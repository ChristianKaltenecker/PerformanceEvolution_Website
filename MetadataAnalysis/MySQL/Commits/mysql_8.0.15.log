Searching between mysql-8.0.13 and mysql-8.0.15
Keywords: slow, fast, time, perf(ormance), optim(ize), regression
Additional keywords: flush,innodb,delayed,dsync,direct
Keywords: slow fast time perf optim regression speed flush innodb delayed dsync direct
For keyword slow:
[33mcommit 02fc593b460c637acc7eab4dc15aa158f950d44b[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Nov 22 07:22:04 2018 +0000

    Bug#28954788: MAIN.SLOW_LOG_EXTRA-BIG FAILS SPORADICALLY ON PB2
    
    Patch attempts to make the test, [1;31mslow[m_log_extra-big, more
    resilient against race conditions. To achieve this, we
    undertake to wait until the thread in question is idle
    before disconnecting it. Previously, the sessions were
    just disconnected after last queries returned its result.
    As a possible [1;31mslow[m query log entry is written after the
    result set is returned, this could result in the session
    being terminated before it could write to the [1;31mslow[m query
    log, which would then end up one line short vis-a-vis the
    test case's expectations.

[33mcommit 5becdf4dd153fe720a70e9955628febe2a2ed9e9[m
Author: Amitabh Das <amitabh.das@oracle.com>
Date:   Mon Oct 8 14:10:42 2018 +0530

    Bug#28493257: ASSERTION `!IS_SET()' FAILED
    
    Calling alter table on a temporary table happens to go through
    dict_create_foreign_constraints_low even if the table has no foreign
    keys. This method returns success even if the table was not opened (in
    this case because there was no space on thread stack), since there were no
    foreign keys to be created. So, mysql_alter_table assumes things went well.
    However, since there was an error reported there is a crash in the
    Diagnostics Area. This is only reproducible in 8.0.12. The patch for
    Bug#28245522 reduced the overall stack usage and this issue is not seen.
    But it's better to skip the tables without FK's.
    
    This patch adds a check to make sure this method is not called if table
    has no foreign keys.
    
    Test changes:
    Changed the tests main.commit_1innodb, main.create, main.partition_locking,
    main.[1;31mslow[m_log_extra, main.foreign_key to account for the new code path.
    
    Change-Id: Ia291c0bc747edd29f87203319db9e823881a7ffd

[33mcommit 575b85d6dc614a48fe4f1116d5cf3426c3a9fb90[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Thu Nov 1 13:37:02 2018 +0100

    Bug#28602237 QUERY_REWRITE_PLUGINS.PERFORMANCE_SCHEMA FAILS SPORADICALLY ON PB2 VALGRIND
    
    Background
     The problem here is similar to bug BUG#28211128.
    
     Initialization of X plugin is async and can happen at any time during
     a test. During initialization the X plugin asks the server about
     various information by using a query:
      SELECT @@skip_networking, @@skip_name_resolve ...
    
     this query will be logged, for the table
     performance_schema.events_statements_history something like this will
     appear:
    
      sql_text : SELECT @@skip_networking, @@skip_name_resolve ...
      source   : srv_session.cc:1107  | <empty>
      digest   : 1ebda4149951... | NULL
    
     For query_rewrite_plugins.performance_schema the query done is:
    
      WHERE sql_text LIKE 'SELECT %'
    
     which leads to test failure when the initialization of the X plugin
     is [1;31mslow[m.
    
    Fix
     Stabilize the test by ignoring rows added by query done in X plugin.

[33mcommit da9d365efb8f91281abf6b07c138693eb7f25255[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Oct 30 04:51:20 2018 +0100

    Bug#28857626 DON'T RUN LEAK CHECKS IN ORDINARY WINDOWS BUILDS
    
    Problem: ordinary Windows debug builds have leak checks turned
    on. These checks are not monitored and hence don't serve any useful
    purpose. In addition they [1;31mslow[m down the shutdown of mysqld, something
    which is potentially problematic on PB2.
    
    Solution: wrap the setup of leak checks in #ifdef MY_MSCRT_DEBUG. This
    flag can be turned on in specialized builds if needed.

[33mcommit 4736e3b67ff3e2915e0c84218899529f20611a4a[m
Author: Sreeharsha Ramanavarapu <sreeharsha.ramanavarapu@oracle.com>
Date:   Fri Oct 12 07:35:31 2018 +0530

    Bug #28086754: OPTIMIZER SKIP THE RANGE SCAN ON SECOND
                   COLUMN IN A COMPOSITE INDEX
    
    Issue:
    ------
    
    CREATE TABLE test_ref (
     a INT NOT NULL,
     b VARCHAR(20),
     c VARCHAR(20),
     d VARCHAR(3),
     id INT,
     PRIMARY KEY (a),
     KEY idx1 (id, c),
     KEY idx2 (id, d));
    
    SELECT *
    FROM test_ref
    WHERE id = 3 and c LIKE 'gh%'
    ORDER BY c LIMIT 1;
    
    
    On 5.7:
    -------
    idx1 with a range access is best option for this. idx1
    covers both the conditions in the WHERE clause and will
    support ordering since column "c" is part of the index.
    But the optimizer choose ref access idx1.
    
    Reasons for this (reproducible only with data from user) :
    1) Table scan is calculated as cheaper than range access on
       idx1. So using range access is ruled out.
    2) The only other option is ref-access on idx1.
    3) Later stage switch back to range access on the same
       index (can_switch_from_ref_to_range) isn't possible
       because range access was rejected altogether in Step 1.
    
    The problem with ref-access is that it uses only one
    keypart of idx1 -> the column "id". This makes the query
    execution [1;31mslow[mer.
    
    On trunk:
    ---------
    1) Table scan is cheaper than range access on idx1. Range
       access on idx1 is rejected.
    2) But since range scan on idx1 uses more key parts than
       ref, the ref-access on idx1 is also rejected.
    3) Initially optimizer chooses a ref-access on idx2.
    4) While evaluating options for "ORDER BY" it notices that
       idx1 provides ordering and also doesn't change the
       current access method (ref). At this stage the optimizer
       is unaware of the reasons for rejecting ref-access on
       idx1 earlier (Step 2) and chooses it.
    
    Solution On both 5.7 and trunk:
    -------------------------------
    In test_quick_select() add a new parameter to ignore the
    cost of table scan. Range optimizer on the relevant key
    will be re-run by ignoring the cost of table scan (since
    it was calculated to be cheaper).
    
    
    On 5.7:
    -------
    If the "dodgy_ref_cost" flag is set, that is
    ref-access might be using less key-parts than range but was
    calculated to be cheaper, then re-run the range optimizer
    and check the same.
    
    On trunk:
    ---------
    When a ref-access is chosen, check if it is possible to
    switch to range-access if more keyparts will be used. To
    achieve this call can_switch_from_ref_to_range() from
    test_if_skip_sort_order() when index for ref-access has
    changed.
    
    In can_switch_from_ref_to_range(), add a new parameter to
    force the switch to range even if it was rejected
    previously because a table scan is cheaper.

[33mcommit 76f2ce528f23cfa8579f2ce63ba07fa8ddc53c37[m
Author: Srikanth B R <srikanth.b.r@oracle.com>
Date:   Tue Oct 2 18:11:48 2018 +0530

    WL#12393: Logging: Add new global variable, log_[1;31mslow[m_extra, for richer [1;31mslow[m logging
    
    Post push fix for main.all_persisted_variables
    
    Reviewed by Erlend Dahl <erlend.dahl@oracle.com> over IM

[33mcommit 9c32d824d637de745cfeeac633d467ddd3a507a2[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Sep 28 10:19:04 2018 +0100

    WL#12393: Logging: Add new global variable, log_[1;31mslow[m_extra, for richer [1;31mslow[m logging
    
    This changeset adds a mode in which more attributes are written to the [1;31mslow[m
    query log to aid in performance debugging.
    
    The traditional output is:
    
    If --log-short-format is not used:
      # Time: <ISO8601_timestamp(current_utime)>
      # User@Host: <user@host>  Id: <connection_ID>
    
    Always (1):
      # Query_time: <duration>  Lock_time: <duration> \
        Rows_sent: <number>  Rows_examined: <number>
    
    If the database has changed:
      use <newDB>;
    
    Always (2):
      SET [last_insert_id=1,][insert_id=1,]timestamp=<current_utime>
    
      [# administrator command: ] <sql_text> ";\n"
    
    If --log-[1;31mslow[m-extra is used, the section "Always (1)" is now modified to
    provide additional information as per below (all in the same line), while
    keeping all other sections as before:
    
      # Query_time: 0  Lock_time: 0  Rows_sent: 0  Rows_examined: 0
            Thread_id: 3 Errno: 0 Killed: 0 Bytes_received: 110
            Bytes_sent: 134 Read_first: 0 Read_last: 0 Read_key: 2
            Read_next: 0 Read_prev: 0 Read_rnd: 0 Read_rnd_next: 0
            Sort_merge_passes: 0 Sort_range_count: 0 Sort_rows: 0
            Sort_scan_count: 0 Created_tmp_disk_tables: 0
            Created_tmp_tables: 0 Start:  9:22:58 End:  9:22:58
    
    A warning is thrown when a client changes log_[1;31mslow[m_extra, but
    log_output does not include FILE.

[33mcommit 35c3232d98c75ed1ffc7b8b2cc64a544b896911a[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Mon Jul 9 11:27:46 2018 +0200

    WL#11722: Step 46
    -----------------
    When moving the scan pointer due to a tuple being removed it is essential
    to setup the context for scanNext properly. This means that the scan bound
    must be setup and the scan pointer must be set in the context.
    Introduced a new method prepare_move_scan_ctx for this purpose.
    Broke out prepare_all_tup_ptrs from prepare_scan_bounds to avoid
    double work.
    Kept double work of re-preparing ctx.keyAttrs since that would [1;31mslow[m down
    the common path in the code.

[33mcommit fbeece32479de86e9418f46c2d85c4a3c0ada57f[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Fri Aug 17 18:08:44 2018 -0700

    Tests introduced in WL9508 still fail periodically in PB2.
    Make ALTER UNDO TABLESPACE SET INACTIVE more indempotent.
    Help truncate_recover_xx tests by doing a [1;31mslow[m shutdown restart first in order
    to reduce the  size of the undo logs so that the shutdown in the test does not timeout.

[33mcommit 2911f214b533dc1dd140e6154ec39b5a0954f19a[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Jul 23 22:42:30 2018 -0700

    WL9508 - Post Fix.  Tests innodb_undo.trunc_multi_client_01 &
    innodb_undo.trunc_multi_client_02 were failing for page sizes 32k and 64k.
    A [1;31mslow[m shutdown stalled with continuous undo truncation because
    undo::needs_truncation() constantly returned true.  The initial size
    of an undo space with those page sizes is larger than 10Mb.
    Used ut_max(to make sure the size must be larger than the initial size.
For keyword fast:
[33mcommit 8bd55fc0b809080defa4408c1e00aa97b7c55b6a[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Tue Nov 20 16:28:12 2018 +0100

    Bug#28949700: SUBOPTIMAL USE OF STRING::RESERVE() IN WRAPPER_TO_STRING()
    
    The one-argument version of String::reserve() increases the capacity
    of the String object linearly and causes a high number of memory
    allocations when converting certain JSON documents to text
    representation.
    
    This patch changes wrapper_to_string() so that it uses the
    two-argument version of String::reserve() instead, which enables
    exponential increase of the capacity and reduces the number of memory
    allocations.
    
    The patch includes some additional small improvements to make the
    conversion from JSON to text go [1;31mfast[mer:
    
    - When appending a string literal, provide the length as an argument
      to avoid calls to strlen().
    
    - When appending two-character string literals, instead append the two
      characters individually (append(char) is inlined, append(const
      char *, size_t) is not).
    
    Microbenchmarks (64-bit, Intel Core i7-4770 3.4 GHz, GCC 8.2.0):
    
    BM_JsonBooleanArrayToString      36985 ns/iter ->  24632 ns/iter [ +50%]
    BM_JsonDateArrayToString        380330 ns/iter -> 149176 ns/iter [+155%]
    BM_JsonDecimalArrayToString     111906 ns/iter ->  32756 ns/iter [+242%]
    BM_JsonDoubleArrayToString      219352 ns/iter ->  97216 ns/iter [+126%]
    BM_JsonObjectToString             1174 ns/iter ->   1030 ns/iter [ +14%]
    BM_JsonStringToString_Plain         93 ns/iter ->     87 ns/iter [  +7%]
    BM_JsonStringToString_SpecialChars 191 ns/iter ->    185 ns/iter [  +3%]
    
    Change-Id: I6b2e6f1683ba195cc4e0546d6f9421ea1d1c9b00

[33mcommit 0bbfaceaa4242f9772b4eaa648c09342b7f17e0f[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Wed Nov 21 15:50:03 2018 +0100

    Bug#27309336 Backport to 5.7
    
    Problem:
    Hang observed while attempting to create large number of tables as [1;31mfast[m
    as possible if innodb_flush_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash is observed when the innodb_flush_method is set to O_DIRECT.
    The problem seems to be because of out of sync FS meta data as fsync()
    is never called if the flush method is set to O_DIRECT_NO_FSYNC.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20994
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit d7fc4690773ad26da49ba842eca66bc72acf07bd[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Nov 19 05:01:56 2018 +0100

    Bug#27309336 INNODB 8.0 WITH O_DIRECT_NO_FSYNC MAY CRASH THE SERVER DUE NEW DD ACTIVITY
    
    Problem:
    Hang observed while attempting to create large number of tables as [1;31mfast[m
    as possible if innodb_flush_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash observer when the innodb_flush_method is set to O_DIRECT.
    
    The problem seems to be related to the fact that fsync() is never called
    in O_DIRECT_NO_FSYNC mode, which could lead to a situation where FS metadata
    may go out of sync causing this hang.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20246
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit fe20e7a88dc99faf5147a43060fd5222736d1a0c[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Oct 18 09:08:02 2018 -0700

    Bug#28813453: INNODB: ADJUST THE CALCULATION FOR DML DELAY TO REDUCE PURGE LAGGING
    
    This adjustment is requested by Dimitri Kravtchuk.  As always, this change is
    well tested.
    
    "Historically, we used 5ms as the minimal delay for purge lagging
    "workaround", while on todays systems things are going much [1;31mfast[mer, and 5ms
    is too big -- it should be rather 5 usec now."
    "With sleeps starting from 5ms we're sleeping too much, so the impact on TPS
    becomes much bigger than it should be"
    
    Patch approved by Dimitri on RB#20753

[33mcommit ec5e3a30a92b7e61a4294058b32343bd535ac6ff[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Mon Oct 8 18:48:59 2018 +0200

    BUG#27511404: NODE MAY NOT SWITCH TO ONLINE UNDER CONSISTENT LOAD
    
    When a member joined a group that had a constant peak load, the
    member might not be able to move from RECOVERING to ONLINE state.
    This was due to:
     1) The member was waiting on a loop for the complete queue of
        transactions that did arrive during recovery to be applied.
        while new transactions were still arriving;
     2) Even when the complete queue was applied, the member was also
        checking that the applier was paused, what is unlikely to happen
        on a continuous peak workload.
    
    To allow a [1;31mfast[mer join to a group, when the recovery completion
    policy is wait for transactions apply, the member will first wait
    until one of the conditions is fulfilled:
     a) the transactions to apply do fit on the flow control control
        configuration, that is, the transactions to apply can be applied
        on the next flow control iteration;
     b) no transactions are being queued or applied, the case of empty
        recovery.
    Then, the member will wait for the apply of the currently queued
    transactions on the group_replication_applier channel, before the
    member state changes to ONLINE.

[33mcommit 58a8ee30f3099c14e7cd2c1b918c8c20620c7290[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Fri Sep 28 23:36:56 2018 +0300

    Fix for bug#28714367 "5.7.21+ LF_NODE METADATA LOCK LEAK WHEN USING GET_LOCK".
    
    Calls to GET_LOCK() function with zero timeout argument which failed due to
    concurrent connections holding the same user-level lock, left underlying
    metadata lock structure in state which prevented future reuse of its memory
    for other metadata locks (or release of this memory before server shutdown).
    As result memory was hogged by some of workloads which involved user-level
    locks with random/constantly changing names, attempted to lock by different
    connections with zero timeout.
    
    The problem was introduced by fix for bug@26739438 "DEADLOCK ON
    GET_LOCK(..., 0)". This fix added short-cut to MDL_context::acquire_lock()
    for case when we failed to acquire lock instantly and zero timeout was used.
    However, proper cleanup of MDL_lock [1;31mfast[m path state and obtrusive lock
    count was not performed in this case, which led to MDL_lock object being
    always marked as used.
    
    This fix solves the problem by changing acquire_lock() code to resort
    to calling try_acquire_lock() in case of zero timeout. The latter call
    performs cleanup properly.
    
    It is hard to write robust test case for this bug for our test suite.
    So no test case provided as part of the patch. However, this fix was
    tested manually.

[33mcommit 65f43bdcac0b111c50cd8e200acbba435505cb5d[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Wed Sep 12 12:09:34 2018 +0200

    WL#12438 add exec_time handling to server greeting
    
    allow to modify the content of the authentication packets and
    set the execution time for each packet
    
    Only "exec_time" is exposed to the json and js layer, the other
    fields can be added later.
    
    * moved the data of the authentication process into the statement
      handler layer
    * split network-layer of authentication and data of authentication
      packets
    * moved auth-[1;31mfast[m encoder into mysql_protocol_encoder

[33mcommit d487ab73cf8f9f639a3731d3d9f2203f914f46f0[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 14 11:55:53 2018 +0100

    WL#11722: Step 25
    -----------------
    Increase the number of rows executed per real-time break by two.
    This is based on that we execute rows now twice as [1;31mfast[m when
    the first value was set. Setting it even higher causes worse
    performance. This setting improved performance by 4% in a scan
    filtering benchmark.

[33mcommit 6d65bacc18c6a16aa8b3c7a2732b93fbe45b046b[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Mon Feb 26 14:02:07 2018 +0100

    WL#11722 Step 3
    ---------------
    Avoid translation from physical row id to row id and back again to
    physical row id when scanning in DBTUX. It makes sense to use row ids
    in DBACC and DBTUP since it is closely related to sending messages to
    other nodes and recovery that deals with logical rowids rather than
    physical rowids.
    
    For DBTUX we only get a row id to be able to read the row in TUP and TUP
    is [1;31mfast[mer to use physical row ids. So since TUX stores physical row ids
    there is no use in converting it to a logical row id. Given that
    LQH now uses different row id dependent on who is scanning we added a
    number of comments on this fact. It is important for LQH to keep track
    of what row ids are logical and which are physical.

[33mcommit 870434552974473c6d03cc1e0f309794e270db17[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:51:19 2018 +0100

    WL#11722
    First preparatory step to optimise scans and particularly
    longer scans with many rows checked but not sent back to
    the MySQL server.
    1) Removed a number of jam's, replaced most by jamDebug and
       added a few jamDebug's
    2) Thoroughly investigated all real-time breaks for scans and
       key operations. Added a long comment describing all of those
       real-time breaks and when they happen.
    3) Introduced a preparatory phase after returning after each
       real-time break. This preparatory phase sets up all pointers
       in TUP and LQH. Later we will add also preparatory phases for
       the scanning block (TUX, TUP and ACC).
    4) An important step is to prepare for longer scan executions.
       To handle this we must unwind the stack after executing one
       row scan. To clarify this we added a number of returns in a
       number of places to ensure we get tail-call optimisations as well
       as ensuring that it is clear that it is intended to return.
    5) Made the code a bit more readable by placing { on new line.
    6) Added many more likely/unlikely for optimisation of the code.
    7) Added a real-time break to handle restarts of scan from
       queue on fragments.
    8) Prepared code in TUPKEYREQ for optimised read row handling.
    9) Optimised TUPKEYREF for scan handling, more optimised handling
       of scans in TUPKEYCONF.
    10)Made sure that new (and old) block pointers are overwritten
       with each new signal executed (in debug mode) for [1;31mfast[mer
       bug finding in this area.
    11)Removed a number of parameters to call that weren't needed
    12)Split prepareTUPKEYREQ into prepare_table_pointers and
       prepare_scanTUPKEYREQ to avoid initialising fragment and
       table pointer on each new row.
    13)Introduced new block variable prim_tab_fragptr to point at
       fragment record for primary table. The scan block scans on
       the index table, but reads from the primary table, so needed
       quick access to this at times as well.
    14)Clarified the delayed signal to be used when rows are locked
For keyword time:
[33mcommit 8b9671b2a686fa659c5d3d1f79370e3962c633fc[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Thu Dec 6 03:36:22 2018 -0500

    Bug #28922795 CHECKING METADATA CACHE READINESS SUFFERS FROM TIMING ISSUES
    
    When the routing plugin starts and runs its metadata-cache destinations
    they need to know when the metadata-cache is ready to call API funtions on it.
    There is API function that returns that information that we use to probe for
    that every 1 ms for 100 ms. If that's not enough we leave with an error.
    
    Changed 2 things:
     - moved seting when the metadata-cache is initialized from plugin's start()
       to init()
     - extended probing [1;31mtime[m in the routing plugin from 100ms to 1s

[33mcommit 1e2718fc0219c85fd6136ac2522b7bb3ba4125a1[m
Author: Piotr Obrzut <piotr.obrzut@oracle.com>
Date:   Thu Dec 13 08:55:19 2018 +0100

    BUG#28345281 MYSQL 8.0 INSTALLER DOES NOT DETECT VISUAL C++ REDISTRIBUTABLE ALREADY INSTALLED
    
    The prerequisite detection for VC++ 2015 run[1;31mtime[m was enhanced to workaround
    the current bug in the VC++ 2015 run[1;31mtime[m installer where it clashes with VC++
    2017 run[1;31mtime[m.

[33mcommit e7638fe58e5964a1ddaf1e4f5dca3711cc41cdac[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Nov 23 07:21:00 2018 +0000

    Bug#28632725: MYSQL LOG TIMESTAMPS DO NOT REFLECT DAYLIGHT SAVINGS TIME IN HOUR OFFSET DST
    
    When using --log-[1;31mtime[mstamps=system, ISO8601 [1;31mtime[mstamps in logging
    would always show the [1;31mtime[mzone's default offset to UTC, even when
    DST was active.

[33mcommit 2a8f8e4ef6e6d937f530e7fc41eca39e105d9407[m
Author: Tiago Vale <tiago.vale@oracle.com>
Date:   Tue Nov 20 17:51:43 2018 +0000

    Bug#28602835 WHEN GROUP_SEEDS HAS BOTH WHITELISTED & NON WHITELISTED HOSTS,START GR FAILS
    
    Post-push fix.
    
    Problem
    -------
    The following tests are failing on Windows:
    
    * group_replication.gr_member_expel_during_shutdown
    * group_replication.gr_single_primary_async_gr_start_on_boot
    * audit_log.audit_log_gr
    
    Analysis
    --------
    Before the original fix, the behaviour was to go through the entire
    seed list and try to connect to each seed.
    If all connections failed, we repeated this process up to 10 [1;31mtime[ms.
    After the original fix, the behaviour is to try to connect to each seed
    10 [1;31mtime[ms, then move to the next seed.
    The new behaviour led, on some tests, to GR timing out waiting for the join
    process to finish, because the first seed is unconnectable.
    
    Solution
    --------
    Revert back to the original behaviour, meaning that we go through the
    entire seed list on every attempt, instead of exhausting each seed's
    attempt immediately.
    
    RB: 20990
    Reviewed-by: Filipe Campos <filipe.campos@oracle.com>
    Reviewed-by: Tiago Jorge <tiago.jorge@oracle.com>

[33mcommit 45e7bf71f1392a3372c21a3c3da884e118d21a03[m
Author: Karthik Kamath <karthik.kamath@oracle.com>
Date:   Thu Nov 22 11:17:37 2018 +0530

    BUG#27703912: EXCESSIVE MEMORY USAGE WITH MANY PREPARE
                  STATEMENT PLACEHOLDERS
    
    ANALYSIS:
    =========
    Executing a prepared statement to do a multi row insertion
    with large number of placeholders consumes excessive
    memory.
    
    With binlog enabled, the execution takes a long [1;31mtime[m too.
    To write a statement to binlog, the query string is
    prepared during the execution. The query string preparation
    in this scenario involves many string append calls to
    expand heap allocated memory and memcpy. Hence a delay is
    observed during the statement execution.
    
    FIX:
    ====
    A reasonable size of memory is reserved for the query
    string. Memory of size "original statement string size + 32
    [1;31mtime[ms the number of parameters" is reserved to avoid
    reallocations during string appends in common use cases.
    
    The String class enforces a limit of 4 GigaBytes for
    strings. This can overflow in 64-bit platforms. A check has
    been added for 64-bit platforms to handle the overflow. It
    is the responsibility of the caller to ensure that the
    memory buffer required to hold the string doesn't exceed
    4GB. An error is reported if it exceeds 4GB.
    
    A test case was not written for this issue due to
    enviromental limitations.
    
    Note:
    This patch includes a backport of patch for Bug#27699248.

[33mcommit 06b6a83958f3c58b7c800402491e7bedbfb42f71[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Nov 21 10:33:28 2018 +0100

    Bug #28860795   ERRMSG-UTF8.TXT USES WRONG FORMAT SPECIFIERS
    
    The format specifiers used in errmsg-utf8.txt did not correspond correctly to actual types passed to ib::info, ib::warn, ib:error and ib::fatal.
    This issue was not very visible because the format strings are not available to compiler at compile [1;31mtime[m, types of integers dependent on many ifdef/typedefs which were platform specific, and the issue manifested only for really large numbers on certain platforms.
    This patch:
    - adds a run-[1;31mtime[m verification of datatypes in debug build
    - fixes places where format specifier were wrong
    - fixes some obvious typeos in some error messages
    - fix for one place where string was not null terminated
    - an MTR for the above fix
    
    This patch also observes quite simple rules, which prevent typing problems:
    
    1. When calling ib::info/warn/error/fatal pass arguments which have types which directly correspond to format specifiers:
    
    %lld  - long long int (a.k.a. longlong)
    %llu, %llx - unsigned long long int (a.k.a. ulonglong)
    %ld - long int (a.k.a. long)
    %lu, %lx, %lX - unsigned long int (a.k.a. ulong)
    %lf - double
    %d, %i - int
    %u, %x, %X - unsigned int (a.k.a. uint)
    %f - float
    %c - char
    %p - a pointer
    %s - char *, char const *, char[N], or any other thing which points to array of chars
    %zu, %zx - size_t
    %zd - ssize_t
    
    2. When the data you want to print is not guaranteed to have one of the types listed above, convert it using an initializer list:
    
            ulonglong{start_lsn}i
    
    as oposed to using static_cast<ulonglong>, this has an advantage that compiler will warn you if on your platform the destination type can not hold the source type (a.k.a. narrowing).
    
    Reviewed-by: Sunny Bains <Sunny.Bains@oracle.com>

[33mcommit 57aaeb1b0e73e40c6c31e78f1f72005e3ab52181[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Nov 14 12:16:41 2018 +0100

    Bug #28637472   IMPLICIT-TO-EXPLICIT CONVERSION LOGIC COULD BE SIMPLIFIED AND OPTIMIZED
    
    We had two different ways to determine if a transaction is already
    commited and thus no longer holds implicit locks:
    (1) checking if trx is already removed from the rw trxs list
    (2) checking if trx->status is TRX_STATE_COMMITED_IN_MEMORY
    and the two some[1;31mtime[ms produced inconsistent results, because changing
    the trx->state happened a bit later than removal from rw trxs list and
    meanwhile no relevant latches were held.
    In some cases it led to two transactions holding exclusive lock on same
    record, one of them being the trx being committed.
    (This patch adds MTRs which demonstrate such scenarios).
    This was mostly harmless, as the committed trx was not going to access the
    record anymore.
    A baind-aid solution used in the past was to introduce
    can_trx_be_ignored(trx) which checked if the trx is already in the
    committing phase, to ease some debug assertions.
    
    This patch solves the underlying issue instead, by making sure that
    state transition occurs together with removal from the list in the same
    critical section, so the two methods (1) and (2) return same result.
    Therefore it also removes can_trx_be_ignored, strenghtening our
    assertions and simplifing code and reasoning.
    
    In order to move the trx->status change closer to removal from list, the
    patch also refactors the lock_trx_release_locks() so that critical
    sections protected by lock_mutex_enter/exit() and trx_mutex_enter/exit()
    are no longer nested, which greatly simplifies the body of the function.
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>
    Reviewed-by: Darshan M N <darshan.m.n@oracle.com>

[33mcommit 35e414e87bd6e4a033595ca3e8f1ec56192ca067[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Nov 7 15:13:10 2018 +0100

    Bug#28892711 MYSQL BUILD FAILED ON MSVC ON WINDOWS
    
    include <stdexcept> for files using std::run[1;31mtime[m_error.
    
    Change-Id: I44c7ba0f929a18ee4cc21e7f731010558eb68183

[33mcommit 9d7fc0de485b83672f4cc1815bb6e93157fd9437[m
Author: Lukasz Kotula <lukasz.kotula@oracle.com>
Date:   Thu Nov 8 20:48:28 2018 +0100

    BUG#28900849 - X.MYSQL_SESSION_USER FAILS WITH VALGRIND AND DEFAULT TEST-CASE-TIMEOUT
    
    Description
    ===========
    
    The test-case hits per-test-case-[1;31mtime[mout when started with valgrind.
    
    Fix
    ===
    
    The test-case is going to be skipped when started with "--valgrind".
    Added "not_valgrind.inc".
    
    RB: 20899
    Reviewed-by: Tomasz Stepniak <tomasz.s.stepniak@oracle.com>
    Reviewed-by: Grzegorz Szwarc <grzegorz.szwarc@oracle.com>

[33mcommit 78f25d2809ad457e81f90342239c9bc32a36cdfa[m
Author: Venkatesh Venugopal <venkatesh.venugopal@oracle.com>
Date:   Mon Nov 12 16:26:59 2018 +0530

    Bug#26997096: RELAY_LOG_SPACE IS INACCURATE AND LEAKS
    
    Problem
    -------
    The Relay_Log_Space variable shown in SHOW SLAVE STATUS is
    some[1;31mtime[ms much higher than the actual disk space used by relay
    logs.
    
    Analysis
    --------
    This is because we are not writing to
    Relay_log_info::log_space_total in a synchronized manner. i.e, no
    lock is being taken by the IO thread while updating the variable.
    
    Fix
    ---
    The Relay_log_info::log_space_total is now guarded by the
    Relay_log_info::log_space_lock and this protects concurrent
    update on Relay_log_info::log_space_total.

[33mcommit 73a28bcf8340811e0c104ca91615df91f4ddacf7[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Nov 6 08:48:45 2018 +0100

    Bug#28805764 STABILIZE THE 8.0 RELEASE TEST [noclose]
    
    More [1;31mtime[mout increases for weak platforms.

[33mcommit c1d4469592cdb793d73c7802650045fc66b5f3f9[m
Author: Sivert Sorumgard <sivert.sorumgaard@oracle.com>
Date:   Tue Sep 11 15:38:08 2018 +0200

    Bug#28211486: Sql layer must check/set server version number in dd tablespace
    
    This patch will:
    
    - Add two new handlerton functions to get and set the server version number
      stored in the dictionary tablespace header.
    - Implement the two functions for InnnoDB. Setting the server version is
      implemented by modifying the existing function 'upgrade_space_version()'.
    - Add a new function to the bootstrap context singleton to keep track of
      server versions from which upgrade is supported. Currently, this set
      contains all post-GA 8.0 versions.
    - Add a check during server bootstrap where we get hold of the server version
      number from the dictionary tablespace, and verify that this is a version
      from which we are allowed to upgrade.
    - Move the update of the actual server version number in the bootstrap context
      from the end of do_upgrade_checks() to update_versions(), and also update the
      server version number in the dictionary tablespace header at the same [1;31mtime[m.
    - Introduce new MTR include predicate not_freebsd.inc and add that one and
      freebsd.inc to excludenoskip.list.
    
    Semi-related to the issues above, this patch will also:
    
    - Add a new MTR test to list the contents of the DD table 'mysql.dd_properties'
      in a human readable format. This will catch accidential changes to the
      contents of this table.
    - Add a zipped datadir from 8.0.13 and extend the relevant dd_schema_definition
      test to verify correct upgrade from 8.0.13.
    - Refactor the existing test dd_schema_definition_after_upgrade_debug and
      separate out common code as an include file. The test will also verify that
      the contents of the mysql.dd_properties table is the same for an upgraded
      server as it is for a newly initialized server (with the exception of ids,
      root page numbers, etc.).
    - Swap the order of the steps in migrate_meta_data() adressing the mysql.tables
      DD table. We need to first complete the in place updates before inserting the
      meta data into the new table version.
    
    Unrelated to the above, this patch will also:
    
    - Update and enable a test cases in main.partition, but keep the original
      test case commented out until support for partitions in shared tablespaces
      is re-implemented.
    - Remove sql/dd/mtr_readme.txt since this file is now obsolete.
    - Change have_mac_os.inc to check also for MacOS, not just the obsoleted OSX.
    - Fix the test resource_group_unsupported, which was using illegal syntax.
    
    Change-Id: Idac1ea7bf3a264e7035e048179f7de89c435f272

[33mcommit 92e2ad58db7fecd27e44f03dccc35b72f64485dd[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Mon Nov 5 10:56:55 2018 +0100

    Bug#28805764 STABILIZE THE 8.0 RELEASE TEST [noclose]
    
    Give more [1;31mtime[m for a server restart in main.dd_upgrade_partition.

[33mcommit 575b85d6dc614a48fe4f1116d5cf3426c3a9fb90[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Thu Nov 1 13:37:02 2018 +0100

    Bug#28602237 QUERY_REWRITE_PLUGINS.PERFORMANCE_SCHEMA FAILS SPORADICALLY ON PB2 VALGRIND
    
    Background
     The problem here is similar to bug BUG#28211128.
    
     Initialization of X plugin is async and can happen at any [1;31mtime[m during
     a test. During initialization the X plugin asks the server about
     various information by using a query:
      SELECT @@skip_networking, @@skip_name_resolve ...
    
     this query will be logged, for the table
     performance_schema.events_statements_history something like this will
     appear:
    
      sql_text : SELECT @@skip_networking, @@skip_name_resolve ...
      source   : srv_session.cc:1107  | <empty>
      digest   : 1ebda4149951... | NULL
    
     For query_rewrite_plugins.performance_schema the query done is:
    
      WHERE sql_text LIKE 'SELECT %'
    
     which leads to test failure when the initialization of the X plugin
     is slow.
    
    Fix
     Stabilize the test by ignoring rows added by query done in X plugin.

[33mcommit 9827f13989dcd470d8e8c88164b34e48afc18666[m
Author: Neha Kumari <neha.n.kumari@oracle.com>
Date:   Thu Nov 1 11:23:53 2018 +0530

    Bug#28606948: BACKPORT OF BUG#24670909 TO 5.7.22
    
    Backported the changes from mysql-8.0 to mysql-5.7
    
    BUG#24670909:
    
    Problem:
    Previously, a session disconnect causes DROP TEMPORARY TABLE IF EXISTS
    to be binlogged for all the opened temp tables in that session. Even
    though temporary table operation are not otherwise binlogged in row or
    mixed mode, this was done regardless of binary log format in use, as
    it was not tracked, whether a particular temp table was not created in
    STATEMENT mode - in which case it does need the DROP.
    For ROW/MIXED users, this behavior causes spurious binlog writes
    and GTIDs generated on otherwise read only servers.
    
    Fix:
    Track the binlog format at temporary table create [1;31mtime[m
    (open_table_uncached and after final decide_logging_format call for
    CREATE ... SELECT), and that can be used to decide whether a DROP should be
    logged or not in method close_temporary_tables.

[33mcommit 9a228186b682551e880cab027155d320cfb7dfe6[m
Author: Daniel Blanchard <daniel.blanchard@oracle.com>
Date:   Wed Oct 31 11:47:37 2018 +0000

    BUG#28861140 MTR TESTING ON WINDOWS EMITS FREQUENT "COULDN'T DELETE FILE..." ERROR MESSAGES
    
    Increase the mysql-test-run.pl default shutdown [1;31mtime[mout and
    modify the shutdown subroutine to apply this [1;31mtime[mout to all
    processes in a collection, not just the first. This is because
    the first process in a collection may shut down more quickly
    than subsequent processes, and killing processes should
    remain a last resort.
    
    Patch approved by Pavan <pavan.naik@oracle.com> over email.

[33mcommit f2e71629b02f1c48447d33ab60422c3ad0b358ec[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Wed Oct 31 05:33:22 2018 +0100

    Increase the test suite / test case [1;31mtime[mout on per-push to improve the
    situation on weak platforms like Solaris.

[33mcommit a3012f66722cb9946117452505f7ce01be7d2247[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Fri Oct 26 15:42:44 2018 +0200

    BUG#27961928 TEST_SERVICE_SQL_API.TEST_SESSION_GENERAL_LOG FAILS SPORADICALLY ON PB2
    
    Background
     The problem here is similar to bug BUG#28211128.
    
     Initialization of X plugin is async and can happen at any [1;31mtime[m during
     a test. During initialization the X plugin asks the server about
     various information by using a query:
    
     SELECT @@skip_networking, @@skip_name_resolve ...
    
     this query will be logged, for the general_log something like this
     will appear:
    
     event_[1;31mtime[m   : 2018-10-26 11:17:33.419096
     user_host    : mysql.session[mysql.session] @ localhost []
     thread_id    : 10
     server_id    : 1
     command_type : Connect | Quit
     argument     :
    
     For test_session_general_log, the query above will add rows and
     increase the result from the count(*) query, moving them from 18 to
     19 as seen in bug report.
    
    Fix
     Stabilize the test by ignoring rows added by mysql.session user.

[33mcommit bc0cce1fda024aa9fa1121c7b433d55fe00589cb[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Fri Oct 26 13:08:22 2018 +0200

    BUG#28377456 PB2 PERFSCHEMA.STATEMENT_DIGEST_CONSUMERS FAILING SPORADICALLY FROM LAST 7 MONTH
    
    Init of X plugin is async and can happen at any [1;31mtime[m during a test,
    during init X plugin ask server about various information by using a
    query:
    
     SELECT @@skip_networking, @@skip_name_resolve ...
    
    this query will be logged, for performance_schema.events_statements_summary_by_digest
    something like this will appear:
    
     schema_name : NULL
     digest      : 1ebda414995104bec2d678a2f3a337c8cc504cafa93982ba395404ef9dbc5419
     digest_text : SELECT @@`skip_networking` , @@`skip_name_resolve` ...
     count_star  : 1
    
    Fix
      Stabilize output by ignoring rows where schema_name is NULL or
      source is srv_session.cc
    
    See also
      BUG#28211128

[33mcommit da492cadc88835bd143e43a063b1d34111ea6d46[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Fri Oct 26 14:58:29 2018 +0200

    BUG#28211128 AUTH_SEC.MYSQL_PROTOCOL_TYPES UNSTABLE ON PB2
    
    Init of X plugin is async and can happen at any [1;31mtime[m during a test,
    during init X plugin ask server about various information by using a
    query:
    
     SELECT @@skip_networking, @@skip_name_resolve ...
    
    this query will be logged, for general_log something like this will
    appear:
    
     event_[1;31mtime[m   : 2018-10-26 11:17:33.419096
     user_host    : mysql.session[mysql.session] @ localhost []
     thread_id    : 10
     server_id    : 1
     command_type : Connect
     argument     :
    
    Note that argument is empty, so in test output only symptom is an
    extra empty line.
    
    Fix:
     Stabilize output by ignoring rows where user_host contains mysql.session

[33mcommit e5b96670ed2aa075bab2eb33c6e5d675100979dd[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Fri Oct 12 10:58:35 2018 +0200

    BUG#28706307: CHARACTER SET OF ENUM DATA TYPE IS NOT AVAILABLE AS PART OF OPTIONAL METADATA
    BUG#28774144: MYSQLBINLOG --PRINT-TABLE-METADATA SHOW WRONG INFO ABOUT DEFAULT CHARSET
    
    Problems:
    
    BUG#28706307: When using binlog_row_metadata=FULL, the character set
    for ENUM and SET was missing in the Table_map_log_event.
    
    BUG#28774144: When mysqlbinlog --print-row-metadata prints information
    about character sets for character columns, it some[1;31mtime[ms incorrectly
    represented the most frequently used character set as the default
    character set for the table.  But the default character set is a table
    attribute that is unrelated to the frequency of the character set
    among the columns.
    
    Fix:
    
    - Added character set metadata for ENUM and SET types.  These appear in
      a new field, not the same field as the character sets for character
      types.  It has the same format as the fields for character sets for
      character types, but includes a different set of columns.  It
      uses new type codes for this.  The field appears before the field
      for ENUM and SET string values, so that a decoder can convert
      charset directly when reading the strings, if needed.
    
      This includes the following code changes:
      - All changes in libbinlogevents.
      - New inner classes in Table_map_log_event, used to iterate over
        character sets when mysqlbinlog prints metadata. These encapsulate
        the logic for iterating, and allows us to reuse the same logic
        when iterating over character set information for enum and set
        columns as we use for iterating over character set information for
        character columns.
      - changes in Table_map_log_event::init_*, to capture the character
        set for enum and set columns
      - moved the printing of character sets until after the printing of
        SET and ENUM string values.
    
    - Don't print character sets for columns as default characters. Just
      print the character set for every column.
    
      This includes the following code changes in
      Table_map_log_event::print_columns:
      - Remove the declaration, initialization, and check related to the
        is_default_cs variable
      - Remove the my_b_printf(... DEFAULT CHARSET ...) code that occurred
        after the main loop.
    
    - Bonus: added newline between column definitions in the output from
      mysqlbinlog.
    
      This accounts for the majority of the changes in the
      test's .result file, and the addition of "\n#         " in a string
      in Table_map_log_event::print_columns.
    
    - Bonus: made mysqlbinlog escape backtick identifiers in identifier
      names.
    
      This accounts for changes in log_event.cc:pretty_print_str. It may
      affect some other tests - will run Jenkins and update the patch
      accordingly.
    
    - Added a test case to cover also non-ascii and escape characters
      occurring in column names.

[33mcommit bb99aeeae62078650e49c45a5842fa43b9c34ac7[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Wed Oct 24 07:33:54 2018 +0200

    Bug#28805764 STABILIZE THE 8.0 RELEASE TEST [noclose]
    
    Give more [1;31mtime[m for the debug part of the testsuite to accomodate weak
    platforms.

[33mcommit cc63b5143cae267f579f83e1ee4020648f593e85[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Oct 8 15:23:12 2018 -0700

    28726286 [INNODB] SEMAPHORE WAIT HAS LASTED > 903 SECONDS during RQG test
    
    1) All undo tablespaces became inactive. The code in
    innodb_alter_undo_tablespace_inactive() that is supposed to keep at least
    2 undo spaces active at all [1;31mtime[ms did not work when there are multiple
    undo spaces marked inactive implicit.  It assumed there was only one.
    This change takes away that assumption and only counts the
    innactive_implicit space that is marked.
    
    2) In this hang, half of the undo spaces were marked inactive_implicit.
    The purge thread only marks one of those at a [1;31mtime[m.  The others are
    explicitly SET INACTIVE and then SET ACTIVE again before they became empty.
    This changes the alter_active() function to check if the inactive_explicit
    space has been marked. If so, it is set to inactive_implicit. If not
    marked yet, it is set directly to active.
    
    Approved by Rahul in RB#20682

[33mcommit b23672edddde2dc63a9398f9803714b4e10bdcc4[m
Author: Pedro Figueiredo <pedro.figueiredo@oracle.com>
Date:   Fri Oct 19 16:42:49 2018 +0100

    BUG#27928837 `HEAD->VARIABLES.GTID_NEXT.TYPE != UNDEFINED_GTID'
    
    [post-push]
    
    Problem
    -------
    The test script `binlog_gtid.binlog_gtid_unknown_xid` is failling sporadically
    on PB2 due to a difference in expect error codes:
    
         mysqltest: At line 110: Query 'XA ROLLBACK 'xa1'' failed with wrong error
         1397: 'XAER_NOTA: Unknown XID', should have failed with error '1399'.
    
    Analysis
    --------
    This problem happens when the test is ran several [1;31mtime[ms in a row, leading to the
    conclusion that something has not been cleaned up properly.
    
    After analysis, the conclusion is that an XA transaction that is started in the
    test script is never commited or rolled back.
    
    Fix
    ---
    1) Rollback the XA transaction at the end of the test script
    2) Verify that are no pending XA transactions

[33mcommit 0795a9a0e69a17a217eebcaba973f8406a7a9be1[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Thu Oct 18 07:22:41 2018 +0200

    Adjustments to the weekly testsuite
    
    - Give more [1;31mtime[m to the testcases from the default suite on the basic platforms.
    
    - Run the GR tests both in debug and non-debug mode (not twice in debug).
    
    Approved by Shipra Jain <shipra.x.jain@oracle.com>

[33mcommit 6438c3b996b5c54e9305c18d38d3175ad52a69e6[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Oct 16 12:11:46 2018 +0200

    Bug #28799559: TESTS DON'T WORK WITH DEFAULT SETTING OF MAX OPEN FILE
                   DESCRIPTORS
    
    Problem: Default value for variable innodb_open_files is different on different
    platforms which causes result file mismatch. max_open_files vlaue is calculated
    on run[1;31mtime[m based on parameters like max_connections, table_open_cache. So if
    max_open_files limit is set to a value more than the calculated value, mysqld
    reports a warning.
    
    Fix: Fix is to add a suppresion for the warning, and removed the
    innodb_open_files variable as this is not needed in the test file.

[33mcommit 72749629e972e25ad048392221de3e5bd5201fa3[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Oct 12 15:24:00 2018 +0200

    Bug#28786981: EXPLAIN ... FOR CONNECTION MODIFIES OTHER CONNECTION
    
    The implementation of EXPLAIN some[1;31mtime[ms uses TABLE::in_use to get
    hold of a THD pointer in order to use THD::mem_root and THD::variables.
    The problem was that during EXPLAIN ... FOR CONNECTION,
    TABLE::in_use points to the THD representing the connection
    running the query, *not* the connection running EXPLAIN.
    
    This leads EXPLAIN ... FOR CONNECTION allocating on a not-thread-safe
    MEM_ROOT owned by another connection and to modifying of
    another connection's sql_mode.
    
    This patch fixes the problem by using the THD for the connection
    running EXPLAIN instead.
    
    Problem introduced by
    WL#1075 Add support for functional indexes
    
    Change-Id: I49653ccdffe2b38be772df2ae88e8f255884b211

[33mcommit 69fc00557c7d3329003d4e75f32e07a695133f36[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Tue Oct 9 18:27:29 2018 +0200

    BUG#28747936: CONSISTENT TRANSACTION OPTIONS ARE BEING ALLOWED ON A NOT ONLINE MEMBER
    
    The begin hook is only invoked once per transaction, to ensure that
    it sets a flag on the first [1;31mtime[m that it is used.
    That flag was incorrectly being set when the hook was throwing
    errors, this was allowing the hook to be bypassed after the first
    error.
    
    To fix the above issue, the flag is only set when the hook is run
    without errors.

[33mcommit 8b0ada5eb01798d20f3288b6cf07b299dd0d7545[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177 MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could some[1;31mtime[ms fail due to missing characterset comments.
    
    The root cause was the regression test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source directory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the regression test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different directory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    See also duplicate
    
    Bug#19372763 SPORADIC FAILURE IN MAIN.INTERNAL_TMP_DISK_STORAGE_ENGINE
    
    Backport patch from 8.0.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit 99edc920d71e1e419456f47e0a16b828d43cf69e[m
Author: Thayumanavar S <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri Oct 5 05:53:32 2018 +0200

    BUG#27966483 - SIGHUP CAUSE MYSQL SERVER CRASH.
    
    When SIGHUP signal is sent to mysqld and the server
    is admitting new connections at this [1;31mtime[m, it results
    in the server exit. The thread cache is purged as part
    of SIGHUP handling. This involves raising signal to the
    threads that are blocked waiting for new connections to be
    in thread cache. We purge the channel queue as part of this
    making it empty. New connections can be admitted after which
    result in waking up a thread in the thread cache and this attempts
    to dequeue a connection for handling from an empty channel info queue.
    This results in the behaviour show in the bug.
    
    The fix is not to drain the channel info queue in the kill_blocked_pthreads
    and instead let it be handled by the unblocked thread.

[33mcommit b18eae3de87ab45e481b23e06a9d9f4a0a1fe81d[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Thu Oct 4 14:19:40 2018 +0200

    Bug#28509306 - DIAGNOSTIC AREA NOT POPULATED ON PREPARE STATEMENT ERROR 1615.
    
    Issue here is, SQL condition in diagnostics area is not pushed
    when re-prepare fails for the prepared statements.
    
    When table is opened for the prepared statement, the tables version
    is verified to check if they change since statement prepare.
    If any table is changed then parse tree may no longer be valid
    and it should be reprepared. In such situation, ER_NEED_REPREPARE
    error is set in the diagnostics area by the table version change
    observer of the prepared statement. The intention of this to
    inform executor method to re-prepare the statement and execute
    again. Since it is internal to the server, observer just sets the
    diagnostics area but exception handlers are not invoked and
    SQL condition is not set.
    
    The prepared statement executor attempts only three [1;31mtime[ms to
    re-prepare the statement and reports ER_NEED_REPREPARE to user
    if tables version mismatches again. Since the error is set in the
    diagnostics area, proper error is reported to the user but
    GET DIAGNOSTIC statement to access SQL condition fails as it
    is not pushed in this situation.
    
    This issue can be observed with stored routine instructions too
    as similar logic is used for it.
    
    To fix this issue for the prepared statement and stored routine
    instructions, now SQL condition is pushed if the execution fails
    with ER_NEED_REPREPARE for more than three [1;31mtime[ms.

[33mcommit 60a6ab982ace6c1c40d91634779c38209ea89658[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177: MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could some[1;31mtime[ms fail due to missing characterset comments.
    
    The root cause was the regression test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source directory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the regression test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different directory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit 58a8ee30f3099c14e7cd2c1b918c8c20620c7290[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Fri Sep 28 23:36:56 2018 +0300

    Fix for bug#28714367 "5.7.21+ LF_NODE METADATA LOCK LEAK WHEN USING GET_LOCK".
    
    Calls to GET_LOCK() function with zero [1;31mtime[mout argument which failed due to
    concurrent connections holding the same user-level lock, left underlying
    metadata lock structure in state which prevented future reuse of its memory
    for other metadata locks (or release of this memory before server shutdown).
    As result memory was hogged by some of workloads which involved user-level
    locks with random/constantly changing names, attempted to lock by different
    connections with zero [1;31mtime[mout.
    
    The problem was introduced by fix for bug@26739438 "DEADLOCK ON
    GET_LOCK(..., 0)". This fix added short-cut to MDL_context::acquire_lock()
    for case when we failed to acquire lock instantly and zero [1;31mtime[mout was used.
    However, proper cleanup of MDL_lock fast path state and obtrusive lock
    count was not performed in this case, which led to MDL_lock object being
    always marked as used.
    
    This fix solves the problem by changing acquire_lock() code to resort
    to calling try_acquire_lock() in case of zero [1;31mtime[mout. The latter call
    performs cleanup properly.
    
    It is hard to write robust test case for this bug for our test suite.
    So no test case provided as part of the patch. However, this fix was
    tested manually.

[33mcommit d698f080a3f989c1c81bf2f45fcea4df071a78e3[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Mon Oct 1 07:17:45 2018 -0400

    Disabling Router UT: LifecycleTest::_wait_for_stop
    
    This test needs to be revised, currently it very often fails on macos
    with a message like:
    
    Failure Expected: (200) > ([1;31mtime[m_diff( t0, t1)), actual: 200 vs 239

[33mcommit 70c79e172d5a6f00bacd8a4bde0484a0dfb5146f[m
Author: Tiago Jorge <tiago.jorge@oracle.com>
Date:   Fri Sep 28 17:54:29 2018 +0100

    WL#11926 GR: IPv6 support
    
    EXECUTIVE SUMMARY
    =================
    
    This worklog implements IPv6 support for MySQL Group
    Replication. After this worklog, the user will be able to fully deploy
    Group Replication not only in an IPv4 but also on a IPv6 network.
    
    USER/DEV STORIES
    ================
    
    As a system administrator I want to deploy an IPv6 network while
    running MySQL Group Replication at the same [1;31mtime[m, so I can make use of
    IPv6 features.
    
    SCOPE
    =====
    
    The scope of this worklog is to make XCom support IPv6.

[33mcommit 9c32d824d637de745cfeeac633d467ddd3a507a2[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Sep 28 10:19:04 2018 +0100

    WL#12393: Logging: Add new global variable, log_slow_extra, for richer slow logging
    
    This changeset adds a mode in which more attributes are written to the slow
    query log to aid in performance debugging.
    
    The traditional output is:
    
    If --log-short-format is not used:
      # Time: <ISO8601_[1;31mtime[mstamp(current_u[1;31mtime[m)>
      # User@Host: <user@host>  Id: <connection_ID>
    
    Always (1):
      # Query_[1;31mtime[m: <duration>  Lock_[1;31mtime[m: <duration> \
        Rows_sent: <number>  Rows_examined: <number>
    
    If the database has changed:
      use <newDB>;
    
    Always (2):
      SET [last_insert_id=1,][insert_id=1,][1;31mtime[mstamp=<current_u[1;31mtime[m>
    
      [# administrator command: ] <sql_text> ";\n"
    
    If --log-slow-extra is used, the section "Always (1)" is now modified to
    provide additional information as per below (all in the same line), while
    keeping all other sections as before:
    
      # Query_[1;31mtime[m: 0  Lock_[1;31mtime[m: 0  Rows_sent: 0  Rows_examined: 0
            Thread_id: 3 Errno: 0 Killed: 0 Bytes_received: 110
            Bytes_sent: 134 Read_first: 0 Read_last: 0 Read_key: 2
            Read_next: 0 Read_prev: 0 Read_rnd: 0 Read_rnd_next: 0
            Sort_merge_passes: 0 Sort_range_count: 0 Sort_rows: 0
            Sort_scan_count: 0 Created_tmp_disk_tables: 0
            Created_tmp_tables: 0 Start:  9:22:58 End:  9:22:58
    
    A warning is thrown when a client changes log_slow_extra, but
    log_output does not include FILE.

[33mcommit 9b7a6e6556356c800c838a891cf0ef0746934b5a[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Sep 27 20:26:05 2018 +0200

    WL#461 allow non-LATERAL outer references in derived table (SQL92?)
    example:
    select * from t1
    where t1.a = (select dt.b from (select t3.c*t1.d from t3) as dt(b));
                                                ^^^^
    the reference to t1.d is now allowed. If "dt" is materialized, it
    will be re-materialized for every execution of the query block which
    contains the definition of "dt" (here, that means the =(...)
    subquery).
    "dt" may also be merged.
    Also possible in a CTEs:
    select * from t1
    where t1.a = (with dt(b) as (select t3.c*t1.d from t3) select dt.b from dt);
                                          ^^^^
    If the CTE is materialized, it
    will be re-materialized for every execution of the query block which
    contains the definition of "dt" (here, that means the =(...)
    subquery). The word "definition" matters as a CTE is defined in a place
    and possibly used elsewhere.
    
    WL#8652 allow LATERAL derived tables (SQL:99)
    example:
    select * from t1,
    lateral (select t3.c*t1.d from t3) as dt(b));
                         ^^^^
    the reference to t1.d is now allowed. If "dt" is materialized, it
    will be re-materialized for every row read from t1.
    "dt" may also be merged.
    A CTE must not have the LATERAL keyword.
    
    The difference between the two WLs is the origin of t1.d:
    a non-lateral derived table DT can reference tables which are
    already read when the subquery containing the definition of DT starts
    executing;
    a lateral derived table DT can also reference tables which are not
    already read when the subquery containing the definition of DT starts
    executing: tables which are read only right before when the nested-loop-join
    execution comes to the stage of reading DT.
    Rematerialization of lateral table is mentioned in EXPLAIN and opt trace,
    with a mention of when in the nested-loop this happens.
    
    Non-obvious changes:
    - mark_select_range_as_dependent() was unused, removed.
    - changed resolution code in Item_field::fix_outer_field() and Item_ref::fix_fields()
    to allow derived tables to find outer references: the places where they
    can search are a _strict_ subset of "all subqueries above".
    - a SELECT_LEX_UNIT underlying a derived table keeps track of its
    lateral outer refs (with table bits); if it has any outer ref it uses
    flag UNCACHEABLE_DEPENDENT.
    - order of merging of derived tables is changed; used to be:
    merge all possible - materialize rest; now it's:
    merge one if possible - if not materialize it; go to next table.
    - when we merge a non-lateral derived table having an outer ref,
    it may make it lateral, if the outer ref is now on same FROM clause.
    - place of lateral table in query plan is decided cost-based based
    on the table is depends on (to minimize the number of rematerializations);
    this implies keeping track of dependencies-already-in-join-prefix
    when in greedy search.
    - 'got_final_plan' introduced to simplify functions.
    - in planning, join buffering is disabled for all tables between a lateral
    table (incl.) and the first of its dependencies (excl.), so that rows
    of that first dependency don't get shuffled (to minimize the number of
    rematerializations).
    - printing of ON conditions in opt trace / EXPLAIN was some[1;31mtime[ms missing
    (EXPLAIN even showed a one-table query when it was actually a left join)
    - TABLE::empty_result_table() contains commonly used code.

[33mcommit f62be3eb4d05fc02df9cdbb7277559e545c64a28[m
Author: Erik Froseth <erik.froseth@oracle.com>
Date:   Thu Sep 6 08:24:18 2018 +0200

    Bug#28333657 MAKE CREATE_FIELD::LENGTH PRIVATE
    
    Create_field::length has for a very long [1;31mtime[m had the comment
    "At various stages in execution this can be length of field in
    bytes or max number of characters.". It has been difficult to know
    whether this variable has been in number of bytes, number of
    characters, number of code points etc. when using it, and this has
    also been a source of several bugs. Furthermore, there are comments
    like "Divide by four here, so we can multiply by four later" in the
    code which makes it even more confusing. This patch tries to
    resolve some of this mess by:
    
    1) Renaming Create_field::length to Create_field::m_max_display_width_in_codepoints.
       We have chosen code points instead of bytes since that is what the
       user enters when writing DDL statements (VARCHAR(20) etc...).
    
    2) Making the variable private, so it can't be modified from the
       outside. Two getters are added; max_display_width_in_bytes(),
       max_display_width_in_codepoints(). This removes all this "divide
       by four so we can multiply by four later".
    
    3) Remove create_length_to_internal_length() which is used throuhout
       the code to convert Create_field::length from number of code points
       to number of bytes.
    
    4) Move Create_field out to a separate file "sql/create_field.{h|cc}"
    
    As a side-effect of the above, Create_field::pack_length and
    Create_field::key_length is removed and replaced by member functions
    pack_length() and key_length() since they both are calculated from
    max_display_widht_in_bytes().
    
    By making it much more explicit what we get from Create_field, we notice
    some interesting things such as "set_char_length" expects the number
    of bytes etc. This patch does not try to figure out whether or not this
    is correct, but merely makes it a lot more explicit what's going on.
    
    Change-Id: I5ad725649e86b38ff32b5c4f07c9be42c7995c64

[33mcommit ce612933853efe573d47ed3fdf11a05f0a40d4f4[m
Author: Dinesh Surya Prakash <dinesh.prakash@oracle.com>
Date:   Fri Sep 21 13:51:29 2018 +0530

    Bug#28647699 SOME NDB TESTS ARE FAILING IN PB2 DUE TO LIBRARY MISMATCH
    
    Most of the ndb api test executable uses mysql and cluster dynamic libraries.
    But as the rpath is not set, during run[1;31mtime[m the linker will look for the
    libraries only in LD_LIBRARY_PATH and it throws library not found error.
    
    Modifed the make file to add the path of shared libraries to the executable.

[33mcommit b186620478d85c44f5e2c16a47d46b2b85df58d5[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Sat Sep 1 13:13:12 2018 +0100

    WL#10957: Binary log encryption at rest (Step 2)
    
    This patch introduces a new option 'binlog_encryption' that allows to
    enable and disable the generation of encrypted binary and relay log
    files.
    
    It also introduces a new dynamic privilege 'BINLOG_ENCRYPTION_ADMIN'. A
    user must have this privilege to change the option in a client session.
    
    When enabling the option for the first [1;31mtime[m for a server instance, the
    server shall generate a new binlog master key that will be used to
    encrypt the encrypted binary and relay log files passwords.
    
    @ share/errmsg-utf8.txt
    
      Introduced the following errors:
      - ER[_SERVER]_RPL_ENCRYPTION_FAILED_TO_ROTATE_LOGS;
      - ER[_SERVER]_RPL_ENCRYPTION_KEY_EXISTS_UNEXPECTED;
      - ER[_SERVER]_RPL_ENCRYPTION_FAILED_TO_GENERATE_KEY;
      - ER[_SERVER]_RPL_ENCRYPTION_FAILED_TO_STORE_KEY;
      - ER[_SERVER]_RPL_ENCRYPTION_FAILED_TO_REMOVE_KEY;
      - ER[_SERVER]_RPL_ENCRYPTION_MASTER_KEY_RECOVERY_FAILED;
      - ER_RPL_ENCRYPTION_UNABLE_TO_CHANGE_OPTION;
      - ER_SERVER_RPL_ENCRYPTION_UNABLE_TO_INITIALIZE;
    
    @ sql/basic_istream.cc
    
      Did minor refactoring.
    
    @ sql/binlog.cc
    
      Binlog_ofile can now create new encrypted binary or relay log files
      depending on binlog_encryption option.
    
      Made read_gtids_and_update_trx_parser_from_relaylog to not ignore
      errors when trying to decrypt relay log files.
    
    @ sql/binlog_istream.cc
    
      Did minor refactoring.
    
    @ sql/binlog_ostream.{h|cc}
    
      Introduced the new 'open' and 'get_header_size' functions to
      Binlog_encryption_ostream.
    
    @ sql/binlog_reader.h
    
      Did minor refactoring.
    
    @ sql/mysqld.cc
    
      Moved server UUID initialization to an early stage and added a call to
      initialize binlog encryption infrastructure right after it, as it must
      be initialized before generating any new binary or relay log file.
    
    @ sql/rpl_log_encryption.{h|cc}
    
      Extended Rpl_encryption class to handle enabling and disabling the
      binlog_encryption option, master key generation and recovery.
    
      Extended Rpl_encryption_header and child to generate encryption
      headers for new files and to serialize them into a down stream.
    
      Extended and fixed code comments.
    
    @ sql/rpl_rli.{h|cc}
    
      Add a new parameter to rli_init_info() to make it skip reading from
      any existing relay log files.
    
    @ sql/rpl_slave.{h|cc}
    
      Add a new parameter to load_mi_and_rli_from_repositories() to make it
      skip reading from any existing relay log files.
    
      Refactored the generic error message reported by the applier thread
      when facing errors reading events from relay log files to also mention
      possible issues with keyring/encryption.
    
      Made change_master() to call load_mi_and_rli_from_repositories()
      telling it to skip reading from relay log files when they are expected
      to be purged in a later stage.
    
    @ sql/set_var.h
    
      Refactored a comment.
    
    @ sql/sys_vars.{h|cc}
    
      Added a new Sys_var_binlog_encryption to handle binlog_encryption
      option.
    
    @ sql/auth/dynamic_privileges_impl.cc
    
      Introuduced the new BINLOG_ENCRYPTION_ADMIN dynamic privilege.
    
    Test cases
    ==========
    
    @ binlog.binlog_encryption_random_access
    
      Tests random access to an encrypted binary log file.
    
    @ rpl.rpl_encryption
    
      Test many scenarios of enabling the option and errors a slave can face
      if the keyring is uninstalled while the binlog_encryption option is
      ON.
    
    @ rpl.rpl_encryption_master_key_generation_recovery
    
      Test possible failures and how the server recover from issues while
      generating a new master key (enabling the option for the first [1;31mtime[m in
      a given server instance).
    
    Some other test cases had to be re-recorded or fixed because of the new
    option or the new dynamic privilege.

[33mcommit b0955c74d4d027f2838ae6c48cd3dfbed639cbaf[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Wed Sep 26 15:45:27 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    Eventum 61197 revealed performance regression introduced for DDL intensive
    workloads when innodb_flush_log_at_trx_commit = 2.
    
    This is because we over-simplified in the previous fix for BUG#28616442.
    
    This patch reverts related part of the previous fix:
    - don't sleep for more than 1ms in log threads (trx=0 case could suffer),
    - when trx=2 and user thread is waiting for flushed redo (ddl), we need
      to first wait for redo written and wake up log_flusher only afterwards
      (otherwise we could wake up log_flusher too early - that was problem).
    
    We also skip disabling spin-delays for two cases:
    - waiting for written redo when trx=1,
    - waiting for flushed redo when trx=2.
    
    We also checked if we could use two-steps waiting always - that is always
    first wait for written redo and only afterwards fallback to wait on
    flushed redo. We can't afford that.
    
    This would result in more CS when busy-waiting is disabled and trx=1
    (it's disabled when total cpu usage is more than the hwm (default 50%)).
    
    However we observed that two-steps waiting increased TPS for workloads
    with small number of connections. We confirmed that it was because of
    increased total [1;31mtime[m for busy-waiting (two [1;31mtime[ms more spin rounds).
    On the other hand, we can't afford making more spin rounds for higher
    number of connections, because we would risk wasting too much cpu,
    before we reach the hwm which disables the busy waiting.
    
    Because of that we introduced new mechanism:
    1. Use increased spin rounds (compute in range 250k..25k) when
       cpu usage is below 50% of the hwm.
    2. Use default spin rounds (25k) when cpu usage is in 50%..100% of hwm.
    3. Do not use busy waiting when cpu usage is higher than hwm.
    
    Points 2,3 - we behave as it was in the past.
    
    Point 1 - we may use increased number of spin rounds when cpu usage
    is lower than 50% of the hwm.

[33mcommit 2e6e6688c96e4938f97be8c7ce74caaf505d0a1a[m
Author: Anibal Pinto <anibal.pinto@oracle.com>
Date:   Wed Sep 26 13:56:52 2018 +0200

    WL#11123: Group Replication: hold reads and writes when the new primary has replication backlog to apply
    
    This worklog implements a fencing mechanism when a new primary is being
    promoted in Group Replication (GR). The fencing will restrict connections from
    writing and reading from the new primary until it has applied all the pending
    backlog of changes that came from the old primary. Applications will not read
    stale data or do writes for a short period of [1;31mtime[m (during the new primary
    promotion).

[33mcommit d301f2d00e245aee838825042d14fb098fa19a2f[m
Author: Erik Froseth <erik.froseth@oracle.com>
Date:   Thu Sep 6 08:24:18 2018 +0200

    Bug#28333657 MAKE CREATE_FIELD::LENGTH PRIVATE
    
    Create_field::length has for a very long [1;31mtime[m had the comment
    "At various stages in execution this can be length of field in
    bytes or max number of characters.". It has been difficult to know
    whether this variable has been in number of bytes, number of
    characters, number of code points etc. when using it, and this has
    also been a source of several bugs. Furthermore, there are comments
    like "Divide by four here, so we can multiply by four later" in the
    code which makes it even more confusing. This patch tries to
    resolve some of this mess by:
    
    1) Renaming Create_field::length to Create_field::m_max_display_width_in_codepoints.
       We have chosen code points instead of bytes since that is what the
       user enters when writing DDL statements (VARCHAR(20) etc...).
    
    2) Making the variable private, so it can't be modified from the
       outside. Two getters are added; max_display_width_in_bytes(),
       max_display_width_in_codepoints(). This removes all this "divide
       by four so we can multiply by four later".
    
    3) Remove create_length_to_internal_length() which is used throuhout
       the code to convert Create_field::length from number of code points
       to number of bytes.
    
    4) Move Create_field out to a separate file "sql/create_field.{h|cc}"
    
    As a side-effect of the above, Create_field::pack_length and
    Create_field::key_length is removed and replaced by member functions
    pack_length() and key_length() since they both are calculated from
    max_display_widht_in_bytes().
    
    By making it much more explicit what we get from Create_field, we notice
    some interesting things such as "set_char_length" expects the number
    of bytes etc. This patch does not try to figure out whether or not this
    is correct, but merely makes it a lot more explicit what's going on.
    
    Change-Id: I5ad725649e86b38ff32b5c4f07c9be42c7995c64

[33mcommit f7cbc8efe912ebc19a13241d530f140fd77bd776[m
Author: Filipe Campos <filipe.campos@oracle.com>
Date:   Mon Sep 17 15:31:38 2018 +0100

    BUG#28656750 - CHANGE MAXIMUM VALUE OF DELAYED EXPEL TIMEOUT
    
    Set maximum value of group_replication_member_expel_[1;31mtime[mout to 3600 seconds,
    and defined a new validation function for the parameter.

[33mcommit c7b0cb6a1cd95e7e6cfb143678847cf9b2378c41[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Sep 24 17:02:35 2018 +0200

    Increase [1;31mtime[mout value for unit test
    
    Change-Id: I3ae3a8d2024ab5967894fbf31ab86fbaa2f192d1

[33mcommit 38a6d1d082fe09d94e45540a56cbecbd115ae7df[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Sep 24 20:16:47 2018 +0530

    Bug#28511326: DEADLOCK DURING PURGE_LOGS_BEFORE_DATE
    
    Problem:
    =======
    Access to the following variables is protected by LOCK_log.
    
    binlog-transaction-dependency-tracking
    binlog-transaction-dependency-history-size
    
    'LOCK_log' is held when these variables are being set or read.
    Holding 'LOCK_log' results in following Deadlock scenario.
    
    Analysis:
    =========
    1) SELECT * FROM performance_schema.session_variables WHERE
    VARIABLE_NAME LIKE 'binlog_transaction_dependency_tracking';
    
    The above query acquires a lock on thread data and tries to
    read the values of 'binlog_transaction_dependency_tracking'.
    Inorder to read this variable 'LOCK_log' is required.
    
    Owns: THD::LOCK_thd_data (acquired in
          PFS_system_variable_cache::do_materialize_all
          ->PFS_variable_cache<Var_type>::get_THD
          -> Find_THD_variable::operator())
    Waits for: MYSQL_BIN_LOG::LOCK_log
    
    2) SHOW BINARY LOGS
    
    Above command acquires 'LOCK_log' to read current active
    binary log specific information and then goes on to acquire
    LOCK_index, so that it can list the rest of binary logs.
    
    Owns: MYSQL_BIN_LOG::LOCK_log (acquired in show_binlogs())
    Waits for: MYSQL_BIN_LOG::LOCK_index
    
    3) PURGE LOGS BEFORE date
    
    Above command acquires 'LOCK_index' and reads one log at a
    [1;31mtime[m from index. For each log it tries to identify how many
    threads are accessing this log. Inorder to do this it
    acquires a lock on global thread list and iterates through
    the entire thread list. For each thread it tries to acquire
    LOCK_thd_data and verify if the log is being used by the
    tread or not. Hence it waits for the lock.
    
    Owns: MYSQL_BIN_LOG::LOCK_index (acquired in
          MYSQL_BIN_LOG::purge_logs_before_date)
    Waits for: THD::LOCK_thd_data
    
    Fix:
    ===
    Transaction dependency tracking information is updated
    based on 'max_committed_transaction' object contents.
    'max_committed_transaction' holds the transaction
    sequence_number. This transaction_sequence number is updated
    during the flush stage of the commit and it is used to
    update the transaction dependency tracking through
    'update_max_committed' function call.
    
    'LOCK_log' needs to be held when the active binary log is
    being modified. Where as to protect concurrent access to set
    or read dependency tracking information a less granular lock
    should be sufficient. Hence a new lock named
    'LOCK_slave_trans_dep_tracker' has been introduced to
    protect concurrent access to transaction dependency tracking
    information.

[33mcommit acca7798297dc6786cf16a1afe64800a9d3ec71b[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Aug 27 18:13:47 2018 +0200

    Bug#28489407 INNODB: ASSERTION FAILURE: SRV0START.CC:1482:0
    
    Problem:
    
    Server hits an assert while shutting down if the value of innodb_spin_wait_delay
    variable is set to a very high value. This happens because while shutting down,
    the server has to wait for all the server threads to shutdown. The server writer
    server threads attempt to acquire exclusive write lock and wait in a loop for
    the lock to be released. This wait [1;31mtime[m may vary from 0 to the value specified by
    innodb_spin_wait_delay variable. If this value is large, then the main server may
    hit the assert after waiting for 100 seconds.
    
    Solution:
    
    Defined the maximum value of innodb_spin_wait_delay to 1000000 microseconds.

[33mcommit 89f30e91ed17d895f25f75dc60e5e52d662f6b5a[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Mon Sep 17 10:04:08 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    The fix to BUG#28062382 was saving CPU by doing longer waits in log threads,
    when there was nothing to be done for the log threads and the frequency of
    requests to write / flush redo was detected as very low. The longer waits
    meant: no spin-delay and initial [1;31mtime[mout = 10ms.
    
    The log threads were waiting on their events (cond_var), so if there was
    a task for them, user threads were trying to wake them up.
    
    The problem was because user threads were trying to find out if they
    should wake up log threads. This leaded to very complex logic which
    has already proved to be error prone.
    
    The whole logic became simplified in this patch. User thread checks
    if write_lsn is not advanced enough and wakes up log_writer in such
    case - no matter if frequency is high/low or CPU usage on server is
    below 80% threshold or not.
    
    There was also a bug in monitor of the frequency - when there were
    no counted calls to log_write_up_to(), the average [1;31mtime[m between calls
    stayed unchanged. It should be reset to +inf if that happened for
    longer period.
    
    Additionally all calls to log_write_up_to() should always be counted.
    The calls from page cleaners were the only that we wanted to exclude
    from being counted, so page cleaners should not call log_write_up_to()
    if the flushed_to_disk_lsn is already advanced enough (most cases).

[33mcommit 3876d06b60ec2a0ad624697e59e94d38e18e81bd[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Thu Sep 13 09:35:55 2018 -0400

    Bug#28642289 MYSQL_SERVER_MOCK DOES NOT SHUTDOWN GRACEFULLY WHEN KILLED ON WINDOWS.
    
    In the Router component tests when we try to stop the child process
    we use kill() method. This method on Windows is implemented with
    the call to TerminateProcess(). That does not allow the process
    that is being killed to do a graceful shutdown, like closing
    the open sockets etc... That has an impact on the tests stability
    as for example trying to connect to the port that killed process
    was using takes long [1;31mtime[m to [1;31mtime[mout while it should fail
    immediately.
    
    This patch replaces use of TerminateProcess() with sending
    Ctrl+Break singal and handling it in the child process
    to emulate SIGTERM behavior.

[33mcommit 53b90065d8dcd9d8c0fea4a5b9b5e60915a43390[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Wed Sep 19 22:03:48 2018 +0700

    Bug#28363362: PB2 ASAN FAILURE IN RPL TESTS
    
    Memory leaks reported by ASAN were caused by improper destruction of MDL_context
    objects belonging to MDL_context_backup instances which used to store metadata
    locks belonging to prepared XA transactions without active connection.
    
    The problem was occurring only occasionally since structures representing
    metadata locks acquired were still properly cleaned up in this case.
    The problem was caused by not releasing LF_PIN object associated with
    such MDL_context, which some[1;31mtime[ms had additional memory tied to it.
    
    This patch ensures that destructor of MDL_context_backup class fully
    destroys its MDL_context by calling destroy() method.

[33mcommit c27e6c736957a0febbf591e6cdd51affcb7bc415[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Sep 13 15:20:26 2018 -0700

    BUG#28516776 NDBIMPORT REMOVE LOGX DEFINES
    
    One problem with the macro names log1(), log2() etc. is that log2
    may also be the name of a math function.
    
    In this patch, log1(msg) and log2(msg) are universally replaced with
    log_debug(1, msg) and log_debug(2, msg). This prevents any conflict
    with a math library, and removes one level of preprocessor indirection.
    
    log3(msg), which is conditionally defined at compile-[1;31mtime[m, is changed
    to log_debug_3(msg).
    
    log4(msg), which is not used in the code, is removed.

[33mcommit 827b9604fd222a6625df76c038eecbaa24923ac1[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Sep 13 14:32:05 2018 -0700

    BUG#28516776 NDBIMPORT REMOVE LOGX DEFINES
    
    Here we are dealing with diagnostic logging messages in a form like
    
      log(level, message)
    
    As a general rule here, we prefer not to evaluate the expression
    "message" unless the current diagnostic logging level is set to
    "level" or higher. "message" may be expensive to evaluate, and
    log statements may occur in tight inner loops of the program logic.
    
    The bug report asks to remove the macro, but using a macro is perhaps
    the best way to prevent the message from being evaluated unnecessarily.
    
    Nonetheless, we can definitely improve the macro by moving its mutex and
    [1;31mtime[mr calls somewhere else. This patch moves them into special "start" and
    "stop" tokens in the message.
    
    As a side effect, the elapsed [1;31mtime[m is moved to the leftmost position in
    the log message and displayed in a fixed-width format.

[33mcommit 187ac1872803cfa3a1d8b3da7a79bd11ec68e19d[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Sep 17 17:28:33 2018 +0200

    Bug#28663887 ALWAYS INLINE BOOST/GEOMETRY FOR /OPT/DEVELOPERSTUDIO12.6/BIN/CC
    
    Always add " -g0 -xdebuginfo=no%decl" for gis source files,
    i.e. those using boost/geometry.
    This avoids link failures, and cuts down link [1;31mtime[m.
    
    Change-Id: I9e7dccc863e343316f453068a187cd39bba9153a

[33mcommit 172d9263e35d57a0495348b4a25428dd636b56c8[m
Author: Lukasz Kotula <lukasz.kotula@oracle.com>
Date:   Mon Sep 17 18:15:28 2018 +0200

    Bug#28637947 - XPLUGIN CRASH FOR LARGE NUMBER OF SESSION OPEN/CLOSE OPERATIONS
    
    Description
    ===========
    
    Assigning, copying 'std::shared_ptr' by multiple threads is an undefined
    behavior. X Plugin is resassigning Client::m_session field, after succesful
    reset-session operation. In the same [1;31mtime[m other thread copyied m_session
    to do THD verification. Both instances of shared_ptr were trying to free
    same shared-ptr control block.
    
    Fix
    ===
    
    Access to m_session from other thread was wrapped in 'm_session_exit_mutex'
    lock.
    
    Change-Id: Ifefcfd71d4e2a47a9422e13c582491bdecf0fadb
    RB: 20543
    Reviewed-by: Grzegorz Szwarc <grzegorz.szwarc@oracle.com>

[33mcommit 1211bb78179bcbd38478d6b64007a028555f9b5c[m
Author: Srikanth B R <srikanth.b.r@oracle.com>
Date:   Tue Sep 18 11:02:15 2018 +0530

    Bug#28660978 MTR TEST FOR FLUSH OF BUFFERED ERROR LOG MSGS ON INITIAL AND INCREMENTAL TIMEOUT
    
    The patch adds testcases to validate flushing of buffered messages
    to the error log file if logging components cannot be activated at
    all or are activated after waiting for some [1;31mtime[m.
    
    It also includes a post-push fix for main.log_options_cmdline.
    
    Reviewed-by: Tatiana Nuernberg <tatjana.nuernberg@oracle.com>
    RB: 20538

[33mcommit 915131e45c9838298f42c44324c322259bd72342[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Mon Sep 17 12:56:32 2018 +0200

    WL#12438: add exec_[1;31mtime[m handling to server greeting
    
    Post-push fix: Fix new Clang build warnings (-Wextra-semi).

[33mcommit 041631b87e61d1c5169a5519b39ac93c89c1c136[m
Author: Maitrayi Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed Sep 12 17:47:04 2018 +0200

    Bug#28597417 - NDB : FIX/ADD VALGRIND SUPPRESSIONS FOR
     BINLOG_SHARE LEAK IN PRE-TRUNK RELEASES
    
    Supress memory_leak at bitmap_init in binlog/Ndb_share in
    pre-trunk releases.
    
    The problem seems to occur during a "discover" and those should be
    rare so there should not be much memory leak over [1;31mtime[m. In trunk, this
    is not a problem due to code rewrites.

[33mcommit 53e14aa794bfcf8586b098b6666ef178bb1a143f[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Thu Sep 13 10:24:18 2018 +0300

    Fix for bug#27353767 "FOREIGN KEY IS ALWAYS IN LOWER CASE".
    
    Names of referenced columns of foreign keys were always shown in lower
    case in SHOW CREATE TABLE output and I_S.KEY_COLUMN_USAGE table.
    While such name is equivalent to name in original case such behavior
    still was incompatible with behavior of server versions before 8.0.
    These versions use version of referenced column name from the parent
    table definition if possible (i.e. when parent table exist at the
    [1;31mtime[m when foreign key is created).
    
    The problem was caused by incorrect fix for bug@25587256 "NEWDD: FK:
    NEED TO SET PROPER REFERENCED COLUMN CASE" which added lowercasing
    of referenced column names.
    
    This patch solves this problem by reverting this incorrect fix
    and adding code which retrieves version of referenced column
    name from the parent table definition.

[33mcommit 65f43bdcac0b111c50cd8e200acbba435505cb5d[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Wed Sep 12 12:09:34 2018 +0200

    WL#12438 add exec_[1;31mtime[m handling to server greeting
    
    allow to modify the content of the authentication packets and
    set the execution [1;31mtime[m for each packet
    
    Only "exec_[1;31mtime[m" is exposed to the json and js layer, the other
    fields can be added later.
    
    * moved the data of the authentication process into the statement
      handler layer
    * split network-layer of authentication and data of authentication
      packets
    * moved auth-fast encoder into mysql_protocol_encoder

[33mcommit 81f255acd572fa2119fab691864a2b200696fd40[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Fri Sep 7 19:51:04 2018 +0200

    WL#12438 add exec_[1;31mtime[m handling to server greeting
    
    Tests

[33mcommit 93320081f9837554ffd61752b42177f562f22091[m
Author: Dyre Tjeldvoll <Dyre.Tjeldvoll@oracle.com>
Date:   Wed Sep 12 12:23:07 2018 +0200

    Bug#28590623: INCORRECT TIMEZONE OBJECT USED TO CREATE DD TIMESTAMPS FOR ROUTINES AND VIEWS
    
    Problem: Setting the current [1;31mtime[m zone to a negative offset and
    setting the current [1;31mtime[mstamp to a very low value would trigger assert
    when altering routines and views.
    
    Solution: Make sure the query start [1;31mtime[mstamp value is correctly
    converted to when altering routines and views (using my_tz_OFFSET0,
    rather than the THD's current [1;31mtime[mzone).
    
    Change-Id: I9b96adea836dd0ebd4c1582df7041c1cb150cfe9

[33mcommit d435b18f95150d5701cb44bd0f4cd85f490aa7f0[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Wed Aug 29 16:18:35 2018 +0200

    Bug#28570122 SLOW SHUTDOWN AFTER SHUTDOWN SIGNAL
    
    handle SIGINT and SIGTERM without any [1;31mtime[mout
    
    Loader::main_loop() was waiting 100ms for the first loaded plugin exit
    before it checked other plugins or the global shutdown flag.
    
    As it relied on futures and std::future having no way to wait on
    multiple futures at the same [1;31mtime[m the new approach uses
    
    * a concurrent queue for plugins's exceptions
      * using a lock-free multi-producer single-consumer queue
    * conditional_vars that are set if either the global shutdown is set
      or a plugin added something to the concurrent queue
    
    If one of
    
    * a plugin exits with an error
    * all plugins exit
    * shutdown is requested
    
    happens, the conditional_vars will be triggered and the loader initiates
    the shutdown sequence.
    
    routertest_component_rest_mock_server test-run[1;31mtime[m changed
    
    * from 8.8 seconds
    * to 1.3 seconds
    
    Overall, test run[1;31mtime[m drops
    
    * from 147sec
    * to 92sec
    
    RB: 20422
    Reviewed-By: pawel.mroszczyk@oracle.com

[33mcommit 8e6b6fdddcf5c212828426644d1669ae3a35e016[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Fri Sep 7 08:52:55 2018 +0200

    Bug#28610484 RACE CONDITION ON SHUTDOWN IN ROUTING/SRC/CONNECTION.CC
    
    When router received a SIGTERM and a new connection at the same [1;31mtime[m
    there was a chance that mainloop exits while the new connection was
    starting up.
    
    The mainloop waits for context_.active_client_threads_ to run to zero.
    
    The fix ensures that counter is incremented before a new thread is
    started.

[33mcommit 1b63cbcad36c2a8dbc8d54579929bfe165462ede[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Fri Sep 7 15:09:10 2018 +0200

    Bug #26399073: MYSQL DOES NOT COMPILE WITH CLANG ON WINDOWS [noclose]
    
    Post-push fix: Unbreak NDB by removing these warnings from NDB builds
    for the [1;31mtime[m being. (The work to actually fix them is relatively big.)
    
    Change-Id: I1720a0290a862516134347d42d6f0422638fde84

[33mcommit de4ba4d1f5c2e6ad1eb3e8b655744664778f772a[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Aug 22 15:01:34 2018 +0200

    BUG#28372628: Fix LCP_SKIP issue
    
    When performing a node restart we perform a copy from live node
    to starting node of changed rows. This copy process also involves
    sending deleted rowids to the starting node.
    
    In a node an update, delete or insert of a row sets the GCI of the
    row to the GCI it was written in. There is one exception to this
    rule, this is during node restart and copy from live node to
    starting nodes for deleted rows. These rows can at [1;31mtime[ms have
    GCI equal to 0. This GCI is then copied over to the starting node.
    
    If a row existed in this position AND a full local LCP was started
    due to UNDO log getting full, we could set the LCP_SKIP bit on a
    row with GCI = 0.

[33mcommit 92836e18d1078a91a552964a3e575bd6856fd1ba[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Jul 14 13:43:34 2018 +0200

    WL#11722: Step 57
    -----------------
    One more real-[1;31mtime[m break found in disk_page_tup_scan_callback
    that needed a call to setup scan environment.

[33mcommit 5ac739be89b29fe5f0b76e50f1937edeccad89de[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Jul 7 19:22:49 2018 +0200

    WL#11722: Step 45
    -----------------
    We retrieved TreeEnt using selectNode but forgot to set it in
    c_ctx.m_current_ent. This caused all sorts of errors when
    working with the index with locking scans.
    
    At the same [1;31mtime[m we removed some debugging messages from VM_TRACE
    builds in LQH and TC, added some more jam's, added a bit more
    debug messages before crashes and a few changed debug messages and
    finally a few likely/unlikely.

[33mcommit 9d9bdd24f958d69134d9104fff511453b4696eb5[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Tue Jul 3 13:34:14 2018 +0200

    WL#11722: Step 43
    -----------------
    For scan with locked rows we needed to call prepare_tux_scan_TUPKEYREQ from
    scanFind since we can have a real-[1;31mtime[m break before arriving in this code.

[33mcommit 22308356f8329f78dbd09bdaa03b78e0f23a855f[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Jun 15 18:00:42 2018 +0200

    WL#11722: Step 33
    -----------------
    Introduce new readKeyAttrs that simply read using normal Attrinfo.
    This attrinfo is fed into a new NdbPack data type DataArray that
    is an array of pointers and lengths where length = 0 means NULL.
    We also change prefix storage in T-tree to use the Attrinfo format
    for storing the prefix. This means an extra overhead of at least
    4 bytes per column stored in the prefix and a bit more at [1;31mtime[ms,
    but never more than 7 bytes. At the same [1;31mtime[m the efficiency in
    comparing is a lot higher using this method.
    
    There is also only one prefix per node and we can win this extra
    storage back most likely by increasing the size of nodes in
    the T-tree.
    
    Ensured that this method is now used for inserts and deletes in the
    T-tree, for index builds during restart and for initial search of
    a range scan.

[33mcommit d487ab73cf8f9f639a3731d3d9f2203f914f46f0[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 14 11:55:53 2018 +0100

    WL#11722: Step 25
    -----------------
    Increase the number of rows executed per real-[1;31mtime[m break by two.
    This is based on that we execute rows now twice as fast when
    the first value was set. Setting it even higher causes worse
    performance. This setting improved performance by 4% in a scan
    filtering benchmark.

[33mcommit 86854aed46910774a6ec6994ef7a8d0c517d36ee[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Mar 10 14:21:28 2018 +0100

    WL#11722: Step 17
    -----------------
    Improve scanFind a bit more by removing unnecessary parameters,
    adding a bit more likely/unlikely, converting some ndbrequire
    to ndbassert's, ensuring that state is kept in local variable
    all the [1;31mtime[m in scanFind.

[33mcommit 419bba30ed216cae9ef6a74a30bebf46e7826221[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 7 13:57:12 2018 +0100

    WL#11722: Step 15
    -----------------
    Fixed two bugs in Step 11.
    1) It is not ok to send SCAN_HBREP from setup after a real-[1;31mtime[m break.
       SCAN_HBREP should only be sent when there is real progress. So if
       we are locked on a locked row we will come back from real-[1;31mtime[m
       breaks, but there is no real progress. So removed this optimisation
       that wasn't ok.
    2) Important to also copy Attrinfo as part of Copy fragment.

[33mcommit 89b25c806be9c13b31fca465cfbfb4b7fe60f147[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Mar 3 01:15:52 2018 +0100

    WL#11722: Step 14
    -----------------
    During a scan we use two bounds, the starting boundary. This is used
    once in scanFirst to find the first row in the range. There is no
    need to precompute this boundary since it is only used once in a
    scan. Similarly we make no special optimisations for now for
    scans for table statistics.
    
    The boundary used to check if the next row is outside the boundary
    is however used many [1;31mtime[ms, most of the [1;31mtime[m at least once, but
    often it can be used many [1;31mtime[ms over and over again. So this
    boundary condition we precompute the objects for and use them
    multiple [1;31mtime[ms.
    
    This optimises the scanCheck method greatly.

[33mcommit 7cad184f855c5f9472e28b6df2ce6d05bcf28256[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Mar 2 18:28:33 2018 +0100

    WL#11722: Step 13
    -----------------
    Remove initialisation of changeMask in KeyReqStruct in most places.
    Don't initialise it for Read queries in TUPKEYREQ. changeMask is
    32 bytes, so takes some [1;31mtime[m initialise even with 0s.

[33mcommit 521242277579d0d8e246566b98f2f6662760d4cf[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Mar 2 18:26:47 2018 +0100

    WL#11722: Step 11
    -----------------
    Fill in attrinfo in DBTUP once per real-[1;31mtime[m break to avoid cost of
    repeatedly copyinfo from signal segments to linear attrinfo buffer.

[33mcommit 5266096589af64ab0ba16527c66a4beda4497b90[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:55:10 2018 +0100

    WL#11722
    Reorganised some functions in Dblqh to make code more readable.
    
    Increase the maximum scan direct count. Counted normal rows
    reported back as 2 and those that only report that condition
    was not met will only count as 1 row. Increased max scan
    direct count to 16 and default from 6 to 16.
    
    Made the amount of rows scanned in one real-[1;31mtime[m break adaptive.
    If the load on JBB level is low we are allowed to execute a bit
    more compared to otherwise.
    
    More work on preparing code for next step of scan optimisations.

[33mcommit 870434552974473c6d03cc1e0f309794e270db17[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:51:19 2018 +0100

    WL#11722
    First preparatory step to optimise scans and particularly
    longer scans with many rows checked but not sent back to
    the MySQL server.
    1) Removed a number of jam's, replaced most by jamDebug and
       added a few jamDebug's
    2) Thoroughly investigated all real-[1;31mtime[m breaks for scans and
       key operations. Added a long comment describing all of those
       real-[1;31mtime[m breaks and when they happen.
    3) Introduced a preparatory phase after returning after each
       real-[1;31mtime[m break. This preparatory phase sets up all pointers
       in TUP and LQH. Later we will add also preparatory phases for
       the scanning block (TUX, TUP and ACC).
    4) An important step is to prepare for longer scan executions.
       To handle this we must unwind the stack after executing one
       row scan. To clarify this we added a number of returns in a
       number of places to ensure we get tail-call optimisations as well
       as ensuring that it is clear that it is intended to return.
    5) Made the code a bit more readable by placing { on new line.
    6) Added many more likely/unlikely for optimisation of the code.
    7) Added a real-[1;31mtime[m break to handle restarts of scan from
       queue on fragments.
    8) Prepared code in TUPKEYREQ for optimised read row handling.
    9) Optimised TUPKEYREF for scan handling, more optimised handling
       of scans in TUPKEYCONF.
    10)Made sure that new (and old) block pointers are overwritten
       with each new signal executed (in debug mode) for faster
       bug finding in this area.
    11)Removed a number of parameters to call that weren't needed
    12)Split prepareTUPKEYREQ into prepare_table_pointers and
       prepare_scanTUPKEYREQ to avoid initialising fragment and
       table pointer on each new row.
    13)Introduced new block variable prim_tab_fragptr to point at
       fragment record for primary table. The scan block scans on
       the index table, but reads from the primary table, so needed
       quick access to this at [1;31mtime[ms as well.
    14)Clarified the delayed signal to be used when rows are locked

[33mcommit 02a02b43ea0d615e8322944cef360a6a447c8029[m
Author: Varun Nagaraju <varun.nagaraju@oracle.com>
Date:   Fri Sep 7 02:17:02 2018 +0530

    WL#12233 MANUALLY START/STOP PROCESSES FROM TESTS
    
    Adds stop, switchConfig, and start atrt commands that can be called from
    tests.
    
    These commands allow support of downgrade tests in which we are required
    to stop all management nodes, and then start them one at the [1;31mtime[m with
    new configuration (different procedure than rolling restart).
    
    Rolling restarts can still be done with the previously existent command
    "change_version". Additionally, the stop and start commands can be use
    for any other test scenarios that requiring stopping and/or starting a
    data node.
    
    Minor formatting issues in AtrtClient.cpp are also fixed.

[33mcommit fabcd365b5533178ba7378c1be572096f1bc2732[m
Author: Thayumanavar S <thayumanavar.x.sachithanantha@oracle.com>
Date:   Wed Sep 5 07:16:36 2018 +0200

    BUG#27966483 - SIGHUP CAUSE MYSQL SERVER CRASH.
    
    When SIGHUP signal is sent to mysqld and the server
    is admitting new connections at this [1;31mtime[m, it results
    in the server crash. The thread cache is purged as part
    of SIGHUP handling. This involves raising signal to the
    threads that are blocked waiting for new connections to be
    in thread cache. We purge the channel queue as part of this
    making it empty. New connections can be admitted after which
    result in waking up a thread in the thread cache and this attempts
    to dequeue a connection for handling from an empty channel info queue.
    This result in the behaviour bug.
    The fix is not to drain the channel info queue in the kill_blocked_pthreads
    and instead let it be handled by the unblocked thread.

[33mcommit 6ab93e4c4e55e74b63c69117224ce9f40a04880e[m
Author: Jan Kneschke <jan.kneschke@oracle.com>
Date:   Wed Aug 29 16:18:35 2018 +0200

    Bug#28570122 SLOW SHUTDOWN AFTER SHUTDOWN SIGNAL
    
    handle SIGINT and SIGTERM without any [1;31mtime[mout
    
    Loader::main_loop() was waiting 100ms for the first loaded plugin exit
    before it checked other plugins or the global shutdown flag.
    
    As it relied on futures and std::future having no way to wait on
    multiple futures at the same [1;31mtime[m the new approach uses
    
    * a concurrent queue for plugins's exceptions
      * using a lock-free multi-producer single-consumer queue
    * conditional_vars that are set if either the global shutdown is set
      or a plugin added something to the concurrent queue
    
    If one of
    
    * a plugin exits with an error
    * all plugins exit
    * shutdown is requested
    
    happens, the conditional_vars will be triggered and the loader initiates
    the shutdown sequence.
    
    routertest_component_rest_mock_server test-run[1;31mtime[m changed
    
    * from 8.8 seconds
    * to 1.3 seconds
    
    Overall, test run[1;31mtime[m drops
    
    * from 147sec
    * to 92sec
    
    RB: 20422
    Reviewed-By: pawel.mroszczyk@oracle.com

[33mcommit 0a583765d31fe6531aadfb20ee49226da9700e9c[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Sep 4 08:13:10 2018 +0200

    WL#11720 - InnoDB: Parallel read of index
    
    Post-push fix for clang/UBSAN:
    storage/innobase/buf/buf0buf.cc:3522:30:
    run[1;31mtime[m error: member access within null pointer of type 'buf_block_t'
    
    Change-Id: I9e0b6f3fb0f1e0cd2b29a2c2fa2b4050f7ed3ce5

[33mcommit 13ae611131d279038f790c7b8e7be124836008f4[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Sun Sep 2 05:39:34 2018 -0400

    Bug#28585914 ROUTER TEST ISSUES REPORTED FOR PB2 8.0.13 RELEASE BRANCH - SOLARIS X86
    
    There seems to be a compiler issue that is making the binary SEGFAULT
    each [1;31mtime[m std::condition_variable::wait_for gets called.
    This can be observed on a simple example:
    
    std::condition_variable cv;
    std::mutex cv_m;
    int main()
    {
      std::unique_lock<std::mutex> lk(cv_m);
      cv.wait_for(lk, std::chrono::milliseconds(10), [](){return true;});
    }
    
    If you compile it with:
    $ /opt/developerstudio12.6/bin/CC -std=c++11 -m64 -xO2 test.cc -o testapp
    you'll get a crash when running the binary.
    
    -xO1 doesn't crash
    -xO2 crashes
    -xO3 doesn't crash
    -xO4 doesn't crash
    -xO5 doesn't crash
    
    This patch is really only a workaround to change the optimization level to -xO1
    for the router sources to avoid this issue.
    
    Reviewed-by: Tor Didriksen <tor.didriksen@oracle.com>
    RB: #20426

[33mcommit 9d66c24308089a9a6d52f4f9fb93c77b4a76c125[m
Author: Pawel Mroszczyk <pawel.mroszczyk@oracle.com>
Date:   Thu Aug 30 13:20:45 2018 -0400

    [Router] BUG#27326466 Router blocks for many seconds on shutdown on bad cluster connectivity
    
    After user requests Router to shut down, it may hang for many seconds if
    connecting to metadata servers is troublesome. This is because Metadata
    Cache's refresh thread may be blocked on mysql_real_connect() trying to
    establish a new connection, while Router waits for the thread to finish
    running.
    
    Before this patch, refresh thread would query the "should shutdown" flag
    only between full cycles (meaning if connecting to metadata servers is
    unsuccessful, it will iterate through the entire list (ususally 3 in a
    3-node cluster), trying to connect to each one, before checking if it
    should shut down. And since the default [1;31mtime[mout was 30 seconds, that's
    typically what the user had, which meant this shutdown could take up to
    3 x 30 = 90 seconds and appear to hang.
    
    There is no real good way of fixing this, because the line it blocks on
    (mysql_real_connect()) cannot be easily interrupted without resorting to
    signals, cancelling threads and the like. So to reduce the impact, this
    patch introduces checking "should shutdown" flag before trying each
    new connection.
    
    There is also a separate patch to reduce default connection [1;31mtime[mout, to
    further reduce the impact if the user does not alter this default.

[33mcommit 980b234c28b8a153ad44f7d7b985ca936bb0a59f[m
Author: Pawel Mroszczyk <pawel.mroszczyk@oracle.com>
Date:   Thu Aug 30 13:11:43 2018 -0400

    [Router] Post-WL#10799 fix: Change default connect [1;31mtime[mout from 30 to 15 seconds (see also BUG#27326466)
    
    Change default connection [1;31mtime[mout to something closer to Server's (which
    is 10 seconds). We added extra 5 seconds to Server's 10 to allow extra
    [1;31mtime[m to establish TCP handshake, before Server's [1;31mtime[mout countdown
    begins to tick.
    
    This reduced [1;31mtime[mout also helps to alleviate the problem in
    BUG#27326466 - see its commit message for more details.

[33mcommit 9935037ced71f324f408bb579581b62ca1faa734[m
Author: Shishir Jaiswal <shishir.j.jaiswal@oracle.com>
Date:   Sat Sep 1 23:53:17 2018 +0530

    Bug#27820277 - MYSQL SERVER CRASH WHILE SET PASSWORD
    
    DESCRIPTION
    ===========
    When a user is deleted from mysql.user which is followed by
    creating few users and finally SET PASSWORD is fired, it
    results in server exit.
    
    ANALYSIS
    ========
    Each [1;31mtime[m we execute CREATE USER .., it pushes the user to
    in-memory acl_users list. When the size of the list equals
    the capacity, it mallocs a new buffer with twice the
    capacity, copies all the elements and frees the old buffer.
    This freed stale memory is returned by find_acl_user()
    during SET PASSWORD and later accessed in
    update_sctx_cache() resulting in the issue.
    
    replace_user_table() calls ha_index_read_idx_map(), which
    tells if or not a given user exists in mysql.user table.
    When SET PASSWORD is called, this func throws an error which
    signifies that the user is not in the mysql table (as it
    was deleted in the very 1st step)
    
    As of now we're not handling this error and proceeding. We
    can avoid the reported issue altogether if we handle the
    error and return from that point.
    
    FIX
    ===
    Above condition is handled in replace_user_table(). Also
    that the call to this function is modified in
    change_password() and mysql_alter_user(). The parameter
    'can_create_user' is passed as false instead of true so
    that its handled accordingly in the patch.

[33mcommit d911e398e00e7c503fe3b79679db637cef67a889[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Mar 5 07:56:42 2018 -0800

    Remove the unused min & max bounds checking feature from SimpleProperties
    
    SimpleProperties::pack() and unpack() can check that serialized values
    fall within the range of a bounded min and max. The check can be
    disabled by setting the the ignoreMinMax flag to true. unpack()
    also takes a second flag, ignoreUnknownKeys.
    
    In practice, both "ignore" flags are always set to true. This should
    mean that the checks for bounds and for unknown keys are never used.
    Nonetheless, unpack() treats the maxValue as a length limit on
    string and binary values.
    
    This patch brings the names and the API into line with the practice.
    All of the unused bounds-checking code is removed.
    ignoreMinMax and ignoreUnknownKeys are removed as flags to pack()
    and unpack(), but are retained as compile-[1;31mtime[m constants.
    minValue is removed from SP2StructMapping, and maxValue is renamed
    to maxLength.  The status code ValueTooHigh is renamed ValueTooLong.

[33mcommit 3f2778640cc51136e178437bc606bc5bed7a3b0c[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average [1;31mtime[m between
       consecutive requests to write or flush redo log.
    
    3. When the average [1;31mtime[m between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial [1;31mtime[mout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7ac28149f725a63d1b1042ee9d9a1d26f566f1b6[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average [1;31mtime[m between
       consecutive requests to write or flush redo log.
    
    3. When the average [1;31mtime[m between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial [1;31mtime[mout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7dadfe88b403d8341de6472e742d705d14de6eed[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Wed Aug 29 15:57:22 2018 +0200

    Bug #26927386: REDUCE COMPILATION TIME [noclose]
    
    Win back some of the extra compilation [1;31mtime[m induced by Bug#28245522
    (‚ÄúAllocate auto releaser maps on demand‚Äù).
    
    Change-Id: Ief0881adf291ffeed997674e1396e3e4a01dea86

[33mcommit 63fc08e24509725c708b86ea5c2f545ea68de331[m
Author: Venkatesh Venugopal <venkatesh.venugopal@oracle.com>
Date:   Thu Aug 30 19:20:41 2018 +0530

    Bug#25839610 ABORT OCCUR DURING SLAVE BACKUP WHEN RELAY LOG INDEX IS LOCK
    
    Problem
    -------
    When Relay log index file is locked by other process,
    server is unable to do any changes to the index file.
    Server treats this situation as fatal and executing
    intentional abort.
    
    
    Analysis & Fix
    --------------
    Some[1;31mtime[ms an outsider can lock index files for temporary
    viewing purpose. For eg: MEB locks
    binlog.index/relaylog.index to view the content of the file.
    During that small period of [1;31mtime[m, deletion/rename of the
    file is not possible on some platforms(Eg: Windows) Server
    should retry the delete operation for few [1;31mtime[ms instead of
    panicking immediately.

[33mcommit 78e067e69ab7afa9f02b03e1c3f398e4da4d7377[m
Author: Kailasnath Nagarkar <kailasnath.nagarkar@oracle.com>
Date:   Thu Aug 30 17:19:34 2018 +0530

    Bug #27659490 : SELECT USING DYNAMIC RANGE AND INDEX
                    MERGE USE TOO MUCH MEMORY(OOM)
    
    Issue:
    While creating a handler object, index-merge access
    creates it in statement MEM_ROOT.
    However when this is used with "Dynamic range access method",
    as range optimizer gets invoked multiple [1;31mtime[ms, mysql ends up
    consuming a lot of memory.
    
    Solution:
    Instead of using statement MEM_ROOT to allocate the handler
    object, use the local MEM_ROOT of the range optimizer which
    gets destroyed at the end of range optimizer's  usage.

[33mcommit cae634fe79e4c19902cd25248b1be24cad2c9a34[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a life[1;31mtime[m of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over [1;31mtime[m (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit 1fa54adb13755947f94b86de1eb428f1bf7f4569[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a life[1;31mtime[m of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over [1;31mtime[m (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit f654475bf10349d13cde6720e6bf8dfec1a2d443[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Wed Aug 29 09:09:46 2018 +0800

    Bug #28258992  FUNCTION CALL NOT WRITTEN TO BINLOG IF IT CONTAIN DML ALONG WITH DROP TEMP TABLE - post fix
    
    Problem 1
    =========
    Calling a function updating temporary table is not compatible with
    FLUSH TABLES WITH READ LOCK in mixed mode, because we set binlog
    format to statement for writing a 'function call' top statement
    into binary log if the function contains DML statement(s) on
    temporary table in mixed mode after fixing Bug#28258992. Which
    causes an known [1;31mtime[mout issue in statement binlog format.
    
    Fix 1
    =====
    To fix the problem, split the case from original script and
    only run the case in row binlog format.
    
    Problem 2
    =========
    There is an assertion `thd->binlog_evt_union.do_union' failure in
    MYSQL_BIN_LOG::stop_union_events(...) when restoring sub statement
    state. The root cause is that we change the binlog format after
    resetting sub statement state when executing a function which
    contains DML statement(s) on temporary table.
    
    Fix 2
    =====
    To fix the problem, we change the binlog format before resetting
    sub statement state when executing a function which contains DML
    statement(s) on temporary table.

[33mcommit c37d0cd10c7d38fdb2a6ec313a0bf8b5ff05ec12[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Mon Aug 27 16:26:14 2018 +0200

    Bug #26927386: REDUCE COMPILATION TIME [noclose]
    
    Remove sql/json_dom.h from a few files that don't need its full definition
    (it is large and takes a long [1;31mtime[m to compile). This takes the number of
    build targets depending on json_dom.h down from 567 to 56.
    
    Change-Id: I41c3b172af3875b90c5737c415634da77c3cf472

[33mcommit 4a4e484980f2c993c12a045c64d22ff2fa837685[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Mon Aug 27 11:49:55 2018 +0200

    Bug#28555665 TSAN: INCREASE SERVER RESTART TIMEOUT
    
    The problem was that tests that restarts the server [1;31mtime[ms out
    when run by a server built with Thread Sanitizer. Fix this by
    using a longer [1;31mtime[mout - similar to what is already done for Valgrind.
    
    Change-Id: I76a5f2f8a7a1b3f5adbe1edbb83fb9845a963b3b

[33mcommit 164980a64b5eb4b296f0c6c6ee053fcabbd3530c[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Sun Aug 26 18:22:48 2018 +0530

    Bug#28424366 : ADAPT CLUSTERJ TO USE CONNECTOR/J 8.0.12 FOR TESTING
    
    Few Date, Time and Timestamp ClusterJ testcases fail, when using the
    latest Connector/J. The reason for this is the complete removal of
    the legacy date and [1;31mtime[m code, in the latest version of the
    Connector/J. The legacy code, which was previously present and
    enabled by default, prevented any conversion of the date/[1;31mtime[m types
    between the mysqld and the ClusterJ test app [1;31mtime[mzones. After its
    removal, the date and [1;31mtime[m types are now converted back and forth
    between the mysqld and ClusterJ test app [1;31mtime[mzones, if they are
    different. This becomes an issue with MTR as the mysqld is always run
    in GMT and the ClusterJ test app is always run in local system [1;31mtime[m.
    This conversion causes the tests to fail when the same value read
    through JDBC and ClusterJ are compared and found not equal.
    
    This patch fixes the issue by setting the session [1;31mtime[mzone of the
    mysqld to the same as the [1;31mtime[mzone of the ClusterJ test app. Due to
    both of them now running in same [1;31mtime[mzone, no conversion happens and
    the values are intact. This patch also enables all the tests
    previously disabled due to this issue. Also, the testcase
    Timestamp2AsSqlTimestampTypesTest is cleaned up in this patch.
    
    Change-Id: I4a87ac77dc29a348973f532064df74287e26150c

[33mcommit ddf2ce815c5ac0fcc20d28d0b0a62144bcb94163[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Thu Aug 23 16:04:24 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a life[1;31mtime[m of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over [1;31mtime[m (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit aa79a62277f476c12dc9cddc0791fdbae0387c90[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Mon Jun 4 13:53:39 2018 +0200

    WL #12131: Deprecate setting user variables within expressions
    
    MySQL has an SQL extension called user variables, where the user can do
    something like
    
      SET @a := 3;
      SELECT @a + 123;
    
    However, it also allows setting these variables within expressions:
    
      SELECT (@a := @a + 1) + 456;
    
    Given that SQL does not contain any notion of sequence points, the timing of
    when these variables are set is undefined. Furthermore, since they can change
    _type_ at any [1;31mtime[m, semantics can become extremely confusing and/or
    undefined, and it's been the source of a number of bugs.
    
    The most popular use of such assignments is to simulate windowing functions
    (e.g. row_number()), but now, we have true windowing functions.
    
    Thus, deprecate it, staging it for later removal.
    
    Change-Id: Ic6aa9bca2c4cb39ed8fa292401745b06fa42265c

[33mcommit c62a35421a31c28f8ac371e4f66f98e47ceade6a[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Wed Jun 20 15:59:08 2018 +0530

    Bug #26574003 : NODE CRASH WITH ERROR 2335 DUE TO TOO MANY DROP_TRIG_IMPL_REQ IN SQ FROM SUMA
    
    When SUMA receives the SUB_STOP_REQ signal, it executes it and sends
    back the SUB_STOP_CONF. The SUB_STOP_CONF ultimately is relayed back to
    the API and the API is now open to send more SUB_STOP_REQs.
    
    The SUMA, only after sending the SUB_STOP_CONF, asynchronously decides
    to drop the subscription altogether if no subscribers are present. This
    drop involves sending multiple DROP_TRIG_IMPL_REQs to DBTUP. With the
    way the DbtupProxy is implemented, it can handle 21 DROP_TRIG_IMPL_REQs
    in parallel. Any more subsequent requests are queued in the DbtupProxy
    thread short [1;31mtime[m queue. If the execution of the DROP_TRIG_IMPL_REQ is
    delayed, there is a chance where the short [1;31mtime[m queue gets overflowed
    with too many DROP_TRIG_IMPL_REQs and crashes the data node with a
    "Error in short [1;31mtime[m queue" error.
    
    A similar scenario exists in SUMA node failure handling, where a node
    failure can cause a lot of subscriptions to stop and can end up
    overflowing the DbtupProxy with DROP_TRIG_IMPL_REQ request. This
    possible overflow during node failure handling is already guarded
    against by limiting the maximum DROP_TRIG_IMPL_REQs allowed to execute
    parallely. But the SUB_STOP_REQ handling lacks such a guard.
    
    This patch fixes that by delaying the execution of the SUB_STOP_REQ when
    there are a number of DROP_TRIG_IMPL_REQs outstanding.
    As a result,
     i.  This prevents SUB_STOP_REQ from adding 3x DROP_TRIG_IMPL_REQ to the
         DbtupProxy thread short [1;31mtime[m queue, at the cost of 1x SUB_STOP_REQ
         in the SUMA thread short [1;31mtime[m queue.
     ii. This delays sending SUB_STOP_CONF, which will stall any further
         request from the same API source, as API sources are usually
         synchronous. If multiple APIs are separately submitting
         SUB_STOP_REQs to SUMA, and are causing different subricptions to
         become STOPPED, then they will also be queued at SUMA when the
         number of DROP_TRIG_IMPL_REQs are higher than the threshold.
    
    The threshold, maximum number of DROP_TRIG_IMPL_REQs that can be
    executed parallely, is set to 6. Given that each SUB_STOP_REQ might
    send 3x DROP_TRIG_IMPL_REQ to DbtupProxy and the maximum threshold set
    at 6, the number of subscriptions that can be dropped in parallel within
    a data node at a [1;31mtime[m is limited to 2.
    
    Change-Id: I8879eb00af3c739b7774af5a9162427b3a40bfdd

[33mcommit 5f27bdb5f4f06dfc3b4382408cf3131bd70ff56d[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Tue Aug 21 15:50:40 2018 +0530

    Bug#28346712: FIX TESTSUITE ABORTED ERROR DUE TO TIMEOUT IN PB2:DAILY-TRUNK-CLUSTER BRANCH
    
    PB2 runs integration testing on the branch daily-trunk-cluster everyday.
    Despite running successfully, it reports failure often in different
    platforms with error - "Test suite [1;31mtime[mout". This patch fixes that by
    doubling the suite [1;31mtime[mout value if the ndb test suites are added and
    run by default.
    
    Change-Id: Icefdd1fcc44e5e5ad41c3d99c37bfc3c16dd77b2

[33mcommit 21a1a505efcb03734be4e3f9d78fe4bfe64810d7[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Aug 20 14:48:16 2018 +0200

    Bug#28486390 False possitive 'Key constraint violations' reported
    
    The root cause for this bug has been tracked down to how the
    'savePointId' is set for the parent/child triggers.
    
    The savePointId is an internal mechanism controlled by TC which
    decide which 'version' of a tuple which should be seen if the
    tuple is modified (possible multiple [1;31mtime[ms) within a transaction.
    For an 'immediate trigger' the parent/child triggers should see
    the tuple contents as immediately *after* the operation which updated
    it. This was implemented by setting the triggers LqhKeyReq::savePointId
    from ApiConnectRecord::currSavePointId. This relied in that
    ApiConnectRecord::currSavePointId was incremented when TC executed
    the 'last' of the operations sent from API.
    
    However, TC could start generating the LqhKeyRequests for the
    row-triggers at the receipt of LQHKEYCONF,  before it had
    executed the 'last' operation. Thus they were produced with an
    incorrect savePointId, effectively resulting in the 'before'
    image value of the tuple being used when validating the parent/child
    relations. (Effectively we had implemented an 'intermediate' trigger
    instead of an 'immediate (after)' trigger'.
    
    The handling of deferred triggers was not affected by this bug.
    These triggers will be fired as part of the commit processing,
    where all preceeding *KEYREQ's has completed, and the
    ApiConnectRecord::currSavePointId correctly contains the
    last savePointId in the transaction.
    
    This patch fix how the LqhKeyReq::savePointId is set in the
    parent/child *read* triggers: If the trigger is an immediate trigger,
    the savePointId is set a '+1' relative to the originating update
    operation. This ensures that the read operation verifying the parent/child
    consistency (only) sees the updated 'after' value.
    
    Patch also changes a few places where we checked for the apiConnectstates
    CS_SEND_FIRE_TRIG_REQ or CS_WAIT_FIRE_TRIG_REQ. This is what the method
    isExecutingDeferredTriggers() is intended to do, so rewrote to call this
    method instead.
    
    The unused '#define INTERNAL_TRIGGER_TCKEYREQ_JBA' was also removed.

[33mcommit 167de447d4efc075e67e7ba07494246b9fd3e977[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 7 14:44:54 2018 +0200

    Bug#28262259: TRIGGERS ON SAME RECORD FIRED FOREVER -> ERROR: 233 OUT OF OPERATION RECORDS
    
      There are two variants of deferred trigger/constraints:
    
     1) Triggers created by a 'NO ACTION' foreign key are deferred by
        declaration, and managed by deferred<Op>Triggers list.
        These are always fired at commit [1;31mtime[m by
        Dbtup::checkDeferredTriggers() calling ::fireDeferredTriggers()
    
     2) In addition any constraints defined to be 'immediate' may be
        deferred by setting the 'ndb_deferred_constraints' or
       'TupKeyReq::deferred_constraints' flag in the TUPKEYREQ signal.
    
    In either case, when TUP find that some triggers should be executed as deferred,
    it informs TC about these outstanding triggers in the FIRE_TRIG_CONF and LQHKEYCONF
    replies. At commit [1;31mtime[m TC will then send a FIRE_TRIG_REQ to TUP which is intended
    to start the deferred trigger execution.
    
    However, it turned out that deferred trigger firing did not distinguish between
    whether it should fire only the triggers deferred by declaration (1), or if
    the immediate triggers had been deferred. So it unconditionally re-fired the
    immediate triggers, even if they already had been executed.
    
    If the immediate triggers contained a self referencing cascading update constraint,
    the update was then effectively re-executed in an endless loop. For other flavours
    of immediate triggers, it was probably only an extra overhead firing the trigger twice.
    
    This patch introduce a ::deferred_constraint flag in the Dbtup::Operationrec
    object. This object is persistent across the initial LQHKEYREQ, and later FIRE_TRIG_REQ
    which may fire and deferred triggers. It keeps track of whether the initial
    LQHKEYREQ requested the immediate triggers to be deferred. The later FIRE_TRIG_REQ
    will the conditionally fire the immediate trigger only when they has been deferred.
    
    (cherry picked from commit 5fa4c67f400a24d429eaf7f1af81c37868c471cc)

[33mcommit fbeece32479de86e9418f46c2d85c4a3c0ada57f[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Fri Aug 17 18:08:44 2018 -0700

    Tests introduced in WL9508 still fail periodically in PB2.
    Make ALTER UNDO TABLESPACE SET INACTIVE more indempotent.
    Help truncate_recover_xx tests by doing a slow shutdown restart first in order
    to reduce the  size of the undo logs so that the shutdown in the test does not [1;31mtime[mout.

[33mcommit e16a06e116442330468093ded4d8c7794693c20b[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Aug 14 14:28:37 2018 +0200

    Bug#28501016 TSAN: DATA RACE IN TC_LOG_MMAP AND MY_TIMER UNIT TESTS
    
    Fix data races and wrong mutex order warnings in tc_log_mmap-t,
    my_[1;31mtime[mr-t and mysys_lf-t unit tests.
    
    tc_log.cc: Move DBUG_ASSERT until after LOCK_tc is locked so
    that active is read protected by LOCK_tc.
    
    my_[1;31mtime[mr-t.cc: Write to notify_function before the [1;31mtime[mr is initialized,
    not after. Also reduce scope of mutex locking to avoid locking
    mutexes in different order.
    
    mysys_lf-t.cc: Make two variables std::atomic. Remaining issues
    with this unit test is now related to the LF implementation, not
    the unit test itself.
    
    Change-Id: I460263c8378bbf7fc93fb65c7c09617888017b9d

[33mcommit 64bcccf87d9f61fd6f1a4627c808a1a8a6a42cc3[m
Author: Arnab Ray <arnab.r.ray@oracle.com>
Date:   Thu Aug 9 16:21:57 2018 +0530

    WL#8500 Adapt MySQL Cluster to 8.0
    
    Enable all tests previously disabled under WL#10167 On the fly frm
    translation and a few others disabled under WL#9185 MySQL Cluster
    support for new DD. The tests that couldn't be enabled are now
    disabled under new/other bugs/worklogs.
    
    - ndb_alter_table_online
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS statement
      - Inplace ALTER ADD of a TIMESTAMP column is now supported.
        Remove this test case from the unsupported cases section and
        reimplement it as a postive test. Change due to WL#9687:
        Change default for explicit_defaults_for_[1;31mtime[mstamp to ON
      - Specify latin1 as character set for a large varchar(10000)
        column
    
    - ndb_column_properties_compat
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP DATABASE IF EXISTS statement
    
    - ndb_fk_restore
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS statement
      - Update result file with new SHOW CREATE TABLE output which
        shows the new default charset
    
    - ndb_restore_misc
      - Modify test to use new way to trigger schema synchronization
      - Update result file with new SHOW CREATE TABLE output which
        shows the new default charset
      - Specify latin1 as character set for a large varchar(11000)
        column
    
    - ndb_restore_compat_compression
      - Same changes as the ones made to ndb_restore_misc
      - This test re-used ndb_restore_misc.result file previously
        which is no longer supported. Created a new result file
        for the test
    
    - ndb_restore_schema_subsets
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary SHOW TABLES statements
    
    - ndb_alter_table_backup
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS statement
    
    - ndb_dd_restore_check_tablespace_mdl
      - Modify test to use new way to trigger schema synchronization
    
    - ndb_restore_compat
      - Modify test to use new way to trigger schema synchronization
    
    - ndb_native_default_support
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS and DROP DATABASE IF
        EXISTS statements
    
    - ndb_restore_compat_downward
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP DATABASE IF EXISTS statement
    
    - ndb_restore_compat_endianness
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS statement
    
    - ndb_restore_undolog
      - Modify test to use new way to trigger schema synchronization
      - Remove unnecessary DROP TABLE IF EXISTS statement
      - Disable test under WL#12172 Schema distribution synchronization
        of tablespaces and logfile groups
    
    - ndb_wl946_pre
      - Modify test to use new way to trigger schema synchronization
      - Set backup_data_dir
    
    - ndb_wl946_post
      - Modify test to use new way to trigger schema synchronization
      - Set backup_data_dir
      - Remove unnecessary DROP TABLE IF EXISTS statement
    
    Change-Id: I48b7cf5976bcda72361ec9c76fa73c6f6b05fc80

[33mcommit c3f19761bd38ca02b23c1ea75acea164f9948136[m
Author: Sanjana DS <sanjana.ds@oracle.com>
Date:   Fri Aug 10 12:05:57 2018 +0530

    Bug #27584165 TUPLE CORRUPTION DETECTED AND DATA NODE CRASH IN DBTUPEXECQUERY.CPP
    
    Cause:
    -----
    1) A COMMIT of a delete arrives.
    2) The disk page must be paged in.
    3) While the page in is happening the scan comes around.
       The scan sees an ok state to scan since the COMMIT delete didn't
       perform the actual delete before the disk page had been paged in.
    4) The page in completes.
    5) The actual delete happens through a callback
    6) The scan happens through a callback and we crash due to tuple already
       deleted.
    
    Fix:
    ---
    1) Introduce DELETE_WAIT flag to mark the tuple header.
    2) Set this flag when we can get a real-[1;31mtime[m break after we decide to delete but
       before the actual delete.
    3) ACC scans will never see any rows with this flag since the row is deleted from ACC index
       before the real-[1;31mtime[m break is invoked in TUP.
    4) TUX scans similarly since TUX indexes are updated before the real-[1;31mtime[m break in TUP
    5) LCP scans need not consider this flag either as the COMMIT point for LCP scans is after the
       real-[1;31mtime[m break reading in the disk page.
    
    So only TUP scans ordered from NDB API and TUP disk scans and Node Restart scans (NR scans)
    are made to consider DELETE_WAIT and don't attempt to read the tuple when this flag is set.

[33mcommit cda095f108ed4b629fb29c0929018614fb58dfb7[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Aug 9 18:27:53 2018 +0200

    Bug#28019228 CPCD FAILS TO START PROCESSES DUE TO INVALID TEMPORARY PID FILES
    
    Problem:
    
    When CPCD starts processes some[1;31mtime[ms there is a pid file left over.
    
    When CPCD starts a new process it waits for the new process to write a
    pid file.
    
    If there is an old pid file CPCD may misread the old one as a new pid
    file.
    
    If the pid in file and what is expected differ, CPCD left the newly
    started process running and ignored the pid file.
    
    This made things worse, leaving processes running typically interferred
    with other processes starting later since they might request same system
    resources like TCP ports.
    
    Solution:
    
    If the found pid file has no running process with that pid, remove the
    pid file.
    
    If there are some process running with pid from pid file, we at least do
    not start new processes that will be forgotten.

[33mcommit a384020bcf4801a5ee19781d7eb1959a79930471[m
Author: Sanjana DS <sanjana.ds@oracle.com>
Date:   Wed Aug 8 17:45:30 2018 +0530

    Bug #17732772 NDB_MGMD : CLUSTER LOG SYNCHRONOUSLY WRITTEN BY SIGNAL HANDLING THREAD
    
    ndbout and ndberr become invalid after exiting from mgmd_run()
    and redirecting to them before the next call to mgmd_run() causes a
    segmentation fault. This can happen during mgmd service restart.
    This fix ensures that ndbout and ndberr are valid all the [1;31mtime[m.

[33mcommit ffeb1995f809e847f18403848e7f50e5ed6830c0[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 7 14:44:54 2018 +0200

    Bug 28262259: TRIGGERS ON SAME RECORD FIRED FOREVER -> ERROR: 233 OUT OF OPERATION RECORDS
    
      There are two variants of deferred trigger/constraints:
    
     1) Triggers created by a 'NO ACTION' foreign key are deferred by
        declaration, and managed by deferred<Op>Triggers list.
        These are always fired at commit [1;31mtime[m by
        Dbtup::checkDeferredTriggers() calling ::fireDeferredTriggers()
    
     2) In addition any constraints defined to be 'immediate' may be
        deferred by setting the 'ndb_deferred_constraints' or
       'TupKeyReq::deferred_constraints' flag in the TUPKEYREQ signal.
    
    In either case, when TUP find that some triggers should be executed as deferred,
    it informs TC about these outstanding triggers in the FIRE_TRIG_CONF and LQHKEYCONF
    replies. At commit [1;31mtime[m TC will then send a FIRE_TRIG_REQ to TUP which is intended
    to start the deferred trigger execution.
    
    However, it turned out that deferred trigger firing did not distinguish between
    whether it should fire only the triggers deferred by declaration (1), or if
    the immediate triggers had been deferred. So it unconditionally re-fired the
    immediate triggers, even if they already had been executed.
    
    If the immediate triggers contained a self referencing cascading update constraint,
    the update was then effectively re-executed in an endless loop. For other flavours
    of immediate triggers, it was probably only an extra overhead firing the trigger twice.
    
    This patch introduce a ::deferred_constraint flag in the Dbtup::Operationrec
    object. This object is persistent across the initial LQHKEYREQ, and later FIRE_TRIG_REQ
    which may fire and deferred triggers. It keeps track of whether the initial
    LQHKEYREQ requested the immediate triggers to be deferred. The later FIRE_TRIG_REQ
    will the conditionally fire the immediate trigger only when they has been deferred.

[33mcommit ab7e2cd84644d223cf9f7841ff6bb8087b6b5ddd[m
Author: Sanjana DS <sanjana.ds@oracle.com>
Date:   Wed Aug 1 19:15:09 2018 +0530

    Bug #17732772 NDB_MGMD : CLUSTER LOG SYNCHRONOUSLY WRITTEN BY SIGNAL HANDLING THREAD
    
    During mgmd shutdown/restart, the cluster logging thread will get blocked for
    some [1;31mtime[m. Since there's no more logging done at one point after we decide
    to shut down, the cluster logging thread will get blocked on the get() call
    to the log buffer.
    Hence, to prevent this a new function is added to the LogBuffer class - stop()
    which makes it possible to wakeup and return immediately from a blocking get()
    call.

[33mcommit d74e1c084a652973d72011ba36e973fcffaa9027[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Thu Aug 2 01:36:43 2018 +0530

    Bug#28051645 : CLUSTERJ EXCEPTION "THE METHOD IS NOT VALID IN CURRENT BLOB STATE"
    
    If a table contains a Blob or a Text field, and the table is queried
    using ClusterJ for a record that doesn't exist, the exception "The
    method is not valid in current blob state" is thrown.
    
    The postExecuteCallback method defined in the NdbRecordKeyOperationImpl
    attempts to read the blob values every[1;31mtime[m after a query execution
    without checking if the operation to fetch the key returned any errors.
    Due to this, the above exception is thrown when there is no record to
    read the blob from. This patch fixes that by updating the function to
    read the blob values only when there is no error in executing the
    ndbOperation. A very similar issue in the NdbRecordUniqueKeyOperationImpl
    is also fixed in this patch. Also updated testcases.

[33mcommit 9b721e8c1b573bb30a70eb921fd16b0ac2113120[m
Author: Ragasudha Chillara <ragasudha.chillara@oracle.com>
Date:   Tue Jul 24 20:25:50 2018 +0530

    WL#12229 ATRT SUPPORTS TEST FAILURE RETRIES
    
    Adds support for ATRT to re-run the test cases in case of failure.
    
    Modifies ATRT to accept a configuration file where user can specifies the number
    of retries if a test case fails. In case of failure test case, all the processes
    will be restarted and the test will retried up to the no of [1;31mtime[ms as specified in
    config file. Setting a retry value in config file is optional, by default retry value
    is '0' , where as the retry value less than zero will be treated as an error and retry
    value greater than five, issues an warning.
    
    Updated the storage/ndb/test/run-test/README general algorithm to include retry
    algorithm.
    
    changes made in storage/ndb/test/run-test/check-tests.sh to support configuration
    if "max-retries" parameter is specified.

[33mcommit 971d944474bdebc345232688534d92fad40d7f6b[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Jul 19 11:02:55 2018 -0700

    WL9508 Post-fix
    innodb.poratability_basic sporadically fails in daily_trunk with a [1;31mtime[mout
    waiting for an undo tablespace to become empty.  Most of these waits
    override the default 30 seconds with 300 in other tests that use explicit
    undo tablespaces in wl9508.  Found 2 other tests that also were missing
    the longer wait_[1;31mtime[mout.
    Fixed 5 Windows compile warnings in clone code.

[33mcommit db1423594e6bd0c193caea35f219a1b099eed729[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Fri Jul 13 19:11:29 2018 +0530

    Bug#26484136 : Condition pushed down differs in different platforms [noclose]
    
    The testcases ndb.ndb_bushy_joins and ndb.ndb_many_fragments fail in
    multiple platform due to result content mismatch caused by this bug.
    The condition push down sent to the engine, which can also be examined
    through explain, varies in different platforms. The difference is not in
    the entire pushed down condition rather a same condition is pushed down
    multiple [1;31mtime[ms and the number of [1;31mtime[ms the same condition gets pushed
    varies in different platforms due to this bug.
    
    This patch adds a hack to both the failing testcases to replace all
    instances of a same condition by just one instance. Thus regardless of
    the number of [1;31mtime[ms the condition gets pushed, the tests will pass.
    This should be reverted when an actual fix is implemented.

[33mcommit d52edf0e63054223788311bdbea76f39f67536c5[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Jul 6 13:21:36 2018 +0200

    BUG28303209
    -----------
    Also restore bug reported in BUG#90609.
    When the following conditions are true we will crash on a segfault in restore
    of an LCP.
    1) More than 2002 parts in LCP to restore
    2) Two LCP ctl files are found in recovery
    3) The LCP ctl file to use is the first one
    
    This can easily happen after long [1;31mtime[m of operating a 7.6 cluster and can happen
    in many nodes at the same [1;31mtime[m.
    
    Problem is that the m_lcp_ctl_file_data array is big enough to read the data in
    from disk, but not big enough to also decompress the partPairs array from 24 bits
    per part to 32 bits per part. This requires an additional 180 bytes.
    
    Fixed by extending the size by the size of the LCP ctl header to ensure that the
    parts array have at least 2048 32-bit words to its access.
    
    (cherry picked from commit 5c5e3582a66c01ed55a9b61d8774e3839475c6d0)
For keyword perf:
[33mcommit ec2d54af4f84a2c794e6b74d93215559bf8dab6a[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Tue Nov 27 09:38:17 2018 +0100

    Bug#28975640: REDUCE STRING ALLOCATIONS IN JSON_WRAPPER_OBJECT_ITERATOR
    
    Introduce microbenchmarks for testing the [1;31mperf[mormance of
    Json_wrapper_object_iterator when iterating over a JSON object, both
    in binary format and in DOM format.
    
    Initial results (64-bit, Intel Core i7-4770 3.4 GHz, GCC 8.2.0):
    
    BM_JsonWrapperObjectIteratorBinary        45733 ns/iter
    BM_JsonWrapperObjectIteratorDOM           41063 ns/iter
    
    Change-Id: I1c2dd55317fac45288b9916614674636d1aeee8d

[33mcommit 69b68980f5fd60c15c6a7fd06e2b25be7910fa00[m
Author: V S Murthy Sidagam <venkata.sidagam@oracle.com>
Date:   Thu Nov 22 07:07:50 2018 +0100

    Bug #27325622 HANG IN XPLUGIN FOR ALTER USER+XCONNECTION+AUDIT_LOG LOAD
    AND UNLOAD
    
    Description:
    observing server hang when below operations [1;31mperf[morming on two terminals.
    a) Install and uninstall audit log plugin continuously
    b) Doing selects for some sys variable continuously.
    
    Analysis:
    THREAD1 is installing audit_log plugin (it doesn't matter if
    its audit_log or other)
    THREAD2 selects for some sys variable.
    
    Both threads do following:
    THREAD1: mysql_install_plugin
    THREAD1: mysql_mutex_lock(&LOCK_plugin);
    THREAD1: mysql_rwlock_wrlock(&LOCK_system_variables_hash);
    THREAD1: plugin_add
    THREAD1: plugin_add, fails because (Function 'audit_log' already exists)
    THREAD1: mysql_mutex_unlock(&LOCK_plugin);
    THREAD2: mysql_parse(SELECT @@require_secure_transport)
    THREAD2: find_sys_var_ex
    THREAD2: mysql_mutex_lock(&LOCK_plugin);
    THREAD2: mysql_rwlock_rdlock(&LOCK_system_variables_hash); WAITS IS
    LOCKED BY THREAD1
    THREAD1: report_error -> my_message_sql -> mysql_audit_acquire_plugins
    THREAD1: mysql_mutex_lock(&LOCK_plugin); WAITS IS LOCKED BY THREAD2
    DEAD-LOCK
    
    The problem concerns following mutexes (deadlock):
    LOCK_system_variables_hash, LOCK_plugin.
    
    Fix:
    In case of plugin_add() failure LOCK_system_variables_hash should be
    released before LOCK_plugin.
    Similarly, in the other places also, maintained the same sequence of
    releasing the lock order.

[33mcommit 0c8b4258e0dd93cda1ef1b7102816135edc514c8[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Mon Nov 19 21:50:26 2018 +0100

    Bug#28805764 STABILIZE THE 8.0 RELEASE TEST [noclose]
    
    Stabilize
    
     query_rewrite_plugins.logging_general
     [1;31mperf[mschema.digest_table_full
     test_service_sql_api.test_sql_general_log
    
    Approved by Terje R√∏sten <terje.rosten@oracle.com>

[33mcommit 575b85d6dc614a48fe4f1116d5cf3426c3a9fb90[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Thu Nov 1 13:37:02 2018 +0100

    Bug#28602237 QUERY_REWRITE_PLUGINS.PERFORMANCE_SCHEMA FAILS SPORADICALLY ON PB2 VALGRIND
    
    Background
     The problem here is similar to bug BUG#28211128.
    
     Initialization of X plugin is async and can happen at any time during
     a test. During initialization the X plugin asks the server about
     various information by using a query:
      SELECT @@skip_networking, @@skip_name_resolve ...
    
     this query will be logged, for the table
     [1;31mperf[mormance_schema.events_statements_history something like this will
     appear:
    
      sql_text : SELECT @@skip_networking, @@skip_name_resolve ...
      source   : srv_session.cc:1107  | <empty>
      digest   : 1ebda4149951... | NULL
    
     For query_rewrite_plugins.[1;31mperf[mormance_schema the query done is:
    
      WHERE sql_text LIKE 'SELECT %'
    
     which leads to test failure when the initialization of the X plugin
     is slow.
    
    Fix
     Stabilize the test by ignoring rows added by query done in X plugin.

[33mcommit 83556c149883457a1841fe11570461812decc57e[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Mon Oct 29 08:03:00 2018 +0100

    Bug#28122841 - CREATE EVENT/PROCEDURE/FUNCTION CRASHES WITH ACCENT INSENSITIVE
    NAMES.
    
    Issue here is, stored routines, events and resource group names
    are case and accent insensitive. While creating MDL_key and
    data-dictionary cache keys, only case sensitiveness is considered
    but not the accent sensitiveness for these object names. Hence
    key comparison of accent insensitive name yielded the wrong
    result and hit the asserts mentioned in the bug report.
    
    While creating MDL_key and DD cache key, currently the case
    is stripped off by lower casing the names. But for key comparisons
    to yield correct result, even the accent should be stripped off
    from the names.
    
    So to fix this issue, while creating keys for MDL and DD cache
    the object names are normalized (to strip off both case and accent).
    The normalized name contains weights of each character of the
    name (strnxfrm() of collation is used to normalize the name).
    utf8_general_ci collation is used by these objects and normalized
    name contains two byte value for each character with the
    utf8_general_ci collation. The normalized name is used in the
    comparison and it yields the expected result.
    
    MDL_key with normalized object name:
    
      MDL_key::mdl_key_init() is overloaded to accept the normalized
      object name and form a key with a quadruplet
    
       @<mdl_namespace@>+@<database name@>+@<normalized object name@>+
                                                  @<object name@>
    
      For key compare operations, key length only till end of
      normalized object name is used.
    
      Object name in the MDL_key is used by the [1;31mperf[mormance_schema
      for metadata locks listing. Similar to MDL_key from for columns,
      object name is stored only if there is sufficient space.
      Not listing a object name from the [1;31mperf[mormance schema when
      there is no space should be OK instead of increasing the MDL_key
      size to store full object name.
    
    DD cache key with normalized object name:
    
      DD Global_name_key, Item_name_key and Routine_name_key class's
      operator<() is updated to compare object names using my_strnncoll().
      New static method name_collation() in dd::tables::* classes gets the
      collation used for the name field. Same collation is used with
      my_strnncoll() to yield the correct results.
    
      TODO: Object_table_definition_impl::name_collation() is implemented
            as a static function now.
            Changing name collation is not supported during upgrade now.
            To support this, static definition of this method should be
            avoided and should provide a possibility to have different
            collations for actual and target table definition.
    
    Sroutine_has_entry with normalized routine object name:
    
      The key for the Sroutine_has_entry hash is modified to hold the
      normalized names for the stored routines now. Binary comparison
      of routine keys yield the correct results.
    
    Change-Id: I70276c4128422168ba81c9d5e28efe8d80244d9b

[33mcommit bc0cce1fda024aa9fa1121c7b433d55fe00589cb[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Fri Oct 26 13:08:22 2018 +0200

    BUG#28377456 PB2 PERFSCHEMA.STATEMENT_DIGEST_CONSUMERS FAILING SPORADICALLY FROM LAST 7 MONTH
    
    Init of X plugin is async and can happen at any time during a test,
    during init X plugin ask server about various information by using a
    query:
    
     SELECT @@skip_networking, @@skip_name_resolve ...
    
    this query will be logged, for [1;31mperf[mormance_schema.events_statements_summary_by_digest
    something like this will appear:
    
     schema_name : NULL
     digest      : 1ebda414995104bec2d678a2f3a337c8cc504cafa93982ba395404ef9dbc5419
     digest_text : SELECT @@`skip_networking` , @@`skip_name_resolve` ...
     count_star  : 1
    
    Fix
      Stabilize output by ignoring rows where schema_name is NULL or
      source is srv_session.cc
    
    See also
      BUG#28211128

[33mcommit 1ccc485d35c5811677dafe8a3440bfa30047493e[m
Author: Aakanksha Verma <aakanksha.verma@oracle.com>
Date:   Thu Oct 25 16:43:24 2018 +0530

    Bug #28517843   INDEX->TYPE & 32 || ROOT != FIL_NULL ||
    DICT_TABLE_IS_DISCARDED(M_TABLE)
    
    PROBLEM
    
    We trying to [1;31mperf[morm an instant add column on a discarded tablespace.
    There is crash as the table id for new table during alter is not updated
    to the data dictionary (because tablespace is discarded).
    
    FIX
    
    As is the case of non instant add column we plan to return error from
    the prepare phase in case of instant add column as well.
    
    Reviewed by : Bin Su<bin.x.su@oracle.com>
                  Kevin Lewis<kevin.lewis@oracle.com>
    
    RB: 20742

[33mcommit b9cb76395177ca88c5a092993a52864ad5df8145[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Wed Oct 24 07:09:40 2018 +0200

    Bug#28805764 STABILIZE THE 8.0 RELEASE TEST [noclose]
    
    Make [1;31mperf[mschema.histograms run only on 64bit platforms (32-bit
    non-debug gives different results).

[33mcommit a18b7636b7c51c767b40cb306a4920971f2844b3[m
Author: Thayumanavar S <thayumanavar.x.sachithanantha@oracle.com>
Date:   Thu Oct 18 07:04:53 2018 +0200

    BUG#27148580 - RESOURCE GROUP namespace should use scoped
                   lock strategy in unobtrusive lock incr.
    
    RESOURCE_GROUPS namespace should use scoped lock strategy in the function
    MDL_lock::get_unobtrusive_lock_increment. Thus this patch introduces
    get_strategy to return either the scoped or object lock strategy
    correspoding to MDL key namespace.
    The patch does not add any test cases as it has impact relating to
    scalability/[1;31mperf[mormance only.

[33mcommit 0567d8f5a583cfcaf22adfe97fb2a48efc660d07[m
Author: Hemant Dangi <hemant.dangi@oracle.com>
Date:   Mon Oct 8 17:51:16 2018 +0530

    Bug#28545795: CRASH(SIG6) PURE VIRTUAL METHOD CALLED TERMINATE WITHOUT ACTIVE EXCEPTION
    
    Issue:
    ======
    Querying [1;31mperf[mormance_schema.replication_group_members table while member is
    rejoining and reallocation of group_member_mgr object happens was
    resulting in crash.
    
    Solution
    ========
    Removed deallocation of group_member_mgr object and instead updated
    new member information.

[33mcommit 58a8ee30f3099c14e7cd2c1b918c8c20620c7290[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Fri Sep 28 23:36:56 2018 +0300

    Fix for bug#28714367 "5.7.21+ LF_NODE METADATA LOCK LEAK WHEN USING GET_LOCK".
    
    Calls to GET_LOCK() function with zero timeout argument which failed due to
    concurrent connections holding the same user-level lock, left underlying
    metadata lock structure in state which prevented future reuse of its memory
    for other metadata locks (or release of this memory before server shutdown).
    As result memory was hogged by some of workloads which involved user-level
    locks with random/constantly changing names, attempted to lock by different
    connections with zero timeout.
    
    The problem was introduced by fix for bug@26739438 "DEADLOCK ON
    GET_LOCK(..., 0)". This fix added short-cut to MDL_context::acquire_lock()
    for case when we failed to acquire lock instantly and zero timeout was used.
    However, proper cleanup of MDL_lock fast path state and obtrusive lock
    count was not [1;31mperf[mormed in this case, which led to MDL_lock object being
    always marked as used.
    
    This fix solves the problem by changing acquire_lock() code to resort
    to calling try_acquire_lock() in case of zero timeout. The latter call
    [1;31mperf[morms cleanup properly.
    
    It is hard to write robust test case for this bug for our test suite.
    So no test case provided as part of the patch. However, this fix was
    tested manually.

[33mcommit b0c88e244250186f44b46f73a0e221c25fcd38ef[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Tue Sep 25 18:01:16 2018 +0300

    Fix for bug#28608460 "POSSIBLE TO ADD INCONSISTENT PARENT TO ORPHAN FOREIGN KEY IF SE CHANGED".
    
    It was possible to add inconsistent parent table to orphan foreign key
    by creating parent table in storage engine different than one of child
    table and then changing parent storage engine to be same as of child.
    
    The problem occurred because code in ALTER TABLE which is responsible
    for checking that newly introduced parent for orphan foreign keys
    is consistent for them didn't take into account the above scenario.
    Locks on orphan tables and checks were [1;31mperf[mormed only for case when
    parent was introduced through ALTER TABLE ... RENAME.
    
    This fix adjusts ALTER TABLE code to take into account that parent table
    for orphan foreign keys can be added by changing table storage engine.
    We now acquire locks on orphan tables in this case and check if new
    parent is consistent with these foreign keys.

[33mcommit 1aed687110edaf83a7556c797bdfb8fd1d4c5ac2[m
Author: Anibal Pinto <anibal.pinto@oracle.com>
Date:   Mon Oct 1 12:00:17 2018 +0200

    WL#11123: Group Replication: hold reads and writes when the new primary has replication backlog to apply
    
    Post fix:  rpl.rpl_[1;31mperf[mschema_threads_processlist_status failing due
    changing last element of array.

[33mcommit 9c32d824d637de745cfeeac633d467ddd3a507a2[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Sep 28 10:19:04 2018 +0100

    WL#12393: Logging: Add new global variable, log_slow_extra, for richer slow logging
    
    This changeset adds a mode in which more attributes are written to the slow
    query log to aid in [1;31mperf[mormance debugging.
    
    The traditional output is:
    
    If --log-short-format is not used:
      # Time: <ISO8601_timestamp(current_utime)>
      # User@Host: <user@host>  Id: <connection_ID>
    
    Always (1):
      # Query_time: <duration>  Lock_time: <duration> \
        Rows_sent: <number>  Rows_examined: <number>
    
    If the database has changed:
      use <newDB>;
    
    Always (2):
      SET [last_insert_id=1,][insert_id=1,]timestamp=<current_utime>
    
      [# administrator command: ] <sql_text> ";\n"
    
    If --log-slow-extra is used, the section "Always (1)" is now modified to
    provide additional information as per below (all in the same line), while
    keeping all other sections as before:
    
      # Query_time: 0  Lock_time: 0  Rows_sent: 0  Rows_examined: 0
            Thread_id: 3 Errno: 0 Killed: 0 Bytes_received: 110
            Bytes_sent: 134 Read_first: 0 Read_last: 0 Read_key: 2
            Read_next: 0 Read_prev: 0 Read_rnd: 0 Read_rnd_next: 0
            Sort_merge_passes: 0 Sort_range_count: 0 Sort_rows: 0
            Sort_scan_count: 0 Created_tmp_disk_tables: 0
            Created_tmp_tables: 0 Start:  9:22:58 End:  9:22:58
    
    A warning is thrown when a client changes log_slow_extra, but
    log_output does not include FILE.

[33mcommit 65ba353425852e89290e5ada9dc212dcb0257591[m
Author: Arnab Ray <arnab.r.ray@oracle.com>
Date:   Thu Sep 27 13:57:10 2018 +0530

    WL#12333 Schema distribution of tablespaces and logfile groups
    
    - Schema operations on tablespaces and logfile groups were previously
      not distributed across all MySQL servers connected to the cluster
      since they were handled only by the SE earlier and retrieved on the
      fly when DDL was [1;31mperf[mormed on them. After the new DD was implemented,
      they need to be installed in each MySQL Server. This worklog
      implements support for distributing tablespaces and logfile groups
      to all connected MySQL servers.
    - The current implementation of the schema distribution protocol is
      extended to handle different DDL operations on tablespaces and logfile
      groups. Changes are made to the client and participant sections of the
      protocol with the coordinator remaining unchanged
    - New MTR test ndb_dd_schema_distribution has been added to test the new
      functionality. The ndb_single_user test has been enabled again
    
    Change-Id: I6748d0a1ff294bf94073bbd3be7c516ffb07e5c6

[33mcommit b0955c74d4d027f2838ae6c48cd3dfbed639cbaf[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Wed Sep 26 15:45:27 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    Eventum 61197 revealed [1;31mperf[mormance regression introduced for DDL intensive
    workloads when innodb_flush_log_at_trx_commit = 2.
    
    This is because we over-simplified in the previous fix for BUG#28616442.
    
    This patch reverts related part of the previous fix:
    - don't sleep for more than 1ms in log threads (trx=0 case could suffer),
    - when trx=2 and user thread is waiting for flushed redo (ddl), we need
      to first wait for redo written and wake up log_flusher only afterwards
      (otherwise we could wake up log_flusher too early - that was problem).
    
    We also skip disabling spin-delays for two cases:
    - waiting for written redo when trx=1,
    - waiting for flushed redo when trx=2.
    
    We also checked if we could use two-steps waiting always - that is always
    first wait for written redo and only afterwards fallback to wait on
    flushed redo. We can't afford that.
    
    This would result in more CS when busy-waiting is disabled and trx=1
    (it's disabled when total cpu usage is more than the hwm (default 50%)).
    
    However we observed that two-steps waiting increased TPS for workloads
    with small number of connections. We confirmed that it was because of
    increased total time for busy-waiting (two times more spin rounds).
    On the other hand, we can't afford making more spin rounds for higher
    number of connections, because we would risk wasting too much cpu,
    before we reach the hwm which disables the busy waiting.
    
    Because of that we introduced new mechanism:
    1. Use increased spin rounds (compute in range 250k..25k) when
       cpu usage is below 50% of the hwm.
    2. Use default spin rounds (25k) when cpu usage is in 50%..100% of hwm.
    3. Do not use busy waiting when cpu usage is higher than hwm.
    
    Points 2,3 - we behave as it was in the past.
    
    Point 1 - we may use increased number of spin rounds when cpu usage
    is lower than 50% of the hwm.

[33mcommit 95fae9a2ce66d6f06a4ef39778d490308bdd950b[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Sep 19 17:41:53 2018 +0200

    Bug#28677954 CLEANUP VALGRIND.SUPP
    
    valgrind.supp contains lots of obsolete suppressions for [1;31mperf[mormance schema
    (missing shutdown)
    Also: add suppressions for signal handling (used by some gr tests).
    
    Change-Id: Id4c32f4b59ae263a61c6ccb8a092833b083bf3b1

[33mcommit 38a6d1d082fe09d94e45540a56cbecbd115ae7df[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Sep 24 20:16:47 2018 +0530

    Bug#28511326: DEADLOCK DURING PURGE_LOGS_BEFORE_DATE
    
    Problem:
    =======
    Access to the following variables is protected by LOCK_log.
    
    binlog-transaction-dependency-tracking
    binlog-transaction-dependency-history-size
    
    'LOCK_log' is held when these variables are being set or read.
    Holding 'LOCK_log' results in following Deadlock scenario.
    
    Analysis:
    =========
    1) SELECT * FROM [1;31mperf[mormance_schema.session_variables WHERE
    VARIABLE_NAME LIKE 'binlog_transaction_dependency_tracking';
    
    The above query acquires a lock on thread data and tries to
    read the values of 'binlog_transaction_dependency_tracking'.
    Inorder to read this variable 'LOCK_log' is required.
    
    Owns: THD::LOCK_thd_data (acquired in
          PFS_system_variable_cache::do_materialize_all
          ->PFS_variable_cache<Var_type>::get_THD
          -> Find_THD_variable::operator())
    Waits for: MYSQL_BIN_LOG::LOCK_log
    
    2) SHOW BINARY LOGS
    
    Above command acquires 'LOCK_log' to read current active
    binary log specific information and then goes on to acquire
    LOCK_index, so that it can list the rest of binary logs.
    
    Owns: MYSQL_BIN_LOG::LOCK_log (acquired in show_binlogs())
    Waits for: MYSQL_BIN_LOG::LOCK_index
    
    3) PURGE LOGS BEFORE date
    
    Above command acquires 'LOCK_index' and reads one log at a
    time from index. For each log it tries to identify how many
    threads are accessing this log. Inorder to do this it
    acquires a lock on global thread list and iterates through
    the entire thread list. For each thread it tries to acquire
    LOCK_thd_data and verify if the log is being used by the
    tread or not. Hence it waits for the lock.
    
    Owns: MYSQL_BIN_LOG::LOCK_index (acquired in
          MYSQL_BIN_LOG::purge_logs_before_date)
    Waits for: THD::LOCK_thd_data
    
    Fix:
    ===
    Transaction dependency tracking information is updated
    based on 'max_committed_transaction' object contents.
    'max_committed_transaction' holds the transaction
    sequence_number. This transaction_sequence number is updated
    during the flush stage of the commit and it is used to
    update the transaction dependency tracking through
    'update_max_committed' function call.
    
    'LOCK_log' needs to be held when the active binary log is
    being modified. Where as to protect concurrent access to set
    or read dependency tracking information a less granular lock
    should be sufficient. Hence a new lock named
    'LOCK_slave_trans_dep_tracker' has been introduced to
    protect concurrent access to transaction dependency tracking
    information.

[33mcommit 1d5f028dc06e01d32f7697664c5a05991eba16d4[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Sep 14 10:44:07 2018 -0700

    wl#10665 post-push fix: Improve test coverage in MTR
    
    Expand the ndb_large_metadata test to [1;31mperf[morm some DDL and DML
    statements against a table with large metadata, including both
    "copying" and "inplace" ALTER TABLE statements.

[33mcommit de4ba4d1f5c2e6ad1eb3e8b655744664778f772a[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Aug 22 15:01:34 2018 +0200

    BUG#28372628: Fix LCP_SKIP issue
    
    When [1;31mperf[morming a node restart we [1;31mperf[morm a copy from live node
    to starting node of changed rows. This copy process also involves
    sending deleted rowids to the starting node.
    
    In a node an update, delete or insert of a row sets the GCI of the
    row to the GCI it was written in. There is one exception to this
    rule, this is during node restart and copy from live node to
    starting nodes for deleted rows. These rows can at times have
    GCI equal to 0. This GCI is then copied over to the starting node.
    
    If a row existed in this position AND a full local LCP was started
    due to UNDO log getting full, we could set the LCP_SKIP bit on a
    row with GCI = 0.

[33mcommit d487ab73cf8f9f639a3731d3d9f2203f914f46f0[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 14 11:55:53 2018 +0100

    WL#11722: Step 25
    -----------------
    Increase the number of rows executed per real-time break by two.
    This is based on that we execute rows now twice as fast when
    the first value was set. Setting it even higher causes worse
    [1;31mperf[mormance. This setting improved [1;31mperf[mormance by 4% in a scan
    filtering benchmark.

[33mcommit cca78162778c8913934cc7c80ded2446fdfeb27c[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 14 11:52:09 2018 +0100

    WL#11722: Step 24
    -----------------
    Based upon data from [1;31mperf[m it was seen that the CPU stalled
    extensively around calls to scanCheck. To handle this the
    scanCheck method was inlined and this showed a 3% [1;31mperf[mormance
    improvement in the scan filtering benchmark.

[33mcommit edbae5ae22eb826733ef5d814afe0bb7e3f61e00[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Tue Mar 13 18:37:28 2018 +0100

    WL#11722: Step 23
    -----------------
    Rewrote the scanCheck function. This function [1;31mperf[morms the end of
    range check. It has the end boundary that is static for the
    scan operation and compares this to each key that is to be reported
    back. The compare operation previously used functions in NdbPack.cpp
    that built up a lot of context again and again.
    
    The new compare function is very simple. It builds up an array of
    objects that have two parts:
    1) m_data_ptr: Pointing to the column value
    2) m_data_len: The length of the column value
    NULL is represented by length equal to 0.
    For the end of range boundary this is calculated by first unacpking the
    boundary and then using the NdbPack::Desc and NdbPack::BoundC objects
    to create a new NdbPack::BoundArray. The above array of representations
    of the bound object is placed in a new NdbPack::DataArray object.
    
    The key is read into an array with Attrinfo setup. The Attrinfo is
    kept and a NdbPack::DataArray is created that points to data in
    the Attrinfo object. This method is very quick and only consumes
    20-30 instructions for a simple boundary with 1 column in the
    bound condition. This mapping is done in the new function
    NdbPack::DataArray::init_poai (poai stands for plain old attribut
    info).
    
    In addition the readKeyAttrsCurr function was dropped and scanCheck
    calls tuxReadAttrsCurr directly from scanCheck.
    
    This change improved [1;31mperf[mormance by 25% in a scan filtering
    benchmark.

[33mcommit 4e1f0997d4f876aeadb571dd4f3afc26b7149f91[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Mar 2 18:24:00 2018 +0100

    WL#11722: Step 9
    ----------------
    It turns out that the NDB API alwayse use the initial read region for
    reads [1;31mperf[mormed in scan operations. This is correct, but very inefficient
    when the interpreter region [1;31mperf[morms substantial filtering. The reason
    is that we're reading the data before we know if we are interested in the
    data.
    
    As it turns out we can safely delay the reading until after the
    interpreted region has been executed. This is no change to the
    NDB protocol and is thus a safe change. It is a significant
    [1;31mperf[mormance enhancements of scans with high degree of filtering.

[33mcommit 6799e5442990b21b59120992179f76fa08676a45[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Feb 28 01:13:49 2018 +0100

    WL#11722 Step 8
    ---------------
    Various optimisations found by analysing [1;31mperf[m reports on
    L1-icache-load-misses.

[33mcommit a2afa9a97454903ca93ac109b65e8d2f0dcbfa5a[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Mon Feb 26 15:18:29 2018 +0100

    WL#11722 Step 4
    ---------------
    Move prepare of read of tuple earlier in process in TUX scans.
    The tuple will be read already in scanCheck, so it is important
    to set the prefetch values before reaching TUP for reading and
    thus [1;31mperf[morm the prefetch operation as early as possible.

[33mcommit 3f2778640cc51136e178437bc606bc5bed7a3b0c[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The [1;31mperf[mormance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7ac28149f725a63d1b1042ee9d9a1d26f566f1b6[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The [1;31mperf[mormance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 572a178e973c5ba2c336a94cbfecf8d883060857[m
Author: Anushree Prakash B <anushree.prakash.b@oracle.com>
Date:   Wed Aug 29 20:05:51 2018 +0530

    Bug#27443252 - THE SELECT WILL DEADLOCK IN THE STORED
                   PROCEDURE, IF THE RESULT SET IS EMPTY.
    
    Description:
    ============
    When executing a prepared statement with a procedure call
    with CURSOR_TYPE_READ_ONLY flag set, libmysqlclient hangs
    when the procedure [1;31mperf[morms a SELECT query that returns
    an empty result set.
    
    Analysis:
    =========
    Generally, metadata result sets are no longer terminated
    with EOF(now OK) packet. However, we have an exception when
    a cursor is opened by the server in which case we can
    have a metadata packet terminated by an EOF(now OK) packet.
    
    For a simple SELECT query prepared to be executed, when a
    client requests a cursor to be opened, the server can decide
    to either open a cursor or send the result set rows.
    1. If no cursor is opened by the server,
    the packet sequence is -> <metadata><0 or more rows><OK>
    2. If server opens a cursor, the packet sequence is
    <metadata><OK><0 or more rows><OK>
    
    The hang in the bug report happens for cases when
    the server doesn't open a cursor and we have an empty
    result set. In this case, the packet sequence sent by the
    server is <metadata><OK>. Client should be able to figure
    out that the OK packet following the metadata is the last
    OK packet of the sequence and it indicates the end of rows
    result-set. Hence, it need not [1;31mperf[morm any additional reads
    which can cause such hangs.
    
    Fix:
    ====
    The solution is to check whether the server responded with
    cursor or not and [1;31mperf[morm packet reads accordingly.

[33mcommit cae634fe79e4c19902cd25248b1be24cad2c9a34[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    [1;31mperf[mormance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only [1;31mperf[mormance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - [1;31mperf[mormance_schema.data_lock_wait.requesting_engine_lock_id
    - [1;31mperf[mormance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit 1fa54adb13755947f94b86de1eb428f1bf7f4569[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    [1;31mperf[mormance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only [1;31mperf[mormance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - [1;31mperf[mormance_schema.data_lock_wait.requesting_engine_lock_id
    - [1;31mperf[mormance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit f509ae81178981b14dc677a1e35379545c1529c2[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 28 14:59:32 2018 +0200

    Bug#27808758 Where cond covered by REF-access not optimized away in ORDER/GROUP BY queries
    
      and the related bug: (see further below)
    
    Bug#27814026 MTR temptable.test crash in indexed_cells_to_string()
    
    Intended behaviour:
    ------------------
    The query optimizer identifies predicates to be pushed down to
    each table as 'table conditions'. During this process it will also check
    if some predicates in the table conditions are already known to be
    'true' by the selected access path for the table. (using part_of_refkey())
    In such cases this predicate could be ignored.
    
    One such example is the query 'select * from t1 where pk=1',
    where 'pk' is the primary key of 't1'. The REF access method
    will then be selected, and as we know that it will only return
    rows where 'pk=1', further evaluation of 'pk=1' as a where-filter
    (Using where) should be eliminated.
    
    Problem:
    --------
    When optimizing a query where GROUP / ORDER BY is specified we
    do a late optimizer step where we check if sorting could be skiped
    by using a sorted index instead. This is done after we already may
    have selected another index for accessing the table to be sorted.
    Thus we may already have removed some predicates as they were
    belived to be redundant due to the previous access path selected.
    To compensate for that, prior to the optimizer calling
    test_skip_sort() we:
    
    1) Reconstructed a table condition (add_ref_to_table_cond())
      containing predicates we had eliminated due to the
      ref-access already selected.
    
    2) test_skip_sort() checked if any sorted index existed such that
      sorting could be avoided, possibly modifying the access plan.
    
    3) Whether the access plan was modified or not, the extra predicates
       added back in 1) became a permanent part of the table condition.
    
    4) If test_skip_sort() changed the access plan to use another
      (sorted) index, no part_of_refkey()-analysis was [1;31mperf[mormed
      for the new index in order to remove predicates now being
      obsolete by the new index.
    
    Both 3) and 4) above broke the intended behavior.
    
    A further problem existed for storage engines implementing
    condition pushdown(ndbcluster): The pushed down conditions were
    generated from the table condition before test_skip_sort() analysis.
    If test_skip_sort() later decided to change the access path, the
    pushed condition did not contain the predicates already removed
    due to 'part_of_refkey()'.
    Resulting in a less efficient condition pushdown.
    
    Root cause:
    -----------
    The root cause is that part_of_refkey()-analysis is [1;31mperf[mormed
    on table predicates before the access method for this
    table has been finaly decided.
    
    Fix:
    ----
    This patch:
    - Removes the (too) early elimination of part_of_refkey's when
      building table conditions. (See changes in make_cond_for_table_from_pred())
    - This made the usage of the compensating add_ref_to_table_cond()
      before test_skip_sort() obsolete - removed.
    - Introduced reduce_cond_for_table() to identify predicates being
      part_of_refkey's. As trig_cond has been added as the point where
      this function is called, these has also to be included in the analysis.
    - reduce_cond_for_table() is called from the new method
      JOIN::finalize_table_conditions() which do the final step of
      removing redundant predicates from the table conditions.
      To avoid an extra iteration over the JOIN_TABs, the cache_const_expr()
      code was also moved into this new method
    - JOIN::cache_const_exprs() now obsolete and removed.
    - Replaced usage of the too simplistic JOIN::remove_subq_pushed_predicates()
      with finalize_table_conditions() which does the same ... and more...
    - make_join_readinfo() does the SE-condition pushdown
      of the now 'finalized' table conditions.
    
    Patch result in a rather large *.result diff in the MTR tests.
    It should be rather regular and demonstrate how 'Using where'
    and 'Using index condition' has been eliminated or reduced from
    lots of group by and order by queries.
    
    A rather large part of the diff is also due to JOIN::remove_subq_pushed_predicates()
    missed a lots of oportunites to reduce the conditions in subqueries.
    Now using reduce_cond_for_tables() here instead result in a more
    'reduced' condition having to be evaluated.
    
    *opt_trace* result files has changes due to:
    
    - 'attaching_conditions' will now also contain boolean terms previously
      eliminated too early.
    
    - Removed the 'added_back_ref_condition' as there is no such
     'add back' any more.
    
    - Added the new trace section 'finalizing_table_conditions' which now
      report the table conditions before and after redundant boolean terms
      has been eliminated.
    
    =========================================================
    
    Bug#27814026 MTR temptable.test crash in indexed_cells_to_string()
    
    The debug utility function indexed_cells_to_string() retrieve the
    values of the ref-key used to lookup row(s) to be fetched.
    These values are fetched by using the virtual Field::val_str() method.
    These methods typically contains an ASSERT_COLUMN_MARKED_FOR_READ,
    which check that we do not attempt to read any Field values not being
    part of the tables 'read_set'
    
    The 'read_set' contains the columns which has to be read from the
    storage engine due to being requested by the client, or somehow
    is required internally by the execution engine.
    
    In the stack trace being part of this bug report, we find that
    a temporary table/handle has been created as part of preparing
    for a sort. The ../sql/filesort.cc function register_used_fields()
    has been called to fill in the read_set for the temporary table.
    (Note; not using the read_set for the source table being filled
    in during resolve phase.)
    
    register_used_fields() will only add the fields needed in the
    sorted result set to the read_set: That is the fields being sorted on,
    other select'ed (non-sorted) fields and any fields being used in conditions
    later being evaluated on the sorted result.
    
    Predicates being covered by the REF access on the source tables
    has been eliminated at this stage. So any fields referred by these are
    not added by register_used_fields().
    
    Note that prior to patch for bug#27808758, the REF'ed fields were added
    back as conditions as part of test_skip_sort() handling, and later never
    eliminated. As a side effect of this all fields in the REF key would be
    added to the read_set by register_used_fields(). Thus, this bug was
    effectively hidden by the defects fixed for bug#27808758.
    
    This patch introduce a check of whether a field is part of the
    read_set before its value is being used by indexed_cells_to_string().

[33mcommit ddf2ce815c5ac0fcc20d28d0b0a62144bcb94163[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Thu Aug 23 16:04:24 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    [1;31mperf[mormance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only [1;31mperf[mormance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - [1;31mperf[mormance_schema.data_lock_wait.requesting_engine_lock_id
    - [1;31mperf[mormance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.innodb_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit 666925fca2377d5c5ac4d2b5b888c145e6ee67f2[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Aug 14 16:23:14 2018 +0200

    Bug#28501563 CMAKE -DWITHOUT_SERVER=1 IS BROKEN
    
    A few recent build errors when building client-only
     - [1;31mperf[mormance schema mismatch
     - router plugin should be excluded
    
    Change-Id: I74b2c8c825c1b0b847b1ea07cba225287e0f5685

[33mcommit c3f19761bd38ca02b23c1ea75acea164f9948136[m
Author: Sanjana DS <sanjana.ds@oracle.com>
Date:   Fri Aug 10 12:05:57 2018 +0530

    Bug #27584165 TUPLE CORRUPTION DETECTED AND DATA NODE CRASH IN DBTUPEXECQUERY.CPP
    
    Cause:
    -----
    1) A COMMIT of a delete arrives.
    2) The disk page must be paged in.
    3) While the page in is happening the scan comes around.
       The scan sees an ok state to scan since the COMMIT delete didn't
       [1;31mperf[morm the actual delete before the disk page had been paged in.
    4) The page in completes.
    5) The actual delete happens through a callback
    6) The scan happens through a callback and we crash due to tuple already
       deleted.
    
    Fix:
    ---
    1) Introduce DELETE_WAIT flag to mark the tuple header.
    2) Set this flag when we can get a real-time break after we decide to delete but
       before the actual delete.
    3) ACC scans will never see any rows with this flag since the row is deleted from ACC index
       before the real-time break is invoked in TUP.
    4) TUX scans similarly since TUX indexes are updated before the real-time break in TUP
    5) LCP scans need not consider this flag either as the COMMIT point for LCP scans is after the
       real-time break reading in the disk page.
    
    So only TUP scans ordered from NDB API and TUP disk scans and Node Restart scans (NR scans)
    are made to consider DELETE_WAIT and don't attempt to read the tuple when this flag is set.

[33mcommit 040e6d88d52a97edcb1fd8593700718179c9ddbf[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Aug 3 12:17:38 2018 +0200

    Bug #28387760 BUILD FAILURE WITH -WERROR ENABLED USING GCC-8
    
    Follow up patch eliminating the need for setting the 'Wno-ignored-qualifiers'
    and '-Wno-cast-function-type' comiler flags.
    
    Root cause for requiring the '-Wno-cast-function-type' option was the implicit_cast between
    incompatible function pointers [1;31mperf[mormed by  CachedArrayPool c'tor:
    
    -  explicit CachedArrayPool(CallBack* seizeErrorHandler=NULL)
    -     : ArrayPool<T>(reinterpret_cast<typename ArrayPool<T>::CallBack*>(seizeErrorHandler)) { }
    
    Here we casted a pointer to a function taking a 'CachedArrayPoll<T>&' as its argument,
    to a function taking an 'ArrayPoll<T>&' argument. Even if class CachedArrayPoll
    is a subclass of ArrayPoll, and casting between these classes is allowed, the same is
    NOT true for pointer to functions taking these classes as arguments. Thus
    Gcc will throw a warning for the above cast.
    
    The fix is to instantiate an abstract class ErrorHandler with different
    implementation for the different 'seizeErrorHandler' functions required.
    A virtual 'failure()' function is defined in this class which call the
    provided  'seizeErrorHandler' functions upon a seize-failure.

[33mcommit 70f052dcf97069990dae14d8dac4efae7fa932fc[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Thu Aug 2 01:29:14 2018 +0530

    Bug#28199372 : NPE FROM CLUSTERJ WHEN "GET ALL" ON A TABLE CONTAINING TEXT OR BLOB FIELD
    
    ClusterJ throws NullPointerException when a full table scan is
    [1;31mperf[mormed on a table with Blob or Text fields.
    
    Any transaction involving the blob or text fields should initialise the
    blob fields using activateBlobs function before actually executing the
    transaction. The activateBlobs function internally calls the
    getBlobHandle function on the respective ndbOperation and stores the
    blob handle for reading later after transaction execution. In this
    particular scenario involving full table scan, the activateBlobs or any
    similar function is never called on the ndbOperation and the blob handles
    never gets stored. Due to this when ClusterJ attempts to read the value
    of the blob later, after the transaction execution, it throws a
    NullPointerException. This patch fixes that by updating the
    NdbRecordTableScanOperationImpl class to call activateBlobs when the
    operation is being defined.
    
    This patch also updates the QueryBlobIndexScanTest. The test previously
    only tests index scans to retrieve blobs. Now it is updated to test also
    table scans. For this purpose, a new column without any index is added
    to the `blobtypes` table and also the related model is updated. The
    test name is also updated to QueryBlobTypesTest reflecting these
    changes.

[33mcommit 03fd3c9e73f4263a4b8c506a19ca8b19532b5bb9[m
Author: Dyre Tjeldvoll <Dyre.Tjeldvoll@oracle.com>
Date:   Tue Jul 24 13:40:07 2018 +0200

    Bug#28393385: MAIN.BASEDIR FAILS FREQUENTLY
    
    Problem: The test main.basedir used to fail sporadically. Lately the
    failures have become more frequent.
    
    Root cause appears to be that running mysqld -D --init-file=x where x issues
    a shutdown can make the daemon go away before it has signaled a successful
    start to the launching grand-parent process.
    
    Solution: Re-write test case to launch the daemon against the MTR
    datadir, connect and then [1;31mperf[morm a normal shutdown using the
    --shutdown-server command.
    
    Change-Id: I64d1f3e4cd846094433837eab72a1220e58fe10c

[33mcommit 96d0b6ce4a5522b351c9875c65bf54b66efde2fb[m
Author: Tiago Alves <tiago.alves@oracle.com>
Date:   Tue Jul 17 12:15:20 2018 +0100

    WL#12279 SUPPORTS RUNNING TESTSUITE SUBSET
    
    Supports running a testsuite subset via testsuite_suffix parameter that
    selects a specific file from a testsuite instead of reading all files
    belonging to the same testsuite.
    
    This change synchronizes changes already done for [1;31mperf[mormance testsuite
    supporting testsuite subsets.

[33mcommit 0b7705cc7070152e2136a678fb0fed4153f72af2[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Thu Jul 5 16:53:51 2018 +0200

    BUG#28297462 REQUESTING CHECKPOINT AT AVAILABLE LSN SHOULD NEVER FLUSH PAGES
    
    BUG#28220222 LOG_CHECKPOINTER IS NOT WILLING TO WRITE CHECKPOINTS ON-TIME
                 WHEN REDO IS SMALL
    
    1. We should not force preflush of dirty pages in log_wait_for_checkpoint().
    The sharp checkpoint mechanism should only force the preflush before it calls
    to log_wait_for_checkpoint().
    
    2. Log checkpointer thread should respect concurrency_margin when deciding
    if next checkpoint write is required.
    
    3. Log checkpointer thread should respect concurrency_margin when deciding
    if it should force sync-flush of dirty pages (wake up page cleaners).
    
    4. Page cleaner threads should respect concurrency margin when deciding
    if they should [1;31mperf[morm sync/async flushing of dirty pages and include
    the concurrency margin in their calculations of how much to flush.

[33mcommit 09ebb77506247692ca63447b07abcea95c799fc2[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Wed May 30 14:54:46 2018 +0530

    BUG#26848813: INDEXED COLUMN CAN'T BE CHANGED FROM VARCHAR(15)
                  TO VARCHAR(40) INSTANTANEOUSLY
    
    Analysis
    ========
    
    Indexed VARCHAR columns are not expanded instantaneously (without index rebuild)for
    InnoDB tables using INPLACE algorithm. The other problems uncovered as part of this
    bug are:
    a) Indexed VARCHAR columns when converted from unpacked
       keys to packed keys(key size > 8 bytes) by expanding
       the VARCHAR column was not instantaneous for InnoDB
       tables using INPLACE algorithm even though pack keys
       is a no-op for InnoDB tables.
    b) CREATE/ALTER of InnoDB tables where the index size
       exceeds the SE limit of 767 bytes for COMPACT or
       REDUNDANT row_format did not report error in STRICT
       mode and warning in non-strict mode.
    
    SQL layer determines if there has been a change in index definition and sets the
    appropriate handler flags which helps the SE to determine whether a index rebuild
    needs to be [1;31mperf[mormed. The 'has_index_def_changed()' did not check if the change
    in length had a compatible packed data representation and marked it as a change in
    index by setting the handler flags DROP_INDEX and ADD_INDEX triggering a recreation
    of indexes in InnoDB.
    
    When 'PACK_KEYS' option is not specified, indexes on columns with string types are
    marked for packing through HA_PACK_KEY. Currently only MyISAM engine supports it.
    Indexes on columns with length greater than 8 bytes were marked for packing
    irrespective of the storage engine. Converting the indexes from non-packed to packed
    i.e expanding the varchar column from lesser than 8 chars to more than 8 chars using
    ALTER was marked as a change in index definition during the check in
    'has_index_def_changed()' even though InnoDB does not support packed keys.
    
    The handler API ha_innobase::max_supported_key_part_length() returned an incorrect
    maximum support key part length since the row format of the table was not taken into
    account. Hence creation of tables using REDUNDANT/COMPACT row format with index size
    exceeding the SE limit were allowed.
    
    Fix:
    ===
    a) A new handler flag 'Alter_inplace_info::ALTER_COLUMN_INDEX_LENGTH"
       is introduced. This is set in 'has_index_def_changed()' when the
       index length differs due to column modification and has compatible
       packed data representation while determining if the index definition
       has changed.
    
    b) Introduced a handlerton flag 'HTON_SUPPORTS_PACKED_KEYS' to check for
       'PACK_KEYS' support which is currently set for MyISAM engine since it
       is the only engine which supports packed keys. The 'HTON_SUPPORTS_PACKED_KEYS'
       is checked prior to setting the index flag 'HA_BINARY_PACK_KEY' for packed keys.
       Hence for InnoDB the packed key flag for indexes is not set.
    
    c) The handler API max_supported_key_part_length() has been modified to take 'create_info'
       as a parameter from which the row_format can be fetched and appropriate index limit can
       be computed and returned by the InnoDB implementation of the handler API. More details
       on the behavior is mentioned in the table below.
    
       - Since the index limit is computed correctly and checked at the SQL layer,
         the error message 'ER_TOO_LONG_KEY' is reported at the SQL layer instead of
         'ER_INDEX_COLUMN_TOO_LONG' (which was reported by the handler interface
         after mapping the error returned by SE).
       - For COMPACT and REDUNDANT row format, when the index size exceeds 767 bytes,
         a warning is reported in non-STRICT mode and the index is truncated to fit
         767 bytes. In a STRICT mode, an error is reported.
    
    This table describes the behavior of INDEX limit, Type of INDEX and
    behavior under STRICT and NON_STRICT mode.
    IL===> Index Limit.
    -------------------------------------------------------------------------------|
      Row Format          |INDEX LIMIT | STRICT MODE(>IL) | NON-STRICT MODE(>IL)   |
    ----------------------|--------------------------------------------------------|
    Compact/Redundant     |  767 bytes |    Error         | Index truncation(767)  |
    (Non Unique Index)    |            |                  | and warning.           |
    --------------------------------------------------------------------------------
    Compact/Redundant     |  767 bytes |    Error         | Error                  |
    (Unique/Primary Index)|            |                  |                        |
    --------------------------------------------------------------------------------
    Dynamic/Compressed    |  3072 byes |    Error         | Index truncation(3072) |
    (Non Unique Index)    |            |                  | and warning            |
    --------------------------------------------------------------------------------
    Dynamic/Compressed    |  3072 bytes|    Error         | Error                  |
    (Unique/Primary Index)|            |                  |                        |
    --------------------------------------------------------------------------------
    
    (cherry picked from commit bdc97b75674ade0251bdbc3ea2dc7d36871f73cd)
    
    "Resolved the copyright year fix"

[33mcommit 5cc80f19f6920f2c2701d611c154b8bb3bbb1f4e[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Wed May 30 14:54:46 2018 +0530

    BUG#26848813: INDEXED COLUMN CAN'T BE CHANGED FROM VARCHAR(15)
                  TO VARCHAR(40) INSTANTANEOUSLY
    
    Analysis
    ========
    
    Indexed VARCHAR columns are not expanded instantaneously (without index rebuild)for
    InnoDB tables using INPLACE algorithm. The other problems uncovered as part of this
    bug are:
    a) Indexed VARCHAR columns when converted from unpacked
       keys to packed keys(key size > 8 bytes) by expanding
       the VARCHAR column was not instantaneous for InnoDB
       tables using INPLACE algorithm even though pack keys
       is a no-op for InnoDB tables.
    b) CREATE/ALTER of InnoDB tables where the index size
       exceeds the SE limit of 767 bytes for COMPACT or
       REDUNDANT row_format did not report error in STRICT
       mode and warning in non-strict mode.
    
    SQL layer determines if there has been a change in index definition and sets the
    appropriate handler flags which helps the SE to determine whether a index rebuild
    needs to be [1;31mperf[mormed. The 'has_index_def_changed()' did not check if the change
    in length had a compatible packed data representation and marked it as a change in
    index by setting the handler flags DROP_INDEX and ADD_INDEX triggering a recreation
    of indexes in InnoDB.
    
    When 'PACK_KEYS' option is not specified, indexes on columns with string types are
    marked for packing through HA_PACK_KEY. Currently only MyISAM engine supports it.
    Indexes on columns with length greater than 8 bytes were marked for packing
    irrespective of the storage engine. Converting the indexes from non-packed to packed
    i.e expanding the varchar column from lesser than 8 chars to more than 8 chars using
    ALTER was marked as a change in index definition during the check in
    'has_index_def_changed()' even though InnoDB does not support packed keys.
    
    The handler API ha_innobase::max_supported_key_part_length() returned an incorrect
    maximum support key part length since the row format of the table was not taken into
    account. Hence creation of tables using REDUNDANT/COMPACT row format with index size
    exceeding the SE limit were allowed.
    
    Fix:
    ===
    a) A new handler flag 'Alter_inplace_info::ALTER_COLUMN_INDEX_LENGTH"
       is introduced. This is set in 'has_index_def_changed()' when the
       index length differs due to column modification and has compatible
       packed data representation while determining if the index definition
       has changed.
    
    b) Introduced a handlerton flag 'HTON_SUPPORTS_PACKED_KEYS' to check for
       'PACK_KEYS' support which is currently set for MyISAM engine since it
       is the only engine which supports packed keys. The 'HTON_SUPPORTS_PACKED_KEYS'
       is checked prior to setting the index flag 'HA_BINARY_PACK_KEY' for packed keys.
       Hence for InnoDB the packed key flag for indexes is not set.
    
    c) The handler API max_supported_key_part_length() has been modified to take 'create_info'
       as a parameter from which the row_format can be fetched and appropriate index limit can
       be computed and returned by the InnoDB implementation of the handler API. More details
       on the behavior is mentioned in the table below.
    
       - Since the index limit is computed correctly and checked at the SQL layer,
         the error message 'ER_TOO_LONG_KEY' is reported at the SQL layer instead of
         'ER_INDEX_COLUMN_TOO_LONG' (which was reported by the handler interface
         after mapping the error returned by SE).
       - For COMPACT and REDUNDANT row format, when the index size exceeds 767 bytes,
         a warning is reported in non-STRICT mode and the index is truncated to fit
         767 bytes. In a STRICT mode, an error is reported.
    
    This table describes the behavior of INDEX limit, Type of INDEX and
    behavior under STRICT and NON_STRICT mode.
    IL===> Index Limit.
    -------------------------------------------------------------------------------|
      Row Format          |INDEX LIMIT | STRICT MODE(>IL) | NON-STRICT MODE(>IL)   |
    ----------------------|--------------------------------------------------------|
    Compact/Redundant     |  767 bytes |    Error         | Index truncation(767)  |
    (Non Unique Index)    |            |                  | and warning.           |
    --------------------------------------------------------------------------------
    Compact/Redundant     |  767 bytes |    Error         | Error                  |
    (Unique/Primary Index)|            |                  |                        |
    --------------------------------------------------------------------------------
    Dynamic/Compressed    |  3072 byes |    Error         | Index truncation(3072) |
    (Non Unique Index)    |            |                  | and warning            |
    --------------------------------------------------------------------------------
    Dynamic/Compressed    |  3072 bytes|    Error         | Error                  |
    (Unique/Primary Index)|            |                  |                        |
    --------------------------------------------------------------------------------
    
    (cherry picked from commit bdc97b75674ade0251bdbc3ea2dc7d36871f73cd)
For keyword optim:
[33mcommit facb23e97b4f87887c0a7b220af56551a2a710ae[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Tue Nov 27 13:28:04 2018 +0100

    Bug#28975640: REDUCE STRING ALLOCATIONS IN JSON_WRAPPER_OBJECT_ITERATOR
    
    Json_wrapper_object_iterator creates a std::string object to hold the
    key for every key/value pair it iterates over. If the key names are
    long, or when running on a platform that doesn't implement the small
    string [1;31moptim[mization, this could result in a heap allocation for every
    member in the JSON object that is being iterated over.
    
    This patch changes Json_wrapper_object_iterator so that it returns a
    pointer to the key name and the length of the key instead of a
    std::string object with the key name. The pointer points into already
    allocated memory for the key in the DOM or binary data structure, so
    that no new heap memory needs to be allocated while iterating.
    
    Microbenchmarks (64-bit, Intel Core i7-4770 3.4 GHz, GCC 8.2.0):
    
    BM_JsonWrapperObjectIteratorBinary  45733 ns/iter -> 21840 ns/iter [+109%]
    BM_JsonWrapperObjectIteratorDOM     41063 ns/iter -> 22504 ns/iter [ +82%]
    
    Change-Id: I8761e23f98f5bf04076f101a041ad2c74c76ef6e

[33mcommit 4736e3b67ff3e2915e0c84218899529f20611a4a[m
Author: Sreeharsha Ramanavarapu <sreeharsha.ramanavarapu@oracle.com>
Date:   Fri Oct 12 07:35:31 2018 +0530

    Bug #28086754: OPTIMIZER SKIP THE RANGE SCAN ON SECOND
                   COLUMN IN A COMPOSITE INDEX
    
    Issue:
    ------
    
    CREATE TABLE test_ref (
     a INT NOT NULL,
     b VARCHAR(20),
     c VARCHAR(20),
     d VARCHAR(3),
     id INT,
     PRIMARY KEY (a),
     KEY idx1 (id, c),
     KEY idx2 (id, d));
    
    SELECT *
    FROM test_ref
    WHERE id = 3 and c LIKE 'gh%'
    ORDER BY c LIMIT 1;
    
    
    On 5.7:
    -------
    idx1 with a range access is best option for this. idx1
    covers both the conditions in the WHERE clause and will
    support ordering since column "c" is part of the index.
    But the [1;31moptim[mizer choose ref access idx1.
    
    Reasons for this (reproducible only with data from user) :
    1) Table scan is calculated as cheaper than range access on
       idx1. So using range access is ruled out.
    2) The only other option is ref-access on idx1.
    3) Later stage switch back to range access on the same
       index (can_switch_from_ref_to_range) isn't possible
       because range access was rejected altogether in Step 1.
    
    The problem with ref-access is that it uses only one
    keypart of idx1 -> the column "id". This makes the query
    execution slower.
    
    On trunk:
    ---------
    1) Table scan is cheaper than range access on idx1. Range
       access on idx1 is rejected.
    2) But since range scan on idx1 uses more key parts than
       ref, the ref-access on idx1 is also rejected.
    3) Initially [1;31moptim[mizer chooses a ref-access on idx2.
    4) While evaluating options for "ORDER BY" it notices that
       idx1 provides ordering and also doesn't change the
       current access method (ref). At this stage the [1;31moptim[mizer
       is unaware of the reasons for rejecting ref-access on
       idx1 earlier (Step 2) and chooses it.
    
    Solution On both 5.7 and trunk:
    -------------------------------
    In test_quick_select() add a new parameter to ignore the
    cost of table scan. Range [1;31moptim[mizer on the relevant key
    will be re-run by ignoring the cost of table scan (since
    it was calculated to be cheaper).
    
    
    On 5.7:
    -------
    If the "dodgy_ref_cost" flag is set, that is
    ref-access might be using less key-parts than range but was
    calculated to be cheaper, then re-run the range [1;31moptim[mizer
    and check the same.
    
    On trunk:
    ---------
    When a ref-access is chosen, check if it is possible to
    switch to range-access if more keyparts will be used. To
    achieve this call can_switch_from_ref_to_range() from
    test_if_skip_sort_order() when index for ref-access has
    changed.
    
    In can_switch_from_ref_to_range(), add a new parameter to
    force the switch to range even if it was rejected
    previously because a table scan is cheaper.

[33mcommit e6b7b8920b128c0e1971048c8bd9e49244071e32[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Wed Sep 19 23:19:14 2018 +1000

    Bug#28445530 RAPID: DISABLE AGGREGATE SUM OPTIMIZATION IN OPT_SUM_QUERY
    
    Disable [1;31moptim[mization of aggregate sum when compiling for secondary engine
    
    [ revised to use LEX::m_statement_options as context for
      SELECT_LEX::make_active_options
    ]
    
    [ revised again to avoid setting base options: set active options instead
      Tests added that show diffs before/after adding the flag
    ]
    
    [ revised after my vacation in response to Knut's and Roy's reviews:
      - added setting option for subqueries as well; not inherited
    ]
    
    Simplified by removing stack logic after Roy's suggestion: no need to resurrect
    active options before we attempted use of secondary engine, since re-prepare
    with InnoDB makes a new SELECT_LEX anyway.
    
    Change-Id: I1d9b40f0aefe814da51c219503455209e11bc6a0

[33mcommit 69c031665b96a12a52c8823bb08fa8572ded4d6a[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Fri Sep 28 16:39:22 2018 +0200

    WL#11540: Support 2 active passwords per user account
    
    Post-push fix: Remove a tautological assert that caused GCC warnings
    if compiling with both [1;31moptim[mization and asserts on.
    
    Change-Id: I37f8fcfbe1f38b244551210d53cd6092ff0bffba

[33mcommit 88b391372e74616bc35edef4bcd238dabea13899[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Thu Sep 27 09:43:25 2018 +0530

    WL#9571: Remove trivial conditions before outer to inner join
             transformation
    
    Currently mysql does not remove trivial conditions (expressions over literals)
    during preparation. This results in poor plans for some queries.
    
    For ex:
    There are two queries of the form
    1.Select * from t1 left join t2 on "cond1" where cond2 or 0 = 1;
    2.Select * from t1 left join t2 on "cond1" where cond2 ;
    
    As 0 = 1 is always false, it is [1;31moptim[mized away. So both
    the queries would be identical. But yet [1;31moptim[mizer gives a better
    plan for the latter. This is because, it converts outer join to
    inner join and re-writes the above query to "Select ..... where
    cond2 and cond1" and thereby resulting in a good plan.
    
    While for the first query, we cannot convert the outer join to
    inner join as not_null_tables_cache is different than the used
    tables ( thanks to "0 = 1" condition). As a result, the join
    condition is left as is. But 0 = 1 gets [1;31moptim[mized away in removal
    of trivial conditions which happens later.
    
    So removing these trivial conditions during preparation is much more
    advantageous than in [1;31moptim[mization phase. The following logic is adapted to
    remove these conditions:
    For an OR expression, if the result of any constant condition is TRUE, then
    the entire condition is collapsed and replaced with a ALWAYS TRUE item.
    Else only the expression over literals is removed.
    Similarly for an AND condition, if the result of any constant condition is
    FALSE, then the entire condition is collapsed and replaced with ALWAYS FALSE
    item. Else only the expression over literals is removed.

[33mcommit df9e2014fc4214cf79b9be01a081ea52a4a46f34[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 26 16:04:49 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
    Addendum patch: We failed to 'update_used_tables' after ::finalize_table_conditions
    had potentialy removed some predicate terms from a table conditions.
    
    Bug affected the later cond_push'ability of (parts of) a predicate
    to be pushed to a storage engine.

[33mcommit 003c567bcbb6d9adb9aae2d2a1f310242c5f22c2[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Sep 7 10:20:08 2018 +0200

    Bug#28614646: FIX -WSHADOW-FIELD WARNINGS [noclose]
    
    Fix Clang compilation warnings about shadowing fields in [1;31moptim[mizer code.
    
    Change-Id: If7bab04055be7f8e5257830f95334378977ca9d4

[33mcommit a5d119b8c3bced28af38fecf149a133cba4ab0a6[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Jun 23 19:05:06 2018 +0200

    WL#11722: Step 40
    -----------------
    A few more [1;31moptim[misations in DbtuxScan.cpp based on variables in context already
    set but not yet used in all places.

[33mcommit d9b5d9a9d087b14cedfbbf4217abd011495d9e5f[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Jun 23 18:55:51 2018 +0200

    WL#11722: Step 39
    -----------------
    Final [1;31moptim[misations making use of context variables instead of calculating
    a variable that has already been calculated.

[33mcommit 1f35e3e78e166f3fa1df6b10080835b1b068e920[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Thu Jun 14 12:45:26 2018 +0200

    WL#11722: Step 32
    -----------------
    Prepare to [1;31moptim[mise searchToScan. This method is used to dive into the
    tree to find the start of a range scan. We [1;31moptim[mise away a few jam's
    and straighten out code in searchToScan, findPosToScan and findNodeToScan.

[33mcommit 419bba30ed216cae9ef6a74a30bebf46e7826221[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Mar 7 13:57:12 2018 +0100

    WL#11722: Step 15
    -----------------
    Fixed two bugs in Step 11.
    1) It is not ok to send SCAN_HBREP from setup after a real-time break.
       SCAN_HBREP should only be sent when there is real progress. So if
       we are locked on a locked row we will come back from real-time
       breaks, but there is no real progress. So removed this [1;31moptim[misation
       that wasn't ok.
    2) Important to also copy Attrinfo as part of Copy fragment.

[33mcommit 89b25c806be9c13b31fca465cfbfb4b7fe60f147[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Mar 3 01:15:52 2018 +0100

    WL#11722: Step 14
    -----------------
    During a scan we use two bounds, the starting boundary. This is used
    once in scanFirst to find the first row in the range. There is no
    need to precompute this boundary since it is only used once in a
    scan. Similarly we make no special [1;31moptim[misations for now for
    scans for table statistics.
    
    The boundary used to check if the next row is outside the boundary
    is however used many times, most of the time at least once, but
    often it can be used many times over and over again. So this
    boundary condition we precompute the objects for and use them
    multiple times.
    
    This [1;31moptim[mises the scanCheck method greatly.

[33mcommit 7e627ef655d285b5437db73e710de65e65f2bf14[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Mar 2 18:27:33 2018 +0100

    WL#11722: Step 12
    -----------------
    Optimise the function readFixedSizeTHOneWordNotNULL to see if
    we can gain some by [1;31moptim[mising the read functions even more.

[33mcommit 6799e5442990b21b59120992179f76fa08676a45[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Wed Feb 28 01:13:49 2018 +0100

    WL#11722 Step 8
    ---------------
    Various [1;31moptim[misations found by analysing perf reports on
    L1-icache-load-misses.

[33mcommit 5266096589af64ab0ba16527c66a4beda4497b90[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:55:10 2018 +0100

    WL#11722
    Reorganised some functions in Dblqh to make code more readable.
    
    Increase the maximum scan direct count. Counted normal rows
    reported back as 2 and those that only report that condition
    was not met will only count as 1 row. Increased max scan
    direct count to 16 and default from 6 to 16.
    
    Made the amount of rows scanned in one real-time break adaptive.
    If the load on JBB level is low we are allowed to execute a bit
    more compared to otherwise.
    
    More work on preparing code for next step of scan [1;31moptim[misations.

[33mcommit 870434552974473c6d03cc1e0f309794e270db17[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:51:19 2018 +0100

    WL#11722
    First preparatory step to [1;31moptim[mise scans and particularly
    longer scans with many rows checked but not sent back to
    the MySQL server.
    1) Removed a number of jam's, replaced most by jamDebug and
       added a few jamDebug's
    2) Thoroughly investigated all real-time breaks for scans and
       key operations. Added a long comment describing all of those
       real-time breaks and when they happen.
    3) Introduced a preparatory phase after returning after each
       real-time break. This preparatory phase sets up all pointers
       in TUP and LQH. Later we will add also preparatory phases for
       the scanning block (TUX, TUP and ACC).
    4) An important step is to prepare for longer scan executions.
       To handle this we must unwind the stack after executing one
       row scan. To clarify this we added a number of returns in a
       number of places to ensure we get tail-call [1;31moptim[misations as well
       as ensuring that it is clear that it is intended to return.
    5) Made the code a bit more readable by placing { on new line.
    6) Added many more likely/unlikely for [1;31moptim[misation of the code.
    7) Added a real-time break to handle restarts of scan from
       queue on fragments.
    8) Prepared code in TUPKEYREQ for [1;31moptim[mised read row handling.
    9) Optimised TUPKEYREF for scan handling, more [1;31moptim[mised handling
       of scans in TUPKEYCONF.
    10)Made sure that new (and old) block pointers are overwritten
       with each new signal executed (in debug mode) for faster
       bug finding in this area.
    11)Removed a number of parameters to call that weren't needed
    12)Split prepareTUPKEYREQ into prepare_table_pointers and
       prepare_scanTUPKEYREQ to avoid initialising fragment and
       table pointer on each new row.
    13)Introduced new block variable prim_tab_fragptr to point at
       fragment record for primary table. The scan block scans on
       the index table, but reads from the primary table, so needed
       quick access to this at times as well.
    14)Clarified the delayed signal to be used when rows are locked

[33mcommit 00d33621cf89c303282d5913d3100a717fa7cab9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Sep 4 15:05:15 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
    Addendum patch fixing Item_func_trig_cond::get_inner_tables() to also
    handle the case where the JOIN:.qep_tab[] had been allocated.
    
    Required due to new usage pattern of the Item_func_trig_cond's
    in one of the WL-branches based on mysql-trunk.

[33mcommit 644db659b4f80019c368a2a51e0076a6fb771085[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Sep 4 14:37:26 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
    Addendum patch fixing Doxygen warnings.

[33mcommit 13ae611131d279038f790c7b8e7be124836008f4[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Sun Sep 2 05:39:34 2018 -0400

    Bug#28585914 ROUTER TEST ISSUES REPORTED FOR PB2 8.0.13 RELEASE BRANCH - SOLARIS X86
    
    There seems to be a compiler issue that is making the binary SEGFAULT
    each time std::condition_variable::wait_for gets called.
    This can be observed on a simple example:
    
    std::condition_variable cv;
    std::mutex cv_m;
    int main()
    {
      std::unique_lock<std::mutex> lk(cv_m);
      cv.wait_for(lk, std::chrono::milliseconds(10), [](){return true;});
    }
    
    If you compile it with:
    $ /opt/developerstudio12.6/bin/CC -std=c++11 -m64 -xO2 test.cc -o testapp
    you'll get a crash when running the binary.
    
    -xO1 doesn't crash
    -xO2 crashes
    -xO3 doesn't crash
    -xO4 doesn't crash
    -xO5 doesn't crash
    
    This patch is really only a workaround to change the [1;31moptim[mization level to -xO1
    for the router sources to avoid this issue.
    
    Reviewed-by: Tor Didriksen <tor.didriksen@oracle.com>
    RB: #20426

[33mcommit 78e067e69ab7afa9f02b03e1c3f398e4da4d7377[m
Author: Kailasnath Nagarkar <kailasnath.nagarkar@oracle.com>
Date:   Thu Aug 30 17:19:34 2018 +0530

    Bug #27659490 : SELECT USING DYNAMIC RANGE AND INDEX
                    MERGE USE TOO MUCH MEMORY(OOM)
    
    Issue:
    While creating a handler object, index-merge access
    creates it in statement MEM_ROOT.
    However when this is used with "Dynamic range access method",
    as range [1;31moptim[mizer gets invoked multiple times, mysql ends up
    consuming a lot of memory.
    
    Solution:
    Instead of using statement MEM_ROOT to allocate the handler
    object, use the local MEM_ROOT of the range [1;31moptim[mizer which
    gets destroyed at the end of range [1;31moptim[mizer's  usage.

[33mcommit f62c704c176a9c106add091b293a097f3484dfac[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Aug 29 10:11:34 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
    Another addendum patch re-recording a changed *.result file:
    
    These diffs are a result of 'Using where' now being [1;31moptim[mized away for
    these queries. Thus, we do not evaluate a condition having a date litteral
    causing a warning to be thrown. Resulting in fewer warnings in this
    result file.

[33mcommit 1105343ebe6b8a0b2297fcf9c21d1f3e302f49bf[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Aug 29 09:01:06 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
    Addendum patch; re-recorded some more *.result files changed due to
    this patch.

[33mcommit f509ae81178981b14dc677a1e35379545c1529c2[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 28 14:59:32 2018 +0200

    Bug#27808758 Where cond covered by REF-access not [1;31moptim[mized away in ORDER/GROUP BY queries
    
      and the related bug: (see further below)
    
    Bug#27814026 MTR temptable.test crash in indexed_cells_to_string()
    
    Intended behaviour:
    ------------------
    The query [1;31moptim[mizer identifies predicates to be pushed down to
    each table as 'table conditions'. During this process it will also check
    if some predicates in the table conditions are already known to be
    'true' by the selected access path for the table. (using part_of_refkey())
    In such cases this predicate could be ignored.
    
    One such example is the query 'select * from t1 where pk=1',
    where 'pk' is the primary key of 't1'. The REF access method
    will then be selected, and as we know that it will only return
    rows where 'pk=1', further evaluation of 'pk=1' as a where-filter
    (Using where) should be eliminated.
    
    Problem:
    --------
    When [1;31moptim[mizing a query where GROUP / ORDER BY is specified we
    do a late [1;31moptim[mizer step where we check if sorting could be skiped
    by using a sorted index instead. This is done after we already may
    have selected another index for accessing the table to be sorted.
    Thus we may already have removed some predicates as they were
    belived to be redundant due to the previous access path selected.
    To compensate for that, prior to the [1;31moptim[mizer calling
    test_skip_sort() we:
    
    1) Reconstructed a table condition (add_ref_to_table_cond())
      containing predicates we had eliminated due to the
      ref-access already selected.
    
    2) test_skip_sort() checked if any sorted index existed such that
      sorting could be avoided, possibly modifying the access plan.
    
    3) Whether the access plan was modified or not, the extra predicates
       added back in 1) became a permanent part of the table condition.
    
    4) If test_skip_sort() changed the access plan to use another
      (sorted) index, no part_of_refkey()-analysis was performed
      for the new index in order to remove predicates now being
      obsolete by the new index.
    
    Both 3) and 4) above broke the intended behavior.
    
    A further problem existed for storage engines implementing
    condition pushdown(ndbcluster): The pushed down conditions were
    generated from the table condition before test_skip_sort() analysis.
    If test_skip_sort() later decided to change the access path, the
    pushed condition did not contain the predicates already removed
    due to 'part_of_refkey()'.
    Resulting in a less efficient condition pushdown.
    
    Root cause:
    -----------
    The root cause is that part_of_refkey()-analysis is performed
    on table predicates before the access method for this
    table has been finaly decided.
    
    Fix:
    ----
    This patch:
    - Removes the (too) early elimination of part_of_refkey's when
      building table conditions. (See changes in make_cond_for_table_from_pred())
    - This made the usage of the compensating add_ref_to_table_cond()
      before test_skip_sort() obsolete - removed.
    - Introduced reduce_cond_for_table() to identify predicates being
      part_of_refkey's. As trig_cond has been added as the point where
      this function is called, these has also to be included in the analysis.
    - reduce_cond_for_table() is called from the new method
      JOIN::finalize_table_conditions() which do the final step of
      removing redundant predicates from the table conditions.
      To avoid an extra iteration over the JOIN_TABs, the cache_const_expr()
      code was also moved into this new method
    - JOIN::cache_const_exprs() now obsolete and removed.
    - Replaced usage of the too simplistic JOIN::remove_subq_pushed_predicates()
      with finalize_table_conditions() which does the same ... and more...
    - make_join_readinfo() does the SE-condition pushdown
      of the now 'finalized' table conditions.
    
    Patch result in a rather large *.result diff in the MTR tests.
    It should be rather regular and demonstrate how 'Using where'
    and 'Using index condition' has been eliminated or reduced from
    lots of group by and order by queries.
    
    A rather large part of the diff is also due to JOIN::remove_subq_pushed_predicates()
    missed a lots of oportunites to reduce the conditions in subqueries.
    Now using reduce_cond_for_tables() here instead result in a more
    'reduced' condition having to be evaluated.
    
    *opt_trace* result files has changes due to:
    
    - 'attaching_conditions' will now also contain boolean terms previously
      eliminated too early.
    
    - Removed the 'added_back_ref_condition' as there is no such
     'add back' any more.
    
    - Added the new trace section 'finalizing_table_conditions' which now
      report the table conditions before and after redundant boolean terms
      has been eliminated.
    
    =========================================================
    
    Bug#27814026 MTR temptable.test crash in indexed_cells_to_string()
    
    The debug utility function indexed_cells_to_string() retrieve the
    values of the ref-key used to lookup row(s) to be fetched.
    These values are fetched by using the virtual Field::val_str() method.
    These methods typically contains an ASSERT_COLUMN_MARKED_FOR_READ,
    which check that we do not attempt to read any Field values not being
    part of the tables 'read_set'
    
    The 'read_set' contains the columns which has to be read from the
    storage engine due to being requested by the client, or somehow
    is required internally by the execution engine.
    
    In the stack trace being part of this bug report, we find that
    a temporary table/handle has been created as part of preparing
    for a sort. The ../sql/filesort.cc function register_used_fields()
    has been called to fill in the read_set for the temporary table.
    (Note; not using the read_set for the source table being filled
    in during resolve phase.)
    
    register_used_fields() will only add the fields needed in the
    sorted result set to the read_set: That is the fields being sorted on,
    other select'ed (non-sorted) fields and any fields being used in conditions
    later being evaluated on the sorted result.
    
    Predicates being covered by the REF access on the source tables
    has been eliminated at this stage. So any fields referred by these are
    not added by register_used_fields().
    
    Note that prior to patch for bug#27808758, the REF'ed fields were added
    back as conditions as part of test_skip_sort() handling, and later never
    eliminated. As a side effect of this all fields in the REF key would be
    added to the read_set by register_used_fields(). Thus, this bug was
    effectively hidden by the defects fixed for bug#27808758.
    
    This patch introduce a check of whether a field is part of the
    read_set before its value is being used by indexed_cells_to_string().

[33mcommit e78371a9f0d9b85b7b2903b52f98af80925fd35c[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Tue Aug 28 13:55:19 2018 +0200

    Bug #26927386: REDUCE COMPILATION TIME [noclose]
    
    Remove a now-obsolete micro[1;31moptim[mization for a few old Japanese character sets
    (bswap is single-cycle these days, not to mention movbe), allowing us to remove
    the link from m_ctype.h to my_byteorder.h. This is relevant because m_ctype.h
    is included from a huge amount of places, and my_byteorder.h pulls in quite a
    few lines transitively. (We remove one of the worst offenders, though, which was
    mistakenly added in an earlier IWYU run.)
    
    Change-Id: I49e2f2de371b9dd7da78a6692992dd9e9bd28e8f

[33mcommit c24b9df85c7c3f077e8337228b3adc7c974d5164[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jul 3 09:27:35 2018 +0200

    Bug#27874068 ADD SUPPORT FOR -DCMAKE_BUILD_TYPE=RELEASE
    
    Move all compiler/language features, like -std=c++11 or
    -fsanitize=address for ASAN builds,
    from CMAKE_<LANG>_FLAGS_<CONFIG> to CMAKE_<LANG>_FLAGS
    
    With this patch, CMAKE_<LANG>_FLAGS_DEBUG contains only debug related flags:
    -- CMAKE_CXX_FLAGS_DEBUG: -DSAFE_MUTEX -DENABLED_DEBUG_SYNC -g
    
    The other CMAKE_<LANG>_FLAGS_<CONFIG> flags will switch off debugging,
    and add our default [1;31moptim[mization flags:
    -- CMAKE_CXX_FLAGS_MINSIZEREL: -DDBUG_OFF -ffunction-sections -fdata-sections -Os -DNDEBUG
    
    Always PREPEND MySQL default flag values to cmake compiler flags,
    so that they can easily be overriden by command line options.
    This fixes
    Bug#28208842 COMPILER OPTIMIZATION HARDWIRED TO O2
    
    To override MySQL default flag values, for e.g. RELEASE builds, do:
    cmake . -DCMAKE_CXX_FLAGS_RELEASE="-O3 -DNDEBUG -fno-function-sections -fno-data-sections"
    
    Change-Id: Ia7786b017fb719c203f4be0e83cf5f9e18b318dc
For keyword regression:
[33mcommit 0ca968fc0d663f1740d5bba3e88f250f536f7677[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Nov 5 16:00:22 2018 +0530

    Bug #28825617 ASSERTION FAILURE: LOB0LOB.CC.*TRX == NULLPTR || TRX->IS_READ_UNCOMMITTED()
    
    Background:
    
    This is a [1;31mregression[m caused by the following commit:
    
    commit 56eb89637db93f16bcf7e7e047d6ec7907cd5bbe
    Author: Kevin Lewis <kevin.lewis@oracle.com>
    Date:   Wed May 23 15:51:08 2018 -0700
    
    Bug#25540277 - INNODB'S MVCC HAS O(N^2) BEHAVIORS
    
    Problem:
    
    The function row_sel_build_prev_vers_for_mysql() goes through the old versions
    of the clustered index record and identifies the one which can be viewed by the
    current transaction (MVCC).  The result is then cached in an object of
    Row_sel_get_clust_rec_for_mysql.
    
    The two things that are cached by the helper class
    Row_sel_get_clust_rec_for_mysql are cached_clust_rec and cached_old_vers.  If
    the latest record is cached_clust_rec, its old version for the given
    transaction that is viewable is cached_old_vers. The important point here is
    that the record offsets for these two records are not cached.
    
    The record offsets for cached_clust_rec and the cached_old_vers need not be the
    same.  This is confirmed by looking at row_sel_build_prev_vers_for_mysql().
    The 6th argument which contains the record offsets is an input/output argument.
    The input is the offsets of the latest version of clust_rec, and the output is
    the record offsets of the older version.  So the record offsets of different
    versions of clust_rec can change.
    
    Solution:
    
    Re-calculate the record offsets for the cached old_vers.
    
    rb#20927 approved by Kevin.

[33mcommit a8f832d78430f50d0c4808dc0bd0817987018ea6[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Fri Nov 23 10:49:29 2018 +0100

    Bug#24481240: FEW RPL TESTS FAIL SPORADICALLY FOR
                  MTS_DB: ERROR IN SYNC_WITH_MASTER.INC
    
    Description: This is a [1;31mregression[m introduced by fix for
                 bug#26475282. Due to the [1;31mregression[m, a race
                 is created when ACL DDL is executed in
                 parallel with another DDL that accesses ACL
                 tables. To avoid this, ACL DDLs will now:
                 1. Open ACL tables
                 2. Take ACL Cache lock

[33mcommit c2333aab9b4ea94ea5ebda0585c68dacfb5dd793[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Fri Oct 12 23:36:42 2018 +0800

    Bug#28581468 - RENAMING PARENT COLUMN IN A FOREIGN KEY FAILS WITH STRANGE ERROR.
    
    The error reported during the ALTER TABLE is because the the foriegn key
    cache is not updated or invalidated properly before opening the table,
    so opening table will find mismatch information between the metadata
    from DD and the column name in foriegn key cache.
    
    This is finally proved to be a [1;31mregression[m from new DD work, which mistakenly
    removed the foreign key cache invalidation for the rebuild ALTER TABLE.
    
    So the fix is just to invalidate the foreign key cache if the foreign key
    is affected by the column rename. And new foreign key cache would be
    re-created on next opening table.
    
    RB: 20711
    Reviewed-by: Allen Lai <zheng.lai@oracle.com>

[33mcommit 8b0ada5eb01798d20f3288b6cf07b299dd0d7545[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177 MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could sometimes fail due to missing characterset comments.
    
    The root cause was the [1;31mregression[m test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source directory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the [1;31mregression[m test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different directory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    See also duplicate
    
    Bug#19372763 SPORADIC FAILURE IN MAIN.INTERNAL_TMP_DISK_STORAGE_ENGINE
    
    Backport patch from 8.0.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit 60a6ab982ace6c1c40d91634779c38209ea89658[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177: MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could sometimes fail due to missing characterset comments.
    
    The root cause was the [1;31mregression[m test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source directory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the [1;31mregression[m test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different directory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit b0955c74d4d027f2838ae6c48cd3dfbed639cbaf[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Wed Sep 26 15:45:27 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    Eventum 61197 revealed performance [1;31mregression[m introduced for DDL intensive
    workloads when innodb_flush_log_at_trx_commit = 2.
    
    This is because we over-simplified in the previous fix for BUG#28616442.
    
    This patch reverts related part of the previous fix:
    - don't sleep for more than 1ms in log threads (trx=0 case could suffer),
    - when trx=2 and user thread is waiting for flushed redo (ddl), we need
      to first wait for redo written and wake up log_flusher only afterwards
      (otherwise we could wake up log_flusher too early - that was problem).
    
    We also skip disabling spin-delays for two cases:
    - waiting for written redo when trx=1,
    - waiting for flushed redo when trx=2.
    
    We also checked if we could use two-steps waiting always - that is always
    first wait for written redo and only afterwards fallback to wait on
    flushed redo. We can't afford that.
    
    This would result in more CS when busy-waiting is disabled and trx=1
    (it's disabled when total cpu usage is more than the hwm (default 50%)).
    
    However we observed that two-steps waiting increased TPS for workloads
    with small number of connections. We confirmed that it was because of
    increased total time for busy-waiting (two times more spin rounds).
    On the other hand, we can't afford making more spin rounds for higher
    number of connections, because we would risk wasting too much cpu,
    before we reach the hwm which disables the busy waiting.
    
    Because of that we introduced new mechanism:
    1. Use increased spin rounds (compute in range 250k..25k) when
       cpu usage is below 50% of the hwm.
    2. Use default spin rounds (25k) when cpu usage is in 50%..100% of hwm.
    3. Do not use busy waiting when cpu usage is higher than hwm.
    
    Points 2,3 - we behave as it was in the past.
    
    Point 1 - we may use increased number of spin rounds when cpu usage
    is lower than 50% of the hwm.

[33mcommit 868b56516042b8d6e25014a449ec1ab38901a279[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Aug 31 16:00:38 2018 -0500

    Bug#28515475 BOGUS DATA WHEN ORDERING RESULTS FROM VARIABLES_BY_THREAD
    
    This fixes a [1;31mregression[m from Bug#21354712 SHOW VARIABLES RETURN WARNING ABOUT @@SESSION.GTID_EXECUTED BEING DEPRECATED, where the logic to deprecate "gtid_executed" misaligns the system variable cache, causing the position of variables following "gtid_executed" to be off by one.
    
    This patch introduces System_variable::m_ignore to identify deprecated system variables that must be accounted for internally but otherwise ignored.

[33mcommit fc5102049cfd9ba0dc61357516af829836e40974[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Sep 18 15:17:17 2018 +0200

    Bug#28614646: FIX -WSHADOW-FIELD WARNINGS
    
    Post-push fix: Fix [1;31mregression[m visible in Cluster tests.
    
    Change-Id: I656aa08fd5e02d21511e78d4085b2e9cedca9862

[33mcommit ad6cfa5d3a18d7bee0b80dec3397613408a095e8[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Sep 7 10:29:37 2018 +0200

    Bug#28105016 SERVER 8.0.12 DOXYGEN BUILDS GENERATED WITH EMPTY REVISION
    NUMBER
    
    Doxygen cleanup, fixed [1;31mregression[m from:
    
    commit a4016740ee3cc6389b8908ce514bc927b6727bee
    Author: Marc Alff <marc.alff@oracle.com>
    Date:   Wed Jan 3 17:33:39 2018 +0100
    
        Doxygen build cleanup
    
        Change doxygen_resources/doxygen-filter-mysqld
        from a bash script to a perl script,
        so it runs also on Windows.
    
    Somehow PUSH_REVISION was renamed to BRANCH_REVISION.
    The correct variable, used in PB2, is PUSH_REVISION.

[33mcommit 3f2778640cc51136e178437bc606bc5bed7a3b0c[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The performance [1;31mregression[m, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7ac28149f725a63d1b1042ee9d9a1d26f566f1b6[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The performance [1;31mregression[m, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.
For keyword speed:
[33mcommit 90f12fcbce2bcc479f049a6935fe02e051897986[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Sat Mar 10 14:20:35 2018 +0100

    WL#11722: Step 16
    -----------------
    Put TreeEnt of current row in block object to ensure that
    we can avoid doing a new selectNode in scanFind.
    
    Ensure that selectNode has context available to ensure it
    knows which tables and index pointers to use.
    
    Make tuxGetNode inline to [1;31mspeed[m things up.
    
    Put a few more things in the context object and ensure this
    object is initialised also for MT-build of ordered indexes.

[33mcommit b21dfb1d20f079d43c2ab9f747ef8b9d845c9a7d[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Mon Feb 26 19:19:33 2018 +0100

    WL#11722 Step 5
    ---------------
    Introduce new configuration variable useChecksum that defaults to 0.
    We can only checksum the fixed part and in addition the overhead of
    this is very significant, so we made it default to not use checksum.
    
    Optimised handling of search condition failures in scan processing
    by [1;31mspeed[ming up handling of errors in TUPKEYREQ.
For keyword flush:
[33mcommit 0bbfaceaa4242f9772b4eaa648c09342b7f17e0f[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Wed Nov 21 15:50:03 2018 +0100

    Bug#27309336 Backport to 5.7
    
    Problem:
    Hang observed while attempting to create large number of tables as fast
    as possible if innodb_[1;31mflush[m_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash is observed when the innodb_[1;31mflush[m_method is set to O_DIRECT.
    The problem seems to be because of out of sync FS meta data as fsync()
    is never called if the [1;31mflush[m method is set to O_DIRECT_NO_FSYNC.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20994
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit d7fc4690773ad26da49ba842eca66bc72acf07bd[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Nov 19 05:01:56 2018 +0100

    Bug#27309336 INNODB 8.0 WITH O_DIRECT_NO_FSYNC MAY CRASH THE SERVER DUE NEW DD ACTIVITY
    
    Problem:
    Hang observed while attempting to create large number of tables as fast
    as possible if innodb_[1;31mflush[m_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash observer when the innodb_[1;31mflush[m_method is set to O_DIRECT.
    
    The problem seems to be related to the fact that fsync() is never called
    in O_DIRECT_NO_FSYNC mode, which could lead to a situation where FS metadata
    may go out of sync causing this hang.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20246
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit 2dd92db3a0a9c3c0e385c38ecc0972717ae93956[m
Author: Hemant Dangi <hemant.dangi@oracle.com>
Date:   Thu Nov 15 18:32:30 2018 +0530

    Bug#28382590: ASSERT `!IS_SET()' AT SQL_ERROR.CC:379 UPON STOP G.R. IN ERRORED MEMBER
    
    Issue:
    ======
    When a member also running asynchronous replication channel goes into
    ERROR state upon execution of
    group_replication_switch_to_single_primary_mode() UDF:
    - The IO and SQL slave thread of asynchronous replication channel should
      not be in running state.
    - In the ERROR state, upon executing STOP GROUP_REPLICATION it should
      not hit ASSERTION `!is_set()' failure at sql_error.cc:379.
    
    Solution:
    =========
    Added missing channel_stop_all() call to stop all running asynchronous
    replication channels, in case when group replication member goes into
    ERROR state.
    The assert error was due to missing skip_readonly_check set for
    STOP GROUP_REPLICATION [1;31mflush[m info tables.

[33mcommit 94a0aab7ea336a7a04ad5ca84e8d6e4f6e99586b[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Fri Nov 2 10:47:29 2018 +0530

    Bug #28607368 INNODB.LOB_NO_SPACE FAILS SPORADICALLY ON PB2 VALGRIND
    
    Problem:
    
    The DBUG_EXECUTE_IF() debug points were placed after the LOB page allocation
    but before its initialization.  Because of this, if these LOB pages gets
    [1;31mflush[med, then it will be considered as corrupted database pages.
    
    Solution:
    
    Place the debug points before the actual LOB page allocations.
    
    rb#20859 approved by Deb.

[33mcommit bfc2e06204d913ca3593b64888747a6334f904c9[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Mon Nov 5 14:44:34 2018 +0530

    Bug25364178: XA PREPARE INCONSISTENT WITH XTRABACKUP
    
    Post push fix for sporadic failure of [1;31mflush[m_read_lock.test

[33mcommit 0c79729d3182a774047d8a3f5cf1d16574e08dc8[m
Author: Pedro Figueiredo <pedro.figueiredo@oracle.com>
Date:   Mon Oct 15 09:18:18 2018 +0100

    BUG#27928837 `HEAD->VARIABLES.GTID_NEXT.TYPE != UNDEFINED_GTID'
    
    Description
    -----------
    The assertion with failing condition `head->variables.gtid_next.type !=
    UNDEFINED_GROUP`, in function ` bool
    MYSQL_BIN_LOG::assign_automatic_gtids_to_[1;31mflush[m_group(THD*)`, is generated while
    running an `XA ROLLBACK` statement with non-existing XID and `GTID_NEXT` is set
    to a specific UUID.
    
    Analysis
    --------
    The *8de09762684577f4a891bba3344c79666d185006* commit altered the behaviour of
    `MYSQL_BIN_LOG::rollback` regarding XA transactions by always invoking
    `gtid_state->update_on_commit()` in order to persist the executed GTID. However,
    the specified code was introduced below the goto tag `end`, the default fallback
    goto tag - even for errors - meaning that the `gtid_state->update_on_commit()`
    will be invoked even if the `XA ROLLBACK` fails with an error - which is the
    case for this bug.
    
    Fix
    ---
    Only invoke `gtid_state->update_on_commit()`, while processing an `XA ROLLBACK`,
    if no error occurs.

[33mcommit 54cb37a1ec8f1be93983512c36408a795448b14a[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Sun Sep 30 08:04:43 2018 +0530

    Bug#22252394: SLAVE I/O THREAD MAY STOP WHEN BINLOG ROTATES
    
    Problem:
    ========
    When binlog rotation occurs before updating
    MYSQL_BIN_LOG::binlog_end_pos in SYNC stage of binlog group
    commit, receiver thread stops with following error.
    
    2015-11-22T17:32:39.386343Z 8 [ERROR] Slave I/O for channel
    '': Got fatal error 1236 from master when reading data from
    binary log: 'unknown error reading log event on the master;
    the first event '' at 4, the last event read from
    './master-bin.000244' at 202, the last byte read from
    './master-bin.000244' at 202.', Error_code: 1236
    
    Analysis:
    ========
    When sync_binlog= 1 durable setting is enabled, the binary
    log end position is updated only upon successful completion
    of sync stage. In binlog group commit the LOCK_log is held
    during [1;31mflush[m stage and it is release during sync stage and
    LOCK_sync is acquired. By releasing LOCK_log earlier more
    parallelization can be achieved on master.
    
    For example let the binlog_file:
    master-bin.000001 and pos: 364
    
    Executing FLUSH LOGS at this moment will grab the LOCK_log
    and it successfully rotates the binary log and does the
    [1;31mflush[m and sync of the new binary log. The binlog end
    position also gets updated to reflect the active binary log
    file and position. At this stage
    
    new binlog_file: master-bin.000002 and pos:154
    
    Now the sync operation proceeds and it tries to update the
    binlog end postion. update_binlog_end_pos(pos) function call
    does the following. It compares if the passed input binlog
    position is greater than previous binlog_end_pos.  If it is
    greater it updates the value.
    
    void update_binlog_end_pos(my_off_t pos)
      lock_binlog_end_pos();
      if (pos > binlog_end_pos)
        binlog_end_pos= pos;
    
    The above code will result as shown below.
    
    Example scenario will become: master-bin.000002 pos: 364.
    
    At this point the binlog sender thread tries to read a non
    existing event at pos 364 from 'master-bin.000002' which
    results in an error.
    
    Associating the position of old binary log file to a new
    binary log file leads to this issue.
    
    Fix:
    ===
    The update_binlog_end_pos function call now receives the
    binary log file name along with the position. The input
    binary log file name will be compared against active binary
    log. If the input file is not the active file then position
    will not be updated. Position update is skipped as active
    binary log has changed.

[33mcommit 66d0aa2238e4852ff74f6120137b695f0b623f4d[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Tue Aug 14 18:49:51 2018 +0100

    WL#10957: Binary log encryption at rest (Step 1)
    
    This patch introduces the infrastructure to allow a server to
    read content from encrypted binary/relay log files and to write
    into an existing encrypted binary log file.
    
    Reading encrypted binary/relay log files requires:
    - Deserializing the file encryption header;
    - Retrieving the replication encryption key from keyring;
    - Decrypting the file password;
    - Reading the encrypted content as a stream.
    
    Writing to existing encrypted binary log files requires:
    - Deserializing the file encryption header;
    - Retrieving the replication encryption key from keyring;
    - Decrypting the file password;
    - Encrypting the new content to be written;
    - Write the encrypted content into the lower stream.
    
    @ client/CMakeLists.txt
    
      As Mysqlbinlog_ifile inherits from Basic_binlog_ifile that depends on
      new sql/rpl_log_encryption.cc, an entry for the latter was added to
      mysqlbinlog dependencies.
    
    @ client/mysqlbinlog.cc
    
      A minor refactoring was needed because of changes in the open_file
      function that is overridden by Mysqlbinlog_ifile.
    
    @ share/errmsg-utf8.txt
    
      Introduced the following errors:
      - ER[_SERVER]_RPL_ENCRYPTION_FAILED_TO_FETCH_KEY;
      - ER[_SERVER]_RPL_ENCRYPTION_KEY_NOT_FOUND;
      - ER[_SERVER]_RPL_ENCRYPTION_INVALID_KEY;
      - ER[_SERVER]_RPL_ENCRYPTION_HEADER_ERROR;
    
    @ sql/CMakeLists.txt
    
      Added an entry for sql/rpl_log_encryption.cc to the BINLOG_SOURCE set
      of dependencies.
    
    @ sql/basic_ostream.h
    
      Added [1;31mflush[m() and sync() virtual function to Truncatable_ostream as
      part of Binlog_ofile refactoring.
    
    @ sql/binlog.h
    
      Added a encrypted_header_size field to LOG_INFO. This is needed to
      display the correct file size in "SHOW BINARY LOGS" for the last
      active binary log file.
    
    @ sql/binlog.cc
    
      Did some refactoring in Binlog_ofile and added a static function
      open_existing() to allow to open an existing encrypted file to change
      its content or truncate it.
    
      Changed any function relying on file size to state the binary log file
      size to consult the size of the "binary log data stream". It is
      actually the file size for plain binary/relay log files and
      (file size - encrypted header size) for encrypted binary/relay log
      files.
    
      Changed binary log file truncation to use Binlog_ofile (that supports
      encryption) when truncating and clearing LOG_EVENT_BINLOG_IN_USE_F
      flag.
    
    @ sql/binlog_istream.{h|cc}
    
      Added new entries to Binlog_read_error:
      - INVALID_ENCRYPTION_HEADER;
      - CANNOT_GET_FILE_PASSWORD;
    
      Introduced the Binlog_encryption_istream class (a Seekable_istream
      with decryption feature).
    
      Did some refactoring in open_file function.
    
    @ sql/binlog_ostream.{h|cc}
    
      Introduced the Binlog_encryption_ostream class (a Truncatable_ostream
      with encryption feature).
    
    @ sql/rpl_master.cc
    
      Made "SHOW BINARY LOGS" to display proper file size for a last
      encrypted binary log file and also added the "Encrypted" column to
      the "SHOW BINARY LOGS" output.
    
    @ sql/rpl_log_encryption.{h|cc}
    
      This is a new pair of files.
    
      They introduce both encryption and decryption logics using
      Aes_ctr_cipher, the Rpl_encryption_header class and general purpose
      Rpl_encryption class.
    
      The Rpl_encryption_header classes shall handle serialization and
      deserialization of the replication log files encryption header.
    
      The Rpl_encryption class shall provide general functionality related
      to encryption for the MySQL server. In this patch, it is providing
      a function to retrieve encryption keys from the keyring.
    
    Test case
    =========
    
    @ rpl_nogtid.rpl_nogtid_encryption_read
    
      This script aims at testing read and truncate operations on encrypted
      binary log files, as well as related error conditions.

[33mcommit b0955c74d4d027f2838ae6c48cd3dfbed639cbaf[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Wed Sep 26 15:45:27 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    Eventum 61197 revealed performance regression introduced for DDL intensive
    workloads when innodb_[1;31mflush[m_log_at_trx_commit = 2.
    
    This is because we over-simplified in the previous fix for BUG#28616442.
    
    This patch reverts related part of the previous fix:
    - don't sleep for more than 1ms in log threads (trx=0 case could suffer),
    - when trx=2 and user thread is waiting for [1;31mflush[med redo (ddl), we need
      to first wait for redo written and wake up log_[1;31mflush[mer only afterwards
      (otherwise we could wake up log_[1;31mflush[mer too early - that was problem).
    
    We also skip disabling spin-delays for two cases:
    - waiting for written redo when trx=1,
    - waiting for [1;31mflush[med redo when trx=2.
    
    We also checked if we could use two-steps waiting always - that is always
    first wait for written redo and only afterwards fallback to wait on
    [1;31mflush[med redo. We can't afford that.
    
    This would result in more CS when busy-waiting is disabled and trx=1
    (it's disabled when total cpu usage is more than the hwm (default 50%)).
    
    However we observed that two-steps waiting increased TPS for workloads
    with small number of connections. We confirmed that it was because of
    increased total time for busy-waiting (two times more spin rounds).
    On the other hand, we can't afford making more spin rounds for higher
    number of connections, because we would risk wasting too much cpu,
    before we reach the hwm which disables the busy waiting.
    
    Because of that we introduced new mechanism:
    1. Use increased spin rounds (compute in range 250k..25k) when
       cpu usage is below 50% of the hwm.
    2. Use default spin rounds (25k) when cpu usage is in 50%..100% of hwm.
    3. Do not use busy waiting when cpu usage is higher than hwm.
    
    Points 2,3 - we behave as it was in the past.
    
    Point 1 - we may use increased number of spin rounds when cpu usage
    is lower than 50% of the hwm.

[33mcommit 38a6d1d082fe09d94e45540a56cbecbd115ae7df[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Sep 24 20:16:47 2018 +0530

    Bug#28511326: DEADLOCK DURING PURGE_LOGS_BEFORE_DATE
    
    Problem:
    =======
    Access to the following variables is protected by LOCK_log.
    
    binlog-transaction-dependency-tracking
    binlog-transaction-dependency-history-size
    
    'LOCK_log' is held when these variables are being set or read.
    Holding 'LOCK_log' results in following Deadlock scenario.
    
    Analysis:
    =========
    1) SELECT * FROM performance_schema.session_variables WHERE
    VARIABLE_NAME LIKE 'binlog_transaction_dependency_tracking';
    
    The above query acquires a lock on thread data and tries to
    read the values of 'binlog_transaction_dependency_tracking'.
    Inorder to read this variable 'LOCK_log' is required.
    
    Owns: THD::LOCK_thd_data (acquired in
          PFS_system_variable_cache::do_materialize_all
          ->PFS_variable_cache<Var_type>::get_THD
          -> Find_THD_variable::operator())
    Waits for: MYSQL_BIN_LOG::LOCK_log
    
    2) SHOW BINARY LOGS
    
    Above command acquires 'LOCK_log' to read current active
    binary log specific information and then goes on to acquire
    LOCK_index, so that it can list the rest of binary logs.
    
    Owns: MYSQL_BIN_LOG::LOCK_log (acquired in show_binlogs())
    Waits for: MYSQL_BIN_LOG::LOCK_index
    
    3) PURGE LOGS BEFORE date
    
    Above command acquires 'LOCK_index' and reads one log at a
    time from index. For each log it tries to identify how many
    threads are accessing this log. Inorder to do this it
    acquires a lock on global thread list and iterates through
    the entire thread list. For each thread it tries to acquire
    LOCK_thd_data and verify if the log is being used by the
    tread or not. Hence it waits for the lock.
    
    Owns: MYSQL_BIN_LOG::LOCK_index (acquired in
          MYSQL_BIN_LOG::purge_logs_before_date)
    Waits for: THD::LOCK_thd_data
    
    Fix:
    ===
    Transaction dependency tracking information is updated
    based on 'max_committed_transaction' object contents.
    'max_committed_transaction' holds the transaction
    sequence_number. This transaction_sequence number is updated
    during the [1;31mflush[m stage of the commit and it is used to
    update the transaction dependency tracking through
    'update_max_committed' function call.
    
    'LOCK_log' needs to be held when the active binary log is
    being modified. Where as to protect concurrent access to set
    or read dependency tracking information a less granular lock
    should be sufficient. Hence a new lock named
    'LOCK_slave_trans_dep_tracker' has been introduced to
    protect concurrent access to transaction dependency tracking
    information.

[33mcommit c8d46fa8014a582fc1f1ad0dfbe59955c6af3712[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Fri Sep 21 12:12:41 2018 +0530

    WL#8500 Adapt MySQL Cluster to 8.0
    
    Implement a wrapper for [1;31mflush[m_error_log_messages() in ndb_log.h.
    
    Change-Id: I6a620c94662a2884b5fd6ddbf07db546cc20535a

[33mcommit 89f30e91ed17d895f25f75dc60e5e52d662f6b5a[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Mon Sep 17 10:04:08 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    The fix to BUG#28062382 was saving CPU by doing longer waits in log threads,
    when there was nothing to be done for the log threads and the frequency of
    requests to write / [1;31mflush[m redo was detected as very low. The longer waits
    meant: no spin-delay and initial timeout = 10ms.
    
    The log threads were waiting on their events (cond_var), so if there was
    a task for them, user threads were trying to wake them up.
    
    The problem was because user threads were trying to find out if they
    should wake up log threads. This leaded to very complex logic which
    has already proved to be error prone.
    
    The whole logic became simplified in this patch. User thread checks
    if write_lsn is not advanced enough and wakes up log_writer in such
    case - no matter if frequency is high/low or CPU usage on server is
    below 80% threshold or not.
    
    There was also a bug in monitor of the frequency - when there were
    no counted calls to log_write_up_to(), the average time between calls
    stayed unchanged. It should be reset to +inf if that happened for
    longer period.
    
    Additionally all calls to log_write_up_to() should always be counted.
    The calls from page cleaners were the only that we wanted to exclude
    from being counted, so page cleaners should not call log_write_up_to()
    if the [1;31mflush[med_to_disk_lsn is already advanced enough (most cases).

[33mcommit 2989f7da59a009c35b56f6f2d6190efef1b433f5[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Wed Sep 19 15:21:43 2018 +0530

    WL#8500 Adapt MySQL Cluster to 8.0
    
    The WL#11875 updates the early buffered logging to make use of the
    newer rich logging system features. Due to this, all the events are now
    buffered, before the logging system gets initialised. Once the system
    gets initialised, the buffer is [1;31mflush[med into the log files. If the
    server shuts down very early, due to some error, before the logging
    system has been initialised, the buffered logs are [1;31mflush[med out to the
    stderr. This patch adapts the same for the ndbcluster plugin when it
    fails during init as it exits by itself rather than returning to server.
    
    Change-Id: I7928ad7f5be523e3e808ea611dafa0b349f3b9d1

[33mcommit 1211bb78179bcbd38478d6b64007a028555f9b5c[m
Author: Srikanth B R <srikanth.b.r@oracle.com>
Date:   Tue Sep 18 11:02:15 2018 +0530

    Bug#28660978 MTR TEST FOR FLUSH OF BUFFERED ERROR LOG MSGS ON INITIAL AND INCREMENTAL TIMEOUT
    
    The patch adds testcases to validate [1;31mflush[ming of buffered messages
    to the error log file if logging components cannot be activated at
    all or are activated after waiting for some time.
    
    It also includes a post-push fix for main.log_options_cmdline.
    
    Reviewed-by: Tatiana Nuernberg <tatjana.nuernberg@oracle.com>
    RB: 20538

[33mcommit 8c3f245d5819ab12aa0971b2e749c17a5f4df440[m
Author: Tatiana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Wed Sep 12 11:30:09 2018 +0100

    WL#11875: update early "buffered" logging to make full use of the new error logging system
    
    Information passed to error-logging before it is fully initialized
    or told where and what to log is collected in a mechanism called
    "buffered logging". This system predates the new rich logging
    system and therefore did not leverage all capabilities of the
    new logger (specifically, it would only buffer a subset of the
    information passed to it, and would not apply any filtering,
    even --log-error-verbosity, to items buffered before those
    options were set).
    
    This patch updates early logging so all (rather than just some)
    structured information of the submitted log-events is saved,
    and then filtered and output as set up by the DBA once all the
    necesary systems are available. If this is not possible (as is
    the case when severe errors occur before the component infrastructure
    is fully initialized), the server will fall back on the built-in
    default log-filter and log-sink to [1;31mflush[m this information so the
    reasons for the early and unexpected shutdown are not lost.
    
    As this way, --log-error-verbosity, --log-error-suppresion-list,
    and, if start-up completes successfully, --log-error-services and
    any configuration specific to any loadable log-services are applied
    to all log-events (rather than that configuraton starting to apply
    halfway through start-up, as is the case without this changeset),
    this should make overall server behaviour less surprising.
    
    Finally, if the start-up takes unexpectedly long, we'll now start
    [1;31mflush[ming buffered messages to the traditional error-log using the
    default log-writer, and will [1;31mflush[m the complete backlog of error
    messages to any other configured log-services once they become
    available. This trade-off should prevent the server from "just
    sitting there" with the DBA receiving no useful information as
    to why. This should not be necessary in normal operation, but is
    intended to be helpful in cases where for instance a starting
    server cannot acquire a lock on a database already in use by
    another instance.

[33mcommit 2809dd8df525fbe26b7df3d2adb30ebf6b3c9637[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Sun Sep 2 12:49:18 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    One more fix (small).
    
    We missed notification when innodb_[1;31mflush[m_log_at_trx_commit = 1 and
    we switched writer to low-latency mode. Because of that writer was
    waiting 10ms before next write and all user threads were delayed.

[33mcommit 3f2778640cc51136e178437bc606bc5bed7a3b0c[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or [1;31mflush[m of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or [1;31mflush[m redo log.
    
    3. When the average time between requests to write or [1;31mflush[m redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7ac28149f725a63d1b1042ee9d9a1d26f566f1b6[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or [1;31mflush[m of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or [1;31mflush[m redo log.
    
    3. When the average time between requests to write or [1;31mflush[m redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait directly
       on the event with initial timeout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 0b7705cc7070152e2136a678fb0fed4153f72af2[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Thu Jul 5 16:53:51 2018 +0200

    BUG#28297462 REQUESTING CHECKPOINT AT AVAILABLE LSN SHOULD NEVER FLUSH PAGES
    
    BUG#28220222 LOG_CHECKPOINTER IS NOT WILLING TO WRITE CHECKPOINTS ON-TIME
                 WHEN REDO IS SMALL
    
    1. We should not force pre[1;31mflush[m of dirty pages in log_wait_for_checkpoint().
    The sharp checkpoint mechanism should only force the pre[1;31mflush[m before it calls
    to log_wait_for_checkpoint().
    
    2. Log checkpointer thread should respect concurrency_margin when deciding
    if next checkpoint write is required.
    
    3. Log checkpointer thread should respect concurrency_margin when deciding
    if it should force sync-[1;31mflush[m of dirty pages (wake up page cleaners).
    
    4. Page cleaner threads should respect concurrency margin when deciding
    if they should perform sync/async [1;31mflush[ming of dirty pages and include
    the concurrency margin in their calculations of how much to [1;31mflush[m.
For keyword innodb:
[33mcommit 0bbfaceaa4242f9772b4eaa648c09342b7f17e0f[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Wed Nov 21 15:50:03 2018 +0100

    Bug#27309336 Backport to 5.7
    
    Problem:
    Hang observed while attempting to create large number of tables as fast
    as possible if [1;31minnodb[m_flush_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash is observed when the [1;31minnodb[m_flush_method is set to O_DIRECT.
    The problem seems to be because of out of sync FS meta data as fsync()
    is never called if the flush method is set to O_DIRECT_NO_FSYNC.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20994
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit d7fc4690773ad26da49ba842eca66bc72acf07bd[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Nov 19 05:01:56 2018 +0100

    Bug#27309336 INNODB 8.0 WITH O_DIRECT_NO_FSYNC MAY CRASH THE SERVER DUE NEW DD ACTIVITY
    
    Problem:
    Hang observed while attempting to create large number of tables as fast
    as possible if [1;31minnodb[m_flush_method is set to O_DIRECT_NO_FSYNC. No hang
    or crash observer when the [1;31minnodb[m_flush_method is set to O_DIRECT.
    
    The problem seems to be related to the fact that fsync() is never called
    in O_DIRECT_NO_FSYNC mode, which could lead to a situation where FS metadata
    may go out of sync causing this hang.
    
    Fix:
    Call fsync in O_DIRECT_NO_FSYNC mode whenever the size of the file changes.
    This will keep the FS metadata in sync.
    
    RB: 20246
    Reviewed by: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit 5becdf4dd153fe720a70e9955628febe2a2ed9e9[m
Author: Amitabh Das <amitabh.das@oracle.com>
Date:   Mon Oct 8 14:10:42 2018 +0530

    Bug#28493257: ASSERTION `!IS_SET()' FAILED
    
    Calling alter table on a temporary table happens to go through
    dict_create_foreign_constraints_low even if the table has no foreign
    keys. This method returns success even if the table was not opened (in
    this case because there was no space on thread stack), since there were no
    foreign keys to be created. So, mysql_alter_table assumes things went well.
    However, since there was an error reported there is a crash in the
    Diagnostics Area. This is only reproducible in 8.0.12. The patch for
    Bug#28245522 reduced the overall stack usage and this issue is not seen.
    But it's better to skip the tables without FK's.
    
    This patch adds a check to make sure this method is not called if table
    has no foreign keys.
    
    Test changes:
    Changed the tests main.commit_1[1;31minnodb[m, main.create, main.partition_locking,
    main.slow_log_extra, main.foreign_key to account for the new code path.
    
    Change-Id: Ia291c0bc747edd29f87203319db9e823881a7ffd

[33mcommit 56550b29c981620cc7acd0eae1ceb710a314fe56[m
Author: Jens Even Berg Blomsoy <jens.even.blomsoy@oracle.com>
Date:   Thu Nov 15 10:34:39 2018 +0100

    Bug #28235668 - ASSERTION AT UT_DBG_ASSERTION_FAILED (EXPR=0X5173C5C "J < NCOL"
    
    Problem:
    EXCHANGE PARTITION would allow two table(partition) with different
    table definition(different virtual column definition) to exchange,
    this caused a crash later on when InnoDB would try to read from a non
    existent virtual column.
    
    Solution:
    Added a check in mysql_compare_tables() to not allow generated
    virtual columns to be exchanged for non generated virtual columns.
    While one is still able to exchange partition for similar table
    definition.
    
    Due to how Innodb renames temporary tables a workaround for how
    special character '#' was handled was implemented in ha_[1;31minnodb[m.cc.
    
    New test file and test result was added.
    
    Change-Id: I6082d78c0af9ed8db30ffca6b5d8c80ef6549376

[33mcommit 78313ea1d2294c78df420f89983c37e59281953e[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Fri Nov 16 06:55:04 2018 +0100

    Bug #28466045: SET PERSIST NOT SETTING CORRECT VALUES FOR VARIABLES LIKE
                   INNODB_STRICT_MODE
    
    Problem: [1;31minnodb[m plugin variable [1;31minnodb[m_strict_mode is not persisted correctly.
    
    Analysis: sys_var_pluginvar::real_value_ptr() this function returns the base
    address for the variable whose value needs to be updated. Based on if THD is
    null or not value can be returned from
    global_system_variables.dynamic_variables_ptr or
    thd->variables.dynamic_variables_ptr. During update to this variable since
    variable type is set to OPT_PERSIST, base address returned from real_value_ptr()
    comes from thd->variables.dynamic_variables_ptr. However when saving this value
    to persisted config file the real_value_ptr() is called with variable type set
    to OPT_GLOBAL, which results is returning wrong base address. Thus the updated
    value is not correctly saved into persisted config file.
    
    Fix: Fix is to ensure that real_value_ptr() returns same base address during
    sys_var::update and during Persisted_variables_cache::get_variable_value().

[33mcommit 97c49a66cea30c96ebc48129f3c4d59ac7a7c913[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Tue Nov 13 12:00:56 2018 +0100

    Bug #28523042   INNODB: ASSERTION FAILURE: LOCK0LOCK.CC:7034 IN DEADLOCKCHECKER::SEARCH
    
    The mysql.[1;31minnodb[m_table_stats and mysql.[1;31minnodb[m_index_stats are visible to the end user,
    and thus can take part in transactions which can acquire shared or exclusive locks,
    and end up in a deadlock.
    There was an assert that locks on internal tables should not take part in deadlock cycle,
    because such deadlocks presumably might not be handled correctly.
    This patch fixes the way we handle deadlocks in background transactions involving
    this two tables, and excludes them from the assertion.
    It also introduces an MTR which checks that user can create a deadlock for any InnoDB table
    in mysql.* database without crashing the server.
    
    Reviewed-by: Bin Su <bin.x.su@oracle.com>

[33mcommit 99a0de4a51cbe6ebed1e847ea8ceb27e3fbc73bd[m
Author: Terje Rosten <terje.rosten@oracle.com>
Date:   Fri Oct 26 10:27:26 2018 +0200

    BUG#28841164 IF CORE_PATTERN IS COMPLEX: SKIP MYSQLD_CORE_DUMP_WITH...
    
    If core pattern is different from simple 'core' it's not possible for mtr
    to find core (and verify size), in such case, just skip tests
    [1;31minnodb[m.mysqld_core_dump_without_buffer_pool*

[33mcommit cd6995e36d33ce7147c91cad1e74be555fdde69e[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Wed Oct 3 09:05:36 2018 -0700

    Bug#28684772: IS.INNODB_TABLESPACES NOT SHOWING TABLESPACE INFO
    AFTER UPGRADE(8.0.12->8.0.14)
    Bug#28684204: UPGRADE FROM 8.0.12 TO 8.0.14 FAILS WHEN
    --INNODB_UNDO_TABLESPACES > 2
    
    There were two problems when upgrading from 8.0.12 to 8.0.14.
    1) dd::Tablespaces::se_private_date is missing key/value pairs for
    the key 'state' and it may contain a kv pair for 'discarded.
    2) The WL9508 code assumed that there can only be 2 implicit undo
    tablespaces. But 8.0.12 allowed and used the setting
    --[1;31minnodb[m-undo-tablespaces to create up to 127 undo tablespaces.
    
    The solution is to make the "state" key optional and add back in the
    possibility of key "discarded".
    Make all calls to get the current tablespace state go through the same
    function, which uses either dd::tablespaces::se_private_data or the
    'discarded' key, or the undo::Tablespaces object in memory.
    Treat implicit undo tablespaces with undo number > 2 as an explicit
    undo space in that they can be made inactive and empty (as the code
    worked before, but they also can be dropped.
    There were other places in code where these guidelines needed to be
    enforced as well.
    
    Approved by Rahul Agarker in RB#20647.

[33mcommit cc63b5143cae267f579f83e1ee4020648f593e85[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Oct 8 15:23:12 2018 -0700

    28726286 [INNODB] SEMAPHORE WAIT HAS LASTED > 903 SECONDS during RQG test
    
    1) All undo tablespaces became inactive. The code in
    [1;31minnodb[m_alter_undo_tablespace_inactive() that is supposed to keep at least
    2 undo spaces active at all times did not work when there are multiple
    undo spaces marked inactive implicit.  It assumed there was only one.
    This change takes away that assumption and only counts the
    innactive_implicit space that is marked.
    
    2) In this hang, half of the undo spaces were marked inactive_implicit.
    The purge thread only marks one of those at a time.  The others are
    explicitly SET INACTIVE and then SET ACTIVE again before they became empty.
    This changes the alter_active() function to check if the inactive_explicit
    space has been marked. If so, it is set to inactive_implicit. If not
    marked yet, it is set directly to active.
    
    Approved by Rahul in RB#20682

[33mcommit 070c307d74ce618e67e23303b9a1f4a84d1e53d8[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Tue Oct 16 14:33:22 2018 -0700

    Bug 28684645: DROP UNDO TABLESPACE AFTER RECOVERY CRASHES
    
    Another problem that shows a similar call stack.  If a database is
    bootstrapped with a remote --[1;31minnodb[m-undo-directory, and a new explicit
    undo space is created in the datadir using an absolute path, then the
    undo::Tablespace::set_file_name() routine did not create the file_name
    correctly.
    After restart, the scan found the file in the datadir and called it
    "./x2.ibu".  undo::Tablespace::set_file_name() did not expect a relative
    path unless the datadir and the undo dr were the same.  So this code
    prepended the remote undo dir to "./x2.ibu".

[33mcommit 6438c3b996b5c54e9305c18d38d3175ad52a69e6[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Oct 16 12:11:46 2018 +0200

    Bug #28799559: TESTS DON'T WORK WITH DEFAULT SETTING OF MAX OPEN FILE
                   DESCRIPTORS
    
    Problem: Default value for variable [1;31minnodb[m_open_files is different on different
    platforms which causes result file mismatch. max_open_files vlaue is calculated
    on runtime based on parameters like max_connections, table_open_cache. So if
    max_open_files limit is set to a value more than the calculated value, mysqld
    reports a warning.
    
    Fix: Fix is to add a suppresion for the warning, and removed the
    [1;31minnodb[m_open_files variable as this is not needed in the test file.

[33mcommit 43acad2a8a1221f589f3bd78c1f09d7e0d937ff3[m
Author: Aditya A <aditya.a@oracle.com>
Date:   Wed Oct 10 18:05:02 2018 +0530

    Bug #22990029 GCOLS: INCORRECT BEHAVIOR AFTER DATA INSERTED WITH IGNORE KEYWORD
    
    PROBLEM
    -------
    
    1. We are inserting a base column entry which causes an invalid value
       by the function provided to generate virtual column,but we go ahead
       and insert this due to ignore keyword.
    2. We then delete this record, making this record delete marked in [1;31minnodb[m.
       If we try to insert another record with the same pk as the deleted
       record and if the rec is not purged ,then we try to undelete mark this
       record and try to build a update vector with previous and updated value
       and while calculating the value of virtual column we get error from
       server that we cannot calculate this from base column.
       Innodb assumes that innobase_get_computed_value() Should always return
       a valid value for the base column present in the row. The failure of
       this call was not handled ,so we were crashing.
    
    FIX
    ---
    Handled the failure of innobase_get_computed_value()
    call. We now return error in this case and prevent a
    crash.

[33mcommit 4e1d10b26c502d407c2ed3fb567eb0c2c15b1464[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Sep 20 12:49:19 2018 -0700

    Bug #28678758   DROP UNDO TABLESPACE FAILS WHEN UNDO TRUNCATE IS OFF
    
    The setting --[1;31minnodb[m-undo-truncate should not affect the ability to truncate
    a specific undo tablespace explicitly with SET INACTIVE.

[33mcommit 5ce335439e2fe2bda98c2292116f41959cfda8da[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Wed Sep 26 10:12:25 2018 +0530

    Bug#28556539 LOCK_ORDER: CYCLE INVOLVING BUF_POOL_FREE_LIST_MUTEX AND
    BUF_POOL_ZIP_FREE_MUTEX
    
    Problem:
    The LOCK_ORDER tool reports a cycle in [1;31minnodb[m mutex locks because of
    incorrectly mapped key while creating a mutex. Because of this incorrect
    mapping, the tool assumes there is a cycle between two mutexes
    potentially leading to a deadlock.
    
    Solution:
    Replaced the incorrect key with the correct one
    
    RB: 20610
    Reviewed By: Debarun Banerjee (debarun.banerjee@oracle.com)

[33mcommit b0955c74d4d027f2838ae6c48cd3dfbed639cbaf[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Wed Sep 26 15:45:27 2018 +0200

    BUG#28616442 FIX TO BUG#28062382 DRAMATICALLY REDUCES THROUGHPUT WHEN
                 THE BINLOG IS ACTIVE
    
    Eventum 61197 revealed performance regression introduced for DDL intensive
    workloads when [1;31minnodb[m_flush_log_at_trx_commit = 2.
    
    This is because we over-simplified in the previous fix for BUG#28616442.
    
    This patch reverts related part of the previous fix:
    - don't sleep for more than 1ms in log threads (trx=0 case could suffer),
    - when trx=2 and user thread is waiting for flushed redo (ddl), we need
      to first wait for redo written and wake up log_flusher only afterwards
      (otherwise we could wake up log_flusher too early - that was problem).
    
    We also skip disabling spin-delays for two cases:
    - waiting for written redo when trx=1,
    - waiting for flushed redo when trx=2.
    
    We also checked if we could use two-steps waiting always - that is always
    first wait for written redo and only afterwards fallback to wait on
    flushed redo. We can't afford that.
    
    This would result in more CS when busy-waiting is disabled and trx=1
    (it's disabled when total cpu usage is more than the hwm (default 50%)).
    
    However we observed that two-steps waiting increased TPS for workloads
    with small number of connections. We confirmed that it was because of
    increased total time for busy-waiting (two times more spin rounds).
    On the other hand, we can't afford making more spin rounds for higher
    number of connections, because we would risk wasting too much cpu,
    before we reach the hwm which disables the busy waiting.
    
    Because of that we introduced new mechanism:
    1. Use increased spin rounds (compute in range 250k..25k) when
       cpu usage is below 50% of the hwm.
    2. Use default spin rounds (25k) when cpu usage is in 50%..100% of hwm.
    3. Do not use busy waiting when cpu usage is higher than hwm.
    
    Points 2,3 - we behave as it was in the past.
    
    Point 1 - we may use increased number of spin rounds when cpu usage
    is lower than 50% of the hwm.

[33mcommit 6dfc6b7868743f8dc8195cc03f746eaea0f8c90a[m
Author: Allen Lai <zheng.lai@oracle.com>
Date:   Wed Sep 26 18:07:55 2018 +0800

    WL#12300 Automatically configure default redo log file size
    
    Currently, we have 2 system variables for setting redo log file size,
    [1;31minnodb[m_log_file_size(default value = 48M) and [1;31minnodb[m_log_files_in_group(default
    value = 2). And the default values of them are fixed.
    
    Proposal for both variables:
    [1;31minnodb[m_log_file_size:
    
    =IF( C2<8, 0.5, IF( C2<=128, 1, 2 ))
    
    [1;31minnodb[m_log_files_in_group:
    
    =IF( C2<8, ROUND(C2), IF( C2<=128, ROUND(C2 * 0.75), 64 ))
    
    C2 is the auto-tuned buffer pool size in GB.
    
    The ROUND(C2) assumes GB rounding of course there too (i.e if the buffer is
    5.5GB, we√Ç‚Äôd want √Ç‚Äú5√Ç‚Äù there, to try and get between 1 and 64 logs).
    
    Note:This feature is only for [1;31minnodb[m dedicated server, which means it works
    once [1;31minnodb[m_dedicated_server is enabled. So, if user has already set those
    options in my.cnf, we will still take user's configuration. User settings
    override dedicated server settings.
    
    RB: 20162
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>

[33mcommit f8fd692d5bfecfbbed957f113892dbb364f21785[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Sep 24 07:12:56 2018 +0200

    Post push fix:
    Bug#28489407 INNODB: ASSERTION FAILURE: SRV0START.CC:1482:0
    
    Fixed the failing test sys_vars.[1;31minnodb[m_spin_wait_delay_basic

[33mcommit acca7798297dc6786cf16a1afe64800a9d3ec71b[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Mon Aug 27 18:13:47 2018 +0200

    Bug#28489407 INNODB: ASSERTION FAILURE: SRV0START.CC:1482:0
    
    Problem:
    
    Server hits an assert while shutting down if the value of [1;31minnodb[m_spin_wait_delay
    variable is set to a very high value. This happens because while shutting down,
    the server has to wait for all the server threads to shutdown. The server writer
    server threads attempt to acquire exclusive write lock and wait in a loop for
    the lock to be released. This wait time may vary from 0 to the value specified by
    [1;31minnodb[m_spin_wait_delay variable. If this value is large, then the main server may
    hit the assert after waiting for 100 seconds.
    
    Solution:
    
    Defined the maximum value of [1;31minnodb[m_spin_wait_delay to 1000000 microseconds.

[33mcommit 6c7a4722b894aa91743f599f7a45ed00c27182eb[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Wed Sep 12 15:04:02 2018 -0700

    Bug 28622708 - SERVER START FAILS TRYING TO RE-CREATE UNDO TABLESPACES
    
    There was an error in string processing in a function called
    Fil_path::is_undo_tablespace_name() when the undo file name had
    a directory attached to it. This happened because
    --[1;31minnodb[m-directories=x and --[1;31minnodb[m-undo-directory=x/undo.
    So that function was trying to parse "undo/undo001" instead of just
    "undo001" like it normally does.  Since it did not recognize this
    as an undo file, it did not open it at file discovery.  Then later,
    InnoDB would try to create the file, but it was already there.
    
    Along with that bug testing showed that --[1;31minnodb[m-undo-directory
    could be assigned a directory above the datadir.  This would
    would increase the scope of files that are scanned at startup
    duriong file discovery.  A new error is added to prevent the
    undo directory from being an ancestor of the datadir.
    --[1;31minnodb[m-directories can still specify ancestors of the datadir.
    
    Approved by Rahul Agarkar in RB#20523

[33mcommit 7c2cc44e21be2c34a24c1ece11fdf3d16f9f43c9[m
Author: Ragasudha Chillara <ragasudha.chillara@oracle.com>
Date:   Tue Sep 11 17:10:26 2018 +0530

    BUG#91480 AUTOTEST DOWNGRADE TESTS FAILS FOR 8.0
    
    Fixes autotest downgrade failures for 8.0 due to refused TCP connection.
    
    In 8.0 the mysqld has been configured with "skip-grant-tables" which enables another
    server option "skip-networking" causing mysqld server not to listen any TCP connections.
    
    Hence this patch modifies the configuration file by removing "skip-grant-tables".
    
    Additionally, changed tables engine from "myisam" to "[1;31minnodb[m" to cope with the default charset
    changes causing error "1071:Specified key was too long; max key length is 1000 bytes.".
     From 5.7 to 8.0 charset was changed from "latin1" to "utf8mb4". This means that a varchar(255)
    key that took 255 * 1 = 255 bytes in MySQL 5.7 now takes 255 * 4 = 1020 bytes (larger than myisam
    limit of 1000 bytes) in MySQL 8.0

[33mcommit f158a8029672cc3ca2b4699b19b3a1e423b081f8[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Tue Sep 11 12:50:45 2018 +0200

    Bug #27724476   CONTRIBUTION BY FACEBOOK: DETACH INNODB BUFFER POOL BEFORE CREATING CORE
    
    Post-push fix to MTRs which failed when run with --mysqld=--[1;31minnodb[m-page-size=4k.
    The smaller the page size, the larger the core file even if we exclude buffer pool from it.
    This fix sets the threshold for the core file sizes to leave as much margin of error as
    possible, which means in equal distance from the core file size for
      --[1;31minnodb[m-page-size=4k --skip-[1;31minnodb[m-buffer-pool-in-core-file
    and
      --[1;31minnodb[m-page-size=64k --[1;31minnodb[m-buffer-pool-in-core-file
    which are respectively the largest core size without buffer pool, and the smallest core
    size with buffer pool.
    
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit 4624a314a7db0e0ee881101795a1e12adb399540[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Tue Sep 11 12:50:45 2018 +0200

    Bug #27724476   CONTRIBUTION BY FACEBOOK: DETACH INNODB BUFFER POOL BEFORE CREATING CORE
    
    Post-push fix to MTRs which failed when run with --mysqld=--[1;31minnodb[m-page-size=4k.
    The smaller the page size, the larger the core file even if we exclude buffer pool from it.
    This fix sets the threshold for the core file sizes to leave as much margin of error as
    possible, which means in equal distance from the core file size for
      --[1;31minnodb[m-page-size=4k --skip-[1;31minnodb[m-buffer-pool-in-core-file
    and
      --[1;31minnodb[m-page-size=64k --[1;31minnodb[m-buffer-pool-in-core-file
    which are respectively the largest core size without buffer pool, and the smallest core
    size with buffer pool.
    
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit 369624b37d3557017e466b9bd08620e681527931[m
Author: Ragasudha Chillara <ragasudha.chillara@oracle.com>
Date:   Tue Sep 11 15:45:25 2018 +0530

    BUG#91480 AUTOTEST DOWNGRADE TESTS FAILS FOR 8.0
    
    Fixes autotest downgrade failures for 8.0 due to refused TCP connection.
    
    In 8.0 the mysqld has been configured with "skip-grant-tables" which enables another
    server option "skip-networking" causing mysqld server not to listen any TCP connections.
    
    Hence this patch modifies the configuration file by removing "skip-grant-tables".
    
    Additionally, changed tables engine from "myisam" to "[1;31minnodb[m" to cope with the default charset
    changes causing error "1071:Specified key was too long; max key length is 1000 bytes.".
     From 5.7 to 8.0 charset was changed from "latin1" to "utf8mb4". This means that a varchar(255)
    key that took 255 * 1 = 255 bytes in MySQL 5.7 now takes 255 * 4 = 1020 bytes (larger than myisam
    limit of 1000 bytes) in MySQL 8.0.

[33mcommit 2809dd8df525fbe26b7df3d2adb30ebf6b3c9637[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Sun Sep 2 12:49:18 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    One more fix (small).
    
    We missed notification when [1;31minnodb[m_flush_log_at_trx_commit = 1 and
    we switched writer to low-latency mode. Because of that writer was
    waiting 10ms before next write and all user threads were delayed.

[33mcommit 64271f1500e3df80813aaa3ec037d5c4b12a1584[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 13:27:14 2018 +0200

    Bug #27724476   CONTRIBUTION BY FACEBOOK: DETACH INNODB BUFFER POOL BEFORE CREATING CORE
    
    Based on contribution https://bugs.mysql.com/file.php?id=26642&bug_id=90144 from
    Facebook, this patch introduces a new dynamic global system variable
    [1;31minnodb[m_buffer_pool_in_core_file which is ON by default.
    When ON, everything should work in a backward compatible way.
    Setting it to OFF (for example by passing --skip-[1;31minnodb[m-buffer-pool-in-core-file)
    means that the user does not want pages allocated for buffer pool to be included in
    the core file in the event of dumping core.
    If this intention can not be satisfied (for example the OS does not support marking
    pages as MADV_DONTDUMP) then an error will be emitted to the log, as soon as this is
    determined, and a core_file will be set to OFF, so no core file will be created at all
    (which seems to be safer given user's intention).
    
    This patch also:
    - introduces linux-version.inc MTR helper file, which makes it easier to
      skip MTR test on old version of Linux.
    - refactors all_persisted_variables.test, so that it outputs and uses correct numbers of
      system variables, lowering the effort required to keep it in sync with reality
    
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit 891460995137598a6e0ae3684ba1cc6ccd0c3ca3[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 13:27:14 2018 +0200

    Bug #27724476   CONTRIBUTION BY FACEBOOK: DETACH INNODB BUFFER POOL BEFORE CREATING CORE
    
    Based on contribution https://bugs.mysql.com/file.php?id=26642&bug_id=90144 from
    Facebook, this patch introduces a new dynamic global system variable
    [1;31minnodb[m_buffer_pool_in_core_file which is ON by default.
    When ON, everything should work in a backward compatible way.
    Setting it to OFF (for example by passing --skip-[1;31minnodb[m-buffer-pool-in-core-file)
    means that the user does not want pages allocated for buffer pool to be included in
    the core file in the event of dumping core.
    If this intention can not be satisfied (for example the OS does not support marking
    pages as MADV_DONTDUMP) then an error will be emitted to the log, as soon as this is
    determined, and a core_file will be set to OFF, so no core file will be created at all
    (which seems to be safer given user's intention).
    
    This patch also:
    - introduces linux-version.inc MTR helper file, which makes it easier to
      skip MTR test on old version of Linux.
    - refactors all_persisted_variables.test, so that it outputs and uses correct numbers of
      system variables, lowering the effort required to keep it in sync with reality
    
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit cae634fe79e4c19902cd25248b1be24cad2c9a34[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.[1;31minnodb[m_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit 1fa54adb13755947f94b86de1eb428f1bf7f4569[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Aug 29 09:51:30 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.[1;31minnodb[m_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit e923ac56b2b43ecc06ce2b4a45e4e48be1bf13c4[m
Author: Sunny Bains <Sunny.Bains@oracle.com>
Date:   Tue Aug 28 19:04:03 2018 -0700

    WL#11720 - InnoDB: Parallel read of index
    
    Search for sub tree from the root node and then traverse the leaf nodes in
    each sub-tree in parallel. Invoke the specified callback for each record found..
    
    Currently only non-locking SELECT COUNT(*) is supported and the second
    phase of CHECK TABLE ...;
    
    New configuration variables: --[1;31minnodb[m-parallel-read-reads=1..256.
    
    Reviewed by Jimmy and Darshan
    RB#18821

[33mcommit dbfc59ffaf8096a2dc5b76766fedb45ff2fb8cbf[m
Author: Sunny Bains <Sunny.Bains@oracle.com>
Date:   Tue Aug 28 19:04:03 2018 -0700

    WL#11720 - InnoDB: Parallel read of index
    
    Search for sub tree from the root node and then traverse the leaf nodes in
    each sub-tree in parallel. Invoke the specified callback for each record found..
    
    Currently only non-locking SELECT COUNT(*) is supported and the second
    phase of CHECK TABLE ...;
    
    New configuration variables: --[1;31minnodb[m-parallel-read-reads=1..256.
    
    Reviewed by Jimmy and Darshan
    RB#18821

[33mcommit ddf2ce815c5ac0fcc20d28d0b0a62144bcb94163[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Thu Aug 23 16:04:24 2018 +0200

    Bug #28176910   PERFORMANCE_SCHEMA.DATA_LOCKS.OBJECT_INSTANCE_BEGIN CHANGES DEPENDING ON ORDER
    
    The main problem fixed by this patch is that engine_lock_id column of
    performance_schema.data_locks was not unique, yet it was treated as such
    in our code, documentation and reported CREATE TABLE.
    
    The old format of engine_lock_id was:
    trx_id:table_id for LOCK_TABLE
    trx_id:space_id:page_no:heap_no for LOCK_REC
    
    which happens to be not really unique, as a single trx can hold multiple
    locks on the same table, or same row, as long as they are in different modes,
    say one is LOCK_S and the other is LOCK_X.
    
    This patch fixes this by using a new format for engine_lock_id:
    
    trx_immutable_id:table_id:lock_immutable_id for LOCK_TABLE
    trx_immutable_id:space_id:page_no:heap_no:lock_immutable_id for LOCK_REC
    
    The newly introduced lock_immutable_id and trx_immutable_id are uint64_t
    which are guaranteed not to change over a lifetime of trx and lock
    respectively and also to be unique among other such objects currently in the
    system. This is in contrast to trx->id which changes over time (when transaction
    switches from read-only to read-write the trx->id changes from 0 to non-zero).
    The reason engine_lock_id contains not only lock_immutable_id but also the
    prefix, is that we need not only to identify a lock, but also be able to find
    it in memory in a safe way, and this is easy to accomplish if we can first
    locate the transaction which owns that lock (URI vs. URL).
    
    This patch affects not only performance_schema.data_locks, but also other
    tables which store engine_lock_id:
    - performance_schema.data_lock_wait.requesting_engine_lock_id
    - performance_schema.data_lock_wait.blocking_engine_lock_id
    - information_schema.[1;31minnodb[m_trx.trx_requested_lock_id
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>

[33mcommit 977fc11a736bad80ef7ea305abe84d60db5b63f2[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Wed Aug 22 11:12:47 2018 -0700

    WL9508 - Another Post-change in order to reduce the number of failures in some
    [1;31minnodb[m_undo tests.

[33mcommit a4ad34a6b38b1985c2ce8229b7149109b894d0cb[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Jul 26 11:42:47 2018 -0600

    WL9508 - Post-Fix
    Several tests fail periodically in PB2. They almost always have to do
    with waiting too long for the purge thread to truncate an inactive_explicit
    undo space and make it empty.
    
    The function [1;31minnodb[m_alter_undo_tablespace_inactive() is refactored here
    to be more directed in what it does. In addition, the rule that the second
    undo space can be made inactive is now changed to the third.  In other words,
    there must always be 2 active undo spaces.  The third one may be explicitly
    set empty. The reason is that while the third is being set inactive, one of
    the other two can be implicitly truncated.  That would leave one more undo
    space to use for new transactions.

[33mcommit 2911f214b533dc1dd140e6154ec39b5a0954f19a[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Jul 23 22:42:30 2018 -0700

    WL9508 - Post Fix.  Tests [1;31minnodb[m_undo.trunc_multi_client_01 &
    [1;31minnodb[m_undo.trunc_multi_client_02 were failing for page sizes 32k and 64k.
    A slow shutdown stalled with continuous undo truncation because
    undo::needs_truncation() constantly returned true.  The initial size
    of an undo space with those page sizes is larger than 10Mb.
    Used ut_max(to make sure the size must be larger than the initial size.

[33mcommit 971d944474bdebc345232688534d92fad40d7f6b[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Jul 19 11:02:55 2018 -0700

    WL9508 Post-fix
    [1;31minnodb[m.poratability_basic sporadically fails in daily_trunk with a timeout
    waiting for an undo tablespace to become empty.  Most of these waits
    override the default 30 seconds with 300 in other tests that use explicit
    undo tablespaces in wl9508.  Found 2 other tests that also were missing
    the longer wait_timeout.
    Fixed 5 Windows compile warnings in clone code.

[33mcommit 1400d3d95042645998ac9b67bb0a60376b0cfc7f[m
Merge: 87b94a2fe0b 3d240baa7b6
Author: Sachin Agarwal <sachin.z.agarwal@oracle.com>
Date:   Wed Jul 18 18:36:02 2018 +0530

    Bug #27753193  ASSERTION `PREBUILT->TRX->ERROR_KEY_NUM <
    HA_ALTER_INFO->KEY_COUNT'
    Merge branch 'mysql-5.7' into mysql-8.0
    Post push fix - adding missing patch & update [1;31minnodb[m-alter-debug.test.

[33mcommit 36b7de0b6138ccd53c8e92f9ad5ad92b199907be[m
Author: Aakanksha Verma <aakanksha.verma@oracle.com>
Date:   Tue May 29 18:11:37 2018 +0530

    BUG#26225783 MYSQL CRASH ON CREATE TABLE (REPRODUCEABLE) -> INNODB: A
        LONG SEMAPHORE WAIT
    
    Post Push fix for test case failure [1;31minnodb[m_gis.rtree_rollback1 on
    weekly-5.7.
    
    Reviewed by : Debarun Banerjee<debarun.banerjee@oracle.com> over IM.
    
    (cherry picked from commit ec6f970e5f1cdf1ea03586f2f30e3f71b133d412)

[33mcommit b67f746e31e4f21bb46970021b45a6bd77fbd0e4[m
Author: Aakanksha Verma <aakanksha.verma@oracle.com>
Date:   Tue May 29 18:11:37 2018 +0530

    BUG#26225783 MYSQL CRASH ON CREATE TABLE (REPRODUCEABLE) -> INNODB: A
        LONG SEMAPHORE WAIT
    
    Post Push fix for test case failure [1;31minnodb[m_gis.rtree_rollback1 on
    weekly-5.7.
    
    Reviewed by : Debarun Banerjee<debarun.banerjee@oracle.com> over IM.
    
    (cherry picked from commit ec6f970e5f1cdf1ea03586f2f30e3f71b133d412)
For keyword delayed:
[33mcommit 870434552974473c6d03cc1e0f309794e270db17[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:51:19 2018 +0100

    WL#11722
    First preparatory step to optimise scans and particularly
    longer scans with many rows checked but not sent back to
    the MySQL server.
    1) Removed a number of jam's, replaced most by jamDebug and
       added a few jamDebug's
    2) Thoroughly investigated all real-time breaks for scans and
       key operations. Added a long comment describing all of those
       real-time breaks and when they happen.
    3) Introduced a preparatory phase after returning after each
       real-time break. This preparatory phase sets up all pointers
       in TUP and LQH. Later we will add also preparatory phases for
       the scanning block (TUX, TUP and ACC).
    4) An important step is to prepare for longer scan executions.
       To handle this we must unwind the stack after executing one
       row scan. To clarify this we added a number of returns in a
       number of places to ensure we get tail-call optimisations as well
       as ensuring that it is clear that it is intended to return.
    5) Made the code a bit more readable by placing { on new line.
    6) Added many more likely/unlikely for optimisation of the code.
    7) Added a real-time break to handle restarts of scan from
       queue on fragments.
    8) Prepared code in TUPKEYREQ for optimised read row handling.
    9) Optimised TUPKEYREF for scan handling, more optimised handling
       of scans in TUPKEYCONF.
    10)Made sure that new (and old) block pointers are overwritten
       with each new signal executed (in debug mode) for faster
       bug finding in this area.
    11)Removed a number of parameters to call that weren't needed
    12)Split prepareTUPKEYREQ into prepare_table_pointers and
       prepare_scanTUPKEYREQ to avoid initialising fragment and
       table pointer on each new row.
    13)Introduced new block variable prim_tab_fragptr to point at
       fragment record for primary table. The scan block scans on
       the index table, but reads from the primary table, so needed
       quick access to this at times as well.
    14)Clarified the [1;31mdelayed[m signal to be used when rows are locked

[33mcommit 2809dd8df525fbe26b7df3d2adb30ebf6b3c9637[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Sun Sep 2 12:49:18 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    One more fix (small).
    
    We missed notification when innodb_flush_log_at_trx_commit = 1 and
    we switched writer to low-latency mode. Because of that writer was
    waiting 10ms before next write and all user threads were [1;31mdelayed[m.

[33mcommit c62a35421a31c28f8ac371e4f66f98e47ceade6a[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Wed Jun 20 15:59:08 2018 +0530

    Bug #26574003 : NODE CRASH WITH ERROR 2335 DUE TO TOO MANY DROP_TRIG_IMPL_REQ IN SQ FROM SUMA
    
    When SUMA receives the SUB_STOP_REQ signal, it executes it and sends
    back the SUB_STOP_CONF. The SUB_STOP_CONF ultimately is relayed back to
    the API and the API is now open to send more SUB_STOP_REQs.
    
    The SUMA, only after sending the SUB_STOP_CONF, asynchronously decides
    to drop the subscription altogether if no subscribers are present. This
    drop involves sending multiple DROP_TRIG_IMPL_REQs to DBTUP. With the
    way the DbtupProxy is implemented, it can handle 21 DROP_TRIG_IMPL_REQs
    in parallel. Any more subsequent requests are queued in the DbtupProxy
    thread short time queue. If the execution of the DROP_TRIG_IMPL_REQ is
    [1;31mdelayed[m, there is a chance where the short time queue gets overflowed
    with too many DROP_TRIG_IMPL_REQs and crashes the data node with a
    "Error in short time queue" error.
    
    A similar scenario exists in SUMA node failure handling, where a node
    failure can cause a lot of subscriptions to stop and can end up
    overflowing the DbtupProxy with DROP_TRIG_IMPL_REQ request. This
    possible overflow during node failure handling is already guarded
    against by limiting the maximum DROP_TRIG_IMPL_REQs allowed to execute
    parallely. But the SUB_STOP_REQ handling lacks such a guard.
    
    This patch fixes that by delaying the execution of the SUB_STOP_REQ when
    there are a number of DROP_TRIG_IMPL_REQs outstanding.
    As a result,
     i.  This prevents SUB_STOP_REQ from adding 3x DROP_TRIG_IMPL_REQ to the
         DbtupProxy thread short time queue, at the cost of 1x SUB_STOP_REQ
         in the SUMA thread short time queue.
     ii. This delays sending SUB_STOP_CONF, which will stall any further
         request from the same API source, as API sources are usually
         synchronous. If multiple APIs are separately submitting
         SUB_STOP_REQs to SUMA, and are causing different subricptions to
         become STOPPED, then they will also be queued at SUMA when the
         number of DROP_TRIG_IMPL_REQs are higher than the threshold.
    
    The threshold, maximum number of DROP_TRIG_IMPL_REQs that can be
    executed parallely, is set to 6. Given that each SUB_STOP_REQ might
    send 3x DROP_TRIG_IMPL_REQ to DbtupProxy and the maximum threshold set
    at 6, the number of subscriptions that can be dropped in parallel within
    a data node at a time is limited to 2.
    
    Change-Id: I8879eb00af3c739b7774af5a9162427b3a40bfdd
For keyword dsync:
For keyword direct:
[33mcommit ca94b993454c86be248fbe180db94647488114e9[m
Author: Bjorn Munch <bjorn.munch@oracle.com>
Date:   Fri Jan 25 23:31:34 2019 +0100

    BUG#29249542 - CANNOT SETUP REPLICATION WITHOUT IPV6 SINCE 8.0.14
    
    Problem
    =========================
    Since Mysql server 8.0.14, we cannot setup a replication group using
    mysql-shell (and probably by using mysql client).
    
    The server fails with these messages (note the "cannot use port xxx"
    and the "adding ipv6 localhost to whitelist" even though no ipv6 is
    available)
    
    Analysis and Fix
    =========================
    When adding ipv6.disable=1 to a into a boot configuration, we are left
    without kernel support for IPv6. As such, we can't have calls that
    use [1;31mdirect[mly AF_INET6 or "::1". Remove all hardcoded AF_INET6
    assumptions.
    
    ReviewBoard: 21337

[33mcommit 06b6a83958f3c58b7c800402491e7bedbfb42f71[m
Author: Jakub ≈Åopusza≈Ñski <jakub.lopuszanski@oracle.com>
Date:   Wed Nov 21 10:33:28 2018 +0100

    Bug #28860795   ERRMSG-UTF8.TXT USES WRONG FORMAT SPECIFIERS
    
    The format specifiers used in errmsg-utf8.txt did not correspond correctly to actual types passed to ib::info, ib::warn, ib:error and ib::fatal.
    This issue was not very visible because the format strings are not available to compiler at compile time, types of integers dependent on many ifdef/typedefs which were platform specific, and the issue manifested only for really large numbers on certain platforms.
    This patch:
    - adds a run-time verification of datatypes in debug build
    - fixes places where format specifier were wrong
    - fixes some obvious typeos in some error messages
    - fix for one place where string was not null terminated
    - an MTR for the above fix
    
    This patch also observes quite simple rules, which prevent typing problems:
    
    1. When calling ib::info/warn/error/fatal pass arguments which have types which [1;31mdirect[mly correspond to format specifiers:
    
    %lld  - long long int (a.k.a. longlong)
    %llu, %llx - unsigned long long int (a.k.a. ulonglong)
    %ld - long int (a.k.a. long)
    %lu, %lx, %lX - unsigned long int (a.k.a. ulong)
    %lf - double
    %d, %i - int
    %u, %x, %X - unsigned int (a.k.a. uint)
    %f - float
    %c - char
    %p - a pointer
    %s - char *, char const *, char[N], or any other thing which points to array of chars
    %zu, %zx - size_t
    %zd - ssize_t
    
    2. When the data you want to print is not guaranteed to have one of the types listed above, convert it using an initializer list:
    
            ulonglong{start_lsn}i
    
    as oposed to using static_cast<ulonglong>, this has an advantage that compiler will warn you if on your platform the destination type can not hold the source type (a.k.a. narrowing).
    
    Reviewed-by: Sunny Bains <Sunny.Bains@oracle.com>

[33mcommit fd0abb16247e70c9edb58ebed7e300e278e5f743[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Tue Oct 16 17:34:13 2018 +0530

    BUG#28022129: NOW() DOESN'T HONOR NO_ZERO_DATE SQL_MODE
    
    Analysis
    ========
    CREATE TABLE..SELECT generates zero date as default value for
    temporal columns in STRICT, NO_ZERO_DATE mode when the columns
    are not based on source table columns [1;31mdirect[mly.
    
    CREATE TABLE..SELECT generates implicit default values for columns
    without a default value which in case of temporal columns contains
    zero date component. The implict defaults generated are marked as
    explicit default value (because the flag 'NO_DEFAULT_VALUE_FLAG'
    is not set), thus skipping the checks which validate the default
    value based on the SQL mode. Hence tables with zero date are
    created in strict mode.
    
    Fix
    ===
    The function "create_table_from_items()" has been modified to
    mark columns having date component which are based on expressions
    and not source table columns [1;31mdirect[mly as having no default value
    in strict mode i.e 'NO_DEFAULT_VALUE_FLAG' flag is set in such
    cases. This ensures that tables are created without zero date
    as default value.

[33mcommit 7354cce6d4d64463f68372698a65a94f4983e55f[m
Author: Rahul Agarkar <rahul.agarkar@oracle.com>
Date:   Thu Nov 1 13:04:03 2018 +0530

    Bug #28598943 TEMPTABLE STORAGE ENGINE DOES NOT HONOUR THE TMPDIR OPTION
    
    Problem:
    MySQL uses temptable engine to create temporary files that it needs. The
    function that creates the temporary files attempts to use the
    environment variable tmpdir on Linux and TEMP on Windows to find the
    location of the temporary [1;31mdirect[mory, if a temp [1;31mdirect[mory path is not
    already provided. If the environment variable tmpdir is not set, then by
    default it picks up /tmp as the temp [1;31mdirect[mory location.
    While attempting to create the temporary file, mysql does not pass any
    predefined temporary [1;31mdirect[mory path and since the environment "tmpdir"
    is also not set, all the temporary files end up in /tmp instead of the
    temp [1;31mdirect[mory mentioned by the startup variable --tmpdir
    
    Solution:
    Pass the value of --tmpdir to the function creating the temp file.
    Since, the parameter specifying the temp [1;31mdirect[mory is not empty, it does
    not try to look for the environment variable "tmpdir" and hence all the
    temp files are created in the [1;31mdirect[mory pointed by "tmpdir".
    
    RB: 20843
    Reviewed By: Kevin Lewis (kevin.lewis@oracle.com)

[33mcommit e5b96670ed2aa075bab2eb33c6e5d675100979dd[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Fri Oct 12 10:58:35 2018 +0200

    BUG#28706307: CHARACTER SET OF ENUM DATA TYPE IS NOT AVAILABLE AS PART OF OPTIONAL METADATA
    BUG#28774144: MYSQLBINLOG --PRINT-TABLE-METADATA SHOW WRONG INFO ABOUT DEFAULT CHARSET
    
    Problems:
    
    BUG#28706307: When using binlog_row_metadata=FULL, the character set
    for ENUM and SET was missing in the Table_map_log_event.
    
    BUG#28774144: When mysqlbinlog --print-row-metadata prints information
    about character sets for character columns, it sometimes incorrectly
    represented the most frequently used character set as the default
    character set for the table.  But the default character set is a table
    attribute that is unrelated to the frequency of the character set
    among the columns.
    
    Fix:
    
    - Added character set metadata for ENUM and SET types.  These appear in
      a new field, not the same field as the character sets for character
      types.  It has the same format as the fields for character sets for
      character types, but includes a different set of columns.  It
      uses new type codes for this.  The field appears before the field
      for ENUM and SET string values, so that a decoder can convert
      charset [1;31mdirect[mly when reading the strings, if needed.
    
      This includes the following code changes:
      - All changes in libbinlogevents.
      - New inner classes in Table_map_log_event, used to iterate over
        character sets when mysqlbinlog prints metadata. These encapsulate
        the logic for iterating, and allows us to reuse the same logic
        when iterating over character set information for enum and set
        columns as we use for iterating over character set information for
        character columns.
      - changes in Table_map_log_event::init_*, to capture the character
        set for enum and set columns
      - moved the printing of character sets until after the printing of
        SET and ENUM string values.
    
    - Don't print character sets for columns as default characters. Just
      print the character set for every column.
    
      This includes the following code changes in
      Table_map_log_event::print_columns:
      - Remove the declaration, initialization, and check related to the
        is_default_cs variable
      - Remove the my_b_printf(... DEFAULT CHARSET ...) code that occurred
        after the main loop.
    
    - Bonus: added newline between column definitions in the output from
      mysqlbinlog.
    
      This accounts for the majority of the changes in the
      test's .result file, and the addition of "\n#         " in a string
      in Table_map_log_event::print_columns.
    
    - Bonus: made mysqlbinlog escape backtick identifiers in identifier
      names.
    
      This accounts for changes in log_event.cc:pretty_print_str. It may
      affect some other tests - will run Jenkins and update the patch
      accordingly.
    
    - Added a test case to cover also non-ascii and escape characters
      occurring in column names.

[33mcommit cc63b5143cae267f579f83e1ee4020648f593e85[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Oct 8 15:23:12 2018 -0700

    28726286 [INNODB] SEMAPHORE WAIT HAS LASTED > 903 SECONDS during RQG test
    
    1) All undo tablespaces became inactive. The code in
    innodb_alter_undo_tablespace_inactive() that is supposed to keep at least
    2 undo spaces active at all times did not work when there are multiple
    undo spaces marked inactive implicit.  It assumed there was only one.
    This change takes away that assumption and only counts the
    innactive_implicit space that is marked.
    
    2) In this hang, half of the undo spaces were marked inactive_implicit.
    The purge thread only marks one of those at a time.  The others are
    explicitly SET INACTIVE and then SET ACTIVE again before they became empty.
    This changes the alter_active() function to check if the inactive_explicit
    space has been marked. If so, it is set to inactive_implicit. If not
    marked yet, it is set [1;31mdirect[mly to active.
    
    Approved by Rahul in RB#20682

[33mcommit 070c307d74ce618e67e23303b9a1f4a84d1e53d8[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Tue Oct 16 14:33:22 2018 -0700

    Bug 28684645: DROP UNDO TABLESPACE AFTER RECOVERY CRASHES
    
    Another problem that shows a similar call stack.  If a database is
    bootstrapped with a remote --innodb-undo-[1;31mdirect[mory, and a new explicit
    undo space is created in the datadir using an absolute path, then the
    undo::Tablespace::set_file_name() routine did not create the file_name
    correctly.
    After restart, the scan found the file in the datadir and called it
    "./x2.ibu".  undo::Tablespace::set_file_name() did not expect a relative
    path unless the datadir and the undo dr were the same.  So this code
    prepended the remote undo dir to "./x2.ibu".

[33mcommit 8b0ada5eb01798d20f3288b6cf07b299dd0d7545[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177 MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could sometimes fail due to missing characterset comments.
    
    The root cause was the regression test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source [1;31mdirect[mory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the regression test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different [1;31mdirect[mory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    See also duplicate
    
    Bug#19372763 SPORADIC FAILURE IN MAIN.INTERNAL_TMP_DISK_STORAGE_ENGINE
    
    Backport patch from 8.0.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit f2adb0437e6c38a9b35ddf869ab2f067d1af57aa[m
Author: Sivert Sorumgard <sivert.sorumgaard@oracle.com>
Date:   Mon Mar 12 13:56:12 2018 +0100

    Bug#27309072: Need a stable api for handling dd::properties.
    
    This patch does some refactoring of the dd::Properties interface and
    implementation, and introduces a way of defining which keys are
    valid for a given Property object. Upon the definition of such a set,
    the validity of the keys will be checked upon setting or getting the
    value of the key.
    
    In more detail, the patch implements the following:
    
    1.  Change iterator type names to conform to STL, thus allowing
        e.g. range based loops.
    2.  Remove the type specific get()/set() functions and introduce
        templates and overloads instead.
    3.  Make all get() variants work in the same way regarding non-existing
        keys: Assert in debug builds, return 'true' in non-debug builds.
        Previously, the behavior was not uniform.
    4.  Remove the value() methods, support only get() instead.
    5.  Embed the std::map into the Properties_impl class [1;31mdirect[mly without
        using a unique_ptr.
    6.  Add a std::set to Properties_impl for storing valid key names.
    7.  When calling the set() and get() variants, verify that the submitted
        key is valid, unless the set of valid keys is empty. If the key is
        invalid, write a warning to the error log, assert in debug builds,
        return 'true' to the caller in release builds. Note that an error is
        not reported here, just written to the error log. It is up to the
        caller to decide how to handle this.
    8.  Filter out any invalid keys when generating a raw string. This might
        be relevant in e.g. an upgrade/downgrade situation.
    9.  Filter out invalid keys when assignning from another property object
        or from a raw string, using the copy_values() methods. This is done
        silently without error reporting or asserts, since it is likely to
        happen e.g. when reading persistent meta data in an upgrade/downgrade
        situation. Note that copy_values() will copy only the key-value pairs,
        the set of valid keys in the source object are not copied.
    10. In the DD objects using Properties, we now embed the property objects
        as proper data members instead of using a unique_ptr. This also makes
        object cloning simpler.
    11. There is no custom copy constructor, the default will apply, and this
        will copy both the set of valid keys and the key-value pairs
        themselves.
    
    Sets of valid keys are added for the Properties that are currently used
    by the DD object classes, i.e.:
    
        dd::Abstract_table_impl::m_options
        dd::Column_impl::m_options
        dd::Column_impl::m_se_private_data
        dd::Index_impl::m_options
        dd::Index_impl::m_se_private_data
        dd::Parameter_impl::m_options
        dd::Partition_impl::m_options
        dd::Partition_impl::m_se_private_data
        dd::Partition_index_impl::m_se_private_data
        dd::Tablespace_impl::m_options
        dd::Tablespace_impl::m_se_private_data
        dd::Table_impl::m_se_private_data
    
    The infrastructure implemented in this patch also allows similar sets of
    valid keys to be collected and defined for other classes making use of
    the dd::Properties implementation.
    
    Change-Id: I9c80cc3cbea1fb00bf5b96be77b8ebf471954f23

[33mcommit 60a6ab982ace6c1c40d91634779c38209ea89658[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 3 09:52:41 2018 +0200

    Bug#28737177: MAIN.INFORMATION_SCHEMA_CS FAILS WITH RESULT CONTENT MISMATCH
    
    The problem was that main.information_schema_cs (and a handful of other tests)
    could sometimes fail due to missing characterset comments.
    
    The root cause was the regression test for Bug#16204175 in main.ctype_uca.
    This test temporarily replaced share/charset/Index.xml in the source [1;31mdirect[mory
    with its own file. If this happened to coincide with other tests depending
    on the original contents of Index.xml, those tests would fail.
    
    This patch fixes the problem by rewriting the regression test for Bug#16204175
    such that it leaves Index.xml intact and instead restarts the server with
    --character-sets-dir pointing to a different [1;31mdirect[mory. This is similar to
    what is already done in e.g. main.invalid_collation.
    
    Change-Id: Ic1eef7b8f448ecf4620d77123e247f106fcd328a

[33mcommit f6b0dfe2d56a1f5fff5bc6f8c1d1efa5774a00c2[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Wed Sep 5 16:58:36 2018 +0100

    WL#10957: Binary log encryption at rest (Step 3)
    
    Made mysqlbinlog to state that it cannot read encrypted log files.
    
    @ sql/binlog_istream.{h|cc}
    
      Added a new Error_type to Binlog_read_error specific to mysqlbinlog
      client program.
    
    @ sql/binlog_istream.cc
    
      Instead of trying to deserialize the encrypted reader, mysqlbinlog is
      now reporting it cannot read encrypted binary log files [1;31mdirect[mly.

[33mcommit 9537c7d2b61295a05a1b2978ba520e673d778077[m
Author: Venkatesh Venugopal <venkatesh.venugopal@oracle.com>
Date:   Fri Sep 21 12:44:21 2018 +0530

    Bug #28284624 `PURGE BINARY LOGS TO` FAILS WHEN BINARY LOG
                  SPECIFIED IS NOT IN BINLOG BASE DIR
    
    Problem
    -------
    `PURGE BINARY LOGS TO` fails if binary log is not present in
    the data [1;31mdirect[mory.
    
    Analysis
    --------
    When the binary logs are being moved to another location by
    mysqlbinlogmove, the index file is also updated with the
    absolute paths of the binary logs.
    
    When the server is executing `PURGE BINARY LOGS TO` command,
    it ends up in comparing the relative path of the 'to_log'
    against the absolute path in the index file instead of
    comparing the basenames of the files.
    
    Fix
    ---
    Added a wrapper function to compare the log names.

[33mcommit b1291457b2998bdc123c867cac3e3fc93755e337[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Sep 20 14:24:11 2018 +0200

    Bug#28649178: MAKE ITEM::STR_VALUE PROTECTED
    
    Post-push fix: Fix test failure in ndb.ndb_condition_pushdown
    
    Ndb_item::get_val() relied on Item::str_value containing nullptr after
    save_in_field() was called on an Item_cache_str object. After get_val()
    was changed to call val_str() instead of inspecting Item::str_value [1;31mdirect[mly,
    it saw the string value contained in the Item_cache_str object instead of
    nullptr, which caused the test to fail. This patch fixes the problem by
    calling val_str() only for Item::STRING_ITEM.
    
    Change-Id: I650a318272293cd68dc15b052ba42e64e198dc03

[33mcommit c27e6c736957a0febbf591e6cdd51affcb7bc415[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Sep 13 15:20:26 2018 -0700

    BUG#28516776 NDBIMPORT REMOVE LOGX DEFINES
    
    One problem with the macro names log1(), log2() etc. is that log2
    may also be the name of a math function.
    
    In this patch, log1(msg) and log2(msg) are universally replaced with
    log_debug(1, msg) and log_debug(2, msg). This prevents any conflict
    with a math library, and removes one level of preprocessor in[1;31mdirect[mion.
    
    log3(msg), which is conditionally defined at compile-time, is changed
    to log_debug_3(msg).
    
    log4(msg), which is not used in the code, is removed.

[33mcommit 2eb9b747f18d0fae5d3af360683c657256abc75a[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Sep 6 10:23:01 2018 +0200

    Bug#28649178: MAKE ITEM::STR_VALUE PROTECTED
    
    Currently Item::str_value is public and is read and modified by non-Item
    classes. Yet not all Item subclasses use str_value but instead have their
    own Strings. This can give unexpected results if str_value is accessed
    for Item subclasses which does not use it. Also, having it public makes
    future refactorings (e.g. changing how Item state is stored) more difficult.
    
    This patch makes Item::str_value protected so that it can no
    longer be accessed [1;31mdirect[mly by other classes.
    
    Change-Id: I660a35b0a4966ba0b8cc04efcc225c9eac7ce6a0

[33mcommit 6c7a4722b894aa91743f599f7a45ed00c27182eb[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Wed Sep 12 15:04:02 2018 -0700

    Bug 28622708 - SERVER START FAILS TRYING TO RE-CREATE UNDO TABLESPACES
    
    There was an error in string processing in a function called
    Fil_path::is_undo_tablespace_name() when the undo file name had
    a [1;31mdirect[mory attached to it. This happened because
    --innodb-[1;31mdirect[mories=x and --innodb-undo-[1;31mdirect[mory=x/undo.
    So that function was trying to parse "undo/undo001" instead of just
    "undo001" like it normally does.  Since it did not recognize this
    as an undo file, it did not open it at file discovery.  Then later,
    InnoDB would try to create the file, but it was already there.
    
    Along with that bug testing showed that --innodb-undo-[1;31mdirect[mory
    could be assigned a [1;31mdirect[mory above the datadir.  This would
    would increase the scope of files that are scanned at startup
    duriong file discovery.  A new error is added to prevent the
    undo [1;31mdirect[mory from being an ancestor of the datadir.
    --innodb-[1;31mdirect[mories can still specify ancestors of the datadir.
    
    Approved by Rahul Agarkar in RB#20523

[33mcommit 8ab83f64681f421fc36840927e6a012c0a941982[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Sep 11 09:53:45 2018 +0200

    Bug#28629132: MTR SHOULD RUN UNIT TESTS IN PARALLEL
    
    Before this patch, MTR did not run unit tests in parallel.
    This is inefficient as the number of unit tests continues to
    grow.
    
    Fix this by having MTR set CTEST_PARALLEL_LEVEL to the same
    number of threads used for running MTR tests. This makes ctest
    run unit tests in parallel instead of single threaded.
    
    Note that this only affects unit tests run from MTR.
    Pushbuild currenly runs ctest [1;31mdirect[mly and is thus not
    affected by this patch.
    
    Change-Id: I29e0a4c071085183f9f5e4d9f9301eaf6ded0543

[33mcommit 4b624bb1582cce19c6caab1f3f56267781e5e747[m
Author: Grzegorz Szwarc <grzegorz.szwarc@oracle.com>
Date:   Fri Aug 24 16:43:35 2018 +0200

    Bug #28351540: BUNDLED PROTOBUF IS OUTDATED
    
    Description:
    ------------
    MySQL Server source includes protobuf source code in 'extra/protobuf'
    folder. The bundle is set to version 2.6.1, still it doesn't build
    properly with ubsan and other sanitizers.
    The sanitizer issues are fixed in pb 3.0+, thus as workaround server
    is build with system protobuf. To fix the issue, the bundle should be
    updated to one of recent versions.
    
    Fix:
    ----
    The bundle updated to version 3.6.1.
    
    - adding extra/protobuf/protobuf-3.6.1
    - removing extra/protobuf/protobuf-2.6.1
    - adjusting configuration for new version of protobuf
    - remove [1;31mdirect[mories and files
       protobuf-3.6.1/benchmarks/
       protobuf-3.6.1/conformance/
       protobuf-3.6.1/editors/
       protobuf-3.6.1/examples/
       protobuf-3.6.1/objectivec/
       protobuf-3.6.1/third_party/
       protobuf-3.6.1/python/
       protobuf-3.6.1/util/
       protobuf-3.6.1/ar-lib
       protobuf-3.6.1/test-driver
    - disable configuration check of cmake's mimimum version
    - disable configuration zlib check
    - disable installation script
    - fix minor compilation issues on SunPro 5.14.0
    - fix minor issues to satisfy UBSAN
    
    Change-Id: Ie5532bacc0fb62d1ba4d524b31cbfd5fc5bb5e7d

[33mcommit edbae5ae22eb826733ef5d814afe0bb7e3f61e00[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Tue Mar 13 18:37:28 2018 +0100

    WL#11722: Step 23
    -----------------
    Rewrote the scanCheck function. This function performs the end of
    range check. It has the end boundary that is static for the
    scan operation and compares this to each key that is to be reported
    back. The compare operation previously used functions in NdbPack.cpp
    that built up a lot of context again and again.
    
    The new compare function is very simple. It builds up an array of
    objects that have two parts:
    1) m_data_ptr: Pointing to the column value
    2) m_data_len: The length of the column value
    NULL is represented by length equal to 0.
    For the end of range boundary this is calculated by first unacpking the
    boundary and then using the NdbPack::Desc and NdbPack::BoundC objects
    to create a new NdbPack::BoundArray. The above array of representations
    of the bound object is placed in a new NdbPack::DataArray object.
    
    The key is read into an array with Attrinfo setup. The Attrinfo is
    kept and a NdbPack::DataArray is created that points to data in
    the Attrinfo object. This method is very quick and only consumes
    20-30 instructions for a simple boundary with 1 column in the
    bound condition. This mapping is done in the new function
    NdbPack::DataArray::init_poai (poai stands for plain old attribut
    info).
    
    In addition the readKeyAttrsCurr function was dropped and scanCheck
    calls tuxReadAttrsCurr [1;31mdirect[mly from scanCheck.
    
    This change improved performance by 25% in a scan filtering
    benchmark.

[33mcommit 5266096589af64ab0ba16527c66a4beda4497b90[m
Author: Mikael Ronstr√∂m <mikael.ronstrom@oracle.com>
Date:   Fri Feb 23 17:55:10 2018 +0100

    WL#11722
    Reorganised some functions in Dblqh to make code more readable.
    
    Increase the maximum scan [1;31mdirect[m count. Counted normal rows
    reported back as 2 and those that only report that condition
    was not met will only count as 1 row. Increased max scan
    [1;31mdirect[m count to 16 and default from 6 to 16.
    
    Made the amount of rows scanned in one real-time break adaptive.
    If the load on JBB level is low we are allowed to execute a bit
    more compared to otherwise.
    
    More work on preparing code for next step of scan optimisations.

[33mcommit 2680ef35fadeb539bdbde09a7e884cf5d1e626bd[m
Author: Pawel Mroszczyk <pawel.mroszczyk@oracle.com>
Date:   Wed Aug 29 07:46:35 2018 -0400

    [Router] Post-WL#10799 fix: Refactored tests; fixed create_config_file() leaving temp [1;31mdirect[mories lingering around
    
    Refactoring of Router's test suite. Notable changes:
    - component tests: fixed uses of create_config_file() leaving temp [1;31mdirect[mories lingering around
    - component tests: moved common keyring init code to a new function RouterComponentTest::init_keyring()
    - component tests: renamed RouterComponentTest::SetUp() -> RouterComponentTest::init()
    - server mock: renamed threads to ease debugging

[33mcommit 18e40d3a3c9607b27b0570fc1ca95dae9a88c9be[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Mar 7 08:35:27 2018 -0800

    WL#10665 Remove fixed-length copying of FRM data from kernel and API
    
    In NdbDictionaryImpl on NDB API side, the FRM data is copied [1;31mdirect[mly
    between signal and NdbTableImpl. On the kernel side in DbDict,
    in packTableIntoPages(), Rope-to-SimpleProperties copying is used
    to write data from TableRecord into signal. In handleTabInfoInit(),
    a new temporary structure called TabInfoLongData holds a LocalRope
    containing FRM data from the signal, which is later assigned to
    frmData in the TableRecord.

[33mcommit d9a28df7cca96a499ad0a8e4f4a03f931e272d67[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Mar 5 10:49:35 2018 -0800

    wl#10665 Add SimpleProperties feature to pack and unpack in[1;31mdirect[m values
    
    Define SimpleProperties callback function types In[1;31mdirect[mReader and In[1;31mdirect[mWriter.
    
    pack() now takes an optional In[1;31mdirect[mWriter and void pointer.
    unpack() now takes an optional In[1;31mdirect[mReader and void pointer.
    
    Define a flag value ExternalData in SP2StructMapping.
    Setting Length_Offset (which would be unused in this case) to ExternalData
    in the mapping indicates that a value is not stored [1;31mdirect[mly in the
    mapped struct, but rather accessed in[1;31mdirect[mly through the callbacks.

[33mcommit b52ef81917a2a595447f21e806bacc2fb88a54d9[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Mar 2 10:20:31 2018 -0800

    wl#10665 Add buffered-style reading and writing to Rope
    
    New methods ConstRope::readBuffered() and LocalRope::appendBuffer()
    
    Allow buffered read and write of Rope so that we will be able to copy
    data [1;31mdirect[mly between Rope and SimpleProperties, without the size of
    the data being limited by an intermediate fixed-size buffer.

[33mcommit 3f2778640cc51136e178437bc606bc5bed7a3b0c[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait [1;31mdirect[mly
       on the event with initial timeout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit 7ac28149f725a63d1b1042ee9d9a1d26f566f1b6[m
Author: Pawe≈Ç Olchawa <pawel.olchawa@oracle.com>
Date:   Fri Aug 31 10:54:04 2018 +0200

    BUG#28062382 CPU OVERHEAD FOR INSERT BENCHMARK LOAD INCREASED BY 2X IN 8.0.11
    
    This patch introduces following mechanism:
    
    1. Whenever we request write or flush of redo during commit of
       transaction, we increment counter of requests.
    
    2. Log writer thread watches the counter and computes average time between
       consecutive requests to write or flush redo log.
    
    3. When the average time between requests to write or flush redo, is higher
       than 100us, we do not use spin-delay in log threads and we wait [1;31mdirect[mly
       on the event with initial timeout equal to 10ms.
    
    The performance regression, which was also revealed in the bug report,
    will be fixed in a new seperate bug report. That's because that solution
    is more complex, we have it ready but prefer to review it carefully and
    push in next window.

[33mcommit e17a2d887c4eb79c257873d22e96d1415609cbb0[m
Author: Steinar H. Gunderson <steinar.gunderson@oracle.com>
Date:   Wed Aug 29 13:51:30 2018 +0200

    Bug #26927386: REDUCE COMPILATION TIME [noclose]
    
    Make plugin_ftparser.h no longer include plugin.h; it is included from
    way too many places in[1;31mdirect[mly (more than 350 files) and uses nothing
    from it. Keep the #include for external users, so that we don't change
    the API.
    
    Change-Id: Ifc1078a796da8ba1f7d64d067f11c19fd3ddec83

[33mcommit 562b005dac376c5571d41b4367e5bdc739d1435d[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Aug 23 16:03:44 2018 +0200

    Bug#28544220 CLEANUP INCLUDE OF NON-EXISTING DIRECTORIES
    
    Remove INCLUDE of misc non-existing [1;31mdirect[mories.
    
    Change-Id: I7d5da39b39a3247d967e25703bc11a1c7244b310

[33mcommit b7272ef34c7469668ce09ee59dea2ff7a4258f89[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Wed Aug 29 09:30:17 2018 -0400

    Bug#28537733 ROUTER INSTALLS DATA DIRECTORY IN DEFAULT DB LOCATION
    
    Using different [1;31mdirect[mory for 'data' when installing the Router built from the sources
    so that it didn't collide with the server's data [1;31mdirect[mory.
    
    Reviewed-by: Jan Kneschke <jan.kneschke@oracle.com>
    RB: 20377

[33mcommit ba4b620cc30d5dd9537ede502697bf2f15e0ef7b[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Mon Aug 27 23:49:34 2018 -0700

    Bug#28341514: RENAME OF TABLE BETWEEN DATABASES FAILS WHEN
    IBD FILE STORED OUTSIDE DATADIR
    
    When renaming a file-per-table table that uses data [1;31mdirect[mory,
    create the database [1;31mdirect[mory if it is needed.

[33mcommit bedce576901ac94a24722b7d50381cf4dd2c22ad[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 28 09:21:43 2018 +0200

    Bug#28530928 ERROR 631: 'SCAN TAKE OVER ERROR', WHERE IT SHOULD NOT HAPPEN.
    
    Considder the following case in the ACC block (access/lock manager)
    
    * Transaction T1, have some operations in run queue.
    
    * Transaction T2, have some operations piling up one by one in
      serial queue. Some of them need exclusive lock and blocks on T1
    
    * All operations in run queue not part of T2 eventually completes
      or aborts. Leaving run queue empty of with only T2 operations.
    
    * One by one, operations from T2 are put into parallel/run queue
      by Dbacc::startNext().
    
    Dbacc::startNext() may move an OperationRec from the serial to the
    parallel queue if it is allowed to run in parallel with the
    other op's in this queue.
    
    When doing so, it failed to update the Operationrec::OP_ACC_LOCK_MODE flag
    which is required to reflect the accumulated OP_LOCK_MODE of all
    previous operations in the parallel queue. Note that when
    an OperationRec is inserted [1;31mdirect[mly into the parallel queue, without
    first waiting in the serial queue, this is handled by
    Dbacc::placeReadInLockQueue(). In this case the OP_ACC_LOCK_MODE
    flag *is* set correctly.
    
    In turn the above inconsistency in the ACC lock queues caused
    the 'scan lock take over' mechanism to fail, as it incorrectly
    concluded that a lock to 'take over' was not held. The same
    failure caused an assert to be hit when we aborted an operation
    being a member in such an inconsistent parallel lock queue.
    
    Patch also do some small refactoring in ::startNext() to avoid
    checking 'opbits' flags where its state should already be known
    due to previous checks.

[33mcommit aa06db86c6f9e5dd5ed143b4fcbf16f6069202b6[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Aug 20 14:41:03 2018 +0200

    Bug#28525796 ADAPT CMAKE SCRIPTS TO CMAKE 3.12
    
    Fix policies, add VERSION to PROJECT, don't try to copy non-existing
    [1;31mdirect[mories.
    
    Change-Id: If6a83e41da1260fcdb0f3ea2e912023f8246900f

[33mcommit d415ef77d686bd1524eb4b3c6db5165ffc8822d8[m
Author: Magnus Bl√•udd <magnus.blaudd@oracle.com>
Date:   Fri Aug 17 14:08:55 2018 +0200

    Bug#28517747 MISSING SETUP OF BACKUP_DATA_DIR
    
    The ndb_restore_conv_more test is using the backup_data_dir variable
    without setting up the variable first. The backup_data_dir variable is
    supposed to point out saved NDB backups. Thus it cant find the location
    of the saved NDB backups and fails.
    
    Fix by setting up the backup_data_dir variable to point at the saved
    NDB backups [1;31mdirect[mory. Actually since all tests using backup and restore
    is now using the ndb_backup_restore_setup.inc file, introduce a new
    NDB_SAVED_BACKUPS variable and use that in all tests. This should
    improve readability of testcases to understand where a saved backup is
    used.
    
    Change-Id: I8fc31685866dbd19ea7c566dd5778b30da6d7619

[33mcommit b1e6a82c404c6f301caf539e826e64245da5be6d[m
Author: Andrzej Religa <andrzej.religa@oracle.com>
Date:   Mon Aug 13 09:40:24 2018 +0200

    Fix for 2 MySQLRouter UTs failing on MacOS.
    
    Added creating of symbolic link to library_output_[1;31mdirect[mory to
    satisfy the dependency to ssl library @loader_path/../lib
    
    Reviewed-by: Grzegorz Szwarc <grzegorz.szwarc@oracle.com>
    RB: 20303

[33mcommit a384020bcf4801a5ee19781d7eb1959a79930471[m
Author: Sanjana DS <sanjana.ds@oracle.com>
Date:   Wed Aug 8 17:45:30 2018 +0530

    Bug #17732772 NDB_MGMD : CLUSTER LOG SYNCHRONOUSLY WRITTEN BY SIGNAL HANDLING THREAD
    
    ndbout and ndberr become invalid after exiting from mgmd_run()
    and re[1;31mdirect[ming to them before the next call to mgmd_run() causes a
    segmentation fault. This can happen during mgmd service restart.
    This fix ensures that ndbout and ndberr are valid all the time.

[33mcommit a8fc5292e5f474d4d4c2e0c4ca0cca9f15bf1579[m
Author: Dinesh Surya Prakash <dinesh.prakash@oracle.com>
Date:   Tue Jul 31 12:45:49 2018 +0530

    Bug#28027150 JAVA BASED UNIT TESTS DONT RUN ON WINDOWS IN PB2
    
    PROBLEM:
      1) Classpath seperator for windows is ";" but in add_test() we give as ":".
         So, Ctest test cases fails on windows in Pb2.
      2) Libndbjtie_unit_tests.dll is not found in library_output_[1;31mdirect[mory
         folder. So, the test were not able to load the binary.
    
    FIX:
      1) Modified the make files to give proper Classpath seperator to add_test() based
         on the platform we build.
      2) Fixed by moving libndbjtie_unit_tests.dll to library_output_[1;31mdirect[mory.

[33mcommit a4ad34a6b38b1985c2ce8229b7149109b894d0cb[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Jul 26 11:42:47 2018 -0600

    WL9508 - Post-Fix
    Several tests fail periodically in PB2. They almost always have to do
    with waiting too long for the purge thread to truncate an inactive_explicit
    undo space and make it empty.
    
    The function innodb_alter_undo_tablespace_inactive() is refactored here
    to be more [1;31mdirect[med in what it does. In addition, the rule that the second
    undo space can be made inactive is now changed to the third.  In other words,
    there must always be 2 active undo spaces.  The third one may be explicitly
    set empty. The reason is that while the third is being set inactive, one of
    the other two can be implicitly truncated.  That would leave one more undo
    space to use for new transactions.

[33mcommit aeed0b345309f47ba49b8eee16e92ae0b365f23f[m
Author: Dinesh Surya Prakash <dinesh.prakash@oracle.com>
Date:   Thu Jul 26 09:44:22 2018 +0530

    Bug#28128059 CLEAN NDB_TESTRUN.LOG AFTER EXECUTING TEST CASES
    
    Mysql-test-run.pl sets the path for a temporary file in global variable
    NDB_TOOLS_OUTPUT. This is widely used in ndb testcases to re[1;31mdirect[m the
    output from various ndb tools. But after completion of the test, this file
    is not removed. So, check_testcase fails the test.
    
    Removed the output file from each testcase.

[33mcommit 043715bdc417241a84aea38f21166736bc68a38f[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Jul 25 12:23:15 2018 +0100

    Bug #28387450 DATA NODES CRASH RUNNING INSERTS IN DEBUG MODE
    
        Fix for second crash mentioned in bug report.
    
        Problem is :
         1 New logic in 7.6 partial LCP using the length of
           a signal to encode new behaviours
         2 Existing logic which may periodically send a signal
           at a point before the signal length has been examined.
           - Existing logic saves the trampled data words, but
             does not restore the signal length
    
        Result :
         - Unrelated assertion failure in debug compiled code
           as incorrect path taken due to incorrect length.
         - Undefined result in release compiled code as incorrect
           path taken due to incorrect length.
    
        Most [1;31mdirect[m fix is to also save + restore the signal length.
        Better approach in future : Avoid trampling signal object.

[33mcommit 8a1e19cac4e7a7d78072bdbbbfa96540b75a54bf[m
Author: Dinesh Surya Prakash <dinesh.prakash@oracle.com>
Date:   Tue Jun 26 21:23:22 2018 +0530

    Bug#27882088 JAVA BASED UNIT TESTS FAILS ON MAC OS IN PB2
    
    PROBLEM:
    
    Test case fails because it can not find openssl libs path. The install
    name for the openssl libs were not set. So, the linker was not able to
    locate the libs.
    
    example:
    otool -L ./storage/ndb/src/ndbjtie/jtie/test/myjapi/libmyjapi.dylib
    ./storage/ndb/src/ndbjtie/jtie/test/myjapi/libmyjapi.dylib:
              /home/dsprakas/mysql/mysql/storage/ndb/src/ndbjtie/jtie/test
              /myjapi/libmyjapi.dylib (compatibility version 0.0.0, current
              version 0.0.0)
    
              libssl.1.0.0.dylib (compatibility version 1.0.0, current
              version 1.0.0)
    
              libcrypto.1.0.0.dylib (compatibility version 1.0.0, current
              version 1.0.0)
    
    FIX:
    
    Fixed the makefile to add the proper install name for the openssl libs.
    
    leo04:mysql dsprakas$ otool -L ./storage/ndb/src/ndbjtie/jtie/test/myjapi/libmyjapi.dylib
    ./storage/ndb/src/ndbjtie/jtie/test/myjapi/libmyjapi.dylib:
              /home/dsprakas/mysql/mysql/storage/ndb/src/ndbjtie/jtie/test/
              myjapi/libmyjapi.dylib (compatibility version 0.0.0, current
              version 0.0.0)
    
              /home/dsprakas/mysql/mysql/library_output_[1;31mdirect[mory/libssl.dylib
              (compatibility version 1.0.0, current version 1.0.0)
    
              /home/dsprakas/mysql/mysql/library_output_[1;31mdirect[mory/
              libcrypto.dylib (compatibility version 1.0.0, current
              version 1.0.0)

[33mcommit 912b1417f5fcd0175d3e60a18a6a24382a88f56f[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Wed Jul 18 19:31:17 2018 +0530

    WL#9210: InnoDB: Clone remote replica
    =====================================
    Enhance server clone plugin created in WL#9209 to transfer snapshot
    over network from one mysqld instance to another. Clone plugin
    installed in both severs accomplishes the data capture, transfer
    and apply.
    
    This feature would be used in future to provision a replica node
    by cloning the snapshot from one of the nodes in a replication
    group. It is one of the child worklogs of wl#8953.
    
    [Donor]: mysqld server that provides the snapshot
    -------
    INSTALL PLUGIN clone SONAME 'mysql_clone.so';
    CREATE USER clone_user IDENTIFIED BY 'clone_password';
    GRANT BACKUP_ADMIN on *.* to clone_user;
    
    [Recipient]: mysqld server that receives the snapshot.
    -----------
    INSTALL PLUGIN clone SONAME 'mysql_clone.so';
         CLONE INSTANCE FROM clone_user@HOST:PORT
         IDENTIFIED BY 'clone_password'
         DATA DIRECTORY [=]'<clone [1;31mdirect[mory>'
         [REQUIRE [NO] SSL];
    
    clone_user     : User name to connect to donor mysqld server
    clone_password : Password for the User
    HOST           : The host where donor server is running
    PORT           : Classic protocol PORT of donor server
    DATA DIRECTORY : Recipient [1;31mdirect[mory for cloned data
    
    Reviewed-by: Gleb Shchepa <gleb.shchepa@oracle.com>
    Reviewed-by: Marcin Babij <marcin.babij@oracle.com>
    
    RB: rb#19143

[33mcommit 2ad3c8adefe0890352f41efa9c3c942fba987d7a[m
Author: Piotr Obrzut <piotr.obrzut@oracle.com>
Date:   Tue Jul 17 09:28:36 2018 +0200

    Problem:
    When attampting uninstall of mysql server msi through Control Panel user will
    see a popup message: "The setup must update files or service that cannot be updated
    while the system is running. If you choose to continue, a reboot will be required
    to complete the setup.". This issue is visible when user installs server through
    MySQL Installer (which handles service setup) and not [1;31mdirect[mly from msi.
    
    Analysis:
    The msi handling of the service removal on uninstall got out of sync with MySQL Installer.
    Additionally due to dependency on CRT library we had to remove the custom action dll, which
    handled the removal.
    
    Fix:
    Replace C++ custom action steps with Windows Batch commands updated to follow MySQL Installer,
    while turning off the 'service running' popup.
