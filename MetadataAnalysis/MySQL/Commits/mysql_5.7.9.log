Searching between mysql-5.6.26 and mysql-5.7.9
Keywords: slow, fast, time, perf(ormance), optim(ize), regression
Additional keywords: delayedInnoDBflush,flush,binaryLog
Keywords: slow fast time perf optim regression speed delayedInnoDBflush flush binaryLog
For keyword slow:
[33mcommit 85b63bf26fc2c2088a158f4929670c9130507165[m
Merge: 6e94cc9c6d3 4a3e309aa60
Author: Ajo Robert <ajo.robert@oracle.com>
Date:   Wed Aug 19 16:33:29 2015 +0530

    Merge remote-tracking branch '56repo/mysql-5.6' into mysql-5.7
    
    Because of this change, tests which verifies PROCESSLIST and
    then status variable "threads_connected" might fail on [1;31mslow[mer
    machines. As part of this patch, changed test cases to wait
    until "threads_connected" is set to expected value instead
    of PROCESSLIST count.

[33mcommit fa2994007c68e3ddad092f1ad9a157a242fd4151[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Jun 18 10:20:43 2015 +0200

    Fix for Bug#20957068:
    
    NDB : EVENT API POLLEVENTS() MUST ACTUALLY CAUSE POLL OF TRANSPORTERS
    
    The patch introduce the normal do_poll pattern to be used
    by clients into ::pollEvent(). This also makes the condition member
    in the EvenBuffers obsolete, as do_poll offers the exact same
    functionality.
    
    By doing its own poll of the receiver, we are not amu longer
    dependent of other client threads or the ClusterMgr doing the
    poll for us. This removes a problem where pollEvents were [1;31mslow[m to
    receive events.

[33mcommit 33c8a52eb56f21ee8de4305e84d8be67a49e95e9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Jun 16 23:55:40 2015 +0200

    Part2 (of 2) fix for Bug#18390321
    
      MT-SCHEDULER: POSSIBLE BUSY WAIT IF PERFORMSEND() FAILED TO SEND
    
    This fix is for the above issue when the send threads are
    responsible for doing the sends. For different reason
    the send threads may have no progress when trying to send
    a chunk of data (buffers full, [1;31mslow[m receivers, ++). It is
    then no point in busy-retrying the send. Instead the send thread
    should take a short break. Other theads will then be able
    to do their work which migh help in resolving the stuck send.
    
    Part of this patch is a refactoring / fix of a patch
    introducing a configurable 'max_send_delay' for the
    send threads. The idea behind that patch is that having a
    short delay before sending available data, might allow us
    to piggy back more data to the same node. This was
    a usefull mechanism to extend into a more general send-delay
    mechanism when the send threads should wait due to overloaded
    transporters. However, I found a refactoring was required in
    order to 'open it up' for such reuse, that included:
    
     - Setting of 'delay' was moved from insert_node() into
       own methods: set_max_delay() / set_overload_delay().
       This also fixed a problem in ::run_send_thread() where
       we re-inserted a node having more data to send, *and*
       started a new delay. In this situation it does not make
       sense to delay more, as we were likely unable to complete the
       current send due to too much data.
    
     - Decision of when to take a delay-sleep, or possible ignore it,
       was moved from get_node() into ::run_send_thread().
       get_node() will now first search for a send node not being
       delayed- If that cant be found, the node with the shortest
       delay is returned. It is then up to the send schedulling
       logic in ::run_send_thread() to either wait for this delay to
       expire, or ignore the delay and send immediately (As previous)
    
     - Furthermore, previously get_node() contained logic deciding
       how many send threads should be kept awake. This is now also
       handled by ::run_send_thread().
    
     - There were also a bug as a send delay actually resulted
       in a busy wait for the delay to expire: Even if get_node() didn't
       return any (non-delayed) node, yield() was called with
       the check_callback function check_available_send_data().
       As 'first_node != 0' when there were delayed sends, the yield()
       would not be taken (Assumed more data had arrived)
    
       That problem were fixed by introducing a 'm_more_nodes' flag
       which is set if more send nodes is inserted inbetween we release the
       send_threads_mutex, and yield has grabbed its own mutex.
       (Some memory barrieres required in addition)
    
    The 'overload' delay has been added on top of this delay
    mechanism, Mainly implemented by perform_send() now returning
    'bytes_send'. Together with the existing 'more' return value,
    ::run_send_thread() detect overload as 'more && bytes_sent==0',
    and then request a set_overload_delay(). Much more than
    that is not required with the refactored send-delay...

[33mcommit 39189f1bb323aa0c73996d3fe3f803fda2aa6a0c[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 15 09:55:22 2015 +0200

    Increase timeout for  'testNodeRestart -n Bug27003 T1'
    from 1800 -> 3600sec.
    
    Normal run time for the tests not failing seems to be
    from 20 - 28 min, so timing out after 30min on [1;31mslow[m machines is
    simply too early.

[33mcommit 63ac18af02ff7cfade32d848b84b36286087aae1[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Fri Jun 5 16:58:30 2015 +0100

    BUG#20191813
    
    Post-push fix. Hardening the test case. In this particular
    case, it could be that on [1;31mslow[m platforms the IO thread did
    not have the chance to initialize its structures for the
    given channel before the server was deliberately shutdown.

[33mcommit 302dc52191a93264864df4f44d31c6320dcc5977[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue May 26 10:41:13 2015 +0200

    Patch for bug#21109605
    
    EXCESSIVE NDB KERNEL THREAD IS STUCK IN: PERFORMING SEND; WATCHDOG TERMINATE
    
    It was possible to end up in a livelock on the send_buffer
    mutex if send buffers became a limiting resource. This could be due
    to too little send buffers configured, [1;31mslow[m/failing
    communication network such that all send buffers are filled,
    [1;31mslow[m receiveres which does not consume what is sent and likely other
    reasons.
    
    In this situation (all?) worker threads will fail to
    allocate send buffer memory for the signals, and
    will attempt a ::forceSend() to free up space.
    At the same time the send thread will be very busy
    trying to send to the same node(s). All these threads
    will compete for taking the send_buffer mutex, which
    result in a livelock on it. Send threads stalled due
    to hitting this livelock will be reported by the
    watchdog as 'Stuck in Send'
    
    As 'stuck' send threads also held the global send_thread mutex
    they could even block worker threads trying to grab the same
    mutex while alerting send threads as part of a do_send request.
    Thus, even the worker threads got 'Stuck in Send'
    
    This patch does two things:
    
    1) Code analysys revealed that the send thread does not
       need to hold the global send_thread mutex while grabbing
       the send_buffer mutex. By releasing this global mutex prior
       to locking the SB-mutex the *worker threads* will no
       more be 'Stuck in Send'.
    
    2) Changed the send treads locking of the send_buffer mutex
       to use a trylock. If the try-locking failed, the node to be
       sent to is re-inserted last into the list of send-nodes in
       order to be retried later. This removed the 'Stuck in Send'
       condition for the send threads as well.

[33mcommit d7cbbe9e4c37c533677f7c13a9acf57239ff9d2d[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Tue May 26 11:07:34 2015 +0530

    Bug#20693153 : ACCESS DENIED WITH SSL CONNECTION FROM MYSQL CLIENT
    
    Description : cli_establish_ssl() tries to establish SSL connection
                  with server. While establishing SSL it calls my_net_writes
                  which writes data into net->buff buffer.
                  In case of optimized binary, if --general-log=0 and
                  --[1;31mslow[m-query-log=0 is set, write operation would overwrite
                  portion of the scramble data sent by server. This happens
                  because when above mentioned variables are set to OFF,
                  server version string does not contain "--log" suffix
                  which otherwise would have prevented overwrite of scramble.
                  This data is yet to be read by authentication plugin on
                  client side. Hence, once control reaches to authentication
                  plugin, it consumes wrong scramble which leads to
                  authentication error if user account has a password.
    Solution : In case of SSL connection, copy scrambe data into new
               buffer and pass new buffer to run_pluggable_auth.
    
    Reviewed-By: Georgi Kodinov <georgi.kodinov@oracle.com>
    Reviewed-By: Robert Golebiowski <robert.golebiowski@oracle.com>

[33mcommit a5a22244ac8237c82f85f7f96abe6b2eabc3be98[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Apr 22 15:10:23 2015 +0300

    Fix Bug#20783098 INNODB_CHECKSUM_ALGORITHM=CRC32 IS NOT BYTE ORDER AGNOSTIC
    
    The CRC32 checksum generation code interprets portions of the byte
    string to checksum as a 8-byte integer so that it can process 8 bytes
    at a time (rather than 1 byte at a time). For this, the code uses the
    native byte order of the machine:
    
      crc ^= *(ib_uint64_t*) buf;
    
    and then does numerical calculations with the result (e.g. crc >> N).
    Thus the resulting checksum depends on the byte order of the machine
    and is different on big and little endian machines. This means that
    files written to with --innodb-checksum-algorithm=crc32/strict_crc32 on
    big (little) endian machines are not readable on little (big) endian
    machines because the checksum, though valid, is not recognized.
    
    The simplest solution would be to start writing only e.g. big endian
    checksums and recognize only such ones, but this would introduce an
    unacceptable backwards incompatibility.
    
    The solution implemented is to recognize both big and little endian
    CRC32 checksums during verification, while first calculating and
    checking the little endian one.
    
    Swapping the byteorder in order to calculate "the other" CRC32 checksum
    [1;31mslow[ms down the checksum calculation by about 1-2% (e.g. recognize
    big-endian-CRC32 on little endian machines or recognize
    little-endian-CRC32 on big endian machines).
    
    When generating the checksum (when writing to disk) we now always use
    little endian byteorder (no change in little endian machines, and an
    extra step of swapping the byteorder on big-endian machines).
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>
    RB: 8781

[33mcommit a6e03f378716c684a2846e3de5265bac41dde3bb[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Fri Apr 17 16:37:52 2015 +0200

    BUG#20470724: SET GTID_PURGED SHOULD NOT ROTATE BINLOG
    
    In 5.6, SET GTID_PURGED rotates the binary log and adds GTIDs to the
    Previous_gtids_log_event of the new binary log.  This correctly
    persists GTID_PURGED even in the presence of multiple old binary logs
    because the server scans all binary logs from the oldest to the
    newest, until it finds one that contains either a non-empty
    Previous_gtids_log_event or a Gtid_log_event and then sets GTID_PURGED
    to the set stored in the Previous_gtids_log_event found by this
    procedure.
    
    But scanning old binary log headers can be [1;31mslow[m, especially when there
    are many binary logs.  Therefore the option
    binlog_gtid_simple_recovery was introduced, which reads GTID_PURGED
    from the oldest binary log without scanning multiple binary logs.  But
    this option can generate a wrong value for GTID_PURGED in case SET
    GTID_PURGED has been issued.
    
    In 5.7, there is a better way to persist GTID_PURGED, which makes the
    server initialize it correctly even when binlog_gtid_simple_recovery
    is enabled.  Since we store GTIDs in the mysql.gtid_executed table, we
    can just insert the GTIDs in this table.
    
    The present patch implements this and prevents SET GTID_PURGED from
    rotating the binary log.

[33mcommit b1ae5c5954a388aeceef98c57982c5a602471eff[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Apr 28 08:42:27 2015 +0100

    WL#4601: Remove fastmutex from the server sources
    
    This patch removes the FAST_MUTEX code and the related
    WITH_FAST_MUTEXES CMake option. Note that this option
    was enabled by default for Linux release builds.
    
    Performance tests show that "fast" mutexes were equal to
    or (more often) [1;31mslow[mer than default Linux mutexes.
    
    Removing fast mutexes fixes / makes obsolete:
    Bug#11748914: MYSQL PERFORMANCE WITH AND WITHOUT
                  FAST MUTEXES USING SYSBENCH WORKLOAD
    Bug#13923722: FAST MUTEXES DELAY SHOULD BE CONFIGURABLE -
                  DEFAULT BAD ON SUPRA03/01
    Bug#18870931: MUTEX_DELAY() CREATING EXCESS MEMORY TRAFFIC,
                  GCC MEM BARRIER NEEDED
    Bug#18871517: MUTEX_DELAY() MISSING X86 PAUSE INSTRUCTION
                  OPTIMIZATION
    Bug#18871138: SET THREAD PRIORITY IN MY_PTHREAD_FASTMUTEX_LOCK

[33mcommit 635f8bb7cdf9fb3b8692cbbca6834802b266786e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Mar 3 13:11:08 2015 +0100

    Fix for bug#20538179
    
            NDB : TEST HANDLING OF ARBITRATOR FAILURE DURING ARBITRATION
    
    Fix a failure seen in AutoTest: 'testNodeRestart -n ClusterSplitLatency'.
    This is also a genuine bug where the 'Loser' in the arbitration
    is allowed to reconnect before the arbitration has completed.
    During the reconnect, it hits a ndbrequire in Qmgr::execRECONNECT_REP
    as we are still in phase ZRUNNING.
    
    Frazer describes the failure scenario as:
    
    .........
    I guess that it needs :
     1.  Losing side is slightly [1;31mslow[mer / behind the winning side at detecting the silence / loss of the other side
     2.  Losing side enters state ZPREPARE_FAIL as part of execPREP_FAILREQ() - in 2 node case, this occurs quickly once the 'other node' failure is decided, as we are / must become President. Losing side sends CLOSE_COMREQ to TRPMAN for 'other' node.
     3.  Handling of 'other node' failure causes DUMP 9991 to be sent to TRPMAN, re-enabling communication.
     4.  Unblocking of transporter causes DISCONNECT_REP to be sent to TRPMAN and then QMGR
     5.  DISCONNECT_REP reception calls node_failed() which causes transition from ZPREPARE_FAIL -> ZFAIL_CLOSING due to bad (fixed) switch statement
          Also causes all pending messages to be discarded, so e.g. no FAIL_REP signals are received which would kill us.
     6.  Losing node is in state CHOOSE_ARBIT, waiting for response.
     7.  Arbitrator disconnects - no action yet, still waiting for ArbitTimeout
     8.  As we're in ZFAIL_CLOSING, the 'other side' can disconnect and reconnect
     9.  If the 'other side' has finished handling our failure, and attempts to reconnect then we can generate CONNECT_REP resulting in ndbrequire(false) etc...
    
    So I think it's a genuine bug, but we probably don't often see it as it requires the 'DISCONNECT_REP' to overtake the 'FAIL_REP' signals in the transporter from the 'other' node.  The error insertion / dump codes probably make that more likely.
    
    .........
    
    The fix is to change Qmgr::node_failed(), such that when in phase
    PREPARE_FAIL it does nothing, instead of sending another (redundant)
    CLOSE_COMREQ, which would have replaced the PREPARE_FAIL state with
    ZFAIL_CLOSING, which would have allowed reconnection after the
    3 second grace period.

[33mcommit d8a53b963d9db05fb1354e3168e1befbbc87e252[m
Author: Ajo Robert <ajo.robert@oracle.com>
Date:   Thu Feb 19 15:32:31 2015 +0530

    Bug#18335504:MISSING INFORMATION IN SLOW QUERY LOG FOR SLOW
    
    Analysis
    --------
    For the Handler..READ statements, rows_sent and
    rows_examined are always "0" in "[1;31mslow[m_log" query tables.
    
    The THD class has counter for rows examined and sent
    for a query. But these counters are not incremented for
    HANDLER...READ operations. Hence rows_sent
    and rows_examined were always 0 in [1;31mslow[m_log for
    HANDLER..READ operations.
    
    Fix:
    ---
    Added code to increment row examined and sent counters in
    HANDLER...READ statement executor.

[33mcommit d70f7f9af50b1e8fde8a178b3cc4d0c57ccf238e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Feb 6 12:12:59 2015 +0100

    Fix issue in AutoTest where test are being killed by
    WatchDog during memory allog in startphase 0.
    
    Set config variable 'TimeBetweenWatchDogCheckInitial'
    to 60s, to allow for more [1;31mslow[mnes in this phase.

[33mcommit 8a7b6b9e473fdaca92b9d5a065898ba97769c17e[m
Author: bharathy <bharathy.x.satish@oracle.com>
Date:   Mon Feb 2 17:37:50 2015 +0530

    WL#6409: deprecate PASSWORD() and extend ALTER USER syntax to manage
             authentication attributes.
    
    This WL implements the following:
      1. Separation of authentication from authorization. GRANT statement
         deals with both authorization and authentication. This WL deprecates
         the authentication part from GRANT statement by throwing a warning.
      2. CREATE/ALTER USER statements will be used to provide authentication
         details associated with a user. GRANT statement when used to create
         users will throw a deprecated warning.
      3. PASSWORD() function is removed from SET PASSWORD sytax.
      4. Authentication plugin is extended to add 2 new APIs which will generate
         and validate password hash without depending on old_password variable anymore.
      5. New system variable to ensure backward compatibility when user management
         queries are rewritten to logs ([1;31mslow[m/general/audit/binary logs).
      6. New Plugin services to validate/calculate_strength/set_salt for
         the credentials.
      7. ALTER USER statement is extended to provide authentication/SSL/connection
         attributes.
      8. SHOW CREATE USER <user>
      9. Password column is removed from mysql.user table and authentication_string
         column will be the new password store for any users created.
      10.Deprecate NO_AUTO_CREATE_USER sql mode.
      11.Script to downgrade as requested by sys QA.

[33mcommit 589577345819e77a90b0f6b92f1d9f781449986e[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Mon Jan 19 14:05:21 2015 +0000

    BUG#20380487: RPL_PERFSCHEMA_NO_MASTER_UUID FAILS SPORADICALLY
    
    The test fails due to poor synchronization. The slave thread may
    be [1;31mslow[m reading the uuid from the master. It starts, connects and
    then reads the UUID. The user thread (the one from the test case)
    knows nothing about this, it just waits until the slave thread
    starts and connects to the master. Thence, it may already be
    checking that the UUID exported on the P_S table is the one
    expected, and consequently assert too soon - i.e., before the
    slave thread actually retrieves the UUID from the master.
    
    The fix is to make sure that the user thread keep checking until
    it gets the expected UUID or bails out after a certain amount of
    time. The latter will make the test case fail, thence acts as an
    implicit assertion.
    
    This patch also cleans up the test case a bit.

[33mcommit 2f11308e7ae876880302b5024018918011003c56[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Dec 9 14:23:53 2014 +0000

    Bug #19858151   MAKING DATA NODE SELF-EXCLUDE MECHANISM MORE ROBUST
    
    The LCP Scan Frag watchdog and GCP Monitor can both decide to
    exclude a node if it is too [1;31mslow[m when participating in these
    protocols.
    
    Currently the exclusion is implemented by asking the failing node
    to shutdown.
    
    This allows it to first log some debugging information, and
    shutdown with a clear failure cause.
    
    However in some situations it may be [1;31mslow[m to shutdown, and prolong
    the duration of GCP/LCP stall for the other unaffected nodes.
    
    To minimise this time, this fix adds an isolation mechanism which
    causes the other live nodes to forcibly disconnect the failing
    node after some delay.
    
    This gives the failing node the chance to shutdown with debugging
    info and a good message if possible, but limits the time the others
    must wait for this to occur.
    
    Once the live nodes have processed the disconnection of the failing
    nodes, they can commence failure handling and restart the protocol(s).
    
    Even if the failed node takes a long time to shutdown, the others
    can proceed with processing.
    
    The GcpMonitor and the Lcp Scan Fragment watchdog are enhanced to
    make use of this mechanism.
    
    Three new testcases are added :
     1.  GcpStop
         Testing of GcpStop handling in normal cases
     2.  GcpStopIsolation
         Testing of GcpStop self-shutdown failure so that Isolation is
         required
     3.  LcpScanFragWatchdogIsolation
         Testing of Lcp Scan Fragment Watchdog where Isolation is
         required.
    
    These are added to the daily-devel test suite.
    
    Additionally :
    
    Bug #20128256   NDB : GCP STOP MONITOR HAS ONLY ONE BULLET
    
    This bug was discovered while testing (GcpStop testcase).
    
    The Gcp Monitor did not continue operation after detecting a Gcp stop.
    
    This is fixed so that it does continue operation after detecting
    a Gcp stop, and this is tested by both the GcpStop and GcpStopIsolation
    testcases (where the Master node is not a victim and must detect and handle
    multiple separate GCP stop events).

[33mcommit b510550a6e96ef127ffe0c2c11dc3ca4fb45b986[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Tue Nov 25 08:38:30 2014 +0530

    - Bug#20083612: INNODB_UNDO.TRUNCATE_RECOVER FAILS DUE TO "SERVER FAILED TO
      DISSAPEAR"
    
      Truncate of undo tablespace is an asynchronous activity that happens in
      background as part of purge-action. There is no hard guarantee that the
      action will be completed within the stipulated time.
      Designing a test-case that relies on such an action is big
      challenge but at same time we can make some assumption that even on
      heavily loaded system with some timeout we should hit the action.
      Timeout that was configured currently was not enough for weekly-trunk
      load on all machines so increasing it such that it can cover up
      for [1;31mslow[mer and faster machine.

[33mcommit f3084f125a56f0f9a3ef5d1d890ff2d9ca2a4c8d[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Thu Nov 20 14:03:46 2014 +0100

    WL#7509: Add undocumented interface to change Disk Write Speed in LCPs, change isn't persistent over restarts, useful to handle SRs where restarts are too [1;31mslow[m

[33mcommit 39a34107820565aff4d317c73bf390098ba930b8[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Thu Nov 20 14:02:58 2014 +0100

    WL#7509: Tweaked the adaptive LCP speed parameters to be a bit [1;31mslow[mer in changing

[33mcommit 5c998b136b68ba607704462f2ab820322ccca331[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 8 10:30:26 2014 +0300

    Exclude [1;31mslow[m tests from per-push Valgrind run.

[33mcommit 6d56efaf9dc27c382fdd8ce8f2c0fa9b814b2c23[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Tue Sep 23 14:00:00 2014 +0530

    Bug #19077239   TESTS FOR CONFIG_EDITOR AND SECURE_INSTALLATION WITH SSL ARE UNSTABLE
    
    Changing the .opt file for mysql_secure_installation_ssl.test for proper ssl keys
    Increasing the timeout value for perl Expect in mysql_secure_installation.test to run the test
    successfully on [1;31mslow[mer machines.
    Skipping mysql_config_editor for windows.

[33mcommit bafef71e79e25444419320ec850d9cdf03b25bd0[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Fri Sep 12 14:28:31 2014 +0530

    Bug#17793901 MYSQL_SECURE_INSTALLATION* FAILS ON PB2.
    Increasing the value of timeout for Expect as the problem seems to be due to a [1;31mslow[m machine.

[33mcommit 70a8dac744aeeeb27c1d3326740801194988478d[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri Aug 22 14:07:31 2014 +0530

    WL# 7583 - THD transitional string refactoring.
    This patch does transitional refactoring of string
    types in the THD class. This will ease on the migration to
    the new strings framework. A number of places in the
    server uses char* instead of const char*. A gradual
    transformation of the server code is required to use
    const char* and use const references to pass objects
    around functions. This will enable the smooth transition
    of the server code to use the new string framework classes.
    
    This changeset is responsible for changing the type of
    db name of the Statement class from a raw char* to
    LEX_CSTRING. This results in set of cascade changes across
    several files. The following data members are modified as
    part of this patch:
          1. sp_name
               LEX_CSTRING m_db
          2. Alter_table_ctx
              const char *db
              const char *table_name
              const char *alias
              const char *new_db
              const char *new_alias
          3. Foreign_key
              LEX_CSTRING ref_db
              LEX_CSTRING ref_table
          4. Statement
              LEX_CSTRING m_db
          5. Table_ident
              LEX_CSTRING db
              LEX_CSTRING table
          6. TABLE_LIST
              const char *db, *table_name, *alias
              LEX_CSTRING view_db
              LEX_CSTRING view_name
          7. Trigger
              LEX_CSTRING m_db_name
              LEX_CSTRING m_subject_table_name
          8. user_var_entry
          LEX_CSTRING m_catalog
    
    The functions which are modified as part of this patch include:
          1. create_embedded_thd
          2. check_embedded_connection
          3. acl_getroot
          4. check_routine_access
          5. acl_authenticate
          6. check_access
          7. mysql_table_grant
          8. get_table_grant
          9. acl_notify_htons
          10. log_loaded_block
          11. decide_logging_format
          12. net_after_header_psi
          13. Event_job_data::execute
          14. Events::update_event
          15. Events::drop_schema_events
          16. filesort
          17. write_schema_op_to_binlog
          18. injectApplyStatusWriteRow
          19. ndb_binlog_thread_func
          20. handler::ha_external_lock
          21. Item_field::fix_fields
          23. Item_func_sp::itemize
          24. Item_func_database::val_str
          25. File_query_log::write_[1;31mslow[m
          26. set_thd_db
          27. Query_log_event::Query_log_event
          28. Query_log_event::do_apply_event
          29. Load_log_event::do_apply_event
          30. Log_event::get_db
          31. handle_slave_io
          32. handle_slave_sql
          33. Current_schema_tracker::store
          34. db_load_routine
          35. lock_db_routines
          36. sp_drop_db_routines
          37. sp_load_for_information_schema
          38. sp_head::execute
          39. sp_head::execute_trigger
          40. sp_instr_stmt::exec_core
          41. mysql_admin_table
          42. Alter_table_ctx::Alter_table_ctx
          43. MYSQL_AUDIT_NOTIFY_CONNECTION_CONNECT (macro)
          44. MYSQL_AUDIT_NOTIFY_CONNECTION_CHANGE_USER (macro)
          45. list_open_tables
          46. open_table
          47. make_cache_key
          48. Query_cache::invalidate
          49. THD::~THD
          50. thd_binlog_filter_ok
          51. db
          52. set_db
          53. reset_db
          54. copy_db_to
          55. Table_ident::Table_ident
          56. Table_ident::change_db
          57. mysql_open_cursor
          58. mysql_change_db_impl
          59. write_to_binlog
          60. get_default_db_collation
          61. mysql_create_db
          62. mysql_alter_db
          63. mysql_rm_db
          64. mysql_change_db_impl
          65. backup_current_db_name
          66. mysql_change_db
          67. mysql_opt_change_db
          68. mysql_upgrade_db
          69. mysql_derived_prepare
          70. Sql_cmd_handler_open::execute
          71. db_is_default_db
          72. mysql_load
          73. write_execute_load_query_log_event
          74. mysql_load
          75. all_tables_not_ok
          76. dispatch_command
          77. mysql_execute_command
          78. mysql_parse
          79. add_table_to_list
          80. plugin_load
          81. Prepared_statement::set_db
          82. Prepared_statement::prepare
          83. Prepared_statement::reprepare
          84. Prepared_statement::swap_prepared_statement
          85. Prepared_statement::execute
          86. mysql_rename_tables
          87. do_rename
          88. close_cached_connection_tables
          89. mysqld_show_create_db
          90. store_create_info
          91. view_store_create_info
          92. List_process_list::operator()
          93. Fill_process_list::operator()
          94. make_table_list
          95. fill_schema_table_by_open
          96. get_all_tables
          97. make_schema_select
          98. get_trigger_table
          99. initialize_information_schema_acl
          100. mysql_rm_table_no_locks
          101. mysql_alter_table
          102. add_table_for_trigger
          103. udf_init
          104. mysql_create_view
          105. mysql_make_view
          106. mysql_drop_view
          107. get_table_category
          108. open_table_def
          109. TABLE_LIST::prepare_view_securety_context
          110. TABLE_LIST::prepare_security
          111. init_one_table
          112. new_nested_join
          113. TABLE_LIST::get_db_name
          114. TABLE_LIST::get_table_name
          115. Table_trigger_dispatcher::Table_trigger_dispatcher
          116. Trigger::create_from_dd
          117. Trigger::execute
          118. Trigger::parse
          119. Trigger::get_db_name
          120. Trigger::get_subject_table_name
          121. Trigger_creation_ctx::create
          122. Trigger_loader::load_triggers
          123. Trigger_loader::drop_all_triggers
          124. my_tz_init
          125. initialize_performance_schema_acl
          126. ParserTest::parse
          127. ACL_internal_schema_registry::register_schema
          128. mysql_routine_grant
          129. Log_to_csv_event_handler::log_[1;31mslow[m
          130. sp_exist_routines
          131. create_file
          132. Statement::Statement
          133. end_connection
          134. prepare_new_connection_state
          135. mysql_ha_hash_get_key
          136. init_lex_with_single_table
          137. innobase_get_foreign_key_info
          138. ha_myisammrg::append_create_info
          139. initialize_peformance_schema_acl.
    A new helper function to_lex_cstring(const char*) has been
    introduced which converts a const char* null-terminated
    string to LEX_CSTRING type.

[33mcommit 2467c1d97470a6e6e26d213ce57be54d06da6fcf[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Wed Aug 20 20:22:39 2014 +0200

    bug#18900198: SHUTDOWN_SERVER FAILS WITH ERROR -2 IN SOME TESTS
    
    Fix for innodb.innodb_bug60196 which had a too small timeout,
    which failed sometimes on [1;31mslow[m platforms.
    
    Fixed by removing the hardcoded 10 sec shutdown timeout.
    (The default is 60 s)
    
    Missed the second shutdown_server :(

[33mcommit 6db0922b0fc21d43511dbf56a8a21aafd44ae9f2[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Aug 20 16:30:57 2014 +0530

    - WL#6965: Truncate UNDO logs.
      Follow-up checkin. Increasing the timeout so that TCs passes on all
      machine (including [1;31mslow[m one)

[33mcommit 2d62bf27fd819f55dafdbe1d756c8303415c0a61[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Thu Aug 14 11:41:12 2014 +0200

    bug#18900198: SHUTDOWN_SERVER FAILS WITH ERROR -2 IN SOME TESTS
    
    Fix for innodb.innodb_bug60196 which had a too small timeout,
    which failed sometimes on [1;31mslow[m platforms.
    
    Fixed by removing the hardcoded 10 sec shutdown timeout.
    (The default is 60 s)
    
    Approved by Marko on IM.

[33mcommit a5ecc38f44abb66aa2024c70e37d1f4aa4c8ace9[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Aug 11 10:43:11 2014 +0300

    Bug#19330255 WL#7142 - CRASH DURING ALTER TABLE LEADS TO
    DATA DICTIONARY INCONSISTENCY
    
    The server crashes on a SELECT because of space id mismatch. The
    mismatch happens if the server crashes during an ALTER TABLE.
    
    There are actually two cases of inconsistency, and three fixes needed
    for the InnoDB problems.
    
    We have dictionary data (tablespace or table name) in 3 places:
    
    (a) The *.frm file is for the old table definition.
    (b) The InnoDB data dictionary is for the new table definition.
    (c) The file system did not rename the tablespace files yet.
    
    In this fix, we will not care if the *.frm file is in sync with the
    InnoDB data dictionary and file system. We will concentrate on the
    mismatch between (b) and (c).
    
    Two scenarios have been mentioned in this bug report. The simpler one
    first:
    
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    The files were not yet renamed in the file system.
    2a. The server is killed, without making a log checkpoint.
    3a. The server refuses to start up, because replaying MLOG_FILE_RENAME2
    fails.
    
    I failed to repeat this myself. I repeated step 3a with a saved
    dataset. The problem seems to be that MLOG_FILE_RENAME2 replay is
    incorrectly being skipped when there is no page-redo log or
    MLOG_FILE_NAME record for the old name of the tablespace.
    
    FIX#1: Recover the id-to-name mapping also from MLOG_FILE_RENAME2
    records when scanning the redo log. It is not necessary to write
    MLOG_FILE_NAME records in addition to MLOG_FILE_RENAME2 records for
    renaming tablespace files.
    
    The scenario in the original Description involves a log checkpoint:
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    2. A log checkpoint and a server kill was injected.
    3. Crash recovery will see no records (other than the MLOG_CHECKPOINT).
    4. dict_check_tablespaces_and_store_max_id() will emit a message about
    a non-found table #sql-ib22*.
    5. A mismatch is triggering the assertion failure.
    
    In my test, at step 4 the SYS_TABLES root page (0:8) contains these 3
    records right before the page supremum:
    * delete-marked (committed) name=#sql-ib21* record, with space=10.
    * name=#sql-ib22*, space=9.
    * name=t1, space=10.
    space=10 is the rebuilt table (#sql-ib21*.ibd in the file system).
    space=9 is the old table (t1.ibd in the file system).
    
    The function dict_check_tablespaces_and_store_max_id() will enter
    t1.ibd with space_id=10 into the fil_system cache without noticing
    that t1.ibd contains space_id=9, because it invokes
    fil_open_single_table_tablespace() with validate=false.
    
    In MySQL 5.6, the space_id from all *.ibd files are being read when
    the redo log checkpoint LSN disagrees with the FIL_PAGE_FILE_FLUSH_LSN
    in the system tablespace. This field is only updated during a clean
    shutdown, after performing the final log checkpoint.
    
    FIX#2: dict_check_tablespaces_and_store_max_id() should pass
    validate=true to fil_open_single_table_tablespace() when a non-clean
    shutdown is detected, forcing the first page of each *.ibd file to be
    read. (We do not want to [1;31mslow[m down startup after a normal shutdown.)
    
    With FIX#2, the SELECT would fail to find the table. This would
    introduce a regression, because before WL#7142, a copy of the table
    was accessible after recovery.
    
    FIX#3: Maintain a list of MLOG_FILE_RENAME2 records that have been
    written to the redo log, but not performed yet in the file system.
    When performing a checkpoint, re-emit these records to the redo
    log. In this way, a mismatch between (b) and (c) should be impossible.
    
    fil_name_process(): Refactored from fil_name_parse(). Adds an item to
    the id-to-filename mapping.
    
    fil_name_parse(): Parses and applies a MLOG_FILE_NAME,
    MLOG_FILE_DELETE or MLOG_FILE_RENAME2 record. This implements FIX#1.
    
    fil_name_write_rename(): A wrapper function for writing
    MLOG_FILE_RENAME2 records.
    
    fil_op_replay_rename(): Apply MLOG_FILE_RENAME2 records. Replaces
    fil_op_log_parse_or_replay(), whose logic was moved to fil_name_parse().
    
    fil_tablespace_exists_in_mem(): Return fil_space_t* instead of bool.
    
    dict_check_tablespaces_and_store_max_id(): Add the parameter
    "validate" to implement FIX#2.
    
    log_sys->append_on_checkpoint: Extra log records to append in case of
    a checkpoint. Needed for FIX#3.
    
    log_append_on_checkpoint(): New function, to update
    log_sys->append_on_checkpoint.
    
    mtr_write_log(): New function, to append mtr_buf_t to the redo log.
    
    fil_names_clear(): Append the data from log_sys->append_on_checkpoint
    if needed.
    
    ha_innobase::commit_inplace_alter_table(): Add any MLOG_FILE_RENAME2
    records to log_sys->append_on_checkpoint(), and remove them once the
    files have been renamed in the file system.
    
    mtr_buf_copy_t: A helper functor for copying a mini-transaction log.
    
    rb#6282 approved by Jimmy Yang

[33mcommit 4a4f248e7593400fe2f4223678eaf42a7bf6ba89[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Jul 25 10:02:57 2014 +0530

    Bug#19129559 -  pb2 main.max_statement_time.test crashing the
                    server in weekly trunk.
    
    Analysis:
    ---------
    This test is failing because few reasons,
    
    1. Issue mentioned in the bug report: (Started failing on Window
       from some time 21 June)
    
      qld-debug.exe!timer_notify_thread_func()[win_timers.c:78]
      7ff74fe2b42e    mysqld-debug.exe!pfs_spawn_thread()[pfs.cc:2074]
      7ff74f92d306    mysqld-debug.exe!pthread_start()[my_winthread.c:73]
      7ff74ff964f5    mysqld-debug.exe!_callthreadstartex()[threadex.c:376]
      7ff74ff96747    mysqld-debug.exe!_threadstartex()[threadex.c:359]
      7ff86d0015dd    KERNEL32.DLL!BaseThreadInitThunk()
      7ff86f6343d1    ntdll.dll!RtlUserThreadStart()
    
      => This issue is seen on Windows 64 bit machines. In function
         "timer_notify_thread_func" pointer value of timer object is
         stored in ULONG variable then type casted to pointer of
         my_timer_t. This works well in 32 bit machine and 64 bit machine
         until address crosses 2^32 value.
         Windows 64 is of LLP64 model. So ULONG variable is of 32 bit
         and pointer is of 64 bit. Because of this pointer value was
         getting corrupted when 64 bit value is stored in 32 bit ULONG
         type variable. Accessing this memory was causing an issue on
         Windows 64.
    
         To fix this issue, changed type of variable from ULONG to
         ULONG_PTR.
    
    2.  (Sporadic failure)
        mysqltest: At line 472: query 'SELECT CONVERT(VARIABLE_VALUE, UNSIGNED) INTO @qc_hits
                                       FROM INFORMATION_SCHEMA.GLOBAL_STATUS
                                       WHERE VARIABLE_NAME = 'Qcache_hits''
        failed: 1909: Query execution was interrupted, max_statement_time
                      exceeded
    
       => Issue here is, max_statement_time is set 50 milliseconds
          and assumption was "SELECT CONVERT(VARIABLE_NAME,...."
          returns withing 50 millisecond. But on some [1;31mslow[m platforms
          it took more than that so query terminated.
    
          To fix this issues, Changed session max_statement_time to 0.
    
    
    3.  (Sporadic failure)
       mysqltest: At line 119: query 'SELECT * from t1' succeeded -
                 should have failed with errno 1909...
    
       =>Issue here is, max_statement_time at session level is set
         to 2 milliseconds and query "SELECT * FROM..." is executed.
         Here, expectation is SELECT will take more than 2 milliseconds
         but on some platforms it returned before 2 millisecond.
         So it is not timed out.
    
         To fix this issues, changed query to select *, sleep(0.5) FROM t1;
         Now sleep(0.5) is executed for each row. This will make query
         execution time greater than 2 microsecond.
    
    4.  (Sporadic failure)
        SET @@SESSION.max_statement_time = 950;
        SELECT SLEEP(1);
         SLEEP(1)
         -1
         +0
    
       => Test case 10, checks timer precision. In this test case,
          query calls sleep(1) function and for that timer with
          different timeout values are set. And expectation is, if
          timer is set for less than 1 sec then sleep should be
          interrupted.
    
          But, max_statement_time is a soft hint for queries, on loaded
          machines or [1;31mslow[m machines time taken notify thread after
          timer expiration would be little bit more. Because of this
          sleep() is not interrupted.
    
          We can reduce statement execution timeout value, but test
          case to check timer precision will be incorrect. Not
          changing test case will result in sporadic test case
          failure. Removed this test case to avoid sporadic failure.
    
    
    5. (sporadic failure)
       @@ -151,7 +151,7 @@
       ERROR HY000: Query execution was interrupted, max_statement_time exceeded
       SELECT @a;
        @a
        -1
       +NULL
    
       => Here we set sessions level max_statement_time to 2 and
          execute stored function having "SELECT SLEEP(2) INTO @a".
          After SELECT query interruption value of variable @a is
          printed. But some times, the SELECT is interrupted before
          populating variable.
    
          Actually test case is verified by checking error reported.
          We need not have to print variable value here. To avoid this
          failure, removed "SELECT @a" statement.

[33mcommit 579a88e3636ecfec26bd8f70586c3f1e4a87932b[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Jul 16 15:24:04 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - increase timeout considering windows [1;31mslow[mer run.

[33mcommit ed0a52772fa96108c9c633407c2cb397905caa25[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Jul 8 07:09:02 2014 +0200

    Make
    
     innodb.blob_page_reserve
     innodb.log_file_size_checkpoint
     sys_vars.log_[1;31mslow[m_admin_statements_func
    
    no_valgrind_without_big since they take more than 5 mins to run.

[33mcommit 6073c2319c6fc655c010a31e422aa54498f4ca70[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Thu Jun 12 18:28:31 2014 +0530

    Bug #18806829 OPENING INNODB TABLES WITH MANY FOREIGN KEY REFERENCES IS
    SLOW/CRASHES SEMAPHORE
    
    Problem:
    
    There are 2 lakh tables - fk_000001, fk_000002 ... fk_200000.  All of them
    are related to the same parent_table through a foreign key constraint.
    When the parent_table is loaded into the dictionary cache, all the child table
    will also be loaded.  This is taking lot of time.  Since this operation happens
    when the dictionary latch is taken, the scenario leads to "long semaphore wait"
    situation and the server gets killed.
    
    Analysis:
    
    A simple performance analysis showed that the [1;31mslow[mness is because of the
    dict_foreign_find() function.  It does a linear search on two linked list
    table->foreign_list and table->referenced_list, looking for a particular
    foreign key object based on foreign->id as the key.  This is called two
    times for each foreign key object.
    
    Solution:
    
    Change the linked lists table->foreign_list and table_referenced_list to
    std::set structures table->foreign_set and table->referenced_set.
    
    rb#5673 approved by Vasil.

[33mcommit 098a2a4eba583205d84afbe42d33c20deb1dae37[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 2 10:08:35 2014 +0200

    'testSystemRestart -n SR_FULLDB T6' is really [1;31mslow[m on
    some test rigs (ndb07), and need even long timeout
    to be able to complete.
    
    Increase from 2.5h to 4h

[33mcommit be3dd2cbb0a2f9f92bb60cf8c8af0a302c824bfa[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Tue May 13 18:42:21 2014 +0300

    Bug#12368203 - 60878: mysqladmin doesn't support flushing
      specific logs
    
    mysqladmin currently supports a command 'flush-logs'
    to flush all log types. However, mysqladmin offered
    no support to flush specific logs.
    
    Added support for additional arguments to
    mysqladmin fllush-logs. The syntax is :
    
    flush-logs [specific specific ...]
    specific = binary
                    | engine
                    | error
                    | general
                    | relay
                    | [1;31mslow[m
    
    Any other "specific" is interpretted as the next mysqladmin
    command.
    
    This results in COM_QUERY (vs. COM_REFRESH that
    was used before, but found lacking in functionality).
    
    Test case added.

[33mcommit d9cb2a9a14d7b0cd741e0e400c1ae51fc23269a3[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu May 8 08:02:59 2014 +0200

    Bug#18675099: WINDOWSCACHE.CMAKE MUST BE UPDATED FOR VS2013
    
    WindowsCache.cmake hardcodes results for CMake configure checks so they
    don't have to be performed on Windows (as this is very [1;31mslow[m).
    
    The problem was that the cache file had several values which were no
    longer correct. The consequence that we were using Windows specific
    workarounds in cases where it was no longer needed.
    
    This patch removes the outdated values from the cache file so that the
    checks are actually made. It also reorganizes the cache file so that it
    matches config.h.cmake, removes duplicate entries, removes dead
    entries and adds missing entries. The patch also removes some
    unneeded Visual Studio version checks.

[33mcommit c676cfd1b023fe9f096096c16791af7002347e1a[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 30 12:32:08 2014 +0200

    ndb_backup_rate.test checks that we are able to recover normal
    operation after a 'Redo log full' situation.
    
    This is done by configuring a [1;31mslow[m 'DiskCheckpointSpeed=1M', and
    letting the test program produce lots of logging by inserting and
    deleting rows while waiting for a 'Redo log full' error.
    
    However, when running with debug compiled binaries, the insert + delete
    performance could be so [1;31mslow[m that we are unable to fill the redo log.
    Thus this test often failed in the 'mix-debug' part of the test.
    
    This fix disable the test when run with debug binaries.
    NOTE: It was already disabled for Valgrind for the same reasons.

[33mcommit d477b01e706e06fabab3e8d59fcdfdee8279f753[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Apr 25 23:41:35 2014 -0700

    Delay after (rather than before) the first test iteration.
    This allows V8 to compile all the JavaScript code before dtrace starts running.
    High dtrace profiling rates caused the first iteration to run very [1;31mslow[mly
    and skewed the profile towards compile times rather than run times.

[33mcommit c50659443a0264ce80cdc8ec439bea7a1bb3f42c[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Mar 22 16:45:19 2014 -0700

    Encoders for NDB integer values: if the fast integer conversion fails, try a [1;31mslow[m one, then test whether the result is legal.  This should allow writing valid JS string and float values to int columns.

[33mcommit e79cadbb429ba2164cb1c6f3fe33af7c501887c5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Mar 19 15:07:55 2014 +0200

    Test push of WL#7142 InnoDB: Simplify tablespace discovery during crash recovery
    
    When the setting innodb_file_per_table=ON was introduced in MySQL 4.1,
    InnoDB crash recovery was changed so that the directories will be
    searched for *.ibd files if any redo needs to be applied.
    
    The scanning and opening of all *.ibd files (including ones for which
    no redo log needs to be applied) can be very [1;31mslow[m, especially on
    deployments that contain a large number of *.ibd files. Furthermore,
    if we allow a more liberal placement of tablespace files in the file
    system, we might have to extend the search to an even broader range of
    directories.
    
    This worklog eliminates the *.ibd file scan by guaranteeing the
    following:
    
    If there are redo log records for any non-predefined tablespace, there
    will also be an MLOG_FILE_NAME record.
    
    The InnoDB redo log format will be changed as follows:
    
    MLOG_FILE_NAME(space_id, filename): A new redo log record.
    Replaces MLOG_FILE_CREATE, MLOG_FILE_CREATE2.
    
    MLOG_FILE_RENAME2(space_id, old, new): The names will be file names
    (directory/databasename/tablename.ibd). Replaces MLOG_FILE_RENAME,
    which used table names (databasename/tablename).
    
    NOTE: We will write MLOG_FILE_NAME once since the latest redo log
    checkpoint. Immediately after a checkpoint, the log may contain some
    MLOG_FILE_NAME records that were "copied across the checkpoint" and a
    MLOG_CHECKPOINT marker to signal the end of a checkpoint.
    
    On redo log apply during crash recovery, we will scan the log up to
    three times:
    
    Recovery scan 1: Look for the first MLOG_FILE_CHECKPOINT marker since
    the latest checkpoint.
    
    If there is no MLOG_FILE_CHECKPOINT, we will skip the entire log. The
    data files will correspond to the system state as of the checkpoint.
    
    Recovery scan 2: Read the redo log since the latest checkpoint. Copy
    scanned records to recv_sys->addr_hash, and construct a map of
    recv_spaces, based on MLOG_FILE_NAME and MLOG_FILE_DELETE records.
    
    Before applying the records from recv_sys->addr_hash, we will check if
    any tablespace files are missing. If there are missing tablespaces, we
    will refuse to start up, so that the DBA can intervene, for example to
    manually rename files. This new safeguard of WL#7142 can be disabled
    by setting innodb_force_recovery.
    
    If not all redo log records in recv_sys->addr_hash, we will need a
    third log scan:
    
    Recovery scan 3: Read the redo log since the latest checkpoint. If
    recv_sys->addr_hash fills up, apply the batch of log records and read
    a new one.
    
    mlog_id_t: Remove MLOG_FILE_CREATE, MLOG_FILE_CREATE2, MLOG_FILE_RENAME.
    Add MLOG_FILE_NAME, MLOG_FILE_RENAME2, MLOG_CHECKPOINT.
    
    MLOG_FILE_FLAG_TEMP: Remove. This was a flag for MLOG_FILE_CREATE*.
    
    enum dict_check_t: Remove DICT_CHECK_ALL_LOADED. Crash recovery no
    longer loads all tablespaces.
    
    mtr_t::m_named_space: Associates a tablespace with a
    mini-transaction. A mini-transaction may be associated with up to one
    non-predefined tablespace. It may also modify predefined tablespaces
    for change buffering and undo logging.
    
    mtr_t::set_named_space(ulint space): Sets m_named_space.
    
    mtr_t::is_named_space(ulint space): Checks if the mini-transaction is
    associated with a given tablespace.
    
    mtr_t::Command::prepare_write(): Write an MLOG_FILE_NAME record if
    needed. This is executed as part of mtr_commit().
    
    mtr_t::commit_checkpoint(): A special method to emit redo log records
    to the redo log buffer when the caller already invoked
    log_mutex_enter(). This is only used by fil_names_clear().
    
    fil_space_t::max_lsn: LSN of the most recent fil_names_write() call,
    or 0 if the tablespace has not been dirtied since fil_names_clear().
    
    fil_space_t::named_spaces, fil_system_t::named_spaces: List of
    tablespaces for which MLOG_FILE_NAME has been written since the latest
    checkpoint.
    
    recv_sys_t: mlog_checkpoint_lsn: The LSN of the first scanned
    MLOG_CHECKPOINT record, or 0 if none was read yet.
    
    fil_space_get(): Look up a tablespace. This is invoked during
    mtr_t::Command::prepare_write() while not holding the log mutex, to
    prepare for a fil_names_write() call. The idea is to minimize the
    log_mutex hold time.
    
    fil_space_create(): Add an output parameter for returning a duplicate
    tablespace (same space_id).
    
    fil_space_free(): Make this an externally callable function, to free a
    tablespace from the cache when applying MLOG_FILE_DELETE.
    
    fil_space_free_low(): Renamed from fil_space_free(). The new wrapper
    fil_space_free() will acquire fil_system->mutex.
    
    fil_op_log_parse_or_replay(): Change the order of parameters. Remove
    log_flags, and rename parse_only to replay. We no longer attempt to
    replay log records of a multi-item mini-transaction, unless the
    MLOG_MULTI_REC_END was seen.
    
    fil_rename_tablespace(): Change the function signature. Take old_path,
    new_name, new_path_in. MLOG_FILE_RENAME2 is logging file names, not
    table names like MLOG_FILE_RENAME was. Also invoke fil_name_write().
    
    enum fil_load_status: Outcomes of fil_load_single_table_tablespace().
    
    fil_load_single_table_tablespace(): Do not exit on failure. Instead,
    return a status value to the caller.
    
    fil_load_single_table_tablespaces(): Remove. We no longer try to load
    all *.ibd files.
    
    fil_create_new_single_table_tablespace(): Do not write any
    MLOG_FILE_CREATE or MLOG_FILE_CREATE2. Instead, invoke
    fil_name_write() to write MLOG_FILE_NAME.
    
    fil_mtr_rename_log(): Change the signature. Take dict_table_t instead
    of names. Take a tmp_name.
    
    fil_names_write_low(): Write MLOG_FILE_NAME record(s) for a
    tablespace.
    
    fil_names_write(): Write MLOG_FILE_NAME record(s) for a tablespace if
    not already written since the latest checkpoint.
    
    fil_names_clear(): Write MLOG_FILE_NAME records and MLOG_CHECKPOINT on
    a log checkpoint or at system startup. If do_write=true, writes
    MLOG_CHECKPOINT even if no MLOG_FILE_NAME was written.
    Reset those fil_space_t::max_lsn for which fil_names_write() has not
    been invoked after the checkpoint LSN. Return true to the caller if
    any redo log was written.
    
    fil_op_write_log(): Replace log_flags with first_page_no, and replace
    table names with file paths. The parameter first_page_no is currently
    being passed as 0, because we do not have non-predefined multi-file
    tablespaces yet.
    
    fil_name_write(): Write an MLOG_FILE_NAME record for a file.
    
    Datafile::open_read_only(): Add the parameter bool strict.
    
    fsp_names_write(): Wrapper for mtr->set_named_space(). This must be
    called when a mini-transaction is going to modify a non-predefined
    tablespace.
    
    is_predefined_tablespace(): Check if a tablespace is a predefined one
    (system tablespace, undo tablespace or shared temporary tablespace).
    
    enum recv_addr_state: Add RECV_DISCARDED, so that buffered redo log
    records can be retroactively deleted if an MLOG_FILE_DELETE was
    later recovered for a tablespace.
    
    btr_free_but_not_root(), btr_free_root(): Call fsp_names_write().
    
    btr_cur_ins_lock_and_undo(), btr_cur_optimistic_insert(),
    btr_cur_pessimistic_insert(), btr_cur_update_in_place(),
    btr_cur_optimistic_update(), btr_cur_pessimistic_update(),
    btr_cur_del_mark_set_clust_rec_log(),
    btr_cur_del_mark_set_clust_rec(), btr_cur_optimistic_delete_func(),
    btr_cur_pessimistic_delete(): Call fsp_names_write() after successful
    locking and undo logging.
    
    btr_store_big_rec_extern_fields(), btr_free_externally_stored_field(),
    row_ins_index_entry_big_rec_func(): Call fsp_names_write().
    
    dict_build_tablespace(), dict_create_index_tree_step(),
    dict_recreate_index_tree(), fil_reinit_space_header(): Call
    fsp_names_write().
    
    page_cur_insert_rec_write_log(),
    page_copy_rec_list_to_created_page_write(),
    page_cur_delete_rec_write_log(), page_cur_delete_rec(), page_create():
    Assert that fsp_names_write() has been called.
    
    dict_table_rename_in_cache(): Pass old_path to
    fil_rename_tablespace().
    
    dict_check_tablespaces_and_store_max_id(): Remove the logic for
    DICT_CHECK_ALL_LOADED. We could probably remove this entire function,
    given that the maximum is also stored in the DICT_HDR page.
    
    mlog_write_initial_log_record_for_file_op(): Replaced by
    mlog_write_initial_log_record_low().
    
    log_checkpoint(): Before invoking log_write_up_to(), invoke
    fil_names_clear() to copy any MLOG_FILE_NAME records across the
    checkpoint. Flush the log up to the MLOG_CHECKPOINT marker, instead of
    only up to the checkpoint LSN. Without this step, the log between
    oldest_lsn and log_sys->lsn would be essentially corrupted (missing
    MLOG_FILE_NAME records on redo log apply). When the redo log scanner
    sees the first MLOG_CHECKPOINT since the latest checkpoint, it knows
    that there must be no missing MLOG_FILE_NAME record for any page
    operation on a non-predefined tablespace. If the MLOG_CHECKPOINT
    marker is missing, no redo log will be applied, and the system would
    be at the state of the checkpoint.
    
    fil_name_parse(): New function, to update the recv_spaces map based on
    MLOG_FILE_NAME and MLOG_FILE_DELETE records during recovery.
    
    recv_parse_or_apply_log_rec_body(), recv_parse_log_rec(): Add the
    parameter "apply". Do not apply file-level redo log records unless the
    entire mini-transaction has been recovered. Fail if an MLOG_FILE_NAME
    record is missing for a page-level operation.
    
    recv_recover_page_func(): Assert that no LSN is after the latest
    scanned redo log LSN.
    
    recv_parse_log_rec(): Check for some more log corruption.
    
    recv_parse_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    Add a parameter "apply" to specify whether log records should be applied
    (apply=false during the first scan for MLOG_CHECKPOINT). Return true
    if an MLOG_CHECKPOINT record was seen for the first time.
    Improve DBUG_PRINT output, and detect some more log corruption.
    
    recv_scan_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    
    recv_group_scan_log_recs(): Initialize the variables and data
    structures to begin reading redo log records. Add a parameter
    "last_phase" that is set when a multi-pass recovery is needed and we
    are scanning the redo log for a third time. In last_phase, we will
    invoke recv_apply_hashed_log_recs() to empty recv_sys->addr_hash
    between passes. If last_phase=false, we would stop filling
    recv_sys->addr_hash, only processing file-level redo log records.
    
    recv_init_crash_recovery(): Split some code into
    recv_init_crash_recovery_spaces(), to be invoked after the first call
    to recv_group_scan_log_recs().
    
    recv_recovery_from_checkpoint_start(): Invoke
    recv_group_scan_log_recs() up to 3 times if needed.
    After processing all redo log, write an MLOG_CHECKPOINT marker
    so that in case we will crash before making a checkpoint, the log
    will be replayed by subsequent crash recovery.
    
    checkpoint_now_set(): Avoid an infinite loop in case an MLOG_CHECKPOINT
    marker is the only thing that was written since the latest checkpoint.
    
    rb#4700r6

[33mcommit d620c25aef9254d47031ebf63c7a2a88e2ef46f1[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun Mar 9 19:33:46 2014 +0800

    Bug#17932935 CALLING IS_SEMI_SYNC_SLAVE() IN EACH FUNCTION CALL
                 HAS BAD PERFORMANCE
    
    Semisync master Binlog_transmit_observer needs to know if the
    dump thread is from a semisync slave. Because the logic is
    different between semisync slave and normal slave. So
    is_semi_sync_slave() is called in each Binlog_transmit_observer
    function. is_semi_sync_slave() reads the user variable
    'rpl_semi_sync_slave' through checking the dump thread's user
    variable hash table. That is very [1;31mslow[m and has remarkable impact
    on semisync replication performance.
    
    the user variable 'rpl_semi_sync_slave' is never changed after
    it is initialized. So we optimized the code as below:
    * is_semi_sync_slave() is only called in
      repl_semi_binlog_dump_start() when starting the dump thread.
      And then its value is stored in a pthread_key.
    
    * Other Binlog_transmit_observer functions just get it value
      through reading the pthread_key.

[33mcommit a53f8bdf1d4d11a822b8cfc671c3c3d76ef807ce[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Wed Mar 5 10:32:01 2014 +0530

    WL#6936 - Implementation of server-side statement timeout.
    
    This is a follow up patch for WL#6936 to fix max_statement_time
    test failure on some(may be [1;31mslow[mer) machines.
    
    - Test has SELECTs which checks result of event and triggers.
      These selects were timed out on some machines because of the delay.
      * Changed timeout value for such SELECTs.
    - Disabling timer for non-read only select is done after checking
      SELECT is read only or not at runtime. Since time out value was
      small for non-read only select before disabling timer query is
      timed out.
      Increased timeout value for non-read only select in test file.

[33mcommit 0c60d406c00c598a7c63eb7b962af0b89e432a06[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Feb 27 23:17:23 2014 +0400

    WL#7306 "Improve MDL performance and scalability by implementing lock-free
    lock acquisition for DML" and fix for bug #18077895 "WL7305 PUSH (7249)
    CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP UP TO -8%".
    
    The idea of this task is to change the acquisition of unobtrusive locks on
    the fast path, which currently involves acquisition of MDL_lock::m_rwlock,
    some checks, increment of the packed counter and release of m_rwlock, to
    a single atomic compare-and-swap operation.
    Similarly the release of the lock on the fast path becomes single atomic
    compare-and-swap (in absence of obtrusive locks and assuming we are not
    releasing the last lock for this MDL_lock object) instead of acquisition
    of MDL_lock::m_rwlock, decrement of the packed counter, some checks and
    m_rwlock release.
    As result these operations become at least twice cheaper than their
    old versions which has a nice effect on performance/scalability.
    
    Acquisition/release of locks on the [1;31mslow[m path (i.e. unobtrusive locks in
    presence of obtrusive locks and obtrusive locks) still has to use the old
    approach involving locking/unlocking MDL_lock::m_rwlocks and checks of
    MDL_lock::m_granted/m_waiting bitmaps/lists.
    
    This patch implements the above idea by performing the following
    three transformations:
    
    I)   MDL_lock::m_fast_path_granted_count is replaced with an atomic
         MDL_lock::m_fast_path_state member, which in the ideal case of
         "fast path" acquisition/release is checked and changed using CAS
         without holding any mutexes.
    II)  Since we would like to check in the same atomic CAS operation that
         MDL_lock object was not destroyed, its m_is_destroyed member is
         replaced by a IS_DESTROYED bit flag in the m_fast_path_state
         packed counter.
    III) Similarly, since we also would like to check in the same atomic CAS
         that there are no granted or pending obtrusive locks, we have to
         add a HAS_OBTRUSIVE bit flag in the m_fast_path_state, while
         keeping MDL_lock::m_obtrusive_locks_granted_waiting_count.
         This flag should be set when we are about to try acquiring an obtrusive
         lock and cleared once the last granted or pending obtrusive lock goes
         away.
    
    
    Most of the remaining changes in this patch are necessary in order to fix
    bug #18077895 "WL7305 PUSH (7249) CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP
    UP TO -8%".
    
    This bug manifested itself as a [1;31mslow[mdown for workloads involving 1 connection
    in cases when there were many concurrent connections to the same server in the
    past or there were many dormant connections at the same time as 1 active
    connection.
    
    In such scenarios the release of a metadata lock meant that MDL_lock became
    unused, was removed from lock-free hash with all lock objects and we
    tried to return it back to allocator. The latter operation involved
    scanning pins for all current and past connections, which became fairly
    expensive in this scenario.
    
    This patch solves this problem by avoiding releasing MDL_lock objects
    and removing them from the hash once they are no longer used. Instead we
    keep unused objects in MDL_map and start their eviction only if their
    number passes certain threshold and the ratio of unused/total lock objects
    is big enough. We evict random unused objects so on average objects
    which are used more often will stay in the hash and rarely used objects
    will go away.
    
    The above idea is implemented by:
    
    a) Introducing a new HAS_SLOW_PATH flag in the MDL_lock::m_fast_path_state
       member, which indicates if there any tickets in MDL_lock::m_granted
       and m_waiting lists or we are about try to add one. Thanks to this
       flag, it is possible to distinguish between used and unused MDL_lock
       objects in atomic compare-and-swap operations used to implement fast
       path acquisition and release of locks.
    b) Changing code which releases locks to avoid removing unused MDL_lock
       objects from the hash and deleting them afterwards. Instead we
       atomically increment the newly introduced MDL_map::m_unused_lock_objects
       counter. Similarly, on the first acquisition of lock for MDL_lock which
       was previously unused we atomically decrement this counter.
    c) In cases when the increment of the MDL_map::m_unused_lock_objects counter
       exceeds the threshold value and the unused/total objects ratio is high
       enough, we try to reduce the number of unused objects. We look-up a random
       unused object in MDL_map, mark it as destroyed, remove it from the hash and
       return it back to allocator. As a consequence MDL_map::remove() method
       has became MDL_map::remove_random_unused().
    d) To support the change described in c), a new lf_hash_random_match()
       function was introduced which allows us to efficiently find a random
       object which matches certain condition in LF_HASH.
    e) Also to support the change described in c), a new PRNG was added to
       MDL_context class. This PRNG is used as a source for randomness for
       look-ups of random unused objects.
    
    Unit tests were added covering handling of unused MDL_lock objects and
    for the new lf_hash_random_matches() function.
    
    
    Finally, this patch fixes a violation of the pinning protocol, which was
    introduced by WL7305 and which occured when the MDL subsystem failed
    to look up MDL_lock object in lock free hash. The LF_HASH documentation
    was updated to reflect the need to call lf_hash_search_unpin in this case.

[33mcommit f2ad5c9f4827af15c85738349dad042dc5d61008[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Feb 11 21:25:44 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Fix failed 'rpl.rpl_[1;31mslow[m_query_log' after merge

[33mcommit 1496cc312e678e0c9da3d7f8c2a9ae575a6b184c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Tue Dec 3 14:07:19 2013 +0400

    WL#7304 "Improve MDL performance and scalability by implementing 'fast-path'
    for DML locks".
    
    Since typical user workload consists mostly of DML statements it makes sense
    to improve performance/scalability by optimizing MDL subsystem for such type
    of statements.
    
    This patch implements "fast-path" for metadata locks acquired by DML
    statements. Acquisition/release of DML lock (S, SH, SW, SR locks) are
    converted into counter increment/decrement (under protection of
    MDL_lock::m_rwlock) instead of more complex code involving list
    manipulation (at expense of acquisition/release of locks typical for DDL
    statements).
    
    Such a step reduces size of critical section associated with
    MDL_lock::m_rwlock and increases scalability/performance in benchmarks
    involving DML workload. Particularly, benchmarking of draft patch
    implementing this idea shown that it provides at least 10% performance
    improvement in single-table OLTP_RO/POINT_SELECT SysBench tests.
    
    Details
    =======
    
    We split all lock types for each of MDL namespaces in two sets:
    
    A) "Unobtrusive" lock types
       1) Each type from this set should be compatible with all other
          types from the set (including itself).
       2) These types should be common for DML operations.
    
    We optimize acquisition and release of locks of this type by avoiding
    complex checks and manipulations on m_waiting/m_granted sets/lists and
    replacing it with a check of and increment/decrement of integer counters.
    We will call the latter type of acquisition/release "fast path".
    
    2) "Obtrusive" lock types
       1) Granted or pending lock of those type is incompatible with some
          other lock (including itself).
       2) Not common for DML operations
    
    These locks have to be always acquired in the old fashion - involving
    manipulations with m_waiting/m_granted sets/lists, i.e. using "[1;31mslow[m path".
    Moreover in the presence of active/pending locks from "obtrusive" set we
    have to acquire even locks of "unobtrusive" type using "[1;31mslow[m path".
    
    -------
    
    For GLOBAL/COMMIT and SCHEMA namespaces (i.e. namespaces with lock
    represented by MDL_scoped_lock class):
    
    "Unobtrusive" locks set consists of IX lock type.
    "Obtrusive" locks set consists of S and X lock types.
    
    For all other namespaces (i.e. represented by MDL_object_lock class):
    
    "Unobtrusive" locks set consists of S, SH, SR and SW locks.
    "Obtrusive" locks set consists of SU, SNW, SNRW and X.
    
    -------
    
    To implement the above MDL_lock object got two new members:
    
    - MDL_lock::m_obtrusive_locks_granted_pending_count - number of granted
      or pending locks of "obtrusive" types. Necessary to quickly verify that
      we can grant "unobtrusive" locks without further checking.
    
    - MDL_lock::m_fast_path_granted_count - packed counter of number of
      granted locks of specific "unobtrusive" type which were granted using
      fast-path algorithm and not using "[1;31mslow[m path" (e.g. for MDL_object_lock
      we use 20-bit chunks of this counter to represent number of S/SH, SR and
      SW locks acquired).
    
    The above two members are still protected by MDL_lock::m_rwlock lock.
    
    Essentially this patch replaces:
    
    1) addition/removal of "unobtrusive" lock to MDL_lock::m_granted list during
       lock acquisition/release with and incrementing/decrementing of corresponding
       part of m_fast_path_granted_count.
    2) check for granted or pending locks which conflict with "unobtrusive" lock
       is replaced with check on m_obtrusive_locks_granted_pending_count counter.
    
    We still allocate MDL_ticket objects for requests which are satisfied using
    fast path algoritm, but we mark them using MDL_ticket::m_is_fast_path member,
    so we know that such ticket can be released in simplified fashion.
    
    In order for deadlock detection algorithm to work properly we need to do so
    called "materialization" of fast path tickets for thread which is about to
    start waiting. This process clears MDL_ticket::m_is_fast_path flag, add ticket
    to appropriate m_granted list and decrement corresponding
    m_fast_path_granted_packed_count counter under protection of MDL_lock::m_rwlock.
    We also have to do this when acquiring "obtrusive" locks and locks for the
    threads with open HANDLERs.
    
    -------
    
    Unit tests for MDL subsystem were extended to cover scenarios important for
    the changes described above.
    Also this patch changes perfschema.mdl_func test to make it robust against
    line number changes in MDL code.

[33mcommit b3b0a645fc3b93278f9e1cc312efaba50fe4f9e2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Nov 25 11:02:29 2013 +0200

    Bug#17543588 PERFORMANCE REGRESSION (9%) FOR DBT-3 QUERY 21
    
    This is an unexpected regression from an earlier fix:
    
    Bug#16852278 SIMPLIFY RECORD COMPARISONS
    
    The reason of this regression is that memcmp() can be [1;31mslow[mer than a for loop.
    If memcmp() is a library call, there is an overhead involved,
    which we can avoid by comparing the first few bytes with a loop.
    The built-in memcmp() of GCC on x86 and AMD64 (repz cmpsb) is known to
    perform worse than the GNU libc library function:
    
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43052
    
    cmp_data(): Use a for loop to compare some first bytes.
    Then invoke memcmp() for any remaining common bytes if needed.
    
    innodb.cmake: Pass -fno-builtin-memcmp when compiling rem0cmp.cc
    with GCC on x86 or AMD64.
    
    rb#3931 approved by Jimmy Yang

[33mcommit 34ccf0b63a362d66b4ee96370fb37e70ae99ce1d[m
Author: Pavan Naik <pavan.naik@oracle.com>
Date:   Fri Nov 15 11:29:31 2013 +0530

    Bug#11756699 : MYSQL-TEST MAIN.LOG_TABLES-BIG FAILS ON 5.1.40
    
    Fix :
    
    Added "set @@global.log_output = 'TABLE';" line so that the details of SQL statements that
    take more than long_query_time seconds to execute are stored in mysql.[1;31mslow[m_log table.

[33mcommit 6fa03ee98ca231ac863de9468f8510e2cc9fcb32[m
Author: Libing Song <libing.song@oracle.com>
Date:   Thu Oct 24 14:06:49 2013 +0800

    WL#7169 Semisync: make master wait for more than one slave to ack back
    
    Postfix:
    - Set rpl_semi_sync_master_timeout longer to guarantee it doesn't timeout
      on [1;31mslow[m machines.
    - Adjusted coverage test order, so it can cover more code.

[33mcommit 07bb83ae15047c0c9868c44e4ce6e05390a96506[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Oct 18 09:24:05 2013 +0200

    Bug#17497869: NEW DEPRECATION WARNINGS INTRODUCE UNWANTED
                  PERFORMANCE DEGRADATION SIDE-EFFECT
    
    This patch changes the implementation of deprecation warnings
    to avoid using current_thd. This will slightly speedup
    generation of such warnings.
    
    But note that the generation of deprecation warnings is
    not free (due to e.g. memory allocation and localization).
    So deprecation a function will make it [1;31mslow[mer.

[33mcommit fafdf33d714036f6c59694615dc480478e71fe77[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 16 09:56:18 2013 +0200

    Bug#16924125: LOG_SLOW_STATEMENT COULD BAIL OUT EARLIER IF
                  SLOW LOGGING IS DISABLED
    
    The problem was that checking if the [1;31mslow[m query log was enabled,
    was one of the last checks that was done before writing to the
    [1;31mslow[m query log. Since the [1;31mslow[m query log by default is disabled,
    it makes sense to have this check earlier.
    
    This patch moves the check from Query_logger::[1;31mslow[m_log_write() to
    the start of log_[1;31mslow[m_applicable().
    
    As this is a minor optimization with no changes of behavior,
    no test case is added.
    
    The patch also fixes the --log-throttle-queries-not-using-indexes
    startup option so that it requires a value (=# instead of [=#]).

[33mcommit e6f1e794338833cd546026d4e2cbbfa3d889a4de[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Aug 16 20:17:40 2013 +0100

    WL#6661: Error logging: Allow control of verbosity
    
    In mysqld, all local messages ("things found on
    console / in error log") now go through a central
    function rather than through a lot of fprint()s.
    This means we can properly timestamp those lines
    with ISO 8601 compliant timestamps (likewise for
    [1;31mslow[m and general log files); it also means that
    we can gag those messages with the new system
    variable / startup option --log_error_verbosity,
    which depraces --log_warnings / -W.
    As timestamps go, the DBA can choose between system
    time or zulu time with the new option --log_timestamps.
    
    Client programs now print their warnings prefixed by
    their name (no path, no ".exe") and the severity level
    in brackets, so "Warning: foo" from bin/mysql.exe
    becomes "mysql: [Warning] foo"
    This is useful especially in mysql_upgrade (which in
    turn calls other binaries) so we can see whether a
    warning was thrown by mysql_upgrade or one of its
    children (and in the latter case, which).
    
    On the tech side, we now have a generic hook for
    local messages. This prints to stderr by default
    (and in the case of client apps likely always will),
    but can be overridden to log elsewhere, mangle the
    string, etc. (as is done by the server once the
    facilities are initiliazed; then this goes to
    error_log_print, for better timestamps, filtering by
    verbosity level, and for printing to NT syslog on Win).
    
    We deprecate my_printf_warning() which does something
    similar, with a less clear name.
    
    New test cases added, comments clarified, old test cases
    updated.

[33mcommit f493bbee2c8bc55cf56cf9f8d893c8c8a87209f0[m
Author: viswanatham gudipati <viswanatham.gudipati@oracle.com>
Date:   Thu Jun 20 13:00:01 2013 +0530

    BUG#16988017: disbaled due to [1;31mslow[m machine

[33mcommit 4a87b30d4029ffb4d845554e67f4dbe7c6a31fda[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Thu May 30 13:00:48 2013 -0500

    Bug#16669321 - PERFSCHEMA.PFS_UPGRADE_EVENT UNSTABLE AFTER SWITCHING
                   STORAGE ENGINE TO INNODB
    
    In PFS tests that use mysql_upgrade, disable the general log and
    the [1;31mslow[m query log to avoid spurious occurrences of
    "ERROR 1194: Table 'general_log' marked as crashed".

[33mcommit 3d718e0b1666125e2d9d64a52b33074ea7857777[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed May 29 15:37:52 2013 +0200

    ndb -
    1) increase retires of NdbMgmd::connect() to make testMgm* more resilant to overload on [1;31mslow[m club machines
    2) fix compiler warning in testMgmd.cpp

[33mcommit 97bf1d952cb98028c6549fece6c2c402bf42d6a3[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue May 28 16:19:40 2013 +0530

    - Bug #16664150: INNODB.INNODB-WL6445-2 FAILS - MYSQLD FAILED DURING TEST RUN
      Trying to start innodb instance in read only mode when previous instance
      was not shutdown properly will result in error.
      Previous instance is shutdown using shutdown_server <timeout> command.
      Time taken by server to shutdown on most machines if <= 10 secs and so
      default used when test is written.
      weekly-trunk test frmk exercises lot of tcs with more than feasible
      parallelism and on machines that seems to be dead-[1;31mslow[m.
      In other words 10 secs is not enough so increasing it to 60 so that
      TCs passes on all machine configuration.
    
      Approved by: Sunny (over IM)

[33mcommit 1384db9b5b36a77d579851b2f36726eeb9cfd70f[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu May 16 16:35:06 2013 +0300

    Bug#16723431 REMOVE SRV_LATIN1_ORDERING
    
    Remove the special handling of the latin1_swedish_ci collation.
    
    srv_latin1_ordering, DATA_MYSQL_LATIN1_SWEDISH_CHARSET_COLL: Remove.
    
    fts_get_charset(): Inlined, to replace innobase_get_fts_charset().
    
    innobase_mysql_cmp_prefix(): Remove.
    
    cmp_dfield_dfield_like_prefix(): Call the collation function directly,
    instead of calling innobase_mysql_cmp_prefix().
    
    innobase_mysql_cmp(): Make inline in rem0cmp.cc, remove parameters.
    
    cmp_whole_field(): Call the collation functions directly.
    
    MYSQL_PLUGIN_IMPORT: Remove; unused definition.
    
    eval_cmp_like(): Abort on IB_LIKE_SUFFIX or IB_LIKE_SUBSTR.
    Earlier, the abort was triggered later, in the removed functions
    cmp_data_data_[1;31mslow[m_like_substr() and cmp_data_data_[1;31mslow[m_like_suffix().
    
    fts_utf8_string_cmp(): Replace with innobase_fts_text_cmp(). The
    function name was misleading; this function actually used
    latin1_swedish_ci aka srv_latin1_ordering[] instead of UTF-8.
    
    fts_utf8_string_cmp_prefix(): Remove; unused function.
    
    cmp_data_data_[1;31mslow[m_varchar(), cmp_data_data_[1;31mslow[m_like_prefix(),
    cmp_data_data_[1;31mslow[m_like_suffix(), cmp_data_data_[1;31mslow[m_like_substr(): Remove.
    
    cmp_dfield_dfield_like_prefix(): Make inlined. Always invoke
    innobase_mysql_cmp_prefix() instead of sometimes invoking
    cmp_data_data_like_prefix().
    
    cmp_dfield_dfield_like_substr(), cmp_dfield_dfield_like_suffix(): Remove.
    
    cmp_collate(): Remove.
    
    cmp_whole_field(): Handle DATA_VARCHAR and DATA_CHAR as
    my_charset_latin1.  These types should only be used by the internal
    SQL parser and for the internal data dictionary.
    
    cmp_data_data(), cmp_dtuple_rec_with_match_low(),
    cmp_rec_rec_simple_field(), cmp_rec_rec_with_match():
    Handle most types with cmp_whole_field(). Do not call cmp_collate().
    
    rb#2350 approved by Jimmy Yang

[33mcommit fd7b88c881dc745bfbf399344bc3188e9467b08e[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon May 13 14:57:41 2013 +0200

    Bug#16736776 GET RID OF DYNAMIC_ARRAY IN THE RANGE OPTIMIZER
    
    Get rid of DYNAMIC_ARRAY:
     - it is not type safe, makes for unreadable/unmaintainable code
     - it is [1;31mslow[m: we always malloc in QUICK_RANGE_SELECT::QUICK_RANGE_SELECT
       and malloc is expensive, esp. with many threads.

[33mcommit e8f31383699488df7a7ba60235b2ac6ec36b9b87[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Apr 9 11:11:25 2013 +0200

    Bug#16595917 NDB_DESC FAILS ON FREEBSD 9
    
     - short lived NdbApi connections and [1;31mslow[m machine revealed a case where the receive thread
       tried to use ClusterMgr efter it had been stopped. This resulted in dereferncing a NULL pointer.
     - Fix by reorder the code so that ClusterMgr is stopped after receive and send thread
     - Remove two outdated comments.

[33mcommit f757293bc8bb49e83c3a65125dd90282cbda397c[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Mon Mar 11 18:06:03 2013 +0530

    Bug#14749800 - ERROR 1571 (HY000): ERROR DURING STARTING/
                   STOPPING OF THE SCHEDULER. ERROR CODE.
    
    Bug#14781590 - SET GLOBAL EVENT_SCHEDULER = 1 SHOULD BE
                   SET TO 0 ON ERROR
    
    Analysis:
    -----------
    In cases when we failed to create thread for event scheduler,
    event execution or simply for handling new connection nothing
    was logged into error log.
    
    Also system variable EVENT_SCHEDULER was not reset to OFF when
    we failed to create event scheduler thread. This caused wrong
    impression that event scheduler was running in cases when it is
    not.
    
    Fix:
    -----------
    Added writing message with errno to error log in the above cases.
    Also adjusted other messages which were written to log when we
    failed to create thread in other cases to contain an errno.
    Also reset system variable EVENT_SCHEDULER to OFF on failure to
    event scheduler create thread.
    
    But logging error for thread creation failure for new connection
    may spam error log.Used log throttling mechanism to avoid spamming
    error log in such a situation. To do this kept generic log-throttling
    functionality in Log_throttle class, moved functionality specific
    for [1;31mslow[m-log to Slow_log_throttle class. Added new
    Error_log_throttle class to be used for throttling of writing
    to error log.

[33mcommit 3c9b69e09a894f220b7e24cf67b56e5c85aac7cd[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Mon Mar 18 12:35:26 2013 +0530

    BUG#15930031 - SETTING SLOW_QUERY_LOG_FILE TO A VALUE WITHOUT
                   A "=" SIGN ENABLES GENERAL LOG
    
    DESCRIPTION:
    
    While checking server startup options from configuration file,
    MySQL checks if the option is a struct option, by finding dot
    ('.') and checking if dot lies before or after '='. If dot is
    before '=' then the startup option will be just after dot. If
    dot lies after '=' then it is a part of argument for startup
    option. If '=' is not in the startup option and dot is there,
    then check_struct_option() should return pointer to the begin
    of startup option. But in practice even if the argument of
    startup option has a '.', check_struct_option() treats it as
    key_name. For example if "[1;31mslow[m_query_log_file [1;31mslow[m1.general_log"
    is in configuration file, general log will be turned on.
    
    FIX:
    
    Condition added to make sure that dot is considered as a
    part of startup option only if it is before '=' sign and
    there is no space before dot.

[33mcommit 05fe0cb47bbf6f6dcebde0a0c0dd1bc5a31a3257[m
Author: Akhila Maddukuri <akhila.x.maddukuri@oracle.com>
Date:   Fri Mar 1 11:44:03 2013 +0530

    Enabled embedded run for the following tests as
    "Bug #16029012 MTR SHOULD USE INNODB AS THE DEFAULT STORAGE ENGINE
    FOR EMBEDDED SERVERS" is fixed.
    
    1) partition_exch_qa_2
    2) character_set_database_func
    3) collation_database_func
    4) collation_server_func
    5) log_[1;31mslow[m_admin_statements_func
    6) max_seeks_for_key_func
    7) sql_quote_show_create_func

[33mcommit 5ece4a68df45beb438012d3cb9778328e80ce458[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Jan 24 15:02:44 2013 +0100

    WL#6613: Refactor logging code - split logging and binlogging code
    
    - Move replication code out of log.h and log.cc.
    - Remove MYSQL_LOG and move implementation to MYSQL_QUERY_LOG
      and MYSQL_BINARY_LOG.
    - Move Log_throttle, log_[1;31mslow[m_applicable(), log_[1;31mslow[m_statement()
      and log_[1;31mslow[m_do() to log.h/log.cc.
    - Cleanup interaction between sys_vars and log, move code from sys_vars to log.
    - Make usage of LOCK_global_system_variables & LOCK_logger more consistent.
    - Remove external access to LOCK_logger.
    - Early exit if logs are closed (e.g. don't lock mutexes).
    - Cleanup construction/destruction.
    - Use common code for reopen/flush.
    - Simplify LOGGER implementation - use enum_log_table_type more.
    - Remove some global functions, use LOGGER functions directly.
    - Make everything but LOGGER private.
    - Write to error log without using log_event_handler since we only support
      error logging to file anyway.
    - Splitt error logging and [1;31mslow[m/general log (LOGGER).
    - Simplify error logging functions.
    - Remove sql_print_message_handlers[].
    - Remove dead code.
    - Update log.h/log.cc according to current code standard.
    - Use true/false rather than TRUE/FALSE.
    - Use bool rather than my_bool.
    - Use size_t.
    - Indentation/spacing cleanup.
    - Move function documentation to header file and make into doxygen.
    - Improve code documentation, add missing doxygen.
    - Add documentation of query logging sysvars.
    - Add simple unit test for Log_throttle
    - Renames:
      opt_log => opt_general_log
      opt_logname => opt_general_logname
      opt_log_raw => opt_general_log_raw
      mysql_log => mysql_general_log
      key_file_query_log => key_file_general_log
      LOGGER => Query_logger
      MYSQL_QUERY_LOG => File_query_log
      event_coordinates => rpl_event_coordinates

[33mcommit 7e243db421e79b53c0be2c51a0384048153d8f99[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Jan 14 13:56:56 2013 +0200

    Bug#15920744 USE STD::SORT WHEN POSSIBLE
    
    Replace the InnoDB homebrew UT_SORT_FUNCTION_BODY (a recursive merge
    sort) with the C++ STL std::sort() in most places, because the latter
    needs less memory (avoiding a second copy of the array, and avoiding
    recursion) and is faster due to inlined execution.
    
    The row_merge_buf_sort() of CREATE INDEX will be kept as is, because
    both std::sort() or qsort_r() can apparently cause more calls to our
    costly tuple comparison routine. That is, the results vary depending
    on the table and index definition, and on the compilation flags.
    
    cmp_data_data_[1;31mslow[m(): Replace with cmp_data_data().
    
    buf_dump_cmp(), buf_dump_sort(): Replace with std::sort().
    
    ut_ulint_sort(): Replace with std::sort().
    
    page_zip_dir_cmp(), page_zip_dir_sort(): Replace with std::sort().
    
    page_zip_dir_decode(): Remove recs_aux[].
    
    rb#1844 approved by Jimmy Yang

[33mcommit 885183c8c189bbd1c79cb623f901527031c23f66[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Dec 19 20:17:00 2012 +0100

    ndb - dbacc, calculate the bits needed for quicker exact key operations
    
    this bug should result in extra [1;31mslow[m exact key operations for big
    tables since it relaxed the condition of valid bits in the reduced
    hashvalue stored for each element in the unique hash index.
    this makes the reduced hashvalue less selective for filtering out
    possible matches in a bucket, so more elements are compared by
    value with the key.

[33mcommit e735ee6303765f1a7e1d6fa73c4a34ed3695f112[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Dec 3 12:13:01 2012 +0530

    Bug#11753843:BINARY AND RELAY LOG FILE NAMES DEPEND ON PID
    FILE NAME
    
    Problem:
    =======
    When Binary and relay log names are not specified by user,
    the default binary log .NNNNNN and .index files are created
    in the data directory using the PID file as basename. The
    same is true for the relay log file, [1;31mslow[m log file and query
    log file.
    
    According to the documentation, the default names should be
    based on the hostname.
    
    Analysis:
    ========
    "rpl_make_log_name" is the function which constructs the
    basenames for binary and relay log files.   This function
    takes three arguments, first the "basename" supplied by
    user through option, second the "pid-file" name and third
    the extension to the file name. The second argument
    "pid-file" is used as basename in the absence of user
    specified basename value.
    
    Fix:
    ===
    In "rpl_make_log_name" function call second parameter i.e
    "pid-file" is being replaced with "default_logfile_name".
    "default_logfile_name" holds the hostname. While testing
    the above fix another problem was observed. When user
    specifies the option like "[1;31mslow[m_query_log_file=" or
    "general_log_file=" in the conf file without specifying
    the file name it causes an invalid memory free. The
    code snippet is given below.
    
    if (!VAR || !*VAR)
    {
        my_free(VAR); /* it could be an allocated empty string "" */
        VAR= ALT;
    }
    
    Memory for all options are allocated by using "malloc"
    within a memroot. When user specifies "empty" string for
    "[1;31mslow[mlog" and "querylog" files the above code tries to
    free the memory for the empty string and reassign the pointer.
    Freeing a part of memory within a memroot is incorrect this
    causes the invalid memory free error. Since it is an empty
    string and memroots are automatically cleared upon exit
    the above "my_free" call is not required. Hence removed.

[33mcommit 66bfcd8ae65cf9d191c61606d8935666e54e5339[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu Nov 29 16:35:49 2012 +0000

    Bug #15935206   NDB : IMPROVE VISIBILITY OF TRANSPORTER OVERLOAD OCCURRENCES
    
    NdbInfo.transporters extended with 5 new columns :
    
    connect_count  (Number of times connection established on this transporter)
    overloaded     (Whether this transporter is currently overloaded.  0 or 1)
    overload_count (How many times this transporter has entered overloaded state since connecting)
    [1;31mslow[mdown       (Whether this transporter is currently in scan [1;31mslow[mdown state.  0 or 1)
    [1;31mslow[mdown_count (How many times this transporter has entered scan [1;31mslow[mdown state since connecting)
    
    Note that the counters are reset on *CONNECT*, so will retain their values after
    the remote node disconnects.
    Note also that the existing bytes_send and bytes_received counters are
    *CHANGED* to be reset on *CONNECT* as well, so will retain their values after the
    remote node disconnects.  This is a change to the previous behaviour, where
    they were reset on DISCONNECT.
    
    Additionally 6 new per-LQH-instance counters are added to NDBINFO.COUNTERS :
    
     LQHKEY_OVERLOAD
       Number of primary key requests rejected at LQH block instance due
       to transporter overload
     LQHKEY_OVERLOAD_TC
       Count of instances of LQHKEY_OVERLOAD where the TC node transporter
       was overloaded
     LQHKEY_OVERLOAD_READER
       Count of instances of LQHKEY_OVERLOAD where the Api reader (only reads)
       node was overloaded.
     LQHKEY_OVERLOAD_NODE_PEER
       Count of instances of LQHKEY_OVERLOAD where the next backup data node
       (only writes) was overloaded
     LQHKEY_OVERLOAD_SUBSCRIBER
       Count of instances of LQHKEY_OVERLOAD where a event subscriber
       (only writes) was overloaded.
     LQHSCAN_SLOWDOWNS
       Count of instances where a fragment scan batchsize was reduced
       due to scanning Api transporter overload.
    
    These new values and counters make transporter overload and SendBuffer
    sizing problems more visible and debuggable.

[33mcommit 599e5f57334b34fb469cc6336411e24640531891[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Nov 23 12:57:28 2012 +0100

    Fix failing ndb_bushy_joins.test under Valgrind
    
    Extends the TimeBetweenEpoch (x 10) to avoid failure due to
    [1;31mslow[m valgrind tests exceeding MaxBufferedEpochs

[33mcommit ff362e667df80db29b07a219a9016b6733a4a878[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Sat Nov 17 17:39:06 2012 +0530

    BUG#14512467 - SLOW QUERY LOG: SERVER RESTART NOT NEEDED IN 5.1
                   AND LATER WITH PERMISSION ISSUE
    
    Description:
    
    If MySQL server is started with --[1;31mslow[m_query_log option but the
    user does not have write permission on the [1;31mslow[m query logging
    file, MySQL server starts with an error message. Error message
    says that MySQL server has turned logging off. It also says that
    after fixing the issue, logging can be started again by
    restarting the server. The same problem exists with general query
    log.
    
    Analysis:
    
    Though, error message says that MySQL server has turned logging
    off when sufficient file permission is missing, global variable
    [1;31mslow[m_query_log is not set to 'OFF'. After fixing the file
    permission issue, it is not needed to restart the server.
    Commands can be issued from the client only to restart the query
    logging.
    
    Fix:
    
    Changed the error message to show user both the options: either
    restart query logging or restart the mySQL server.Set the global
    variable [1;31mslow[m_query_log to OFF when this error occurs.

[33mcommit f15f55c05227f9508440c9ad72872dfd6cde47cf[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Sat Nov 17 17:11:34 2012 +0530

    BUG#14711347 - TURNING SLOW QUERY LOG ON WITHOUT FILE PERMISSIONS
                   CRASHES SERVER
    
    
    Description:
    
    If SET GLOBAL [1;31mslow[m_query_log='ON' command is given from MySQL
    client to enable [1;31mslow[m query logging, but the user does not have
    write permission on the [1;31mslow[m query logging file, MySQL server
    crashes due to an assert failure. The same problem exists for
    general_log option also.
    
    Analysis:
    
    When activate_log_handler() opens the file for logging, it is
    not checking whether the file is successfully opened or not for
    query logging.
    
    Fix:
    
    Added the condition on the open_[1;31mslow[m_log() and open_query_log()
    to check for errors in file opening. The return flag value is
    changed based on finding of errors in opening value. Test case
    added.

[33mcommit 6b39379e12cf243525014c14d5cbedff90625a69[m
Author: horst.hunger@oracle.com <>
Date:   Thu Nov 1 16:40:09 2012 +0100

    Slow launch time is too [1;31mslow[m for long running test jobs like in valgrind. increaed it from 1000 to 10000.

[33mcommit 263f00517623b555ab10255ce90399529fcf9c73[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Mon Oct 29 18:34:05 2012 +0000

    Bug #14828998   NDB : SLOW FILESYSTEM CAN CAUSE DIH FILE PAGE EXHAUSTION
    
    Limit number of concurrent table definition updates that DIH can issue.
    This avoids a [1;31mslow[m filesystem exerting pressure on DIH File page buffers which
    can lead to a crash if they're exhausted.
    
    Add a testcase showing the crash behaviour using error insert and
    showing that it fixes the problem.

[33mcommit fd3dfc487952cd7ce1ffca4707a951030032ab3f[m
Author: kevin.lewis@oracle.com <>
Date:   Tue Oct 23 15:51:46 2012 -0500

    Bug #14520559 - INNODB; LARGE BLOB INSERTS ARE SLOW AND TROUBLESOME
    
    Large BLOBS are [1;31mslow[m with debug builds due to the large number of
    calls to mtr_memo_contains() and the extra large size of the main
    MTR.  Non-debug builds do not seem to suffer noticeably. That
    problem is not addressed by this patch.
    
    This patch fixes the 'troublesome' assert that happens with 4k page
    size and with some Compressed tablespaces.  The assert is described
    fully in the bug report.  It has to do with how a system tablespace
    is initially expanded.  At 32Mb, it is normally expanded by more than
    one extent at a time.  But with 4k pages, that must happen sooner.
    
    http://bur03.no.oracle.com/rb/r/1301/ approved by Marko.

[33mcommit aa0203ba8595b3eb5a09e6a5081a859c94bfa1bb[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri Sep 21 13:41:35 2012 +0530

    BUG# 12793893 - binlog_innodb timed out in pushbuild.
    
    === Description ===
    
    The test contained a transaction that ran a loop 2000
    times. In every iteration, a row was being being
    inserted into a table. 2000 iterations to build up the
    long transaction sometimes proved to be too costly on
    [1;31mslow[m platforms.The purpose behind 2000 iterations was to
    make sure that the transaction used the temporary binary
    log cache and also exceeded the value of
    "binlog_cache_size".
    
    === Fix ===
    
    I toned down the loop at the same time I made sure it
    did not break the purpose.
    1) Made the loop run 600 times.
    2) Decreased the value of "binlog_cache_size" to 4096,
       the minimum allowed value defined in the manual.

[33mcommit 1f52dc987c0bf037f3a1fa9f31bdf86fd3a858b7[m
Merge: d102dec9eb4 7e7d6dddd95
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Sep 21 07:57:38 2012 +1000

    Merge from mysql-trunk-wl6445.
    
    Allow InnoDB to work on read-only media e.g., DVD, CDROM . Introduce a new
    configuration parameter:
    
      --innodb-read-only
    
    When this configuration parameter is set:
    
     1. Open ALL the tables in read-only mode
    
     2. If the system was not shutdown cleanly then refuse to start.
    
     3. Disable all background threads except the IO read threads. In a 100%
        pure RO workload deadlocks should not happen.
    
     4. Disable writes to temp files, InnoDB uses temp files to log information
        like deadlocks, monitor output, SHOW ENGINE INNODB STATUS etc.
    
     5. If there are entries in the UNDO log  we should  still be able to start
        normal operation. Purge will not remove the delete marked entries from
        the tables and will not trim the UNDO logs because the purge thread(s)
        will be disabled.
    
     6. Refuse to start if there are entries in the change buffer. The user must
        do a [1;31mslow[m shutdown or disable change buffering when creating the database
        that is going to be shipped on RO media.
    
        --innodb-change_buffering=OFF
    
     7. Users should ensure that they set the REDO log file to the smallest size
        possible (1M) before moving to read-only media. The REDO log is only used
        to check whether the database was shutdown cleanly or not.
    
     8. Some optimisations that are not done yet.
        1. Disable read view creation and deletion.
    
        2. Transaction start and commit, running in AUTO-COMMIT mode should
           achieve a similar effect for now.
    
     9. Should be possible to run multiple RO instances against the same
        set of (shared) RO files.
    
     10. This new parameter is independent of --read-only which is a server
        layer parameter
    
    Limitations:
       SHOW ENGINE INNODB STATUS; wont work, it will return an empty set.

[33mcommit 0daaf8aecd8f84ff1fb400029139222ea1f0d812[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Sep 20 12:34:31 2012 +0530

    BUG#11763447: 'YOU CANNOT 'ALTER' A LOG TABLE IF LOGGING IS ENABLED'
                   EVEN IF I LOG TO FILE.
    
    Analysis:
    ----------
    
    MYSQL_UPGRADE of the master breaks the replication when
    the query logging is enabled with FILE/NONE 'log-output'
    option on the slave.
    
    mysql_upgrade modifies the 'general_log' and '[1;31mslow[m_log'
    tables after the logging is disabled as below:
    
    SET @old_log_state = @@global.general_log;
    SET GLOBAL general_log = 'OFF';
    ALTER TABLE general_log
    MODIFY event_time TIMESTAMP NOT NULL,
    ( .... );
    SET GLOBAL general_log = @old_log_state;
    
    and
    
    SET @old_log_state = @@global.[1;31mslow[m_query_log;
    SET GLOBAL [1;31mslow[m_query_log = 'OFF';
    ALTER TABLE [1;31mslow[m_log
    MODIFY start_time TIMESTAMP NOT NULL,
    ( .... );
    SET GLOBAL [1;31mslow[m_query_log = @old_log_state;
    
    In the binary log, only the ALTER statements are logged
    but not the SET statements which turns ON/OFF the logging.
    So when the slave replays the binary log,the ALTER of LOG
    tables throws an error since the logging is enabled. Also
    the 'log-output' option is not checked to determine
    whether to allow/disallow the ALTER operation.
    
    Fix:
    ----
    The 'log-output' option is included in the check while
    determining whether the query logging happens using the
    log tables.

[33mcommit 9d9be9ff38e5256652e1046b436fbc735376aa03[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu Sep 13 21:18:47 2012 +0100

    Bug #14472648   CONFIGURED DISKCHECKPOINTSPEED EXCEEDED WHEN BACKUPMAXWRITESIZE SET TO HIGH VALU
    
    The DiskCheckpointSpeed mechanism is implemented using 100 millisecond
    periods, which each have 1/10th of the configured quota available.
    
    A period is allowed to overflow its quota, with the excess being taken
    from the next period's quota.
    
    However, this overflow was limited to the next period, after that, any
    further overflow was ignored.
    
    In cases where large overflows were possible, relative to the 1/10
    DiskCheckPointSpeed quota, this could result in excessive disk writing,
    and CPU overhead as a result.
    
    Setting a larger-than standard MaxBackupWriteSize is the primary means
    of causing larger-than 2* quota overflows and triggering this bug.
    
    This bug is fixed by using as many subsequent periods as necessary to
    'pay off' the quota overflow.
    
    This will result in the data node staying within its quota.
    
    This fix may result in [1;31mslow[mer LCP in some systems, and reduced CPU usage
    during LCP.
    
    A testcase, and an internal DiskCheckPointSpeed verification mechanism
    are added to avoid future regressions.

[33mcommit 3ecd7517e951fc7d84cacc0726342d48a08a92ac[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Aug 22 11:09:04 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    Do not select compress_time and uncompress_time because their values are
    nondeterministic. On [1;31mslow[m platforms (like Valgrind) it may end up 1 instead
    of 0, causing the whole test to fail.

[33mcommit f14d2bfcaa224f26b9e7313d13cabd1da2256358[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Aug 10 10:52:51 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    Protect access to the per index stats container with a mutex. We choose to
    use a mutex instead of rwlock because the performance critical part is in
    page_zip_compress() and page_zip_decompress() where we need write access.
    So a rwlock would not increase the concurrency of the performance critical
    code but it is [1;31mslow[mer to acquire than a mutex.
    
    Hide "map<index_id_t, page_zip_stat_t>" behind a typedef.

[33mcommit a8030adf89eac0f71096a3e81d58dff71a80f7c1[m
Merge: 94174cd732f bc757f3c319
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Tue Aug 7 03:38:19 2012 +0100

    auto-merge of
    Bug#14073554: Don't call mysql_rewrite_query if no binlog, general log, [1;31mslow[m log is enabled

[33mcommit c6fb7e0600b4c4581f2939cccc5b30bd00ec901f[m
Merge: 03750bf05a6 802987a413f
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Mon Aug 6 06:32:04 2012 +0100

    Bug#14236124:Wrong connection ID (thread ID) in the general and [1;31mslow[m query logs
    
    manual up-merge of the assorted patches for FILE and TABLE sinks for general and [1;31mslow[m query log from 5.6

[33mcommit 802987a413ff0a880d9d3a55c2c28e4c954e935b[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Mon Aug 6 05:11:36 2012 +0100

    Bug#14236124: WRONG CONNECTION ID (THREAD ID) IN THE GENERAL AND SLOW QUERY LOGS
    
    Optional add-on to previous patch which corrected logging to file)
    wherein we change width of thread-ID field in [1;31mslow[m and general log
    tables from INT to BIGINT (4 to 8 bytes), for correct logging of
    very large thread-IDs.

[33mcommit 0911753c554c875565a88cf36343355d65c0a5b8[m
Author: Manish Kumar <manish.4.kumar@oracle.com>
Date:   Wed Jul 18 11:44:37 2012 +0530

    BUG#14237992 - RPL_SEMI_SYNC FAILURES ON TRUNK AS PART OF WL#5223
    
    Problem: The rpl.rpl_semi_sync test fails sporadically and more
             frequently after wl#5223. The problem is that the slave is
             unable to get in sync with the master on [1;31mslow[m platforms in the
             default duration of the timeout. So the wait_for_status_var fails.
    
    Fix: The problem is fixed by hardening the test case with new checks
         and also increase the timeout duration during the call of the
         .inc file in the test, in order to let the slave catch up with the
         master.

[33mcommit ed45098a82c9f5df21b35de6e42f559be1a068cb[m
Author: Hemant Kumar <hemant.hk.kumar@oracle.com>
Date:   Wed Jul 4 10:26:11 2012 +0530

    Considering the time taken for this test on [1;31mslow[m solaris platforms making it a big test.

[33mcommit aeb811f6f8e26977d24729af098841b0f8a50f54[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Mon Jul 2 04:43:35 2012 +0100

    Bug#14236124: WRONG CONNECTION ID (THREAD ID) IN THE GENERAL AND SLOW QUERY LOGS
    
    Thread-IDs can get very large; patch adjusts general-logging
    and [1;31mslow[m query logging to reflect that and log those IDs correctly.

[33mcommit cd566c86459bc3c71bc295ec0db5fc8c9e166a7b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 7 16:46:55 2012 +0200

    ndb - [1;31mslow[mdown scans
    
    Invokes [1;31mslow[mdown of scans when we reach 60% fill level instead of at blocking level at
    75% where primary key access is even stoppped (this is fill level of send buffer).
    
    Part of Mikael Ronstroms "Patches used in benchmark tree with Intel"

[33mcommit 516ce09ccfafae05ecd1e56994b1bbe193131148[m
Author: Maitrayi Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed May 2 13:31:46 2012 +0200

    Fix NF_Join failure on [1;31mslow[m machines - reduce client load

[33mcommit 8f038bf8d2a662555fde72fa83d0c3a8e2273947[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Feb 1 11:23:10 2012 +0100

    ndb - change ndb_statistics.inc to perform insert into myisam and the alter to ndb, to avoid timeout issue on really [1;31mslow[m machines in PB2

[33mcommit 0f03e2f68e2154963b0b53c0fefd8a7258860c95[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Tue Dec 20 09:42:46 2011 +0100

    ndb - adopt some testSystemRestart tests to [1;31mslow[mer machines

[33mcommit d7cd8e5ec883b4ad76541fc30c28f3cae01823c2[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Fri Dec 16 10:13:46 2011 +0100

    ndb - this adds the following atrt features: 1) export TIMEOUT in environment so that cooperative programs on [1;31mslow[m hosts (guess) can avoid timeouts 2) add cmd-type: mysql, which runs cmd in mysql-client mode (normal is ndbapi) 3) add support for per-test case mysqld options

[33mcommit cdfe79531d552bf3d98b0f60c99362af4f5b371a[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Fri Oct 28 14:49:12 2011 +0100

    bug#13323589 Ndb : Multi node failure during [1;31mslow[m LCP causes crash on node recovery
    
    Fix for problem when allowing nodes to start before DIH Master takeover completed.

[33mcommit 06630b0a5671630941447767743ea82a9ca1002e[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Sep 13 13:27:33 2011 +0200

    ndb_mgm.test
     - the new nodeid allocation scheme where API nodes are always allocated
       in ndbd is slightly [1;31mslow[mer to clean up when a API node disconnects.
       This may cause a restarting mysqld node to get different nodeids
       after a restart.
     - Rewrite test, old test assumed that nodeids would always be allocated
       in sequential order. This is still the case, but not if the requested
       nodeid still is used(or in the process of being released).
For keyword fast:
[33mcommit eaf9bf9aa26b1a598b6128254ad4ede8b1d3637c[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Tue Jul 7 22:12:09 2015 +0530

    BUG#20106837: ALTER TABLE WHICH DROPS AND ADDS THE SAME FULLTEXT
                  INDEX IS NOT INPLACE/FAST.
    
    Analysis
    -------
    
    The ALTER TABLE operation which drops and adds the same FULLTEXT
    index is not a INPLACE/FAST operation.
    
    While determining if the key definition has changed, the index
    algorithm is compared between the old index and new index
    definition. While preparing the new index list during the ALTER
    TABLE operation, the FULLTEXT index algorithm is not set correctly.
    Hence the ALTER TABLE operation for dropping and adding the
    same FULLTEXT index is not a [1;31mfast[m operation.
    
    Also, such [1;31mfast[m operation which drops and adds the same index
    did not clean up the temporary 'frm' created during the
    operation.
    
    Fix
    ---
    a) Set the algorithm correctly for the FULLTEXT index in the
       new index list prepared.
    b) Delete the temporary 'frm' file created for such [1;31mfast[m/INPLACE
       operations.

[33mcommit b1ae5c5954a388aeceef98c57982c5a602471eff[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Apr 28 08:42:27 2015 +0100

    WL#4601: Remove [1;31mfast[mmutex from the server sources
    
    This patch removes the FAST_MUTEX code and the related
    WITH_FAST_MUTEXES CMake option. Note that this option
    was enabled by default for Linux release builds.
    
    Performance tests show that "[1;31mfast[m" mutexes were equal to
    or (more often) slower than default Linux mutexes.
    
    Removing [1;31mfast[m mutexes fixes / makes obsolete:
    Bug#11748914: MYSQL PERFORMANCE WITH AND WITHOUT
                  FAST MUTEXES USING SYSBENCH WORKLOAD
    Bug#13923722: FAST MUTEXES DELAY SHOULD BE CONFIGURABLE -
                  DEFAULT BAD ON SUPRA03/01
    Bug#18870931: MUTEX_DELAY() CREATING EXCESS MEMORY TRAFFIC,
                  GCC MEM BARRIER NEEDED
    Bug#18871517: MUTEX_DELAY() MISSING X86 PAUSE INSTRUCTION
                  OPTIMIZATION
    Bug#18871138: SET THREAD PRIORITY IN MY_PTHREAD_FASTMUTEX_LOCK

[33mcommit a5fe51a39f475294b2af4a31308390f14c7624b5[m
Author: Sunny Bains <Sunny.Bains@oracle.com>
Date:   Mon Mar 23 16:59:53 2015 +1100

    WL#7696 - Make LZ4 decompress safe during recovery
    
    Use LZ4_decompress_safe() when the page is being recovered via the double
    write buffer. If a page is corrupt on disk due to a torn (partial) write
    LZ4_decompress_[1;31mfast[m() can potentially access memory beyond the input buffer
    size.
    
    Use the double write buffer page for recovery only if the original data page
    cannot be decompressed due to corruption.

[33mcommit a3b832e2d348715f59ab28eb94d39822e2b2971d[m
Author: Nisha <nisha.gopalakrishnan@oracle.com>
Date:   Wed Mar 18 09:32:26 2015 +0530

    BUG#20106553: ALTER TABLE WHICH CHANGES INDEX COMMENT IS
                  NOT LONGER INPLACE/FAST OPERATION.
    
    Analysis:
    
    ALTER TABLE operations involving only changes to the INDEX
    comment is no longer INPLACE for MyISAM tables and requires
    index rebuild for InnoDB.
    
    The patch for bug fix BUG#19779365 reflects the modified
    index comment using ALTER TABLE operation by index rebuild
    which cannot be performed using INPLACE algorithm. Hence
    ALTER TABLE operation involving changes only to the INDEX
    comment using INPLACE algorithm reports an error.
    
    Fix:
    
    Changing the index comment using ALTER TABLE operation
    is marked as a [1;31mfast[m/INPLACE operation.

[33mcommit 9d1fd51d81825453a49b6bfcc7d85fd42a097970[m
Author: Anitha <anitha.gopi@oracle.com>
Date:   Thu Jan 29 02:43:07 2015 +0100

    Swtch runs on mysql-trunk-stage to use non debug server. This can make runs
    [1;31mfast[mer. Any debug specific issues will be caught in PerpPush or daily runs

[33mcommit c7c20e3fc870c579b0bec68dcecf4a1e20a98d01[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Dec 12 15:11:43 2014 +0100

    Bug#20075406 TEMPLATE-BASED QUEUE/HEAP IN BOUNDED_QUEUE
    
    Our QUEUE implementation should be replaced by a templatized C++ version.
    Templatized heaps are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Some changed results for queries doing 'ORDER BY CHAR(0) NOT NULL limit N'

[33mcommit 2299e34c786e2a9b33078230709a173ed3dc8096[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Dec 1 09:40:35 2014 +0100

    Bug#20085085 TEMPLATE-BASED QUEUE/HEAP IN HA_PARTITION
    
    Our QUEUE implementation should be replaced by a templatized C++ version.
    Templatized heaps are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit b510550a6e96ef127ffe0c2c11dc3ca4fb45b986[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Tue Nov 25 08:38:30 2014 +0530

    - Bug#20083612: INNODB_UNDO.TRUNCATE_RECOVER FAILS DUE TO "SERVER FAILED TO
      DISSAPEAR"
    
      Truncate of undo tablespace is an asynchronous activity that happens in
      background as part of purge-action. There is no hard guarantee that the
      action will be completed within the stipulated time.
      Designing a test-case that relies on such an action is big
      challenge but at same time we can make some assumption that even on
      heavily loaded system with some timeout we should hit the action.
      Timeout that was configured currently was not enough for weekly-trunk
      load on all machines so increasing it such that it can cover up
      for slower and [1;31mfast[mer machine.

[33mcommit 7b14b067763682f21a5da70b84de9f6f606c412a[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Sat Nov 15 01:36:36 2014 +0100

    BUG#74594: Remove need for long wait for LCP to copy meta data to make restart times more predictable and [1;31mfast[mer

[33mcommit a17051d72718c37bf9e8b7c64a1551a263d0921e[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Nov 10 15:37:44 2014 +0100

    Bug#19975662 TEMPLATE-BASED QUEUE/HEAP IN EVENT_QUEUE
    
    Our QUEUE implementation should be replaced by a templatized C++ version.
    Templatized heaps are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 61b3f7fff1228054dbb928d17f174931fa313310[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Nov 5 11:01:23 2014 +0100

    Bug#19917028 TEMPLATE-BASED QUEUE/HEAP IN MERGE_WALK AND ROR_UNION
    
    Our QUEUE implementation should be replaced by a templatized C++ version.
    Templatized heaps are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 815ee849aaa50d1eb224711c15a2fc322fb9dbff[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Oct 30 08:51:08 2014 +0100

    Bug#19844782 SWITCH TO A TEMPLATE-BASED QUEUE/HEAP
    
    Our QUEUE implementation should be replaced by a templatized C++ version.
    Templatized heaps are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit d39898de8e0de21f64ce94cd4ea698675edfb447[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Oct 2 17:07:07 2014 +0300

    Bug#19702328 INTRODUCE DICT_INDEX_T::IS_COMMITTED()
    
    Since InnoDB Plugin for MySQL 5.1, when the "[1;31mfast[m index creation" was
    introduced, index objects are added to SYS_INDEXES before they were
    committed. The uncommitted entries are identified by prepending them with the
    invalid UTF-8 sequence 0xff (defined as TEMP_INDEX_PREFIX).
    
    The TEMP_INDEX_PREFIX is also being used in the InnoDB data dictionary cache,
    which causes some complication when displaying or comparing index->name.
    
    We will introduce a flag dict_index_t::uncommitted and the accessor methods
    is_committed() and set_committed().
    
    The format of SYS_INDEXES.NAME must remain as is, for maintaining file format
    compatibility.
    
    This is based on a merge of -r8166..8169 from mysql-trunk-wl7141.
    
    rb#6853 approved by Vasil Dimov

[33mcommit ed391b65d76da79a5bafcaef75a2dfa01e29aa9b[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Mon Sep 15 10:51:26 2014 +0300

    Bug#19149177 Remove space_id lookups
    Bug#16831138 69276: fil_system->mutex contention limits block read rate
    with [1;31mfast[m storage (partial fix)
    
    The function calls fil_inc_pending_ops() and fil_decr_pending_ops() form a
    logical pair.
    
    If the first call succeeds, the fil_space_t object cannot be deleted until
    after the second call has been issued. So, we could safely cache the pointer
    here, and avoid looking up the tablespace again.
    
    We replace the two functions with fil_space_acquire() and fil_space_release().
    
    We will also start using these functions in the following code:
    buf_load(): restoring a buffer pool dump
    fsp_get_available_space_in_free_extents(): used in ha_innobase::info()
    lock_rec_block_validate(): debug code
    
    rb#5919 approved by Yasufumi Kinoshita and Jimmy Yang

[33mcommit 319c9326979f0bbfa39566a796eed5547adfd1c5[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Aug 28 08:45:28 2014 +0200

    Bug#19499701 GET RID OF DYNAMIC_ARRAY IN RPL_RLI_PDB
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 3f332c12bd54faa029caf03c7ecff00ddedccc7c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Aug 28 13:44:16 2014 +0900

    Bug#17554489 : UNNECESSARY OVERHEAD FROM PERSISTENT ADAPTIVE HASH INDEX LATCHES
    
    If INNODB_RW_LOCKS_USE_ATOMICS is enabled, rw_lock implementation is [1;31mfast[m enough.
    So no need to keep btr_search_latch also when over 10000 times AHI searches per 1 transaction,
    because just might block the other AHI updates.
    
    Approved by Sunny in rb#6257

[33mcommit 621d6fa46f081ee25a81073c4f2a37f03c3313c5[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Aug 22 16:36:59 2014 +0200

    Bug#19459193 GET RID OF DYNAMIC_ARRAY IN CIRCULAR_BUFFER_QUEUE
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 3a7b7b6b9808725c4c03f2561febdf93b0e18676[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Aug 18 11:09:27 2014 +0200

    Bug#19179338 GET RID OF DYNAMIC_ARRAY IN RELAY_LOG_INFO AND DEFERRED_LOG_EVENTS
    Bug#19191355 PB2 A NUMBER OF RPL TESTS CRASHED THE SERVER IN METHOD LOG_EVENT::APPLY_EVENT
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Also: fix a bug introduced by
    WL#6964 MTS: Support SLAVE_TRANSACTION_RETRIES in MTS mode
    The curr_group_da array now contains Slave_job_item structs,
    rather than Log_event pointers.
    One part of that refactoring was missed, and we were still inserting
    pointers into the array.
    
    Also: make code more type-safe by changing
    Slave_job_item::data from void* to Log_event*

[33mcommit 07613b636a3c0cc8a580777e959d90280b90b1fe[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jul 10 16:15:40 2014 +0200

    Bug#19179338 GET RID OF DYNAMIC_ARRAY IN RELAY_LOG_INFO AND DEFERRED_LOG_EVENTS
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit e1d0ed226d54125dd730412af0e0969ab2dfc945[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jul 8 13:26:56 2014 +0200

    Bug#19157369 GET RID OF DYNAMIC_ARRAY IN RELAY_LOG_INFO::WORKERS
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 25d4db99117b6931e9bf74324a9053af0e75d1f8[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 4 10:32:10 2014 +0200

    Bug#19143857 GET RID OF DYNAMIC_ARRAY IN RPL_FILTER
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 9c66b546b300dd0ca065eea65599f12d4d910bcb[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jul 2 11:29:31 2014 +0200

    Bug#19134997 GET RID OF DYNAMIC_ARRAY IN RPL_GTID
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit d52fe27a0418e16d9935c9641e749e26d202883c[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jul 1 13:23:05 2014 +0200

    Bug#19125864 GET RID OF DYNAMIC_ARRAY IN SQL_PLUGIN.CC
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Also: remove 'struct' keyword from function signatures, not necessary in C++

[33mcommit 193ed818f4f26a9fd84fca9f0861db14d4624a3c[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jun 27 16:48:35 2014 +0200

    Bug#19076686 GET RID OF DYNAMIC_ARRAY IN SID_MAP
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Do not say 'using std::list' at global scope in a header file,
    it pollutes the namespace of every file that happens to include it.

[33mcommit a695cc7f0c94bb34fe2a3d4eb76e40ed791b7dca[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jun 27 11:33:14 2014 +0200

    Bug#19068046 GET RID OF DYNAMIC_ARRAY IN SQL/AUTH/*.CC
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 98fb10f14f8d4ce0b97031e28b1ff338677dbe77[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jun 26 09:46:28 2014 +0200

    Bug#19060799 GET RID OF DYNAMIC_ARRAY IN MY_TMPDIR
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Move mysys/mf_tmpdir.c to mysys_ssl (which is a misnomer, it's the C++ part of mysys)
    Move mutex_lock.h to the include directory, so it can be used outside of server code.

[33mcommit 150c499142644e95e45ad83e131089cd3045c249[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jun 24 17:05:24 2014 +0200

    Bug#19054551 GET RID OF DYNAMIC_ARRAY IN HA_FEDERATED
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit e6730ed19a84dc2a4fc793c64146323d81df0047[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jun 20 17:31:14 2014 +0200

    Bug#19029426 GET RID OF DYNAMIC_ARRAY IN MY_LOAD_DEFAULTS()
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit b19c5000bf993e5a800b9e26b273b1d5e958be3f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jun 20 10:30:24 2014 +0200

    Bug#18994745 GET RID OF DYNAMIC_ARRAY IN MYSQL.CC MYSQLBINLOG.CC
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors/maps are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 182d7b4d7273e3eda44b4d076c99716402b04fa9[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Jun 16 17:16:52 2014 +0200

    Bug#18957679 GET RID OF DYNAMIC_ARRAY IN MYSQLTEST.CC
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit f8b61d9286d1528e170ee2dec1323d621e1e57fb[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Jun 16 15:09:21 2014 +0200

    Bug#18920203 GET RID OF DYNAMIC_ARRAY IN SERVER_IDS::DYNAMIC_I
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Also: remove some dead code.
    The class Database_ids is unused, which means we can remove it,
    and the base class Dynamic_ids.
    The member function Server_ids::do_search_id is also unused,
    and can be removed.
    
    The macro sort_dynamic() is no longer in use, and can be removed.
    
    There was a bug in change_receive_options():
    If we are inserting multiple ids into mi->ignore_server_ids
    then we need to re-sort before doing a binary_search.
    The old code worked "by accident", bsearch() found an element,
    even though the array was no longer sorted after an insert_dynamic()
    
    The fix for the sort/bsearch() bug: implement insertion-sort in Prealloced_array,
    and use insert_unique() to keep the array sorted at all times.
    This is a short array of ints, with pre-allocation, so insert-sort should be fairly efficient.

[33mcommit 7f301a8700cbe36ce65e83419dd8cea7fabc5eca[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Jun 10 09:19:05 2014 +0200

    Bug#18846682: REPLACE MALLOC+MEMSET WITH CALLOC
    
    Use calloc() rather than malloc() + memset(..., 0, ...).
    Microbenchmark shows this is [1;31mfast[mer for both glibc malloc and jemalloc.

[33mcommit 8ce2a7c996e25d41cce06bc29e4687b095e59ec0[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jun 5 15:21:07 2014 +0200

    Bug#18904069 GET RID OF DYNAMIC_ARRAY IN LEAST_OCCUPIED_WORKERS
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 48660eca349788bed4c261faa2661f75e80d5f0e[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jun 5 13:04:26 2014 +0200

    Bug#18855155 GET RID OF RAW ARRAY IN MDL_CONTEXT::ACQUIRE_LOCKS
    
      Our array abstractions should be removed, and substituted with modern data
      structures.
    
      Templatized vectors are:
       - type safe
       - easier to read/maintain
       - [1;31mfast[mer
    
    Using Prealloced_array, we also avoid a malloc/free of the buffer in most cases.

[33mcommit 1007d851b8e2a0e1ff05e0f63f1927623ecae54a[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Wed Jun 4 08:16:09 2014 +0200

    1. Added big-test flag to tests that take more than a minute on the [1;31mfast[mest platform OEL6 on PB2. This will help reduce time taken for per push runs
    2. Disabled rpl.rpl_stm_mixed_mts_rec_crash_safe_small. This is supposed to be shorter version of .rpl_stm_mixed_mts_rec_crash_safe suitable for PerPush runs. This is taking about 2 minutes and hence needs to be tuned down
    3. Moved rpl.rpl_optimize from experimental to disabled list. It is timing out on windows leading to very long running time for windows in daily.

[33mcommit 5203c8f11dcba30c5f07c300c99b1bfe6e07bf23[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jun 11 09:48:30 2014 +0200

    Bug#18947996 GET RID OF DYNAMIC_ARRAY IN IGNORE_DB_DIRS_ARRAY
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit a0c6a3b372d133fd0ed677f7dc53665277db3541[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed May 28 17:39:31 2014 +0200

    Bug#18851597 GET RID OF DYNAMIC_ARRAY IN REPL_IGNORE_SERVER_IDS
    
      Our array abstractions should be removed, and substituted with modern data
      structures.
    
      Templatized vectors are:
       - type safe
       - easier to read/maintain
       - [1;31mfast[mer

[33mcommit ec2ca9f6f2f3e21a0b94f85e8b2431cb7e44b17b[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon May 26 15:32:39 2014 +0200

    Bug#18799511 GET RID OF DYNAMIC_ARRAY IN LEX::PLUGINS
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Also: fix memory leak in unit tests: call cleanup_variables()

[33mcommit 276eddfb494483a13f9727cbe362d5db5fb3d297[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue May 13 17:34:18 2014 +0200

    Bug#18702758 GET RID OF DYNAMIC_ARRAY IN DISPLAY_TABLE_LOCKS
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Extend Prealloced_array to be closer to std::vector

[33mcommit 59290cdd43a1fbacae68784cb6b7d86f61d0ba3d[m
Author: mithun <mithun.c.y@oracle.com>
Date:   Sun May 11 06:52:08 2014 +0530

    Bug #17059925: UNIONS COMPUTES ROWS_EXAMINED INCORRECTLY
    
    Post-push-fix: Added some sleep to selects. If not on
                   some machines queries are not logged to
                   slog_log table because they are executed
                   too [1;31mfast[m.

[33mcommit 68f928173b6bf34d4f1070d2357396de58d03526[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Apr 30 12:58:04 2014 +0200

    Bug#18663095 GET RID OF DYNAMIC_ARRAY IN THD
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 7dd7dec7a4fed42aaf173d7741a0d822a604333a[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Apr 25 14:43:41 2014 +0200

    Bug#18636972 GET RID OF SORT_DYNAMIC IN SQL_SHOW
    
    Use standard algorithms, rather than dynamic_array.
    standard library functions are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit 2b26839d3b78caf973650f665b3ec0b886e2dae7[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Apr 24 15:12:24 2014 +0200

    Bug#18556403 USE STD LIBRARY FOR SORTING AND MATCHING IN SUBCLASSES OF IN_VECTOR, PART 2
    
    As demonstrated by the patch for Bug#18486249
    in_vector and its derived classes can benefit from some code modernization.
    
    standard library functions are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer
    
    Introduce typed vectors of objects, rather than the "blob vector" in the base class in_vector.
    The re-write uncovered a bug in the range optimizer:
    it was building range predicates for unused parts of the IN-vector.

[33mcommit c9798954c243006fcb51a1b96e620a30882174a6[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Apr 9 12:43:01 2014 +0200

    Bug#18503621 USE STANDARD LIBRARY FOR SORTING AND MATCHING IN SUBCLASSES OF IN_VECTOR
    
    As demonstrated by the patch for Bug#18486249
    in_vector and its derived classes can benefit from some code modernization.
    
    standard library functions are
     - type safe
     - easier to read/maintain
     - [1;31mfast[mer

[33mcommit d2bce2fbc2a83c75504c551f7d96561e2f286e7e[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Mar 26 15:17:16 2014 -0700

     NDB encoding-related changes.
     (1) distinguish between 22003 numeric value out of range and HY000 for "value is not numeric"
     (2) provide a (hopefully) [1;31mfast[mer path to encoderWrite and encoderRead by wrapping them as methods of Record

[33mcommit c50659443a0264ce80cdc8ec439bea7a1bb3f42c[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Mar 22 16:45:19 2014 -0700

    Encoders for NDB integer values: if the [1;31mfast[m integer conversion fails, try a slow one, then test whether the result is legal.  This should allow writing valid JS string and float values to int columns.

[33mcommit ec76332a67b1a18bef78f7f1e273d3c243e83f19[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 18 21:12:03 2014 +0200

    Bug#18417128 MINOR REFACTORING OF REDO LOGGING AND RECOVERY CODE NEEDED
    
    Simplify the code flow in InnoDB redo logging and crash recovery, and
    remove some unused definitions.
    
    Use log_mutex_enter(), log_mutex_exit(), log_mutex_own() instead of
    accessing log_sys->mutex directly.
    
    log_release(): Replace by log_mutex_exit().
    
    buf_page_is_corrupted(): Remove type casts.
    
    log_reserve_and_write_[1;31mfast[m(), log_reserve_and_open(): Assume that
    log_mutex_enter() has already been invoked.
    
    log_buffer_extend(): Expose to mtr_t::Command, which is invoked as
    part of mtr_commit(). This must be called before log_mutex_enter().
    
    log_write_checkpoint_info(): Renamed from
    log_groups_write_checkpoint_info(). Add a Boolean parameter to
    indicate whether to wait for the checkpoint to be persistent.
    This will invoke log_mutex_exit().
    
    log_checkpoint(): Replace ibool with bool, clarify some comments, and
    move some code to log_write_checkpoint_info().
    
    log_make_checkpoint_at(): Replace ibool with bool, and clarify the
    function description.
    
    log_checkpoint_margin(): Replace ibool with bool.
    
    log_check_margins(): Use a loop instead of goto.
    
    logs_empty_and_mark_files_at_shutdown(): Remove a duplicated
    log_mutex_exit() call.
    
    log_archive_do(), log_archive_stop(), log_archive_start(),
    log_archive_noarchivelog(), log_archive_archivelog(),
    log_archived_file_name_gen(): Remove. These were orphan declarations,
    with no corresponding function definition.
    
    log_group_write_buf(): Make this a static function.
    
    log_t::check_flush_or_checkpoint: Replace ibool with bool.
    
    recv_sys_t: Remove the unused fields lsn, last_log_buf_size.
    
    recv_synchronize_groups(): Remove some code that is now in
    log_write_checkpoint_info().
    
    recv_recover_page_func(): Add some debug assertions. Add DBUG_PRINT
    output.
    
    recv_recovery_from_checkpoint_start(): Add a missing space to an error
    message. Remove an unreachable condition.
    
    recv_recovery_from_checkpoint_finish(): Move some code to
    innobase_start_or_create_for_mysql().
    
    mtr_t::Command::write(): Split to prepare_write() and finish_write().
    
    rb#4954 approved by Vasil Dimov

[33mcommit 0c60d406c00c598a7c63eb7b962af0b89e432a06[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Feb 27 23:17:23 2014 +0400

    WL#7306 "Improve MDL performance and scalability by implementing lock-free
    lock acquisition for DML" and fix for bug #18077895 "WL7305 PUSH (7249)
    CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP UP TO -8%".
    
    The idea of this task is to change the acquisition of unobtrusive locks on
    the [1;31mfast[m path, which currently involves acquisition of MDL_lock::m_rwlock,
    some checks, increment of the packed counter and release of m_rwlock, to
    a single atomic compare-and-swap operation.
    Similarly the release of the lock on the [1;31mfast[m path becomes single atomic
    compare-and-swap (in absence of obtrusive locks and assuming we are not
    releasing the last lock for this MDL_lock object) instead of acquisition
    of MDL_lock::m_rwlock, decrement of the packed counter, some checks and
    m_rwlock release.
    As result these operations become at least twice cheaper than their
    old versions which has a nice effect on performance/scalability.
    
    Acquisition/release of locks on the slow path (i.e. unobtrusive locks in
    presence of obtrusive locks and obtrusive locks) still has to use the old
    approach involving locking/unlocking MDL_lock::m_rwlocks and checks of
    MDL_lock::m_granted/m_waiting bitmaps/lists.
    
    This patch implements the above idea by performing the following
    three transformations:
    
    I)   MDL_lock::m_[1;31mfast[m_path_granted_count is replaced with an atomic
         MDL_lock::m_[1;31mfast[m_path_state member, which in the ideal case of
         "[1;31mfast[m path" acquisition/release is checked and changed using CAS
         without holding any mutexes.
    II)  Since we would like to check in the same atomic CAS operation that
         MDL_lock object was not destroyed, its m_is_destroyed member is
         replaced by a IS_DESTROYED bit flag in the m_[1;31mfast[m_path_state
         packed counter.
    III) Similarly, since we also would like to check in the same atomic CAS
         that there are no granted or pending obtrusive locks, we have to
         add a HAS_OBTRUSIVE bit flag in the m_[1;31mfast[m_path_state, while
         keeping MDL_lock::m_obtrusive_locks_granted_waiting_count.
         This flag should be set when we are about to try acquiring an obtrusive
         lock and cleared once the last granted or pending obtrusive lock goes
         away.
    
    
    Most of the remaining changes in this patch are necessary in order to fix
    bug #18077895 "WL7305 PUSH (7249) CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP
    UP TO -8%".
    
    This bug manifested itself as a slowdown for workloads involving 1 connection
    in cases when there were many concurrent connections to the same server in the
    past or there were many dormant connections at the same time as 1 active
    connection.
    
    In such scenarios the release of a metadata lock meant that MDL_lock became
    unused, was removed from lock-free hash with all lock objects and we
    tried to return it back to allocator. The latter operation involved
    scanning pins for all current and past connections, which became fairly
    expensive in this scenario.
    
    This patch solves this problem by avoiding releasing MDL_lock objects
    and removing them from the hash once they are no longer used. Instead we
    keep unused objects in MDL_map and start their eviction only if their
    number passes certain threshold and the ratio of unused/total lock objects
    is big enough. We evict random unused objects so on average objects
    which are used more often will stay in the hash and rarely used objects
    will go away.
    
    The above idea is implemented by:
    
    a) Introducing a new HAS_SLOW_PATH flag in the MDL_lock::m_[1;31mfast[m_path_state
       member, which indicates if there any tickets in MDL_lock::m_granted
       and m_waiting lists or we are about try to add one. Thanks to this
       flag, it is possible to distinguish between used and unused MDL_lock
       objects in atomic compare-and-swap operations used to implement [1;31mfast[m
       path acquisition and release of locks.
    b) Changing code which releases locks to avoid removing unused MDL_lock
       objects from the hash and deleting them afterwards. Instead we
       atomically increment the newly introduced MDL_map::m_unused_lock_objects
       counter. Similarly, on the first acquisition of lock for MDL_lock which
       was previously unused we atomically decrement this counter.
    c) In cases when the increment of the MDL_map::m_unused_lock_objects counter
       exceeds the threshold value and the unused/total objects ratio is high
       enough, we try to reduce the number of unused objects. We look-up a random
       unused object in MDL_map, mark it as destroyed, remove it from the hash and
       return it back to allocator. As a consequence MDL_map::remove() method
       has became MDL_map::remove_random_unused().
    d) To support the change described in c), a new lf_hash_random_match()
       function was introduced which allows us to efficiently find a random
       object which matches certain condition in LF_HASH.
    e) Also to support the change described in c), a new PRNG was added to
       MDL_context class. This PRNG is used as a source for randomness for
       look-ups of random unused objects.
    
    Unit tests were added covering handling of unused MDL_lock objects and
    for the new lf_hash_random_matches() function.
    
    
    Finally, this patch fixes a violation of the pinning protocol, which was
    introduced by WL7305 and which occured when the MDL subsystem failed
    to look up MDL_lock object in lock free hash. The LF_HASH documentation
    was updated to reflect the need to call lf_hash_search_unpin in this case.

[33mcommit b910453f9acb91994081a74723dd576e76f6d4e8[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Feb 26 13:31:31 2014 +0100

    WL#7647 Ndb_component usage - thread safe wakeup
     - Add abstract do_wakeup() function which subclasses must implement
       in order to detect the stop as [1;31mfast[m as possible. This involves
       signaling conditions etc.
     - Move the wakeup functionality of our three components into
       do_wakeup() functions.

[33mcommit e468a66d6b5057b177f417ba94ea7b4df509c5e8[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jan 28 13:24:38 2014 +0100

    Bug#18003651 VALGRIND: MEMORY LEAK AT THE TIME OF SERVER START UP
    
    The race conditions during server shutdown are gone,
    so cleanup of of pfs_instr_config_array is simply a matter of
    deleting all allocated objects.
    
    Swich from DYNAMIC_ARRAY to Prealloced_array: it is [1;31mfast[mer, and it is type safe.
    
    Remove valgrind suppressions related pfs_instr_config_array.

[33mcommit bb764e8ca1f93d2d39c20edb9e06a8cdcbd1cfab[m
Author: Neha Kumari <neha.n.kumari@oracle.com>
Date:   Tue Jan 28 12:05:55 2014 +0530

    ISSUE:
    Earlier is_valid was a pure virtual method declared inside the class Log_event and implemented
    by all the event classes.
    
    
    RATIONALE:
    Now it will be bool variable declared inside class Log_event,
    but defined in the constructors of all events.
    
    Removing a pure virtual method with a instance variable will make the execution
    [1;31mfast[mer

[33mcommit 51aabf4ebaaed9445c5bb78c16898876db904d5f[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Dec 12 21:55:53 2013 +0400

    Follow-up patch for WL#7304 "Improve MDL performance and
    scalability by implementing "[1;31mfast[m-path" for DML locks".
    
    Unit test class for MDL-related tests now saves original
    error hook before installing custom one and restores it
    after test execution.

[33mcommit cdff3c67f58eb401e7a326ac42b954a34dac1a98[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 10 15:38:02 2013 +0100

    Fix for Bug#17842035 LCP WATCHDOG SHOULD USE REAL CLOCKTIME
    
    Recommit of fix updated according to Frazers review comments
    
    The LCP watchdog implemented by Dblqh::checkLcpFragWatchdog is intended to be
    called by the scheduler event mechanism every 10th s. Every time it is
    called it increment its own 'pollCounter', assuming that 10s has passed.
    
    When this counts reaches a limit representing that either a 'WarnPeriod'
    (default 20s), or 'Timeout Limit' (60s) has passed, either a LCP warning
    is printed, or the datanode aborted due to 'SCAN_WATCHDOG_FAIL
    
    However, there are no guarantee that this timer fires at the
    specified 10s interval.:
    
    - There might be CPU contention which prevents the job scheduler
      from running. which results in a longer delay.
    
    - If the scheduler was 'late', as above, it will try to make up
      for that by running the internal scheduler timer [1;31mfast[mer.
      In some cases it might even cause a delayed event to fire
      immediately.
    
    To overcome these limitation the watchdog has been refactored to
    keep track of its own starttime, and calculate elapsed time by
    reading the clock every time it is called.
    
    Furthermore, it will now absorbe a 'backtick' (not likely to occure)
    by taking the backwards time as new 'current time' and calcule 'elapsed'
    time for this round as '0'.
    The ill effect of a forward leap, which possibly could expire the watchdog
    timer immediately, is reduced by never calculating an elapsed time
    longer than the requested 'delay' time of the watchdog timer.

[33mcommit 2f959856864cd836c243fc7ca04ec1bc319d4318[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 10 15:37:08 2013 +0100

    Fix for bug#17647469
      'GCP LAG' WATCHDOG (DBDIH::CHECKGCPSTOPLAB) SHOULD USE REAL CLOCK TIME
    
    Recommit of fix updated according to comments from Frazer
    
    The GCP watchdog implemented by Dbdih::checkGcpStopLab is intended to be
    called by the scheduler event mechanism every 100th ms. Every time it is
    called it increment its own 'invocation counter'. When these counts reaches
    (n*10) == <timeout limit>, the GCP LAG warning is printed, or the 'crashSystem'
    action taken.
    
    However, there are no guarantee that such timed events actually are called
    after the specified delay:
    
     - There might be CPU contention which prevents the job scheduler
       from running. which results in a longer delay.
     - If the scheduler was 'late', as above, it will try to make
       up for that by running the internal scheduler timer [1;31mfast[mer.
       In some cases that might even cause a delayed event to fire
       immediately.
     - The resolution of the OS timers may be to coarse to
       reliable being able to hit near the 100ms mark.
    
    To overcome these limitation the watchdog has been refactored to
    keep track of its own starttime, and calculate elapsed time by
    reading the clock every time it is called.
    
    Furthermore, it will now absorbe a 'backtick' (not likely to occure)
    by taking the backwards time as new 'current time' and calcule 'elapsed'
    time for this round as '0'.
    The ill effect of a forward leap, which possibly could expire the watchdog
    timer immediately, is reduced by never calculating an elapsed time
    longer than the requested 'delay' time of the watchdog timer.

[33mcommit 1496cc312e678e0c9da3d7f8c2a9ae575a6b184c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Tue Dec 3 14:07:19 2013 +0400

    WL#7304 "Improve MDL performance and scalability by implementing '[1;31mfast[m-path'
    for DML locks".
    
    Since typical user workload consists mostly of DML statements it makes sense
    to improve performance/scalability by optimizing MDL subsystem for such type
    of statements.
    
    This patch implements "[1;31mfast[m-path" for metadata locks acquired by DML
    statements. Acquisition/release of DML lock (S, SH, SW, SR locks) are
    converted into counter increment/decrement (under protection of
    MDL_lock::m_rwlock) instead of more complex code involving list
    manipulation (at expense of acquisition/release of locks typical for DDL
    statements).
    
    Such a step reduces size of critical section associated with
    MDL_lock::m_rwlock and increases scalability/performance in benchmarks
    involving DML workload. Particularly, benchmarking of draft patch
    implementing this idea shown that it provides at least 10% performance
    improvement in single-table OLTP_RO/POINT_SELECT SysBench tests.
    
    Details
    =======
    
    We split all lock types for each of MDL namespaces in two sets:
    
    A) "Unobtrusive" lock types
       1) Each type from this set should be compatible with all other
          types from the set (including itself).
       2) These types should be common for DML operations.
    
    We optimize acquisition and release of locks of this type by avoiding
    complex checks and manipulations on m_waiting/m_granted sets/lists and
    replacing it with a check of and increment/decrement of integer counters.
    We will call the latter type of acquisition/release "[1;31mfast[m path".
    
    2) "Obtrusive" lock types
       1) Granted or pending lock of those type is incompatible with some
          other lock (including itself).
       2) Not common for DML operations
    
    These locks have to be always acquired in the old fashion - involving
    manipulations with m_waiting/m_granted sets/lists, i.e. using "slow path".
    Moreover in the presence of active/pending locks from "obtrusive" set we
    have to acquire even locks of "unobtrusive" type using "slow path".
    
    -------
    
    For GLOBAL/COMMIT and SCHEMA namespaces (i.e. namespaces with lock
    represented by MDL_scoped_lock class):
    
    "Unobtrusive" locks set consists of IX lock type.
    "Obtrusive" locks set consists of S and X lock types.
    
    For all other namespaces (i.e. represented by MDL_object_lock class):
    
    "Unobtrusive" locks set consists of S, SH, SR and SW locks.
    "Obtrusive" locks set consists of SU, SNW, SNRW and X.
    
    -------
    
    To implement the above MDL_lock object got two new members:
    
    - MDL_lock::m_obtrusive_locks_granted_pending_count - number of granted
      or pending locks of "obtrusive" types. Necessary to quickly verify that
      we can grant "unobtrusive" locks without further checking.
    
    - MDL_lock::m_[1;31mfast[m_path_granted_count - packed counter of number of
      granted locks of specific "unobtrusive" type which were granted using
      [1;31mfast[m-path algorithm and not using "slow path" (e.g. for MDL_object_lock
      we use 20-bit chunks of this counter to represent number of S/SH, SR and
      SW locks acquired).
    
    The above two members are still protected by MDL_lock::m_rwlock lock.
    
    Essentially this patch replaces:
    
    1) addition/removal of "unobtrusive" lock to MDL_lock::m_granted list during
       lock acquisition/release with and incrementing/decrementing of corresponding
       part of m_[1;31mfast[m_path_granted_count.
    2) check for granted or pending locks which conflict with "unobtrusive" lock
       is replaced with check on m_obtrusive_locks_granted_pending_count counter.
    
    We still allocate MDL_ticket objects for requests which are satisfied using
    [1;31mfast[m path algoritm, but we mark them using MDL_ticket::m_is_[1;31mfast[m_path member,
    so we know that such ticket can be released in simplified fashion.
    
    In order for deadlock detection algorithm to work properly we need to do so
    called "materialization" of [1;31mfast[m path tickets for thread which is about to
    start waiting. This process clears MDL_ticket::m_is_[1;31mfast[m_path flag, add ticket
    to appropriate m_granted list and decrement corresponding
    m_[1;31mfast[m_path_granted_packed_count counter under protection of MDL_lock::m_rwlock.
    We also have to do this when acquiring "obtrusive" locks and locks for the
    threads with open HANDLERs.
    
    -------
    
    Unit tests for MDL subsystem were extended to cover scenarios important for
    the changes described above.
    Also this patch changes perfschema.mdl_func test to make it robust against
    line number changes in MDL code.

[33mcommit 028c291ff35560eb3a08a8a6ea2ef1f3bb4797c7[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Aug 26 13:27:29 2013 +0200

    This is a fix for Bug #17177279 'NDB : IMPROVE JAM() COVERAGE AND
    EFFECTIVENESS'. This commit changes the jam trace mechanism such that each
    entry in the jam trace buffer now will refer to a unique file id (JAM_FILE_ID)
    rather than a block number. This bringe the following benefits:
    - It is much easier to map an entry to the right source code location.
      (Trace files will point to the source file rather than the block.)
    - jam should be slightly [1;31mfast[mer, since the jam value can be calculated at
      compile time.
    - Forgetting to call jamEntry() when going from one block to another is no
      longer a problem. Each jam() call records the complete context. From
      now on, there is no need to add jamEntry() calls, jam() is sufficient.
    - There is no need to maintain per-file jam offsets to distinguish between
      different files within a block.
    
    See comments for jamFileNames in Emulator.cpp for a decription of how
    to add new JAM_FILE_IDs.
    
    This commit gathers include directives at the top of the source files
    that have JAM_FILE_Ids. That way, it is easier to add a JAM_FILE_ID
    macro that will not be  redefined by some include file. Also, gathering
    include directives in one place improves code readability.
    
    Notes:
    * There were two cases of the following pattern: A.hpp defines class A. Then
      B.hpp in included, which defines class B and then defines inline methods for
      B which refers members of A. Finally A.hpp defines inline methods for A
      which refers members of B. This mutual dependency means that A.hpp cannot
      include B.hpp until *after* class A has been defined. It turned out that
      both cases of this pattern were due to methods that were not in use.
      This was LogLevel::operator=(const EventSubscribeReq&) and
      SignalCounter::operator=(const NodeReceiverGroup&). These methods have been
      removed such that the corresponding #include directives could be moved to
      the top.
    
    * SimulatedBlock.hpp had an unneeded #include of Mutex.hpp. This would not
      compile unless Mutex.hpp was included after initial definitions in
      SimulatedBlock.hpp. Therefore, the #include was removed from
      SimulatedBlock.hpp. Instead, Mutex.hpp is included directly from the files
      that need it.

[33mcommit a836753aef1d94c797caa626f89c9800368f9e8c[m
Merge: a13786fe47b 8c7c5b92db8
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Aug 13 08:46:32 2013 +1000

    WL#6044 - InnoDB: Policy-based mutex
    
    Create a common interface for all InnoDB mutex types. This change gets rid
    of the distinction between "os_[1;31mfast[m_mutex" and the InnoDB home-brew mutex
    implementation.
    
    Create several classes of mutexes (more to be added later).
    
     1. OSBasicMutex   - POSIX or Windows CRITICAL_SECTION
     2. OSTrackedMutex - Same as #1 but we track the acquire/release
     3. TTASFutexMutex - Where available on Linux, uses futexes instead of condvars
     4. TTASEventMutex - The old home-brew implementation
     5. SpinMutex      - Spin only version that uses TTAS
    
    #3 should get rid of an existing performance issue around condvar broadcasts
    that results in a thundering herd problem. Also, we can now set the mutex type
    to any one of the above for individual mutexes in the code.
    
    Rewrite the os_event_t code, use OSBasicMutex as the underlying mutex type.
    The code uses static polymorphism (C++ templates) instead of inheritance. This
    is to avoid the vptr overhead. This is mainly relevant only for mutexes that are
    currently used for the blocks. For very large buffer pools, e.g., 64G this will
    reduce the mutex overhead significantly. On Linux where futexes are available
    the mutex overhead can be as low as sizeof(lock_word_t).
    
    Rewrite the UNIV_SYNC_DEBUG code. Move all PFS key defines and declarations
    to sync0sync.h and sync0sync.cc.
    
    rb#1913 Approved by Jimmy Yang and Yasufumi Kinoshita.

[33mcommit 9947584d74e193adb21f7732e58a759f796a7e41[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Mon Aug 12 16:22:58 2013 +0200

    Bug#16900947: WHEN REPAIRING A PARTITION REMOVE
    THE DUPLICATES AND LOG THEM
    
    There are no easy/[1;31mfast[m way to repair partitions which have
    duplicate key violations.
    
    Added support for IGNORE in
    ALTER IGNORE TABLE t CHECK/REPAIR PARTITION all_or_list_of_partitions.
    
    For REPAIR, it will remove rows that are impossible to move, due to
    duplicate key violations.
    
    For CHECK, it will write out the contents of the row
    (all fields in the PK/partitioning expression) and does not stop after the
    first misplaced row.

[33mcommit 31e915b111c9a7b365b368273d1b3a32b850fedc[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Mon Jul 15 22:04:10 2013 +0530

    WL#6314:
    fixed timeout issues.
    Fixed rpl.rpl_mts_submode_switch by making D3.t transactional.
    removed MTR runs on Pushbuild branch to make the tests [1;31mfast[mer.

[33mcommit 24e6b53456b48e98158f68499fbf067bb21a4ded[m
Author: Libing Song <libing.song@oracle.com>
Date:   Tue Jul 2 11:09:58 2013 +0800

    BUG#16988017 *GROUP_COMMIT_DEADLOCK* TESTCASES ARE FAILING SPORADICLY
                 ON FREEBSD9-X86-64BIT
    
    Slave IO thread stopped sporadically with an error. The error was caused by an
    unexpected heartbeat. The heartbeat event was between Rotate_log_event and
    Format_description_log_event. But slave IO thread doesn't accept any event
    immediately after receivng a Rotate_log_event,
    except Format_description_log_event.
    
    Binlog rotation has two steps. First, it appends a Rotate_log_event into
    current binlog and close it. Second, it creates a new binlog and appends
    a Format_description_log_event. Immediately after the Rotate_log_event was
    appended, binlog_end_log_pos was updated and dump threads were notified to
    send the event. After the event was sent, dump threads began to wait
    for new events coming until the new binlog file was created. Dump threads
    could get timeout and send heart beat events to slaves if the new
    binlog was not created so [1;31mfast[m.
    
    To solve the problem, the code updating binlog_end_log_pos immediately after
    Rotate_log_event is appended is removed. So dump threads will not know the
    Rotate_log_event is appended until active binlog is switched. Because active
    binlog is switched, dump threads will switch to next binlog file directly
    without any heartbeat after sending Rotate_log_event.

[33mcommit 4fd18025f46e5d473c415433ada500cf44a1d490[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Apr 30 08:25:38 2013 +0200

    Bug#16395778 SUBOPTIMAL CODE IN SKIP_TRAILING_SPACE()
    
    The aligned reading is only necessary on sparc.
    Removed a strange assert(pointer > 4)
    Added assert(end_words > ptr) instead, which shows that we could remove an if()
    
    Added a simpler 8-byte read for non-sparc platforms, which is a lot [1;31mfast[mer.
    
    See the accompanying microbenchmark unit tests.

[33mcommit 6983522a4e98c336ed0393429c9049bb1e0cb34b[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Feb 8 17:40:59 2013 +1100

    Bug#1624555 - PATCH FOR BUG#14729365 CAUSE RO PERFORMANCE REGRESSION FOR 8 CORE HOST(S)
    
    Use a system specific random indexer in the fuzzy counter class. This helps in
    reducing the cache coherency impact when multiple threads try and update
    the same counter.
    
    Note: This patch uses THD::thread_id if a "[1;31mfast[m" random indexer is not available
    for that OS. However, for srv_stats.n_rows_read this "[1;31mfast[m" part is moot because
    a static thread specific identifier should be [1;31mfast[mer than a function call. Given
    that the performance tests were done with the OS specific indexer I've decided
    to leave that as is for now. We should revisit this.
    
    Approved by Jimmy Yang, rb#1950

[33mcommit c572202381b739099f8a33e36d0a2ca835015ae4[m
Merge: 6b9865eebae 5fbaf5123c4
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Jan 29 15:34:02 2013 +1100

    Merge from mysql-5.6. This version of the patch is a rewrite/refactor of the
    deadlock check code. The aim is to make it self contained so that we can
    investigate whether it is possible to make check parallel and [1;31mfast[mer easily.

[33mcommit 7e243db421e79b53c0be2c51a0384048153d8f99[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Jan 14 13:56:56 2013 +0200

    Bug#15920744 USE STD::SORT WHEN POSSIBLE
    
    Replace the InnoDB homebrew UT_SORT_FUNCTION_BODY (a recursive merge
    sort) with the C++ STL std::sort() in most places, because the latter
    needs less memory (avoiding a second copy of the array, and avoiding
    recursion) and is [1;31mfast[mer due to inlined execution.
    
    The row_merge_buf_sort() of CREATE INDEX will be kept as is, because
    both std::sort() or qsort_r() can apparently cause more calls to our
    costly tuple comparison routine. That is, the results vary depending
    on the table and index definition, and on the compilation flags.
    
    cmp_data_data_slow(): Replace with cmp_data_data().
    
    buf_dump_cmp(), buf_dump_sort(): Replace with std::sort().
    
    ut_ulint_sort(): Replace with std::sort().
    
    page_zip_dir_cmp(), page_zip_dir_sort(): Replace with std::sort().
    
    page_zip_dir_decode(): Remove recs_aux[].
    
    rb#1844 approved by Jimmy Yang

[33mcommit 4445da4cc031aa86b2d04d5c2e0cc85a8b84f22d[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Aug 29 10:24:46 2012 +1000

    WL#6445 - Ignore flush to disk during [1;31mfast[m shutdown if --innodb-read-only set.
    Fix log message, move to new format.

[33mcommit 08508cdf14fcab87aa7bb130f322363d08b502ec[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 25 11:34:51 2012 +0200

    SPJ Fix: Only try to 'make_pushed_join()' if there are multiple tables in query
    
    We want to keep a '[1;31mfast[m track' through the optimizer for simple, single table queries.
    Thus we should avoid possible overhead from finding join-pushable parts of the query
    if there are only a single table.

[33mcommit 5a6ad5c2e1866bf4323b898945bd8441871560d1[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Sat Oct 8 19:55:25 2011 +0300

    wl#4124 h04_update.diff
    detect analyze request [1;31mfast[mer
For keyword time:
[33mcommit fb1f0abaeb4fd30c9fd4a62fe7016d979c077b45[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Mon Sep 28 16:47:44 2015 -0500

    Bug#21863597 SHOW STATUS LIKE '%SSL%' CRASHING SERVER
    
    The 5.7 implementation of SHOW STATUS exposed a potential crash in the
    handling of SSL status variables. The crash occurs because the SSL status
    variable callback function, show_ssl_get_server_not_after(), passes a null
    ASN1 [1;31mtime[m pointer to my_asn1_[1;31mtime[m_to_string().
    
    This callback function and its counterpart, show_ssl_get_server_not_before(),
    now check the value of the ASN1 [1;31mtime[m pointer. If NULL, then the callback
    returns an empty string.
    
    (cherry picked from commit 4b3ba08e4cc6067d9f7159c7a31cbbe4b9718b71)

[33mcommit 971849567a7e8dd72b87bb808386e010c034d9dd[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Mon Sep 28 17:51:55 2015 +0800

    Bug#11761559 SUPPORT OGC STANDARD WKT FOR MULTIPOINT
    
    Issue:
    
    Our WKT parser doesn't allow multipoint provided in this form: MULTIPOINT((0 0), (1 1)),
    but this format was defined by OGC as standard WKT format for multipoints.
    
    Fix:
    
    Allow above format, and also allow the old format: MULTIPOINT(0 0, 1 1),
    but do not allow the mixture of both formats in the same multipoint geometry.
    And at the same [1;31mtime[m, output multipoint WKT in OGC standard format.
    
    (cherry picked from commit ac4ee0362856916abb7f06562347296d9dceb659)

[33mcommit 727a5c9229b6f46f15606228a33ce93c19c13d94[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Fri Sep 18 13:17:10 2015 +0200

    Bug#21808680: JSON + GENERATED COLUMN CORRUPTS TABLE CACHE MEMORY, CRASHES
    
    In some cases pointers to stale data could be left in the Item tree of
    the generated column expression, which would lead to problems the next
    [1;31mtime[m the generated column was accessed.
    
    There is already code in place to avoid this situation.
    TABLE::cleanup_gc_items() calls Item::cleanup() on the generated
    column expression upon completion of a statement, and this is supposed
    to revert the Item tree to a pristine state, ready for reuse in a new
    statement.
    
    However, since the Item::cleanup() functions are not recursive, only
    the top-level item in the generated column expression will be cleaned
    up. Children, such as items representing the arguments of a function
    call, will not be cleaned up and can point to stale data inserted into
    the item tree during execution.
    
    The fix changes TABLE::cleanup_gc_items() so that it calls cleanup()
    on all the items in the generated column's item_free_list, and not
    only on the top-level item.
    
    The patch also removes an unused parameter from the function.
    
    (cherry picked from commit a16052827bbe17415cee5969f0fa46247f18e77d)
    
    Conflicts:
            mysql-test/suite/gcol/r/gcol_bugfixes.result
            mysql-test/suite/gcol/t/gcol_bugfixes.test

[33mcommit fe9d5a26c801d0d9660a42028b397aa5fd56d503[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Sep 23 16:06:28 2015 +0300

    Bug #21865901   MTS COORDINATOR AND WORKER THREAD SYNCHRONIZATION LIMITS PARALLELISM
    
    In LC (Logical Clock) MTS mode, Coordinator thread distributes transactions to
    workers respecting at most one transaction to a Worker at a [1;31mtime[m constrain.
    The rule was introduced yet by wl6314 and not been revised since that [1;31mtime[m.
    A part of the rule implementation involved my_sleep() on the Coordinator side
    with the idea the Coordinator to have a nap while every Worker is busy.
    
    This has turned out to be costly for overall slave applier performance and
    my_sleep() is replaced with sched_yield(), except Windows where the latter
    is not defined.
    
    Notice with this method a potential excessive busy-waiting by
    Coordinator, e.g when all Workers are busy to process big
    transactions, happens only when the yielded thread's CPU is not requested by
    any other user thread.
    Otherwise there should be no negative cost for this operation except
    a Worker can become available sooner than Coordinator gets rescheduled.
    by OS.
    
    There're still few alternatives to the current solution, declined
    due to either higher risks or rather significant complexity.

[33mcommit c45c5a08fe4921a4b318d0c1315e97bea39b628f[m
Author: Evgeny Potemkin <evgeny.potemkin@oracle.com>
Date:   Tue Sep 22 16:44:01 2015 +0400

    Bug#21317507:GC: STORED COLUMN REJECTED, BUT VIRTUAL IS ACCEPTED
    
    When a virtual generated column is added to a table, it wasn't ensured that
    data being calculated by GC's expression wouldn't get out of range of the
    column. This might lead to inconsistent data being returned and unexpectedly
    failed statements.
    
    This patch adds two new options to ALTER TABLE:
    1) WITH VALIDATION
    2) WITHOUT VALIDATION (default)
    e.g ALTER TABLE .. ADD COLUMN gc INT AS (...) VIRTUAL, WITH VALIDATION
    These options are statement-level clauses, similarly to ALGORITHM or LOCK
    clauses.
    When the first option is specified, ALTER copies the table and if an
    out-of-range error is thrown, or any other error occurs, the statement
    fails.  Due to copy, such ALTER could take considerable amount of [1;31mtime[m on big
    tables.
    When these options are used without ADD/CHANGE/MODIFY COLUMN clause, the
    ER_WRONG_USAGE error is thrown.
    
    With the 2nd option, in-place ALTER is used (if possible) and data integrity
    isn't checked and the statement finishes quickly. Drawback is that
    errors/warnings might be reported during reading such column.
    
    Options are used only during execution of the ALTER statement and isn't stored
    anywhere in .FRM.
    
    Now fill_alter_inplace_info() check whether validation is asked for and if
    so, sets the newly added flag Alter_inplace_info::VALIDATE_VIRTUAL_COLUMN
    to indicate that a virtual GC needs validation.
    Storage engine's check_if_supported_inplace_alter() should check whether it
    allows such operation to be in-place.
    
    check_if_supported_inplace_alter() in both handler and ha_innodb classes
    don't need any changes because they block in-place changes for all flags
    other than explicitly listed in them.
    
    (cherry picked from commit 29c6552ade0ff140f91b882b7a51940140589596)

[33mcommit 11309edfe930557463bed878257658c0513998b4[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Mon Sep 21 16:19:21 2015 +0530

    Bug#21816041 - ROW_FORMAT=DYNAMIC IS MORE RESTRICTIVE THAN ROW_FORMAT=COMPACT
    
    Problem:
    --------
    Table creation succeeds with ROW_FORMAT=COMPACT and REDUNDANT but fails with
    DYNAMIC and COMPRESSED. This happens because of wrong undo log record size
    calculation
    
    Fix:
    ----
    The undo log record size calculation at DDL [1;31mtime[m(dict_index_too_big_for_undo())
    is conservative some[1;31mtime[ms and aggressive at other [1;31mtime[ms. Since there is a DML
    [1;31mtime[m check and the behaviour of dict_index_too_big_for_undo() causes DDL
    failures(inturn replication breakage), we will remove this undo log record
    size calculation logic at DDLs because there is undo log record size
    check for DMLs which is more accurate.
    
    We also make DYNAMIC and COMPRESSED to adhere strict_mode=OFF and allow
    row length violations at DDL [1;31mtime[m.
    
    After this fix, it is possible to create a table with 'non-updatable'
    columns. i.e. you cannot UPDATE a single column.
    
    Reviewed-by: Marko Makela <marko.makela@oracle.com>
    RB: 10220
    (cherry picked from commit 6f57fad788920634ddf07f80b6ebfe1575f888cc)

[33mcommit 85a541cb6c8676529d5ecb70609cbb3f89e738ca[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Thu Sep 17 15:57:12 2015 +0200

    Bug#21824519: ASSERTION IN DROP TRIGGER WHEN TABLE HAS VIRTUAL GENERATED COLUMN
    
    In some cases a DROP TRIGGER statement could hit a DBUG_ASSERT in
    debug builds if the trigger was defined on a table that contained a
    generated column. The same DBUG_ASSERT could under some circumstances
    be hit when calling a stored function that referenced a table that
    contained a generated column.
    
    The DBUG_ASSERT was hit during re-resolving (aka "refixing") of the
    generated column expression. The first [1;31mtime[m a statement opens a table
    that contains a generated column, all the generated columns in that
    table will be re-resolved to ensure that they are in a pristine state.
    The DBUG_ASSERT (in TABLE_LIST::map()) fails because the TABLE_LIST
    object associated with the Item_field that is being re-resolved, is in
    an inconsistent state where m_map is not in sync with m_tableno.
    
    The TABLE_LIST objects in question are allocated in
    sp_add_to_query_tables() in the DROP TRIGGER case and in
    sp_head::add_used_tables_to_table_list() in the stored function case.
    Common for these two functions is that they initialize the TABLE_LIST
    objects by manually setting the various members, instead of calling
    TABLE_LIST::init_one_table(), which is the common way of initializing
    such objects. This manual initialization forgets to set the m_map and
    m_tableno members to values that are consistent with each other.
    
    The fix makes the functions use init_one_table() to initialize the
    TABLE_LIST objects. init_one_table() also needed a little reorganizing
    to allow more flexible initialization of the associated lock request.
    
    (cherry picked from commit 1aabcfde317a5b96bf9d33226795aa096db2aab8)

[33mcommit 0f6bab956aa90cba4d5aeff6706d8de600bafcf8[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Wed Sep 16 10:36:43 2015 +0200

    Disable innodb.tablespace_crash since it [1;31mtime[ms out in Valgrind.
    
    Approved by Anitha Gopi <anitha.gopi@oracle.com> over IM.
    
    (cherry picked from commit c30aadf9cc8027bdd6939b096f087886290588ff)

[33mcommit 2442299b60bca6bdb0f42f9a35986b1d187a6576[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Sep 14 11:15:29 2015 +0200

    Fix link errors on Ubuntu.
    
    boost::chrono::system_clock::now() depends on clock_get[1;31mtime[m
    On some platforms we need to add an explicit depencency on librt

[33mcommit 9efdc7ced85123c23efef378651ac95956c5a887[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Sep 10 15:40:25 2015 +0200

    Bug#21797776 ASSERTION `BIT < MAP->N_BITS' FAILED.
    
    View v1 is materialized; the Field-s of the corresponding temporary
    tables are created by copying expressions from the SELECT list in the
    view's definition.
    If the expression is a column, the copy is done with
    create_tmp_field_from_field(). This uses the Field copy constructor,
    which copies any gcol_info. This isn't appropriate, and is already
    corrected in instantiate_tmp_table(); but it's too late; before
    instantiate_tmp_table() runs, which happens at JOIN::exec() [1;31mtime[m,
    there is already code which manipulates the new Field-s, for example
    insert_fields().
    
    Fix: implement what was noted as @todo. This zeroes gcol_info as early
    as possible. An assertion is kept to make sure I forgot no case.
    In create_table_from_items(), changing Create_field members isn't needed
    anymore, as Create_field is based on tmp_table_field which, after this fix,
    has no gcol_info.

[33mcommit af0acedd885eb7103e319f79d25fda7386ef1506[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Sep 11 12:29:37 2015 +0300

    WL#8845 Implement an InnoDB redo log format version identifier
    
    InnoDB has several [1;31mtime[ms changed its redo log format by introducing
    new redo log record types. Format changes would lead to misleading
    reports of redo log corruption when processing individual redo log
    records.
    
    In the redo log header (start of the ib_logfile0 file), we will introduce
    a format version identifier and textual representation of the software
    version that created the redo log files.
    
    Furthermore, we change the checksum of redo log checkpoint pages, so that
    older versions of MySQL will refuse to start up on redo log files that
    were created with a MySQL server that includes this fix.
    
    We will also remove a number of unused fields from the redo log
    header and checkpoint pages (pages 0, 1, and 3).
    
    Some tests will be expanded, because with this fix,
    the server must refuse to start up with older redo log files,
    unless they are dirty.
    
    We will also replace the configuration parameter
    innodb_log_checksum_algorithm with the Boolean parameter
    innodb_log_checksums.
    We make CRC-32C the only checksum on the InnoDB redo log pages when
    innodb_log_checksums=ON (the default). Checksums on the header page
    and the checkpoint pages are never disabled.
    
    innodb_log_checksums_func_update(), innodb_log_checksums_update():
    Update triggers for the new global Boolean variable innodb_log_checksums.
    
    innodb_log_checksum_func_update(), innodb_log_checksum_algorithm_update():
    Removed along with the global parameter innodb_log_checksum_algorithm.
    
    Removed definitions:
    LOG_MAX_N_GROUPS
    log_group_read_checkpoint_info()
    log_checkpoint_get_nth_group_info()
    log_checkpoint_set_nth_group_info()
    log_block_calc_checksum_innodb()
    log_block_calc_checksum_crc32_legacy_big_endian()
    recv_check_cp_is_consistent()
    log_block_checksum_weak_validation()
    log_block_checksum_what_matches()
    log_block_checksum_fail_fatal()
    log_block_checksum_is_ok_or_old_format()
    LOG_CHECKPOINT_OFFSET_LOW32
    LOG_CHECKPOINT_ARCHIVED_LSN
    LOG_CHECKPOINT_GROUP_ARRAY
    LOG_CHECKPOINT_ARCHIVED_FILE_NO
    LOG_CHECKPOINT_ARCHIVED_OFFSET
    LOG_CHECKPOINT_ARRAY_END
    LOG_CHECKPOINT_CHECKSUM_1
    LOG_CHECKPOINT_CHECKSUM_2
    LOG_CHECKPOINT_FSP_FREE_LIMIT
    LOG_CHECKPOINT_FSP_MAGIC_N
    LOG_CHECKPOINT_FSP_MAGIC_N_VAL
    LOG_CHECKPOINT_OFFSET_HIGH32
    LOG_CHECKPOINT_SIZE
    LOG_GROUP_ID
    LOG_FILE_START_LSN
    LOG_FILE_NO
    LOG_FILE_WAS_CREATED_BY_HOT_BACKUP
    LOG_FILE_ARCH_COMPLETED
    LOG_FILE_END_LSN
    
    Changed definitions:
    LOG_CHECKPOINT_LOG_BUF_SIZE
    
    Added definitions:
    innodb_log_checksums: The current value of the SET GLOBAL variable.
    LOG_CHECKPOINT_OFFSET
    LOG_HEADER_FORMAT (repurposing LOG_GROUP_ID which was always 0)
    LOG_HEADER_PAD1 (unused 4 bytes, zero-initialized)
    LOG_HEADER_START_LSN
    LOG_HEADER_CREATOR (renamed from LOG_FILE_WAS_CREATED_BY_HOT_BACKUP)
    LOG_HEADER_CREATOR_END
    LOG_HEADER_CREATOR_CURRENT
    LOG_HEADER_FORMAT_CURRENT
    log_group_t::format
    
    log_block_calc_checksum_format_0(): Renamed from
    log_block_calc_checksum_innodb(). This is only used when upgrading the
    redo log from non-tagged format.
    
    recv_find_max_checkpoint_0(): New function, used when upgrading the
    redo log from non-tagged format.
    
    recv_log_format_0_recover(): New function, used when upgrading the
    redo log from non-tagged format. Checks if the redo log is clean.
    
    log_group_header_read(): Replaces log_group_read_checkpoint_info().
    Also used for reading the log header page (page 0).
    
    recv_check_log_header_checksum(): Replaces recv_check_cp_is_consistent().
    Also used for checking the log header page (page 0).
    
    log_block_checksum_is_ok(): Checks a log block checksum. It must either
    be CRC-32C, or we must have innodb_log_checksums=OFF.
    
    Changed functions:
    
    log_group_file_header_flush(): Always zero-initialize the buffer
    and initialize all LOG_HEADER_ fields.
    
    log_group_checkpoint(): Zero-initialize the checkpoint buffer,
    and write the checkpoint in the new format, with CRC-32C checksum.
    
    recv_find_max_checkpoint(): Support both the old log format
    (if the old-format redo log is logically empty) and the new format.
    
    recv_scan_log_recs(): Clean up the logic a bit. Display a message
    when encountering (and terminating log parsing due to) invalid
    log blocks. For now, keep the existing behaviour and do not display
    a message about log block header mismatch.
    
    recv_recovery_from_checkpoint_start(): Replace the whole "ibbackup" label.
    Check if a log upgrade or a normal recovery is needed.
    
    srv_prepare_to_delete_redo_log_files(): Display a message about
    upgrading the redo log.
    
    Other changes to startup:
    
    If we are going to upgrade the redo log, we must avoid writing any
    new redo log records before we have replaced the redo log.
    
    dict_check_sys_tablespaces(), dict_check_sys_tables(): Avoid
    updating SYS_DATAFILES if we are going to upgrade the redo log.
    
    dict_create_or_check_sys_virtual(): Do not modify anything if we are
    running in --innodb-read-only or --innodb-force-recovery=6 mode.
    
    RB: 10096
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit 4c4d41c9435d562ce4b92c217080ddc181f6f0de[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Sep 11 09:34:20 2015 +0200

    Bug#21350125: Wrong results when ORDER BY is removed
    
    This is a regression from the bug fix for bug no. 14358878.
    The fix for that bug was basically to add the nullable property
    for any Item_direct_view_ref object that would wrap an Item
    selected from a derived table/view reference placed on the
    inner side of an outer join, and letting the null value be
    calculated at run [1;31mtime[m by looking at the NULL status of the first
    table of the derived table.
    
    This works well when the derived table is a single table, or an
    inner join, or a left outer join. However, if the derived table
    is itself a right outer join, the first table of the derived table
    may be the inner table of the right outer join, and that table may
    contain a NULL row even though the derived table as a whole is
    producing a row.
    
    Thus, we can get a NULL indication even though the derived table
    is producing a row, if the inner table of the right outer join that
    is part of that derived table is null-extended.
    
    The solution to the problem is to locate a table within the
    derived table that is not outer-joined within the derived table,
    ie locate a table that does not have the outer_join property set.
    A new function TABLE_LIST::any_outer_leaf_table() is implemented
    to locate such a table.
    
    Notice that this bug is limited to derived tables: A view has the
    same properties as a derived table, but there is one important
    difference: When the view is created, its join nests are normalized
    so that right joins are converted into left joins. Hence, the
    first table of the derived table nest will always be an "outer" table,
    which is good for our calculation. An alternative solution might thus
    be to normalize derived tables like views.
    
    (Derived tables are indeed right-join normalized, but the normalization
    is limited to the join_list structures, and not the table_list
    structures, which are used when navigating tables contained in
    a view/derived table. Whereas views are normalized when written to
    stored dictionary and table_list is reconstructed based on the
    normalized form when view is included into a query.)

[33mcommit 89114030d62a70c497211a75e7404bfda88bf815[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Sep 9 11:55:36 2015 +0200

    Bug#21779011 INVALID READS AND SENDING RANDOM SERVER MEMORY BACK TO CLIENT
    
    When a table containing a virtual generated column was stored in the
    InnoDB partition engine, reading from this table could in some cases
    return random data for the virtual generated column. The cause for
    this, was that the code that calculated and filled the value for the
    virtual generated column was not run.
    
    The rows are read using handler::multi_range_read_next() which calls
    read_range_first(). handler::read_range_first() is a virtual
    function. In the case of most other storage engines, the version
    implemented in the handler class is used. This function calls
    handler::ha_index_read_map() which updates the generated column
    values. When using the InnoDB partition engine, the specialized
    version implemented in ha_innopart::read_range_first() is used. This
    function does not use handler::ha_index_read_map() for reading the
    index but instead uses corresponding functions implemented in
    ha_innobase. None of these functions update the generated column
    values. The result is that when returning from
    handler::multi_range_read_next(), the virtual generated columns have
    not been updated.
    
    The fix for this is to add updating of virtual generated columns in
    handler::multi_range_read_next(). To avoid that the virtual generated
    columns are updated multiple [1;31mtime[ms (eg both in
    handler::multi_range_read_next() and handler::ha_index_read_map()) a
    new flag is added to the handler class that keeps track of whether the
    generated columns have been updated or not.

[33mcommit 0c4a1fdcc162c2d4af8e5dda58cc358553c9d2cd[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Aug 19 16:27:24 2015 +0300

    Bug#19630322 MTS: WORKERS MAY NOT ADAPT TO THE MASTER VERSION
    
    There's a flaw in master server version adaptation framework in that
    MTS workers could install another execution context at [1;31mtime[m the old
    context is being used.
    This occurs when Coordinator schedules an event (CREATE TABLE query)
    and after that processes a Format Description event, and does so
    asynchronously with the Worker. That creates a race between execution
    of CREATE TABLE in the old pre-FD context and the new context
    installation to the Worker which was done by the Coordinator thread.
    
    **Fixed** with relocating the whole master
    Format_description_log_event installation
    (set_rli_description_event()), which was previously in the Coordinator
    domain, to the Worker thread.
    
    By the refined logics, when coordinator reads
    Format_description_log_event, it calls the own
    Relay_log_info::set_description_event() method. This function will
    set the "memo" flag Slave_worker::fd_change_notified=false for each
    worker (to mean reset fact of any former notification).
    The flags will be used by Coordinator as a memo to itself
    to notify (see L4 below) Workers who are scheduled with
    a first group of events that follows the new FD.
    The Worker finds (Slave_job_group::new_fd_event !=NULL) the
    notification before it has executed the first group event, to install
    the new FD into its execution context. At once the Worker conducts the
    version adaptation.
    
    The worker logics is received a small block (see L5) consisting of
    a. checking whether a new FD event is supplied, to install it and run version adaptation
       routine;
    b. the Worker is made to "intelligently" destroy "obsolete" FD instances (see
       new ulong Format_description_log_event::usage_counter and Slave_worker::set_rli_description_event(),
       An obsolete FD can also be destroyed by Coordinator when Worker has exited.
       Worker never destroy the very last read FD.
    
    As a side effect, the version adaptation made work correctly for
    a case of mysqlbinlog | mysqld aka direct binlog applying via client session.
    More importantly, the FD installation relocation fixes other potential vulnerabilities
    where being processed by Worker events may depend on a certain FD. The old mechanism
    did not protect the correct execution environment.
    
    On the low-level,
    
    L0.
    
      Format_description_log_event class is extended with
        Format_description_log_event::usage_counter
      a usage counter
      to keep track of concurrent utilization of an instance and its
      correct destruction in Relay_log_info::set_rli_description_event()
      and Slave_worker::set_rli_description_event().
    
    L1.
    
      Relay_log_info::adapt_to_master_version(Format_description_log_event *fdle)
    
      is made to work by Coordinator or SQL threads.
    
    L2.
    
      ulong Relay_log_info::adapt_to_master_version_updown()
    
      is made to be invoked by Workers too. It's extensively commented to
      hopefully make the framework understood better.
    
    L3.
    
      Slave_worker::set_rli_description_event()
    
      is refactored to made the Worker to delete stale FD.
    
    L4.
    
      Slave_worker *Log_event::get_slave_worker()
    
      the new notification logics is deployed.
      A worker can't miss a fact of FD/version change when it takes
      on following events that are bound to the FD/version through
      implied execution context.
    
    L5.
    
      slave_worker_exec_job_group()
    
      augmented with logics of checking out the new notification and
      the new FD installation including version adaptation function (L2).

[33mcommit da65c655e904b04955fd6bf66e227483b567c006[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Sep 9 12:44:30 2015 +0100

    Bug#21374923: NO CYCLE TIMER IMPLEMENTATION FOR AARCH64
    
    This patch adds support for ARM64 for our low level CPU cycle [1;31mtime[mrs.
    Note that this does not work for ARM32.
    
    Based on patch contributed by Alexey Kopytov.

[33mcommit 38c12c2fd9296dd2fa5a18d7128d954fd655aa0a[m
Author: Horst Hunger <horst.hunger@oracle.com>
Date:   Wed Sep 9 11:50:47 2015 +0200

    Replaced the sleep with a conditional wait and reduced [1;31mtime[m in plugin to stabilize test for PB

[33mcommit 4fcdb29c88f24239b2d563403a536054050b755e[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Sep 8 16:53:45 2015 +0200

    Bug#21574933 Extra rows with derived table in subquery + XOR
    
    This problem stems from the transformation of IN subqueries
    in Item_in_subselect::single_value_in_to_exists_transformer().
    
    The source of the regression is the fix for bug no. 14358878, which
    wrapped all selected columns from a derived table that was on the
    inner side of an outer join in Item_direct_view_ref objects.
    The purpose of the wrapping was to make the columns nullable and
    nullability depending on some table from the derived table.
    
    single_value_in_to_exists_transformer() calculated orig_item from one
    such column. Prior to the wrapping, nullability information was present
    in orig_item. But after the bugfix, real_item() would pass the
    Item_direct_view_ref object and return the underlying Item object,
    which is not necessarily nullable.
    
    real_item() was originally used so that we do not build a permanent
    subquery transformation on top of a run[1;31mtime[m item. The wrapped
    Item_direct_view_ref objects are run[1;31mtime[m objects, meaning that
    they cannot be used in a permanent transformation.
    The solution is a kludge: If the Item_direct_view_ref object is
    nullable, we set the item being pointed to from the
    Item_direct_view_ref as nullable as well. This means that we pick up
    correct nullability, but the item does not correctly reflect the
    nullability of the datum. However, we have not been able to identify
    a situation where this causes erroneous or non-optimal execution.
    
    Further on, this means that NULL values from outer joined tables
    are included in the query result, meaning that the IN subquery
    can return NULL instead of FALSE in cases like this.

[33mcommit e5ae7aa96eca9f9f2f6f76dda226bec60b1a18f6[m
Author: Libing Song <libing.song@oracle.com>
Date:   Mon Jul 20 12:23:42 2015 +0800

    BUG#21045848 XA+MEMORY TABLE: POST SERVER RESTART 'XA COMMIT'
                 IS OVERRIDDEN BY 'DELETE' CMD
    
    ANALYSIS
    ========
    Memory table's data will be lost after server restarts. To keep the data
    consistency between master and slave, it just binlogs
    'DELETE FROM `db`.`table_name`' when the memory table is locked first [1;31mtime[m.
    So DELETE statement could be binlogged in many statements.
    
    Because it was put into binlog statement cache of current session,
    so it could cause below troubles:
    - COM_FIELD_LIST
      it doesn't flush binlog cache to binlog file. So the DELETEs were not
      binlogged in COM_FIELD_LIST, it was binlogged with next statement together.
    
    - CREATE TABLE ... LIKE memory_table
    - CREATE TABLE ... SELECT memory_table
      They were binlogged like:
      GTID_log_event | Anonymous_gtid_log_event
      DELETE FROM memory_table
      CREATE TABLE ... LIKE memory_table | CREATE TABLE ... SELECT memory_table
    
      Both statements shared the same gtid event. That was not correct.
    
    - DELETE was binlogged without BEGIN and COMMIT
      It caused some DML binlogged without BEGIN and COMMIT
      e.g.
      INSERT INTO myisam_t1 SELECT * FROM memory_table
      it was binlogged as:
      GTID_log_event | Anonymous_gtid_log_event
      DELETE FROM memory_table
      INSERT INTO myisam_t1 SELECT * FROM memory_table
    
    FIX
    ===
    To fix it, each DELETE will be binlogged separately and wrapped by
    BEGIN/COMMIT pair. It is flush to binlog immediately after it is
    written into binlog statement cache. E.g.
    
    INSERT INTO myisam_t1 SELECT * FROM memory_table will be binlogged as:
    
    GTID_log_event | Anonymous_gtid_log_event
    Query_log_event BEGIN
    Query_log_event DELETE FROM memory_table
    Query_log_event COMMIT
    GTID_log_event | Anonymous_gtid_log_event
    Query_log_event BEGIN
    Query_log_event INSERT INTO myisam_t1 SELECT * FROM memory_table
    Xid_log_event
    
    It also added a debug option build_completion_hash to mysql for test purpose.

[33mcommit c4ce65ca22ee207be543bd07dcbb8cff30853199[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 7 14:44:59 2015 +0200

    BUG#20483278 LAST_QUERY_COST SHOWS DIFFERENT VALUES FOR SHOW_COMPATIBLITY
    FOR ON VS OFF
    BUG#21788549 LAST_QUERY_PARTIAL_PLANS VALUES NOT SAME WITH
    SHOW_COMPATIBILITY_56
    
    Before this fix,
    
    With SHOW_COMPATIBILITY_56=ON,
    
    The statement
      SHOW STATUS like 'Last_query_cost';
    was executed with dedicated code in the information schema
    (sql/sql_show.cc), which is -- not -- implemented as a storage engine.
    As a result, this statement did not execute code in the optimizer,
    which did not overwrite the session status variable 'Last_query_cost'
    for the current select statement.
    
    As a result, the value of 'Last_query_cost' reported was
    for the previous SELECT statement executed in the session.
    
    With SHOW_COMPATIBILITY_56=OFF,
      SHOW STATUS like 'Last_query_cost';
    is executed as a SELECT ... FROM performance_schema.session_status.
    This table is implemented with a storage engine,
    and the optimizer code is used.
    By the [1;31mtime[m the value is returned, the value of 'Last_query_cost'
    reported is the value just overwritten by the optimizer for the current
    statement, which is not the desired result.
    
    The fix is to separate clearly in the session attributes:
    - the value of 'Last_query_cost' for the current query,
    - the value of 'Last_query_cost' for the previous query,
    and only save the former in the later when query execution is complete.
    
    The same issue exists for Last_query_partial_plan,
    which is fixed the same way.

[33mcommit 95a156f6eb78bf59be0d05c7215cc977aa05d03e[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Feb 19 16:53:51 2015 +0100

    BUG#19579811: SET GTID_PURGED DOES NOT STOP WAIT_FOR_EXECUTED_GTID_SET
    
    Background:
    
    WAIT_FOR_EXECUTED_GTID_SET waits until a specified set of GTIDs is
    included in GTID_EXECUTED. SET GTID_PURGED adds GTIDs to
    GTID_EXECUTED. RESET MASTER clears GTID_EXECUTED.
    
    There were multiple issues:
    
     1. Problem:
    
        The change in GTID_EXECUTED implied by SET GTID_PURGED did
        not cause WAIT_FOR_EXECUTED_GTID_SET to stop waiting.
    
        Analysis:
    
        WAIT_FOR_EXECUTED_GTID_SET waits for a signal to be sent.
        But SET GTID_PURGED never sent the signal.
    
        Fix:
    
        Make GTID_PURGED send the signal.
    
        Changes:
        - sql/rpl_gtid_state.cc:Gtid_state::add_lost_gtids
        - sql/rpl_gtid_state.cc: removal of #ifdef HAVE_GTID_NEXT_LIST
        - sql/rpl_gtid.h: removal of #ifdef HAVE_GTID_NEXT_LIST
    
     2. Problem:
    
        There was a race condition where WAIT_FOR_EXECUTED_GTID_SET
        could miss the signal from a commit and go into an infinite
        wait even if GTID_EXECUTED contains all the waited-for GTIDs.
    
        Analysis:
    
        In the bug, WAIT_FOR_EXECUTED_GTID_SET took a lock while
        taking a copy of the global state. Then it released the lock,
        analyzed the copy of the global state, and decided whether it
        should wait.  But if the GTID to wait for was committed after
        the lock was released, WAIT_FOR_EXECUTED_GTID_SET would miss
        the signal and go to an infinite wait even if GTID_EXECUTED
        contains all the waited-for GTIDs.
    
        Fix:
    
        Refactor the code so that it holds the lock all the way from
        before it reads the global state until it goes to the wait.
    
        Changes:
    
        - sql/rpl_gtid_state.cc:Gtid_state::wait_for_gtid_set:
          Most of the changes in this function are to fix this bug.
    
        Note:
    
        When the bug existed, it was possible to create a test case
        for this by placing a debug sync point in the section where
        it does not hold the lock.  However, after the bug has been
        fixed this section does not exist, so there is no way to test
        it deterministically.  The bug would also cause the test to
        fail rarely, so a way to test this is to run the test case
        1000 [1;31mtime[ms.
    
     3. Problem:
    
        The function would take global_sid_lock.wrlock every [1;31mtime[m it has
        to wait, and while holding it takes a copy of the entire
        gtid_executed (which implies allocating memory).  This is not very
        optimal: it may process the entire set each [1;31mtime[m it waits, and it
        may wait once for each member of the set, so in the worst case it
        is O(N^2) where N is the size of the set.
    
        Fix:
    
        This is fixed by the same refactoring that fixes problem #2.  In
        particular, it does not re-process the entire Gtid_set for each
        committed transaction. It only removes all intervals of
        gtid_executed for the current sidno from the remainder of the
        wait-for-set.
    
        Changes:
        - sql/rpl_gtid_set.cc: Add function remove_intervals_for_sidno.
        - sql/rpl_gtid_state.cc: Use remove_intervals_for_sidno and remove
          only intervals for the current sidno. Remove intervals
          incrementally in the innermost while loop, rather than recompute
          the entire set each iteration.
    
     4. Problem:
    
        If the client that executes WAIT_FOR_EXECUTED_GTID_SET owns a
        GTID that is included in the set, then there is no chance for
        another thread to commit it, so it will wait forever.  In
        effect, it deadlocks with itself.
    
        Fix:
    
        Detect the situation and generate an error.
    
        Changes:
        - sql/share/errmsg-utf8.txt: new error code
          ER_CANT_WAIT_FOR_EXECUTED_GTID_SET_WHILE_OWNING_A_GTID
        - sql/item_func.cc: check the condition and generate the new error
    
     5. Various simplfications.
    
        - sql/item_func.cc:Item_wait_for_executed_gtid_set::val_int:
          - Pointless to set null_value when generating an error.
          - add DBUG_ENTER
          - Improve the prototype for Gtid_state::wait_for_gtid_set so
            that it takes a Gtid_set instead of a string, and also so that
            it requires global_sid_lock.
        - sql/rpl_gtid.h:Mutex_cond_array
          - combine wait functions into one and make it return bool
          - improve some comments
        - sql/rpl_gtid_set.cc:Gtid_set::remove_gno_intervals:
          - Optimize so that it returns early if this set becomes empty
    
    @mysql-test/extra/rpl_tests/rpl_wait_for_executed_gtid_set.inc
    - Move all wait_for_executed_gtid_set tests into
      mysql-test/suite/rpl/t/rpl_wait_for_executed_gtid_set.test
    
    @mysql-test/include/kill_wait_for_executed_gtid_set.inc
    @mysql-test/include/wait_for_wait_for_executed_gtid_set.inc
    - New auxiliary scripts.
    
    @mysql-test/include/rpl_init.inc
    - Document undocumented side effect.
    
    @mysql-test/suite/rpl/r/rpl_wait_for_executed_gtid_set.result
    - Update result file.
    
    @mysql-test/suite/rpl/t/rpl_wait_for_executed_gtid_set.test
    - Rewrote the test to improve coverage and cover all parts of this bug.
    
    @sql/item_func.cc
    - Add DBUG_ENTER
    - No point in setting null_value when generating an error.
    - Do the decoding from text to Gtid_set here rather than in Gtid_state.
    - Check for the new error
      ER_CANT_WAIT_FOR_EXECUTED_GTID_SET_WHILE_OWNING_A_GTID
    
    @sql/rpl_gtid.h
    - Simplify the Mutex_cond_array::wait functions in the following ways:
      - Make them one function since they share most code. This also allows
        calling the three-argument function with NULL as the last
        parameter, which simplifies the caller.
      - Make it return bool rather than 0/ETIME/ETIMEOUT, to make it more
        easy to use.
    - Make is_thd_killed private.
    - Add prototype for new Gtid_set::remove_intervals_for_sidno.
    - Add prototype for Gtid_state::wait_for_sidno.
    - Un-ifdef-out lock_sidnos/unlock_sidnos/broadcast_sidnos since we now
      need them.
    - Make wait_for_gtid_set return bool.
    
    @sql/rpl_gtid_mutex_cond_array.cc
    - Remove the now unused check_thd_killed.
    
    @sql/rpl_gtid_set.cc
    - Optimize Gtid_set::remove_gno_intervals, so that it returns early
      if the Interval list becomes empty.
    - Add Gtid_set::remove_intervals_for_sidno. This is just a wrapper
      around the already existing private member function
      Gtid_set::remove_gno_intervals.
    
    @sql/rpl_gtid_state.cc
    - Rewrite wait_for_gtid_set to fix problems 2 and 3. See code
      comment for details.
    - Factor out wait_for_sidno from wait_for_gtid.
    - Enable broadcast_sidnos/lock_sidnos/unlock_sidnos, which were ifdef'ed out.
    - Call broadcast_sidnos after updating the state, to fix issue #1.
    
    @sql/share/errmsg-utf8.txt
    - Add error message used to fix issue #4.

[33mcommit 9676f0df9f70ccd999f293f9a51b1d92ca32867d[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Fri Sep 4 13:04:12 2015 +0200

    Bug#21696206: ASSERTION `TRANSL->ITEM->FIXED' FAILED IN SELECT_LEX::DELETE_UNUSED_MERGED_COLUMN
    
    It turns out that the standalone PREPARE doesn't call
    apply_local_transforms from mysql_prepare_insert, in contrast to
    mysql_prepare_delete and mysql_prepare_update.
    
    This has the effect that this resolve phase operation isn't performed when
    a standalone prepared insert is performed. Now, at PS execution [1;31mtime[m, the code
    in Sql_cmd_insert::mysql_insert does call apply_local_transforms - for the
    first [1;31mtime[m.  The fact that it is the first call implies this method's call to
    fix_prepare_information hasn't been called before.
    
    This again means that the value of st_select_lex::first_execution == true when
    we first enter apply_local_transforms (during EXECUTE).
    
    Now, apply_local_transforms tests on this flag in this stanza:
    
      if (derived_table_count &&
          first_execution &&
          !(thd->lex->context_analysis_only & CONTEXT_ANALYSIS_ONLY_VIEW))
        delete_unused_merged_columns(&top_join_list);
    
    Since first_execution is true, the call to delete_unused_merged_columns is
    done.  This is not supposed to have happened, the unused item is no longer
    fixed, and the ASSERT happens. Lifting the ASSERT doen't help; we crash later
    in sql_authorization.cc:2274 instead.
    
    Adding the call to apply_local_transforms at the end of
    mysql_prepare_insert makes the statement work, since we now do the work of
    apply_local_transforms also at prepare [1;31mtime[m, as for DELETE and UPDATE.
    
    We only do this if mysql_prepare_insert is not called for a INSERT..SELECT
    since for those statements, apply_local_transforms *are* called during
    prepare already (from st_select_lex_unit::prepare), and calling it can cause
    things to break, e.g. one of the tests for WL-5275 in insert.test,
    cf. discussion on the bug entry.
    
    This patch also fixes Bug#21696641, so test cases have been added for
    both 21696206 and 21696641.

[33mcommit 8dbdd0f22156c60c9eec7105dcf031d0f0890c22[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Thu Sep 3 16:26:49 2015 +0530

    Bug#21347001 SEGMENTATION FAULT WHILE CREATING GENERAL
                 TABLESPACE IN DISK FULL LINUX
    
    Description:
    
    When innodb returns disk full error in 'CREATE TABLESPACE'
    statement, sql layer calls my_error(). Here, my_error() is
    some[1;31mtime[ms called without any parameters even if error message
    requires one. In the error message expansion, va_arg() is called
    without required parameter.
    
    va_arg() returns an invalid pointer which leads to segmentation
    fault. Manual for va_arg() says "If there is no next argument,
    or if type is not compatible with the type of the actual next
    argument (as promoted according to the default argument promotions),
    random errors will occur.
    
    Fix:
    
    Added call for possible error messages. Changed my_error() call
    with specific error message to general error message.

[33mcommit 1fbfab58385e4675137a4d0b45aeda8c8f754fa7[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Thu Sep 3 10:42:59 2015 +0530

    Bug#21485997: WRONG INSTRUMENTATION OF CLASS GTID_SET
    
    Analysis:
    ========
    At run[1;31mtime[m, some GTID_SET objects are instrumented with a
    performance schema mutex key of 0, which is incorrect (not
    instrumented).
    
    The root cause is the declaration of constructors, like:
    
    Gtid_set(Sid_map *sid_map, Checkable_rwlock *sid_lock= NULL
    #ifdef HAVE_PSI_INTERFACE
            ,PSI_mutex_key free_intervals_mutex_key= 0
    #endif
            );
    
    The issue is giving a default value of 0 when the mutex key
    is not passed. This allows some caller to not pass any key,
    causing a bug.
    
    
    Fix:
    ===
    Moved the PSI_mutex_key from Gtid_state class to Gtid_set
    class.

[33mcommit 57ee0d9a9d72ed465999bf7e43d9c83a9f000ce1[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 14:07:30 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::optimize()
    
    Patch # 1 of 5.
    
    This is an issue with missing propagation of error values.
    A const subquery is evaluated when removing const items in
    remove_eq_conds(). The subquery fails because it returns more than
    one row. This error code is poorly handled and causes the optimizer
    to attempt to optimize a subquery a second [1;31mtime[m, which causes
    this assert.
    
    This patch changes interfaces for eval_const_cond(), remove_eq_conds()
    and internal_remove_eq_conds(). The interfaces encourage better error
    propagation but requires slightly more code to be written.

[33mcommit 16ef114cc22ef20e50762cb3a777a31811189e07[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Sep 1 16:33:44 2015 +0200

    Bug#21185883 MISSING LIBSTLPORT.SO LINK IN SOLARISSTUDIO 12.4 VERSION
    
    This is a (partial) backport of
      Bug#16555106 FIX BROKEN BUILD WITH SOLARIS/GCC 64BIT MODE
      Bug#17826757 CMAKE SUPPORT FOR GCC ON SOLARIS
      Bug#17954277 BUILD FAILS ON SOLARIS IF NO STL_LIBRARY_NAME FOUND
      Bug#19010286 BUILD WITH SUN C++ 5.13 SUNOS_SPARC BETA
    
    This adds support for building with Solaris Studio 5.13
    
    When building with stlport4 we install the run[1;31mtime[m libraries,
    since these are part of the Sun Studio, and not part of Solaris by defualt.
    When building with stdcxx4, we get dependencies on libstdcxx4.so
    which is part of Solaris by default.
    
    The only new code is:
     - an extra -R'\$ORIGIN/..' in CMAKE_SHARED_LIBRARY_C_FLAGS
       so that plugins are able to find the stlport C++ library
     - new compile flags when cmake is invoked with -DSUNPRO_CXX_LIBRARY=stdcxx4
    
    Additionally:
     - disable building of unittest/gunit/stdcxx, it was exeperimental,
       and un-used anyways.
     - #include <ios> in validate_password.cc

[33mcommit 8a64c5e6cd1c676121e32d9274dec60ca365a624[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Mon Aug 31 16:42:18 2015 +0800

    Bug#21614368 CRASH IN GEOMETRY::CONSTRUCT WITH BLOB INPUT
    
    Issue: The ST_ASWKB() function was given a Field_blob object to read GEOMETRY
    data from, but the row buffer doesn't contain valid row data, for blob fields
    that means the blob buffer pointer&length stored in the row buffer is invalid,
    so when ST_ASWKB() tries to access the GEOMETRY data at this invalid address,
    segment fault occurs.
    
    Fix: GEOMETRY data is never convertible to date[[1;31mtime[m] data or any other type of
    data, we know this for sure, so don't try to detect whether it can be
    convertible in the first place so above issue is avoided.
    
    Doing so works around the above issue but
    doesn't directly solve it --- to solve it we would need to maintain a state in
    TABLE class indicating whether the row buffer has valid row data or not, but
    that's beyond this bug fix I would think.
    
    Also, make the field type of ST_ASWKB() as GEOMETRY. Although the returned blob
    is actually a WKB byte string rather than a GEOMETRY byte string, the
    difference is only relevant to GIS functions, which do not rely on the field
    type anyway --- when we access any blob data we check its content&structure to
    make sure it is valid GEOMETRY data, so GIS functions won't be fooled by the
    field type. To the rest of MySQL where the field type really is concerned,
    WKB data is also geometry data which can't be converted to any other type of
    data either, and is not applicable anywhere the GEOMETRY data isn't applicable.
    
    To distinguish functions producing WKB data from those producing GEOMETRY data,
    add a SP_WKB_FUNC function type and let ST_ASWKB() be of such a function type.

[33mcommit e3b9f29ea945e8fdc88e8bd6af77ce4b3d961697[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Mon Aug 31 12:42:55 2015 +0200

    Bug#21619634 ACCESS OF DEALLOCATED ITEM_OUTER_REF CAUSES CRASH: PREPARED STMT
    
    An Item_outer_ref is substituted for the original Item_field as part of
    binding.  This is registered for rollback with a view to separate prepared
    statements via the usual mechanism: THD::change_item_tree, in
    Item_field::fix_outer_field, ca line 5548:
    
      if (!(rf= new Item_outer_ref(context, this))) ..
      thd->change_item_tree(reference, rf);
    
    A semi-join tranformation produces a permanent node, Item_func_eq ca line 2118
    in sql_resolver.cc, which takes as its left item the transient Item_outer_ref:
    
      new Item_func_eq(in_subq_pred->left_expr->element_index(i),
                       subq_select->ref_pointer_array[i]);
    
    However, this is not registered for rollback, and when we come to execution,
    this pointer points to garbage, hence the crash.
    
    The patch registers this location (in Item_func_eq) as the one which needs the
    rollback, instead of the original location, which has now been made irrelevant
    by the transformation. This lets re-binding at execute [1;31mtime[m proceed without
    seeing the transient object, and all is well.
    
    A new method,
    
      replace_rollback_place_for_value
    
    is introduced for this purpose. A related method, change_item_tree_place has
    been renamed to
    
       replace_rollback_place_for_ref
    
    to show the relationship between them.
    
    We also removed two redundant arguments for nocheck_register_item_tree_change
    and added new constructor to Item_change_record.
    
    The repro test has been added to subquery_sj.inc.

[33mcommit 002fefdf466901d3688566a6e02d46a2f1accb2e[m
Author: Thirunarayanan Balathandayuthapani <thirunarayanan.balathandayuth@oracle.com>
Date:   Sun Aug 30 13:26:08 2015 +0530

    Bug#21143276   Hanging due to async_rollback
    
    Problem:
    =======
    High Priority transaction waits for victim transaction to exit from InnoDB.
    Victim transaction entered InnoDB and waits for asynchronous rollback to
    complete. It leads to hanging state.
    
    If in_depth > 1 then we should allow the thread to continue. Problem arises
    because in_depth variable  can be updated by two threads at the same [1;31mtime[m
    (Async rollback killer thread enter InnoDB on behalf of victim transaction
    and Victim transaction tries to enter InnoDB on its own).
    
    Solution:
    ========
    Don't track the async rollback killer thread to enter into the InnoDB.
    Basically it is way to avoid to enter the InnoDB for the same transaction
    by multiple threads.
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>
    Reviewed-by: Bin Su <bin.x.su@oracle.com>
    RB: 9876

[33mcommit c4e3acb332b7494e2df994b2f92a3683b57be548[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Fri Aug 28 22:47:48 2015 +0530

    BUG#20367116: ALTER TABLE BREAKS ON DELETE CASCADE FOREIGN KEY
                  CONSTRAINT
    
    Analysis
    ========
    ALTER TABLE operation on a table with FOREIGN KEY CASCADE
    constraint while a concurrent DML operation is being
    performed on the table referenced may break the referential
    integrity.
    
    In the case of ALTER TABLE operation using COPY algorithm:
    a) The ALTER TABLE operation on the table with FOREIGN KEY
       CASCADE constraint, the data is copied from the original
       file to temp file and transaction is committed releasing
       InnoDB locks (also, as an optimization, InnoDB commits
       transaction and releases locks every [1;31mtime[m ALTER TABLE
       copies 10000 rows).
    b) Concurrently if DELETE operation is executed on the
       referenced table, it will trigger delete of records
       on the table being altered. This will end up deleting
       records from the original file since the rename of
       the temp file has not yet completed and locks on the
       original file are not held.
    c) The ALTER TABLE operation continues, renaming the temp
       file to the original file leaving behind orphaned rows.
    
    In the case of ALTER TABLE operation using INPLACE algorithm:
    a) The DELETE operation on the referenced table acquires an
       LOCK_IS on the table being altered due to the CASCADE
       FOREIGN KEY constraint.
    b) The ALTER TABLE operation tries to acquire LOCK_X during
       the commit phase of INPLACE alter and is added to the
       waiting queue since LOCK_IS is acquired by DELETE.
    c) DELETE operation upon finding the record to delete, tries
       to acquire LOCK_IX on the table and is added to the wait
       queue. This results in a deadlock with ALTER operation
       being rolled back.
    Fix:
    ====
    a) INNODB has introduced a new handler API
       'get_cascade_foreign_key_table_list()' which returns a full
       closure of all tables ordered by the dependency on FOREIGN
       KEY CASCADE constraint.
    b) A function called 'lock_fk_dependent_tables()' is added
       which locks the list of tables ordered by the FOREIGN
       KEY CASCADE constraint for the table being altered.
       It is invoked in two places.
       1) Before the copy of data is invoked for the ALTER TABLE,
          COPY algorithm.
       2) Before ALTER TABLE, INPLACE commit.
    This avoids the orphaned rows and deadlock.
    
    Please note that this is a temporary workaround which is
    necessary until WL#6049 "Meta-data locking for FOREIGN KEY tables"
    is implemented
    
    NOTE: Some of the test cases have been removed, since the condition
    of concurrent DDL/DML operation on a parent table while a DDL
    operation is performed on the the child table having a CASCADE
    foreign key constraint is restricted with this patch

[33mcommit 342d0b82225ccb7df2d6fc24216d755061ca06fc[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu Aug 27 10:16:57 2015 +0530

    - Bug#21662604: INNODB_ZIP.WL6501 FAILS IN COMPRESSION RUN
      - Further reduce the test-case to bare minimum to reduce the execution [1;31mtime[m
        and in turn warning messages.

[33mcommit 3e688bbf37df137b7eb9316e3eb50e2a8efbe3f8[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu Aug 27 09:59:46 2015 +0530

    - Bug#21744589: MLOG_TRUNCATE IS LOGGING SPACE_ID TWICE
    
      MLOG_TRUNCATE was logging space_id twice.
      Once as part of mlog_write_initial_log_record_low and next [1;31mtime[m
      explicitly using mach_write_to_4.
    
      Avoid this redundant information.
    
      Reviewed by: Jimmy Yang (jimmy.yang@oracle.com)
      RB: 10073

[33mcommit 81e69d8e023d86a58c7c2ca0ed421e666b8aea9f[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Mon Aug 24 18:52:40 2015 +0300

    Fixes for bug#21645001, bug#21645944, bug#21646017 and Bug#21646106
    
    * Demoted the asterisk to a regular symbol (not a wildcard) in
      version_tokens_delete()
    * version_tokens_delete() now trims whitespace around individual token
      names to delete.
    * version_tokens_delete() will ignore an invalid (empty, all spaces)
      token name.
    * version_tokens_delete() will treat NULL arguments as an empty string
      argument.
    * version_tokens_set() and version_tokens_edit() will properly detect
      and ignore empty token names
    * version_tokens_lock_shared() and version_tokens_lock_exclusive()
      will now return error for NULL [1;31mtime[mout values.
    * Test cases added to the above and existing tests updated.

[33mcommit d84945f82aad0619c11f85910f640cc81f5260d0[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Wed Aug 26 10:46:16 2015 +0800

    WL#8756  Deprecate binlog_max_flush_queue_[1;31mtime[m in 5.7  - post fix
    
    On some platforms, the assert_grep.inc in sys_vars/t/binlog_max_flush_queue_[1;31mtime[m_basic.test
    is failing, because it requires enabled binlog and an error log named mysqld.1.err exactly.
    
    To fix the problem, move the test case with assert_grep.inc into replication binlog suite.

[33mcommit 714c0b3d96f273d8b39b8f65250320157899ddf0[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Aug 25 10:47:54 2015 +0100

    Bug#21657078: MYSQL DOES NOT COMPILE WITH VISUAL STUDIO 2015
    
    Fix various issues when building with Visual Studio 2015:
    - Visual Studio 2015 adds support for [1;31mtime[mspec. Add check for
      this and only use our replacement if [1;31mtime[mspec is not defined.
    - Rename lfind/lsearch to my* to avoid redefinition problems.
    - Set default value for TMPDIR to "" on Windows as P_tmpdir
      no longer exists.
    - [1;31mtime[mzone and tzname are no renamed _[1;31mtime[mzone and _tzname.
    - std::hash_map has been deprecated, use std::unordered_map instead.
    - Add support for Wix Toolset 3.10 which is required to make
      MSI packages with Visual Studio 2015.

[33mcommit 9b0e016889c73b8d9db1837f97e03f8ee20fbf19[m
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Tue Aug 25 09:12:33 2015 +0530

    BUG#16418100 ERROR "WHEN GTID_NEXT IS SET TO A GTID" ROW BASED REPLICATION
    
    Problem & Analysis:
    ===================
    When relay log info repository is configured to be persisted in a
    table, it is updated when transaction commits or explicit flush is called,
    like on relay log rotate. If a transaction that uses non-transactional
    engine is split across multiple relay logs, on relay log flush
    it will be partially committed. If GTID_MODE=ON and when partial transaction
    in first relay log is committed, it uses the current GTID value and sets
    GTID value to UNDEFINED GROUP. Hence it causes 'ER_GTID_NEXT_TYPE_UNDEFINED_GROUP'
    error when it is trying to commit the rest of the transaction.
    It also causes many other problems if the repository is updated in between
    the transaction and it should be avoided.
    
    Fix: relay log info repository should be updated on relay log
    rotate. But when the transaction is split across two relay logs,
    update the repository will cause unexpected results and should
    be postponed till the 'commit' of the transaction is executed.
    
    Introduced a flag that set to 'true' when this type of 'forced flush'(at the
    [1;31mtime[m of rotate relay log) is postponed due to transaction split
    across the relay logs. And the flag will be checked in flush_info function
    to see if there is a flush that got postponed earlier. If so, it will
    force the flush now irrespective of sync_relay_log_info value.

[33mcommit ad507e87909a27eac32702bc0c8d8dfad46c8aba[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Thu Aug 20 12:59:12 2015 -0500

    Bug 21662569: INNODB_ZIP.16K FAILS ON SOLARIS
    
    The output in this test changed some[1;31mtime[m recently when the table_ids changed.
    It should be sorted by table name instead.

[33mcommit 69ffc9c3a5d3466e4276a21e657779ba040fdfbf[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Fri Aug 21 11:01:37 2015 +0200

    Bug#21625929: ASSERTION FAILED: FALSE IN CREATE_TMP_FIELD
    WITH UNKNOWN OUTER FIELD
    
    Some assertions in sql_tmp_table.cc were added in a merge
    from 5.6. They ruled out the possibility that a temporary
    table column could be created on the basis of a prepared
    statement parameter.
    
    Fixed by reverting the behavior back to 5.6 in this
    particular case: I.e. not creating columns for parameters
    during PREPARE. During EXECUTE we don't have this problem,
    obviously, since at that [1;31mtime[m the parameters have been
    substituted by constant expressions.

[33mcommit c7991b854331c90db641f51c60eb2ae67ef27efe[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Thu Aug 20 10:54:58 2015 +0800

    WL#8756  Deprecate binlog_max_flush_queue_[1;31mtime[m in 5.7
    
    The variable binlog_max_flush_queue_[1;31mtime[m was introduced in 5.6
    in order to fine-tune the Binlog Group Commit. Due to changes
    in 5.7, the option has no effect any more. Therefore we
    deprecate the variable in 5.7 and plan to remove it in 5.8.
    
    This worklog will generate a deprecation warning in 5.7 and 5.8.
    It will not remove the variable in 5.8.

[33mcommit bea174f0d37132f8a89fa9a56b47ccbe9131467f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Aug 18 12:27:34 2015 +0200

    Bug#21616585 ASSERTION FAILED: !(DECIMAL_IS_ZERO(FROM2) && FROM2->SIGN)
    
    Avoid negative zero for results when converting [1;31mtime[m to decimal.
    This is a followup patch to
    Bug#18411494 WRONG COMPARSION ON BIG DECIMAL VALUES

[33mcommit 7da44d91f63e5c5cb7afa33837db449b9d15256b[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Wed Aug 19 12:31:19 2015 +0530

    - Bug#21629618: RECOVERY FAILURE: PLUGIN 'INNODB' INIT FUNCTION RETURNED ERROR
    
      Likely cause seems to be related to recently added code that check if
      there is out-of-space error while creating new rollback segments.
    
      Unfortunately, we couldn't find other traces in InnoDB log files
      about out-of-file error.
    
      Based on discussion we have improved the logging messages and added
      new error messages to help catch this issue if it occurs next [1;31mtime[m.
    
      Reviewed by: Sunny Bains (sunny.bains@oracle.com)
      RB: 9981

[33mcommit 4b7f7617a9d4a95fbf5abf9b85c8be8a4f6d381b[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Aug 12 14:06:11 2015 +0200

    Bug#21562212 MEMORY/PERFORMANCE_SCHEMA INSTRUMENTS SHOULD BE ALWAYS ENABLED
    
    Before this fix,
    - "memory/%" instruments could be configured with
      TIMED='YES' or TIMED='NO' in table setup_instrument.
    - "memory/performance_schema/%" instruments could be configured with
      ENABLED='YES' or ENABLED='NO' in table setup_instrument.
    
    This does not make sense, because:
    - memory instruments in general are never [1;31mtime[md,
    - performance_schema builtin memory instruments in particular can not be disabled
    
    With this fix,
    - "memory/%" instruments are never [1;31mtime[md.
    - "memory/performance_schema/%" instruments are never disabled.
    
    This fix avoids confusion, by displaying more truthfully in table
    setup_instrument what the server actually does with the memory
    instrumentation.

[33mcommit bdd466d91cb345463644c45aaa263c0c6f0927da[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Aug 14 09:51:18 2015 +0200

    Bug#21613422: Assertion failed: !thd->is_error() in select_lex::prepare()
    
    This is yet another occurrence of a missing error check, this [1;31mtime[m after
    call to find_field_in_table_ref().

[33mcommit ffafad49b76ad3a248cc027ed1728ffd956d307e[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Aug 13 13:33:10 2015 +0100

    Bug#21296553: WARNINGS SENT TO STDERR/OUT EVEN WHEN LOGGING SET TO SYSLOG
    
    The problem was that some server error messages during server boot are
    written to stderr/out even if syslog and/or error log file is enabled.
    This happened because proper error logging is only enabled after the
    relevant options have been parsed and the server knows where to write
    the messages. This is especally a problem if mysqld has been detached
    so that stderr/out messages are simply lost.
    
    This patch fixes the problem by buffering error log messages until
    the relevant options have been parsed. The existing Buffered_logs
    implementation, which was only partially in use, has been removed
    in favour of a similar implementation hidden behind the standard
    error log API. This means that the error log API can be used at
    all [1;31mtime[ms.
    
    Note that if the server terminates during startup before the
    error file log has been opened (we know which file to open),
    error messages will be written to stderr as before.

[33mcommit 982dcfc747a6572704f8108bdef8a583a3629f1b[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Aug 10 09:25:38 2015 +0200

    Bug#21055139 SUBQUERY HAVING COUNT WITH GROUP BY GIVES INCORRECT RESULT
    
    For 'x in (SELECT y)', subquery materialization is used.
    x and y are here a list of expressions.
    
    The result of 'select y' is stored into a temporary table; duplicates
    are eliminated:
    1) by default this is done by creating a unique index over the columns
    of the tmp table
    2) but here the length of "y" is so big that it's not possible; a
    hidden hash column, indexed, is added; because of possible collisions
    (two different "y" having the same hash value), the hash index is not
    unique; if, upon insertion, there is a collision with an existing hash
    value, the values of "y" are compared to decide if this is a duplicate
    or not (see check_unique_constraint() in
    Query_result_union::send_data()).
    Possibility (2) is new in 5.7, added by WL#6711 "Support InnoDB as
    additional storage engine for tmp table".
    
    Then we look "x" up in the tmp table's index (ha_index_read_map()).
    In case (1) above, the index is unique, so a single lookup is enough.
    In case (2), the index is not unique, so we can have:
    hash y
    1    A
    2    B
    and if we are searching for "x=B", the first lookup is a false positive.
    Thus we must do all lookups until we find a match (this matching is
    ensured by the condition "x=y" in
    "subselect_indexsubquery_engine::cond" created in
    subselect_hash_sj_engine::setup()).
    This problem was missed in WL#6711.
    
    Fix: if using hash column, index is not unique, so set unique=false.
    No new testcase is added, as an existing testcase in
    subquery_sj_mat_nosj.test is covering the case (transitions from bad
    to good result). Notice how it has ("[1;31mtime[m","[1;31mtime[m") as value for "y";
    this gives a hash value of 0; so does ("lpjdzvkp","lpjdzvkp"); so does
    any pair of two identical strings (CRC of second string is XORed with
    CRC of first steing). So we have different "y" for a same hash value.
    Changes in EXPLAIN: are about subqueries using hash_field; they must
    try multiple lookups.

[33mcommit 68c80250f30bfd2f6538a694226a8c83c7138f32[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Wed Aug 12 11:44:59 2015 +0530

    - Bug#21569876: SEARCH_LATCH_TIMEOUT DOESN'T MAKE SENSE AFTER SUPPORTING MULTI AHI LATCH
    
      Search Latch Timeout approach was introduce to avoid search latch contention.
      Search latch was released for BTR_SEARCH_TIMEOUT (10000) [1;31mtime[ms before it was kept
      w/o releasing unless any writer is waiting.
    
      This contention is now reduced by using a better and alternative approach of
      ahi partitioning and so we can get rid of old approach.
    
      Reviewed by: Sunny Bains (sunny.bains@oracle.com)
      RB: 9902

[33mcommit 6e16d5a6afd82ba56be9aa73a3e396a233a953da[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Thu Jul 23 19:13:17 2015 +0300

    Bug #21459999 "ASSERTION FAILURE WHEN PREVIOUSLY UNUSED TIME ZONE IS USED WITH GTID + NO BINLOG".
    Bug #21459795 "ASSERTION FAILURE WHEN PREVIOUSLY UNUSED TIME ZONE IS USED IN FUNCTION/TRIGGER".
    
    The first problem was that attempt to use previously unused [1;31mtime[m zone
    in CONVER_TZ function with binary log off but with GTID mode turned on
    caused assertion failure in debug builds and warning/errors in release
    builds.
    
    The second problem was that attempt to use previously unused [1;31mtime[m zone in
    CONVERT_TZ function or for @@[1;31mtime[m_zone system variable inside of stored
    function or trigger has caused assertion failure in debug builds (and
    no effect in release builds).
    
    Both problems stem from the fact that use of previously unused [1;31mtime[m zone
    causes [1;31mtime[m zone cache miss and thus initiates attachable transaction
    to look up [1;31mtime[m zone definition in [1;31mtime[m zone tables. To finish this
    attachable transaction we call trans_commit_stmt() function.
    In case of the first problem this function and functions called by it are
    unaware of attachable transaction being active, so in no-binlog + gtid-mode-on
    mode it will prematurely try to store current GTID (which describes main
    transaction) into special table. Since attachable transaction is read only
    errors will be reported/assertion will fail.
    Similarly trans_commit_stmt() is not supposed to be called inside of stored
    functons/triggers which is enforced by assert, which fails in the second case.
    
    This patch fixes the problem by introducing new trans_commit_attachable()
    function which is used to finish attachable transaction specifically. This
    function is slimmed down version of trans_commit_stmt() which commits
    attachable transactions but skips code which is unnecessary and unsafe for
    them (like dealing with binary log and GTIDs). It is also OK to call this
    function inside stored functions/triggers.

[33mcommit 3648af6ee06c681f1af8fcbd1cf631bb7a0a82b6[m
Author: Benny Wang <benny.wang@oracle.com>
Date:   Wed Jul 29 05:30:27 2015 +0200

    Fixed bug#21390605: VALGRIND ERROR ON DELETE FROM TABLE CONTAINING AN INDEXED
    VIRTUAL COLUMN
    
    Take one test case for an example.
    CREATE TABLE t1 (
     a INTEGER,
     b INTEGER GENERATED ALWAYS AS (a) VIRTUAL,
     c INTEGER GENERATED ALWAYS AS (b) VIRTUAL,
     INDEX idx (c));
    When calling the callback function my_eval_gcolumn_expr_helper() to evaluate
    the value of column 'c', the value of virtual generated column 'b' is needed.
    But the caller only asks for 'c'. So the callback function won't calculate
    column 'b' before evaluating column 'c'.  This results in invalid read of 'b'
    during calculate 'c'.
    
    The solution is to calculate all the base virtual generated columns before
    evaluating the required column.
    
    Instead of adding yet another walk() to gather needed columns, we do the
    walk() a single [1;31mtime[m (when opening the table) and build a bitmap
    (new member Generated_column::base_columns_map).  This bitmap replaces the
    base_columns list. It is different as it lists virtual base columns too
    (base_columns only listed *stored* base columns). So the InnoDB code, which
    wants to see the stored base columns only, has been adapted to filter out the
    bitmap; if it only needs a count of columns, it uses the new member
    Generated_column::num_non_virtual_base_cols.

[33mcommit 489e597d2d887f5c6fe1c015ef77e015a778b88a[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Jul 31 15:06:50 2015 +0200

    Bug#21139402 ASSERTION FAILED: LENGTH > 0 && KEYPARTS != 0, CRASH IN JOIN::OPTIMIZE_KEYUSE
    
    select 1 from t1 where 1 in
    (select (t2.c is null) in (select t1.a from t1 where t1.b)  from t2);
    
    Scenario, in [1;31mtime[m order:
    
    1) FIXING 'c IS NULL':
    Item_func_isnull::fix_fields():
    c is a pk column, cannot be NULL, so this item is marked as constant
    with a value of FALSE:
      virtual void update_used_tables()
      {
        if (!args[0]->maybe_null)
        {
          used_tables_cache= 0;            /* is always false */
          const_item_cache= 1;
          cached_value= (longlong) 0;
        }
    
    2) DOING IN->EXISTS FOR SELECT#3:
    Item_in_subselect::single_value_transformer():
    "c IS NULL" wrapped into Item_ref which is is injected into WHERE:
    "WHERE b AND (c IS NULL)=t1.a"
    Item_ref is NOT marked as an outer reference because 'c IS NULL' is
    constant:
    
        // Make the left expression "outer" relative to the subquery
        if (!left_expr->const_item())
          left->depended_from= select->outer_select();
    
    (this if() is introduced by:
    Bug#16369522: Assertion failed: !tables ||
    thd->lex->is_query_tables_locked() with comment:
    "   Notice also that const items need not be designated as outer.
        Ignoring those occurrences was required to preserve multiple equality
        handling for equalities involving constant values.";
    it might be why this bug is a regression)
    
    Same function does fix_fields() on this new WHERE, which does nothing
    as 'c is NULL' is already fixed.
    
    3) SELECT#2 IS SEMIJOIN-MERGED INTO SELECT#1
    
    We do fix_after_pullout on sj condition and thus on SELECT list of select#2
    and thus on select#3 and thus on WHERE of select#3 and thus on
    the previously injected 'c IS NULL'; this is fix_after_pullout for 'c
    IS NULL':
      if (arg_count)
      {
        for (arg=args, arg_end=args+arg_count; arg != arg_end ; arg++)
        {
          Item *const item= *arg;
          item->fix_after_pullout(parent_select, removed_select);
          used_tables_cache|=     item->used_tables();
          not_null_tables_cache|= item->not_null_tables();
          const_item_cache&=      item->const_item();
    
    'item' is t2.c. The last lines above declare that "IS NULL uses
    t2" and is not constant (anymore!).
    So we now have a non-constant item in select#3, not declared as outer
    reference, involving a table from select#1 (t2 is in select#1 now). This is
    wrong.
    
    Later we have a keyuse for select#3 for '(c IS NULL)=a',
    so keyuse->used_tables= used_tables of 'c is NULL' = map of t2, wrong,
    as t2 is not in select#3. So we try to access a join_tab for t2 in
    select#3 and crash.
    
    Fix:
    when we pull out an Item_func, if it's already constant there's no
    reason it could become non-const; and if it's constant it doesn't
    really use tables, so there's no reason to recalculate
    used_tables_cache and friends.

[33mcommit 7987bd8171fc4c5bf51b5616d09b48c48a7c9009[m
Author: Libing Song <libing.song@oracle.com>
Date:   Mon Jun 29 15:02:05 2015 +0800

    BUG#19641963 RPL.RPL_SEMI_SYNC_GROUP_COMMIT_DEADLOCK FAILS WITH
                 ASSERT IN MTS CODE
    
    It was an sporadically assertion failure on:
    lwm_estim + 1 == rli->gaq->get_job_group(rli->gaq->entry)->sequence_number);
    
    In slave_worker_ends_group(), mts_submode->min_waited_[1;31mtime[mstamp is checked.
    if it is not SEQ_UNINIT then the worker should call get_lwm_[1;31mtime[mstamp and
    signal coordinator. But the checking is not protected by mts_gaq_LOCK,
    coordinator's status may already changed when the worker calls get_lwm_[1;31mtime[mstamp.
    And the status should not appear when get_lwm_[1;31mtime[mstamp is being called.
    
    To fix it, mts_submode->min_waited_[1;31mtime[mstamp is checked again after the
    worker gets mts_gaq_LOCK.

[33mcommit 9e587fbc17edb8b1f004bbc864ee18c2e84d5f0c[m
Author: Benny Wang <benny.wang@oracle.com>
Date:   Fri Jul 24 10:14:06 2015 +0200

    Fixed bug#21469535: VALGRIND ERROR (CONDITIONAL JUMP) WHEN INSERT ROWS INTO
    A PARTITIONED TABLE
    
    When prune partitions, the server only cares which partitions it needs scan.
    During this [1;31mtime[m, only partition expression needs an evaluation. So there is
    an extra argument named 'bitmap' to show which columns needs an evaluation in
    fill_record.
    
    However, update_generated_write_fields tries to update the virtual generated
    columns only if they are in table->write_set and ignore the 'bitmap' in
    fill_record. Because the base columns of vitual generated columns in
    table->write_set don't occur in 'bitmap', it results in an valgrind error
    during generate the value of virtual generated columns.
    
    The solution is to add the 'bitmap' argument to update_generated_write_fields.
    If the field index of virtual generated column doesn't occur in 'bitmap'
    argument, the virtual generated column won't be evaluated.

[33mcommit 1713009bc088cff3cfff8a69f54b72cc87e312ab[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Mon Jul 13 10:18:22 2015 +0200

    Bug#21391781 ASSERT WHEN RUNNING ALTER TABLE ON A TABLE WITH INDEX ON
                 VIRTUAL COLUMN
    
    During ALTER TABLE on a table that has an indexed virtual column, the
    storage engine calls my_eval_gcolumn_expr() to compute the value of
    the virtual column. Information about which columns that should be evaluated
    is specified using an ulonglong used a bit map. This bitmap can only
    define indexes on the 64 first columns.
    
    In the failing test, the virtual column with an index was column
    number 2. In addition column number 66 was a virtual column. Due to a
    bug in how the code checked if a column should be evaluated or not,
    column 66 left-shifted 66 [1;31mtime[ms in the bitmap matched the same
    position as column number 2 in the bitmap. This caused column 66 to be
    evaluated. But since the storage engine had not requested this column
    to be evaluated, it had not provided the values for the base columns
    needed for this evaluation. This caused the evaluation to be based on
    whatever data was in the record buffer for the base columns and in
    some cases this could cause errors and hitting the assert.
    
    The fix for this problem is to replace the ulonglong used as a 64 bit
    bitmap with a general bitmap of type MY_BITMAP that can have a bit for
    all columns in the table.
    
    This change also removes the restriction that for virtual columns,
    only the first 64 columns in a table can be indexed.

[33mcommit 12ada74af397ef78d2738e4278cbda56ab6dacfd[m
Author: Dyre Tjeldvoll <Dyre.Tjeldvoll@oracle.com>
Date:   Tue Jul 14 10:11:09 2015 +0200

    BUG#21021848: ASSERTION `M_STATUS == DA_ERROR' FAILED.
    
    Problem: When I_S query which needed to perform full table open
    encountered a corrupt table it tried to repair it. To do this I_S
    query tried to acquire X metadata lock on the table. When I_S query
    was used in the middle of transaction this some[1;31mtime[ms led to deadlocks
    if there was concurrent DDL. As result transaction was aborted and
    rolled back without appropriate error reported. For XA transaction in
    debug builds assert fired.
    
    Solution: Since it is undesirable to have an I_S query attempt repair
    (or discovery) the patch modifies the behavior of
    Open_table_context::recover_from_failed_open() to skip these
    operations if MYSQL_OPEN_FAIL_ON_MDL_CONFLICT set, and instead sets
    the error ER_WARN_I_S_SKIPPED_TABLE which will be converted to a
    warning. This will avoid deadlocks during I_S query execution.
    
    This returns the behavior of I_S queries to what they were before
    BUG#18075170 was fixed. The assert in conjunction with XA abort, which
    was present even before BUG#18075170, is now fixed.

[33mcommit 3e8202ff443909e93231d453a3f1560b5a5ce3cb[m
Author: Aditya A <aditya.a@oracle.com>
Date:   Tue Jul 14 17:31:10 2015 +0530

    Bug #21040050   PURGE THREAD MUST EXIT SOONER AT SERVER SHUTDOWN
    
    PROBLEM
    
    The purge thread takes too much long [1;31mtime[m during
    shutdown. This is because we call trx_purge()
    three [1;31mtime[ms during shutdown,each [1;31mtime[m it is
    called 300 undo log pages are being processed.
    so even though you set batch size as 300,
    900 undo pages are processed during shutdown,
    whereas in 5.5 it is only called once
    during shutdown.  This is the reason we are
    seeing a 10 minute delay in shutdown [1;31mtime[m.
    
    FIX
    ---
    Instead of calling a separate trx_purge()
    for truncating the history log, do it
    with the trx_purge() in work loop once every
    128 (TRX_SYS_N_RSEGS) [1;31mtime[ms. Also reduce the
    batch size to 20 for trx_purge() which is
    called after detecting shutdown.
    
    [#rb 9359 Approved by jimmy]

[33mcommit df7678c17e4cd955bc3dd2ee571bce2be990a1bb[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Fri Jul 10 11:04:49 2015 +0200

    Bug#21383284: ASSERTION IN SELECT_LEX::SETUP_CONDS
    
    JSON_SEARCH evaluates the escape expression at resolve [1;31mtime[m in
    Item_func_json_search::fix_fields(), but it doesn't check if the
    evaluation is successful, and it could end up returning false
    (indicating success) even though an error has been raised.
    
    SELECT_LEX::setup_conds() has an assert which checks that no error has
    been raised if fix_fields() returns false. This assert gets hit if a
    JSON_SEARCH call is in a WHERE clause and the evaluation of the escape
    expression fails.
    
    Errors could be raised either when fix_fields() calls val_str() on the
    item representing the escape expression, or when it calls fix_fields()
    on its synthetic Item_func_like object.
    
    Item_func_json_search::fix_fields() also allocates some objects
    without checking if the allocation was successful.
    
    This patch makes fix_fields() check for all of the above error
    conditions and return true to indicate that an error has happened.

[33mcommit abf7b4e7be57a32b452bbc65138a75ce429990df[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Mon Jul 13 09:11:09 2015 +0530

    Bug #20728894: MEMORY LEAK IN ADD_DERIVED_KEY()
    
    Problem:
    Possible derived keys are created in stmt_arena's mem_root which
    is the permanent memory area for statement. It does not get deleted
    after execution of the query. In case of stored procedure, if a query
    creating derived table is executed many [1;31mtime[ms, the memory keeps growing.
    
    Solution:
    Allocated derived keys in thd->mem_root which is for run[1;31mtime[m objects instead
    of thd->stmt_arena->mem_root.

[33mcommit 087c2f04c7c94713cea6e5596ec1a946f23f551c[m
Merge: 5639195ee29 49667f04419
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Jul 10 20:58:36 2015 +0200

    Merge branch 'mysql-5.5' into mysql-5.6
    
    Conflicts:
            storage/perfschema/pfs_[1;31mtime[mr.cc

[33mcommit 49667f044197cefdb7c90b8beab3e78ec23deecd[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Jul 10 20:42:33 2015 +0200

    Bug#21374104 SETUP_TIMERS INITIALIZATION ASSUMES CYCLE TIMER IS ALWAYS AVAILABLE
    
    For WAIT events, fall back to other [1;31mtime[mrs if CYCLE is not available.

[33mcommit 5390fd98dfb2cb5da48dd0c8adf727385953a1cc[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Jun 29 01:18:59 2015 +0200

    Bug#21337139 ATRT SILENTLY STOPS TESTING AT FIRST SYNTAX ERROR IN TEST CASE FILE
    
    Added syntax check option --check-testcase-files to atrt.
    
    Added help description for testcase file syntax to atrt.
    
    If atrt aborts testing due to for example syntax errors in testcase
    file a start error on fictive testcase 'critical error' will be
    reported.
    
    This critical error will be visible in autotest webgui.
    
    Build [1;31mtime[m checks of testcase files are added.
    
    Unit test added to check testcase files.

[33mcommit 0fd8555617e8e8d309acf41808610e5d5cb97ef1[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 10 10:08:14 2015 +0200

    Bug#21271736 UNITTEST UT0CRC32-T HANGS ON SOLARIS 10 SPARC
    
    Reduce execution [1;31mtime[m for unit test.
    Change the constant n_megabytes_to_checksum and build optimized when doing
    performance analysis.

[33mcommit 9901e6d07803b0f51d3fc38151324572010f8466[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Thu Jul 9 16:40:11 2015 +0300

    Bug #21034322: RENAME LOCKING UDFS IN THE VERSION_TOKEN PLUGIN
    
    Renamed the locking UDFs as follows:
    vtoken_get_read_locks   -> version_tokens_lock_shared
    vtoken_get_write_locks  -> version_tokens_lock_exclusive
    vtoken_release_locks    -> version_tokens_unlock
    
    Fixed a conversion warning and added range checks for the [1;31mtime[mout
    argument to version_tokens_lock_*.
    
    Tests added to cover the range check.
    
    Bumped plugin version to 1.01.

[33mcommit 5967ab5979639ea8f3980cb91fb31d9c591fcbce[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Jul 8 16:33:54 2015 +0200

    Bug#21277074: crash (segfault) in THD::change_item_tree on exec of prep
    
    The problem here is when resolving a field during execution of a
    prepared statement, when the field is an aggregation argument,
    an outer reference, and comes from a derived table or a view, and
    the aggregation function is aggregated on the outer level.
    
    When this type of field is resolved for the second [1;31mtime[m, some code
    that updates max_arg_level of the aggregation function is triggered.
    In this case, max_arg_level was set to the level according to the
    current select_lex, but should be set to the level which the table
    the field belongs to is resolved from.
    
    For evidence, check the other assignments to max_arg_level inside
    Item_field::fix_fields() and Item_field::fix_outer_field() and
    Item_ref:fix_fields().

[33mcommit ee75e5588f86db9f888b98715f1d59c40a5819e8[m
Author: Mikael Ronström <mikael.ronstrom@oracle.com>
Date:   Sat Jul 4 01:45:45 2015 +0200

    Pushing BUG#21297407 revealed an uninited variable in Fragrecord in DBLQH (lcp_frag_ord_state, was set to LCP_QUEUED == 0 in most cases which led to crash if drop table happened before LCP had [1;31mtime[m to execute

[33mcommit 52db41e94ec7fdd1a01344285b9701c20715a92a[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Tue Jun 23 19:30:19 2015 +0300

    Bug#21095969 RPL+LOCK_WAIT_TIMEOUT: BOOL TRANS_CHECK_STATE ASSERTS `THD->GET_TRANSACTION().
    
    The reported assert in the slave temporary failing transaction block happens
    *every* [1;31mtime[m when the replicated transaction faces a temporary error and
    the slave's recovery tables (aka info repositories) are of the transactional
    type. Indeed, such replicated deadlocked or [1;31mtime[md-out transaction is to
    be rolled back and re-tried whenever @@global.slave_trans_retries > 0.
    
    Before it is rolled back, the applier calls global_init_info where
    thanks to BUG16533802 fixes, global_init_info starts a new (short-lived) transaction
    when relay_log_info_repository_type='TABLE'.
    And that leads to an assertion about improper transaction state
    because the temproary failing one it still active.
    
    Fixed with relocating a part of general cleanup of the slave applier
    (cleanup_context) to be executed before global_init_info().
    Running the former function prior the latter must be safe and actually
    makes much more sense.

[33mcommit 9b5f1b810715cf64448b87f4316006da380931b0[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Apr 22 20:03:55 2015 +0300

    Bug #20920851   ASSERTION `IS_STARTED()' IN HA_TRX_INFO::NEXT() AT TRANSACTION_INFO.H:138
    
    The assert is hit when XA transaction updated only a non-transactional table and
    went to prepare stage.
    At that [1;31mtime[m MYSQL_BIN_LOG::write_binlog_and_commit_engine() is invoked where
    the trx should not attempt any committing. But that's happened.
    The binlog "engine" managed to commit_low() in conditions of the reported case
    which led to the assert few instructions later
    
    #7  0x00007fad151dbe42 in __GI___assert_fail (assertion=0x1e37d16 "is_started()", file=0x1e37c68
    #8  0x0000000000fafcf7 in Ha_trx_info::next (this=0x7fac8c002028) at
    #9  0x0000000000f9dfa9 in ha_prepare (thd=0x7fac8c000bb0)
    
    More analysis proved there's an "inverted" related issue in the rollback branch.
    When the pure non-transactional engine xa transaction is rolled back, this [1;31mtime[m, it
    misses to execute rollback_low() method to leave a screwed state which
    the following query discovers hiting against an assert.
    
       bool trans_commit_stmt(THD*): Assertion
       `thd->in_active_multi_stmt_transaction() || thd->m_transaction_psi
       == __null' failed.
    
    And finally, testing revealed a case of no test coverage so far in combination
    of XA ROLLBACK, no transactional tables involved and no XA PREPARE.
    In such case logging was just incorrect mixing XA START and ROLLBACK.
    
    The original issue is fixed with making
    MYSQL_BIN_LOG::write_binlog_and_commit_engine() to compute a local boolean flag `skip-commit'
    correctly based on the value of the XA state.
    Commit is disallowed when the state is Prepared.
    To satisfy to the ONE-phase XA, the committing XA is also made
    to receive XA_PREPARED status, as intermediate, right after the prepare phase is done.
    in a general commit handler of ha_commit_trans().
    
    The second issue of the rollback part is fixed with relocating an existing explicit
    rollback_low() for xa-rollback to a safer point.
    
    And the final third issue is fixed with augmentment of
    ending_trans()'s the trans_cannot_safely_rollback(thd) branch to
    compose an appropriate Query-log-event.
    Logics of preventing second [1;31mtime[m do_binlog_xa_commit_rollback()
    invocation that is actual to the "externally" committing XA is
    simplified.
    The former idea was in that the first invocation of
    do_binlog_xa_commit_rollback() in the "external" XA commit branch
    would necessarily turn the cache from empty, asserted, to not empty.
    On the other hand, at running do_binlog_xa_commit_rollback() for the
    local xa the cache must be empty (because it should've been flushed at
    prepare), asserted in the rollback case too.
    Hence the state of the cache checking was correct: (the local xa go
    through, the external xa goes through once which is first [1;31mtime[m).  Now
    instead of the above deduction the 1st invocation is just gets
    explicitly flagged. And because we would like to preserve signature of
    MYSQL_BIN_LOG class methods the flag is made to pass as a new member
    of `bool binlog_cache_mngr::has_logged_xid'
    
    Mixing transactional and non-transactional tables in
    rpl.rpl_xa_survive_disconnect_mixed_engines reveal one issue in MTS
    grouping. An XA transaction "prepare" group can be closed
    with XA-ROLLBACK query which was previously missed to capture.
    A use case for that is mixed transactional and non-transactional updates.
    It's been corrected now.
    
    As a side effect is_loggable_xa_prepare() had to be refined to satisfy
    @c simulate_commit_failure.

[33mcommit b5bbc1b38b0319709b3dc5d196e11d708ad6463d[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed May 27 09:03:04 2015 +0200

    BUG#21127308: REPLICATION THREAD STATUSES NOT SHOWN IN PERFORMANCE_SCHEMA
    
    Some thread statuses related to replication were not included in the
    all_server_stages[] array.  That resulted in
    performance_schema.threads showing NULL in the PROCESSLIST_STATUS
    column (but SHOW PROCESSLIST showed the correct status).  Fixed by
    including these thread statuses in the array.
    
    Additionally:
    - Changed "Waiting for slave workers to finish." to "Waiting for slave
      workers to process their queues".
    - Changed "Waiting for GTID to be written to the binary log" to
      "Waiting for GTID to be committed"
    - Changed "Waiting for its turn to commit." to "Waiting for preceding
      transaction to commit"
    - Changed "Waiting for dependent transaction to commit." to "Waiting
      for dependent transaction to commit"
    
    Tests:
    - mysql-test/suite/rpl/t/rpl_mts_logical_clock_crash.test failed
      becuase it was waiting for
      stage_slave_waiting_for_workers_to_finish.  Fixed the test.  Also
      noticed that despite there was a [1;31mtime[mout in wait_condition.inc, it
      just printed a message and continued running the test; it did not
      source show_rpl_debug_info.inc and did not execute 'die'.  Added
      call to show_rpl_debug_info.inc if $show_rpl_debug_info is set (also
      in some other include/wait_*.inc files), and made rpl_init.inc set
      $show_rpl_debug_info.

[33mcommit b9b03fad976289265def3e5fc574a7cf2a2556f6[m
Author: Haixiang Li <haixiang.li@oracle.com>
Date:   Thu Jul 2 10:54:00 2015 +0200

    Bug#21139522 PREPARED STATEMENT EXPLAIN DELETE .. WITH STRICT MODE VIOLATION
                 FLATLINES
    
    Description:
    ------------
    When MySQL calls 'EXECUTE s' at the first [1;31mtime[m to run a prepare statement with
    'EXPLAIN', it did not release 'column_buffer' object, if running the same
    statement at the second [1;31mtime[m, a dead loop appeared, it seems MySQL was hanged.
    
    Fix:
    ----
    Release 'column_buffer' object by a helper class.
    
    Test case added.

[33mcommit 7a5a48db61a16e1aa3aba373afb52238e46b8878[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Thu Jul 2 11:31:41 2015 +0530

    BUG#21283343 FAIL WITH INNODB_UNDO_TABLESPACES SET TO 2 AFTER WL#7943
    
    Problem :
    ---------
    1. Segmentation fault in mysqldump while trying to dump all dbs. This is
    a direct impact of WL#7943 which adds UNDO tablespaces in
    INFORMATION_SCHEMA.FILES with LOGFILE_GROUP_NAME column as NULL. In
    dump_tablespaces(), mysqldump tries to generate "CREATE/ALTER LOGFILE
    GROUP ADD UNDOFILE ..." for rows having non NULL FILE_NAME and
    FILE_TYPE = 'UNDO LOG'. The code has implicit expectation that fetched
    LOGFILE_GROUP_NAME should be non NULL and crashes when the fetch returns
    NULL for LOGFILE_GROUP_NAME.
    
    2. FILE_NAME in INFORMATION_SCHEMA.FILES is ".//undo001" instead of
    "innodb_undo001" for innodb UNDO tablespaces. This is impact of
    [WL#7806 InnoDB: Log-based discovery] where, if redo for undo tablespace
    is to be replayed, we open the undo tablespace first [1;31mtime[m during
    recovery. At this point the name is the UNDO file name (.//undo001)
    rather than the undo tablespace name. srv_undo_tablespaces_init(),
    which opens the undo tablespaces during normal startup, skips the UNDO
    tablespaces if already opened during recovery.
    
    Solution :
    ----------
    Issue-1. This issue is best resolved in mysqldump. In dump_tablespaces()
    add condition "LOGFILE_GROUP_NAME IS NOT NULL" to avoid fetching rows
    without LOGFILE_GROUP_NAME as we don't need to add any dump statements
    for such rows.
    
    Issue-2. One way to solve this is to correct the undo tablespace name
    in srv_undo_tablespaces_init, when it is already opened during recovery.
    Current fix includes ...
        a.Correct undo tablespace name if already opened during recovery
        b.Add a ORDER BY clause in i_s_files.inc to ensure correct order.
    
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    
    RB: 9476

[33mcommit 868d3a1ff096e00d86a501f3b485053ebd5ceef8[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Wed Jul 1 22:28:35 2015 +0200

    BUG#21348278: ENABLE SLAVE PARALLEL APPLIER RECOVERY GTID BASED
    
    Parallel applier recovery, which happens after the sequence:
    STOP SLAVE, START SLAVE; is based on master log name and position,
    on Group Replication we have several masters what makes impossible
    to recover parallel applier from that information.
    Since we always have GTID_MODE=ON on Group Replication, we can
    ignore the positions completely, seek the current relay log to the
    beginning and start from there. Already applied transactions will be
    skipped due to GTIDs auto skip feature and applier will resume from
    the last applied transaction.
    
    This patch fix the above issue and also introduce some more changes:
     1) Parallel applier recovery based on GTIDs instead of
        file+position.
        This change will only affect Group Replication applier.
        Files changed: rpl_slave.cc
    
     2) Sequential execution of transactions on logical clock parallel
        applier, by setting transaction logical [1;31mtime[mstamps to (0,0).
        This was already implemented on server, but since it was not a
        valid code path, there was a warning. Now we have a valid code
        path, so warning was removed.
        Files changed: log_event.cc, rpl_mts_submode.cc
    
     3) Adapt UNTIL_SQL_VIEW_ID to parallel applier.
        Parallel applier cannot be stopped on a ongoing transaction, so
        UNTIL_SQL_VIEW_ID condition, the stop condition for Group
        Replication distributed recovery, was adjusted to take that into
        consideration.
        Files changed: rpl_rli.h, rpl_rli.cc, rpl_slave.cc
    
     4) Disallow FLUSH RELAY LOGS FOR CHANNEL "group_replication_applier".
        To avoid that DBA splits transactions among relay logs, the
        command FLUSH RELAY LOGS was disabled for channel
        "group_replication_applier".
        Files changed: rpl_slave.cc, share/errmsg-utf8.txt
    
     5) Extend Trans_context_info structure with parallel applier
        options.
        Group Replication doesn't support database parallel applier, in
        order to prevent its use, it will check if it is in use and
        error out.
        Files changed: replication.h, rpl_group_replication.cc,
                       rpl_handler.cc
    
     6) Add channel_flush() function to rpl_channel_service_interface
        Provide a API function to flush relay logs from within a plugin,
        so that START GROUP_REPLICATION can rotate applier relay log.
        Files changed: rpl_channel_service_interface.h,
                       rpl_channel_service_interface.cc

[33mcommit 0c86267e059d0aec211e2e0da11158ceccf85660[m
Author: Sivert Sorumgard <sivert.sorumgaard@oracle.com>
Date:   Tue Jun 23 15:26:43 2015 +0200

    Bug #20581228: MYSQLD --HELP --VERBOSE TRIES TO LOCK FILES
    
    Various issues are relevant to consider when running mysqld with
    options --help --verbose:
    
    1. On a loaded system, mysqld --help --verbose will initialize InnoDB in the
    same way as for an ordinary start. It will try to lock the system files
    present, and keeps trying for a hardcoded number of [1;31mtime[ms, with a hardcoded
    sleep inbetween. It will eventually fail, and this will also make mysqld
    --help --verbose fail and report an error.
    
    2. On an existing but empty datadir, InnoDB will create the system files
    while initializing. These files will be left behind after mysqld --help
    --verbose is finished, and will make a subsequent mysqld --initialize fail
    due to the datadir not being empty.
    
    3. On a non-existing datadir, the plugin table will not be found, and mysqld
    --help --verbose will fail too.
    
    InnoDB is initialized in order to enable reading the plugin table, which
    contains the dynamic plugins. However, this is relevant only in issue 1
    above. For issue 2 and 3, there is no point in initializing InnoDB since only
    the builtin plugins will be listed anyway.
    
    This patch applies the same constraint also in the first situation above.
    Thus, we let mysqld --help --verbose only list options belonging to the
    builtin plugins, and skip the dynamic ones. This allows us to execute
    mysqld --help --verbose without initializing InnoDB and without opening the
    plugin table, and this solves all three issues listed above.

[33mcommit 94ea2b6dc0a4d7dec80f35c46c47c1d4cc67a0bb[m
Author: Maria Couceiro <maria.couceiro@oracle.com>
Date:   Wed Jul 1 15:33:15 2015 +0100

    Post-push fix: Bug#20865683 IF AUTOCOMMIT=OFF AND GTID_NEXT='UUID:NO', GTID_MODE SWITCHING SHOULD BE BLOCKED
    
    sys_vars.gtid_executed_basic test was failing because master was not being reset in the begining.
    Changed check on update for variable explicit_defaults_for_[1;31mtime[mstamp so that gtid_next is not considered.

[33mcommit af205c05d8a75ed64bf99a7d6bd055923647d401[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Thu Jun 11 20:58:02 2015 +0300

    Bug #20866059   CAN'T ENABLE EXPLICIT_DEFAULTS_FOR_TIMESTAMPS W ROLLING UPGRADES IN CHAINED RPL
    
    The bug is essentially in that the binlog event applier (slave, or
    mysqlbinlog client) miss out a necessary part of Query event execution
    context. Specifically the part is
    
    MASTER@@session.explicit_defaults_for_[1;31mtime[mstamp
    
    value in the context of CREATE or ALTER table queries whose attribute
    list includes at least one TIMESTAMP column.
    
    ***Here goes some analysis***
    
    More specifically the TIMESTAMP attribute declaration that does not
    have explicit NULL or NOT-NULL is subject to dual interpretation at
    the query execution, which is controlable by the @@session var.
    Which of the two methods (per the two var's values) was chosen by
    the Master the binlog applier might have no idea.
    
    The applier can learn about this type of execution context when events
    are received (relay-logged) immediately from pre-WL6292 server thanks
    to WL6292 introduced server version adaptation framework VAF (and
    Bug19630322 fixing an MTS issue in it).
    
    But once such events are relayed further downstream chain that info gets lost.
    
    This patch addresses the following points of the issue.
    
     Legends:
    
     M - stands for Master server, S - for the applier (slave) server.
     The LaTeX style subscipt `_x' stands for a version `x'.
     `wl6292' is a version value
     corresponding to that of where the WL was pushed to.
     `fixed' is the current being fixed server version.
     `pre#' is for a version before that it prefixes.
     `[]' designates any in a range.
     `()' parameteries the server with a specific values of global var.
    
        Pre-WL6292 master events relayed downstream of chain replication:
    
    M_pre#wl6292 -> S_fixed -> S_fixed -> ...
    
    Data on S:s are consistent with the M.
    
        Cross "@@explicit_defaults_for_[1;31mtime[mstamp"-replication,
        from/to servers having different values of the var in the logger and in the applier.
        The chain replication is covered as well:
    
    M_fixed(the-var := true)  -> S_fixed(false) -> S_fixed(true) ...
    
    Notice that a problem of replicating from the WL6292 and later pre-fixed M
    to fixed S is unsolvable in principle:
    
    M_[wl6292..pre#fixed] --X-> S_fixed
    
    because the var's value can't be learned by S_fixed even through VAF
    (in contrast, on the pre#wl6292 servers the "var"'s value is false,
    "" - as there's no such variable).
    The patch treats this case with an assumption that M's value, awareness
    of which is detected by VAF, is the same as the S' one.
    
    In more details the first link of p.1 M_[0..pre#wl6292] -> S_fixed remains to be fixed by VAF
    (see MTS fixes in bug19630322).
    
    S_fixed -> S_fixed is fulfilled in the current bug's patch with
    enriching the execution context that is recorded along with the Query
    log event. The so called status var:s set is augmented with a specific
    token when the master's parser faces a [1;31mtime[mstamp column
    declaration. The token is essentially a boolean value
    corresponding to @@session.explicit_defaults_for_[1;31mtime[mstamp.
    
    Here is an example how the "old" intepretation settles down
    [1;31mtime[mstamp column definition:
    
    --server master
      start mysqld args: \empty
    
    --connection master
    select @@version;
    =>  5.7.8-rc-debug-log
    SELECT @@global.explicit_defaults_for_[1;31mtime[mstamp;
    => 0
    
    CREATE TABLE tc (ts TIMESTAMP);
    => OK
    
    show create table tc;
    => CREATE TABLE tc (
    ts [1;31mtime[mstamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=latin1
    
    To recollect the docs, by rules of pre#WL6292 server which are in force in above
    as @@global diagnoses, when the explicit NULL is not provided the
    "implicit" NOT-NULL is added up to the actual column definition.
    Moreover, the absence of the explicit NULL is the rules' necessary condition.
    
    ***Solution***
    
    A solution that bases on exhausive analysis of parsed query details (presense of the explicit NULL)
    would be perfect.
    However that is rather tedious and the patch does not attempt precise investigation for that.
    This could be always/possibly refined later.
    A more coarse approach is taken to mark potentially vulnerable queries.
    
    Dwelling into the low level details, a new three-enum-value member added to THD.
    Two values correspond to of the two values of @@session.explicit_defaults_for_[1;31mtime[mstamp.
    When the parser processes TIMESTAMP column declaration it memorizes the actual value of
    the session var.
    Whether the member's value is changed from the default UNSET is checked out
    by the binary logger which further encodes it into a new Query-log-event status
    variable of the bool sematic.
    The meaning of the latter is to instruct the event applier about
    the actual value of @@session.explicit_defaults_for_[1;31mtime[mstamp for executing
    the Query-log-event.
    
    Notice that @@explicit_defaults_for_[1;31mtime[mstamp is turned into writable
    which is necessary because there's no other way to inform the "mysqlbinlog"
    applier about the correct context but through the variable.
    
    This type of approach is rather economic, extending only "suspicious"
    Query-log-events with two bytes. A sign of the original context is
    relayed along replication chain to lift out the reported issue in
    particular.
    
    The fixes enroll correction to main.mysqlbinlog test which dealt with
    binlog files indexed explicitly. That turned to be not future-changes safe,
    as the test limits max binlog size. And now when the new status var is added
    the index have shifted.

[33mcommit faa5b573b799b5d0b81017191e2d8d917b180c1d[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Wed Jun 24 12:21:45 2015 +0400

    Bug #21306646: RENAME MAX_STATEMENT_TIME% VARIABLES TO MAX_EXECUTION_TIME%
    
    Since the MAX_STATEMENT_TIME syntax is removed (replaced with
    MAX_EXECUTION_TIME), it is logical to rename related command line
    parameter and system variables:
    
    * --max-statement-[1;31mtime[m --> --max-execution-[1;31mtime[m
    * @@max_statement_[1;31mtime[m --> max_execution_[1;31mtime[m
    * @@max_statement_[1;31mtime[m_exceeded --> @@max_execution_[1;31mtime[m_exceeded
    * @@max_statement_[1;31mtime[mr_set --> @@max_execution_[1;31mtime[mr_set
    * @@max_statement_[1;31mtime[m_set_failed --> @@max_execution_[1;31mtime[m_set_failed

[33mcommit 3822c8ad77d95d236627db59c1446e6e302cc4e5[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu Jun 25 15:22:59 2015 +0200

    Remove obsolete ifdef
    
     - restarting node using angel on Windows has been supported
       for long [1;31mtime[m now. Remove TODO and obsolete ifdef.

[33mcommit ffa7c5be8e342d607698efb4a8aca58b88ad0ae4[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Fri Jun 26 14:59:56 2015 +0530

    BUG#19881933 :  TABLE CREATION WITH TIMESTAMP IS REJECTED
    
    DESCRIPTION:
    
    I. When server is started without --explicit-defaults-for-[1;31mtime[mstamp
    
     CREATE TABLE statement with more than one TIMESTAMP column fails
     with error 'Invalid default value' in STRICT mode.
     Reason for failure is related to TIMESTAMP behaviour. When server is
     started WITHOUT server option --explicit_defaults_for_[1;31mtime[mstamp,
     first [1;31mtime[mstamp column automatically gets DEFAULT CURRENT_TIMESTAMP
     ON UPDATE CURRENT_TIMESTAMP and rest [1;31mtime[mstamp columns get
     NOT NULL DEFAULT '0000-00-00 00:00:00'. '0000-00-00 00:00:00' value
     is rejected by strict mode.
    
    II. When server is started with --explicit-defaults-for-[1;31mtime[mstamp
    
     CREATE TABLE statement with 'column_name TIMESTAMP NOT NULL' fails.
     Reason is an extra check which was not handled for
     explicit_defaults_for_[1;31mtime[mstamp option.
    
    FIX:
    
    I. When server is started without --explicit-defaults-for-[1;31mtime[mstamp
    
     this isn't fixed here because --explicit-defaults-for-[1;31mtime[mstamp will
     be forced to ON in 5.8
    
    II. When server is started with --explicit-defaults-for-[1;31mtime[mstamp
    
     If the option is on, the column does not get an implicit 0 default.
     Changed check to not throw error in this case.

[33mcommit 65094544381a2a95db1ff13266bfa557d15caec3[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Fri Jun 26 14:25:34 2015 +0530

    - Bug#19873470: SPEND TOO LONG TIME TO TRUNCATE A SMALL TABLE IN 5.7.5
    
      As part of making truncate atomic, checkpoint was added before
      truncate was mark done. This checkpoint helped in making current
      state consistent in-turn helped in skipping application of redo-entries
      from action before truncate (note these redo entries may have reference
      to a page that may not exist post truncate).
    
      Checkpoint takes significant [1;31mtime[m if parallel RW-workload is active
      eventually delaying truncate.
    
      In new approach, a special redo log record is written at start of truncate
      to cache space_id and current-lsn. In case of crash, post truncate
      before system enforced checkpoint this redo log record helps in blocking
      application of redo-entry from action before truncate but allows application
      of redo-entry from action post truncate.
    
      Reviewed by: Jimmy Yang (jimmy.yang@oracle.com)
      RB: 9305

[33mcommit c7d7daa80f5ba1c7c880a6a5d7bc4444163c61ea[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jun 25 11:01:54 2015 +0200

    Bug#21255496 REWRITER PLUGIN BREAKS ONE DEFINITION RULE
    
    Do not link with mysys, it breaks the one-definition-rule
    for everything in my_static.c
    The library is already part of the server,
    so it will be available at load-[1;31mtime[m.

[33mcommit c8a70adf2ccf2973c5b1df77bfb2d6c7f4044958[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jun 24 16:12:05 2015 +0200

    Increase [1;31mtime[mout for ATRT test

[33mcommit e5ce7e46974cfac736ab8cf8341b659cf815f52d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jun 24 16:12:05 2015 +0200

    Increase [1;31mtime[mout for ATRT test

[33mcommit b36a825ede793ad5943cf05c09fc018cc78ab428[m
Author: Mikael Ronström <mikael.ronstrom@oracle.com>
Date:   Wed Jun 24 15:49:20 2015 +0200

    BUG#20904721, WL#8525: Fix of part9, used internal TUP pointer instead of LQH pointer when calling LQH function directly, leads to both wrong handling and some[1;31mtime[ms even a crash when index is not a used scan pointer

[33mcommit 163ff6a8bcfa5eda97cbd8fb77baeb1172adc0a4[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Wed Jun 24 14:34:36 2015 +0800

    Change the default value of innodb_sync_debug from TRUE to FALSE,
    to reduce the execute [1;31mtime[m of MTR when default.

[33mcommit adad957bc270c58dd7000d92e858b3319aff32d8[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Wed Jun 24 10:13:17 2015 +0800

    Bug#20954452  GTID IS NOT RELEASED PROPERLY WHEN PS_PROTOCOL + GTID + BINLOG OFF COMBINATION
    
    When we start server with ps protocol and disabled binlog, executing
    DDL statement did not get a chance to call update_gtids_impl(...) to
    release its specified gtid, due to it miss to call ha_commit_trans(...)
    in trans_commit_stmt(...), since it is not active at that [1;31mtime[m; DDLs
    were using COM_STMT_PREPARE/COM_STMT_EXECUTE, so it also missed to
    call mysql_parse(...) to release its specified gtid.
    
    We fix the problem by moving the call to gtid_end_transaction(...)
    away from mysql_parse(....), into mysql_execute_command(...), since
    these DDLs with ps protocol are calling mysql_execute_command(...)
    by COM_STMT_EXECUTE and other normal transactions are calling
    mysql_execute_command(...) by mysql_parse(...).
    
    BUG#19774317  GTID_NEXT WITH EMPTY TRANSACTIONS DOES NOT WORK INSIDE STORED PROCEDURES
    
    Setting gtid_next inside a stored procedure and committing an empty
    transaction does not work. The root cause is that we failed to log
    the empty group to consume the GTID specified by setting gtid_next
    , due to the transaction owned gtid is cleared before calling
    gtid_end_transaction in mysql_parse(...).
    
    We also can fix the first problem by moving the call to
    gtid_end_transaction(...) away from mysql_parse(....), into
    mysql_execute_command(...), since committing the empty transaction
    with a specified GTID in stored procedure has a chance to call
    gtid_end_transaction(...) in mysql_execute_command(...) to log an
    empty group to consume the specified GTID.
    
    Bug#20343644  ASSERTION AT BINLOG.CC:1120 WHEN GTID_MODE=ON, GTID_NEXT='UUID:NUMBER'
    Bug#20444828  WL7592: ASSERT `THD->VARIABLES.GTID_NEXT.TYPE == GTID_GROUP' ON CREATE..SELECT
    
    Problem1: When gtid_mode is on, Start a transaction with a specified
    gtid, then create a table to cause an error, on commit we are hitting
    an assertion failure "thd->variables.gtid_next.type != UNDEFINED_GROUP".
    Problem2: When gtid_mode is on, set gtid_next with a specified gtid,
    then execute the statement 'CREATE..SELECT' to cause an error, on
    commit we are hitting the above assertion failure. The root cause is
    that we set gtid_next to undefined in gtid_post_statement_checks(...),
    which is called unconditionally after any statement in
    THD::cleanup_after_query(...). This means that gtid_next will be set
    to undefined for implicitly committing statements, even if the
    statement failed with an error before the implicit commit happened.
    Then, the transaction is still open and gtid_next is undefined. This
    is an invalid state, which triggers the assertion at commit [1;31mtime[m.
    
    The gtid_post_statement_checks(...) was introduced in BUG#16223835,
    the reason the function is needed is the following case:
     1. SET GTID_NEXT='<GTID that is already included in gtid_executed>';
     2. BEGIN;
     3. ... dml dml dml ...
     4. COMMIT;
    Then, the commit on line 4 will not invoke update_gtids_impl because
    the GTID is already included in GTID_EXECUTED so the thread does not
    own the GTID. So then the call to set_undefined from
    gtid_post_statement_checks is really needed in this case. But this
    causes it to call set_undefined too often and causes the assertion
    reported in this bug.
    
    To fix the problem, we remove gtid_post_statement_checks, add a call
    to set_undefined() in Gtid_state::update_gtids_impl(...), and add a
    call to update_gtids_impl(...) in gtid_end_transaction(...), in case
    the transaction owned gtid is empty and the gtid_next type is
    GTID_GROUP.

[33mcommit 33f010ace32ac55c954b7303ff6ba1861bb76931[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Tue Jun 23 10:27:51 2015 +0530

    Bug#21108296 : --SSL-CIPHER OPTION CAUSES SSL INITIALIZATION FAILURE
    
    Description : 1. If --ssl-cipher is specified at the [1;31mtime[m of starting
                     Server, SSL certificates are not generated automatically.
                     Automatic generation is limited to OpenSSL linked
                     MySQL server.
    
                  2. If --ssl-cipher is specified at the [1;31mtime[m of starting
                     server, automatic detection does notwork.
    
    Solution : Modified checks performed before certificate generation/detection
               to make sure that --ssl-cipher is not a cause to skip either of
               these actions.

[33mcommit df2a57abf35f56c23977470d5929dc1e2d494517[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Tue Jun 2 12:25:51 2015 +0200

    BUG#16459136 MYSQLD ON SAME HOST MAYY GET SAME SERVER_UUID
    
    - Temporarily "salt" the global variables used for "uuid generator
      initialization" with the values that normally differ when starting
      more than one mysqld on the same host. This avoids that another mysqld
      started at the same [1;31mtime[m on the same host get the same "server_uuid".
    - Debug printouts showing pre and post values of salted variables
    - Remove workaround from mtr.pl

[33mcommit d59c69197d171c6f88fdce18debe3d79d7f21ac7[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Mon Jun 22 12:48:34 2015 +0530

    Bug#20617891: NDB : SUSPICIOUS HANDLING OF SIGNAL-WAIT TIMEOUT
    IN NDBAPI
    
    In ndbapi, a signal [1;31mtime[mout should be handled by setting the
    state to [1;31mtime[md-out. A [1;31mtime[md-out state can then be mapped to
    the error code 4008. However, the [1;31mtime[md-out state is always
    overwritten to a success state.
    
    Modified the signal-[1;31mtime[mout code so that the [1;31mtime[md-out state
    is not overwritten.

[33mcommit 82a8a407cfddf82e7acc230d7813b8c31a3b274c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 18 17:18:32 2015 +0200

    Bug#17312761 MYSQLD WAITS FOR DUMMY NODES (NODEGROUP=65536) TILL NDB-WAIT-CONNECTED PERIOD
    
    On startup mysqld waste [1;31mtime[m waiting on ndb data nodes that have
    not joined the cluster.
    
    Any ndbapi node including mysqld can only connect to a started
    cluster.
    
    Prior this patch mysqld (or a ndbapi application calling
    wait_until_ready/2) waited until all configured data nodes were
    started.
    
    The ndbapi actually only need one connection to a data node with
    DBTC to work, but this is not optimal since all data will be
    routed internally in kernel via that data node.
    
    Data nodes that not yet have joined the cluster will of course
    never send any data until it have joined so no need to wait for
    that node.
    
    If a new data node eventually joins the cluster the ndbapi will
    try to connect to the new node with an API-DB heartbeat interval.
    
    With this patch mysqld will wait only for data nodes that have
    joined the cluster.

[33mcommit 1fa78d30d9c00ffeae0e13f9e0507b25294f464e[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 18 17:18:30 2015 +0200

    Test case for Bug#17312761 MYSQLD WAITS FOR DUMMY NODES (NODEGROUP=65536) TILL NDB-WAIT-CONNECTED PERIOD
    
    On startup mysqld waste [1;31mtime[m waiting on ndb data nodes that have not joined the cluster.

[33mcommit 99a38a7aeaeeff4980c743545739f8c54b972e1f[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Jun 16 22:48:09 2015 +0200

    Part1 (of 2): Fix for Bug#18390321
    
      MT-SCHEDULER: POSSIBLE BUSY WAIT IF PERFORMSEND() FAILED TO SEND
    
    This fix is for the above issue when the worker threads are doing its own
    sends - There will be a part2 for fixing the same issue in the send threads.
    
    We fix two similar issues where the worker threads could more or less
    end up in a busy loop doing (only) send.
    
    1) Other threads may trylock the same send buffer as the worker thread
       has locked while in do_send. The 'force_send' flag will then be set
       by these threads in order to force this worker thread to do the send
       for them.
    
       As the 'force' flag was checked in a while loop on do_send, this worker
       thread could on up looping here for a long [1;31mtime[m without taking care of
       its own work.
    
       This fix removes this busy-loop and instead return to the main worker loop
       with information set up such that it know it has more pending send work to
       this node. Further sending will now be retried **after** it has checked
       for more own 'work' to do.
    
    2) If there are only pending send work remaining in the main work loop:
       (No 'work' / (signals) pending) The worker loop would immediately
       retry the pending send. This is a good idea if it is able to make progress.
       However, if send is not able to send more data (buffers full?), it is
       no point in wasting CPU in a send-retry loop.
    
       This patch now detects this condition, and allows the worker threads
       to yield the CPU for 1ms in this state.

[33mcommit 9b6f1af4a6622119fd949a72c84743062b28e748[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Jun 16 10:07:53 2015 +0200

    Bug#21097485: *insert_table_ref && (*insert_table_ref)->is_insertable
    
    This problem occurs because we do not identify a single table
    for insertion, which causes this assertion. The root cause is
    that the column being assigned to is not checked for updatability.
    Such check is therefore added when setting up the list of columns
    to assign values to. (A column in an insertable-into view may still
    be non-updatable).
    
    This is achieved by centralizing the test for column updatability,
    controlled by an extra argument to setup_fields(). (There are indeed
    many calls that need updates, and if it wasn't for LOAD and one call
    in UPDATE, we could have determined the property by looking at
    required privileges, instead of adding this argument).
    
    Bonus fix 1: A non-updatable column in INSERT does now cause the
    correct error message "Column 'x' is not updatable" instead of
    "The target table v of the INSERT is not insertable-into".
    
    Bonus fix 2: All validation for non-updatable columns (except for
    implicitly generated column lists and LOAD command) are now done in
    setup_fields().
    
    Bonus fix 3: Run[1;31mtime[m checks for non-updatable columns were eliminated
    from fill_record().
    
    Bonus fix 4: Consistent error messages for non-updatable columns used
    in LOAD command, and a test case. Error ER_LOAD_DATA_INVALID_COLUMN
    is marked as unused.

[33mcommit 81baf33034e23bd373b217f301fb15d32e5555ab[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 15 14:24:52 2015 +0200

    Increase [1;31mtime[mout value for several failing
    'testNodeRestart ... DD' tests.

[33mcommit 39189f1bb323aa0c73996d3fe3f803fda2aa6a0c[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 15 09:55:22 2015 +0200

    Increase [1;31mtime[mout for  'testNodeRestart -n Bug27003 T1'
    from 1800 -> 3600sec.
    
    Normal run [1;31mtime[m for the tests not failing seems to be
    from 20 - 28 min, so timing out after 30min on slow machines is
    simply too early.

[33mcommit 61e2172a1885172aec9814fa7ece5a3176547634[m
Author: Libing Song <libing.song@oracle.com>
Date:   Thu Apr 9 16:53:18 2015 +0800

    BUG#20136704 --SLAVE-PRESERVE-COMMIT-ORDER CAUSES SLAVE TO DEADLOCK AND
                 BREAK FOR SOME QUERIE
    
    A corner case caused slave hang when slave_preserve_commit_order is ON.
    
    ANALYSIS
    ========
    CREATE TABLE t1(c1 INT PRIMARY KEY, c2 INT, INDEX(c2)) ENGINE = InnoDB;
    INSERT INTO t1 VALUES(1, NULL),(2, 2), (3, NULL), (4, 4), (5, NULL), (6, 6);
    
    INSERT INTO t1 VALUES(7, NULL);
    DELETE FROM t1 WHERE c2 <= 3;
    
    On master, the INSERT statement acquires its row lock before DELETE STATEMENT.
    Since the INSERT doesn't block the DELETE, Both statements may have same
    commit parent which mean they can be applied parallel on slave.
    
    On slave, they will be applied parallel if MTS is ON. There is a chance that
    the DELETE acquires its lock before the INSERT. The DELETE holds a gap lock
    on INDEX(c2) which blocks the INSERT statement. The INSERT cannot be applied
    unless DELETE is committed or rolled back. And meanwhile the DELETE is waiting
    for the INSERT to commit since slave_preserve_commit_order is ON. That is
    a deadlock and hangs the slave. Here we call the deadlock as a order commit
    deadlock.
    
    FIX
    ===
    A deadlock checking mechanism is introduced. Every [1;31mtime[m when a transaction
    needs to wait for another transaction to release a row lock, innodb will
    call a slave function to check if there is an order commit deadlock. If
    it founds an order commit deadlock, It will set a deadlock flag to the
    slave worker which is holding the row lock. Thereafter, the worker will
    roll its transaction back and retry it again.

[33mcommit 2254618ab4889bf494d946cd1117958270ce1df2[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jun 12 10:12:49 2015 +0200

    Bug#19900900 ASSERTION `!CHECK_DATETIME_RANGE(LTIME)' FAILED TIME_TO_LONGLONG_DATETIME_PACKED
    
    Problem: the function add_[1;31mtime[m could produce results that were out-of-range
    (year >= 10000)
    
    Solution: add a range check.

[33mcommit 6bc1d13227a88c889f7c76ae5d304c226d29e6c0[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Thu Jun 11 16:32:33 2015 +0800

    BUG#20856397 MYSQLD --HELP --VERBOSE TAKES 50 SECONDS ON UNLOADED SYSTEM
    
    It takes most of the [1;31mtime[m to initialize innodb buffer pool(256G).
    So the solution is to use default buffer pool size(128M) when
    mysqld is run with --help --verbose options, if buffer pool size
    is greater than 128M. And we still show the correct value in help.
    
    Reviewed-by: Marko Mäkelä       <marko.makela@oracle.com>
    Reviewed-by: Robert Golebiowski <robert.golebiowski@oracle.com>
    RB: 9252

[33mcommit 9583a3bd86fc0471a7bfc5d24ab43dcfe8edecbb[m
Author: Mark Leith <mark.leith@oracle.com>
Date:   Fri Jun 5 09:50:06 2015 +0100

    Bug #20971120 MTR TESTS FAIL WHILE CREATING SYSTEM TABLES ON NON PS BUILD
    
    If compiling without performance schema, use a generated empty SQL file, that just contains an empty sys schema (like performance_schema does when not compiled in), to compile in to mysql_upgrade, mysql_install_db, and mysqld --initialize. Set the version as 1.0.0 for the sys schema, so that mysql_upgrade etc. will install the empty schema, but will not overwrite an existing schema.
    
    mysql_upgrade and mysql_install_db still have the --skip-sys-schema options, and can still be used to not create the empty schema if desired.
    
    Also add a --skip-sys-schema option to MTR to allow skipping use of the sys schema. This has to be done in this way, as we can't rely on checking the compile [1;31mtime[m option. It also leaves the possibility of not using the sys schema when performance schema is compiled in too, should people want to use it.
    
    Like mysql_install_db etc., this currently creates an empty sys schema when used and removes the sysschema suite from the set of default suites to run.
    
    Reviewed-by: Marc Alff <marc.alff@oracle.com>
    Reviewed-by: Bjorn Munch <bjorn.munch@oracle.com>
    RB: 9009

[33mcommit 38e3aa74d8d2bf882863d9586ad8c9e9ed2c4f00[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Fri May 29 13:32:41 2015 -0500

    wl#7943 - InnoDB: Implement Information_Schema.Files
    
    Implement support for information_schema.files in InnoDB so that it can
    be a replacement for i_s.innodb_sys_datafiles. Add similar SELECT
    statements for i_s.files wherever i_s.innodb_sys_datafiles is used in
    all test files.  Gather information from the fil_system in fil0fil.cc
    one file at a [1;31mtime[m so that the fil_system->mutex is not held very long.
    
    Approved by Marko and Vasil in rb#7396

[33mcommit 70197ead3dac65cceb9069832a3a9a26d06976f0[m
Merge: a3efc61cbff c8243dd3604
Author: Ajo Robert <ajo.robert@oracle.com>
Date:   Mon Jun 1 21:23:08 2015 +0530

    Bug #18591145: SOME MONOTONICALLY INCREASING STATUS
    VARIABLES DICREASES UNEXPECTEDLY
    
    Analysis
    --------
    "SHOW STATUS VARIABLES" shows incorrect status values when
    concurrent connection is executing statement to change user
    or is being disconnected.
    
    During switch user/session disconnect session status
    variables are merged to global status variables. After
    this, session variables are not reset for a while. During
    this [1;31mtime[m select of global status variable from another
    session will result in wrong data as all the session values
    get added to global to calculate final value. Effectively
    current session status variable value gets added twice.
    
    Fix:
    ---
    New flag introduced to represent whether THD::status_var
    addedd to global status. If status_var_aggregated is set,
    THD::status_var is skiped from aggregation.
    Code added to reset session variable after adding to global
    status variable for change user scenario.
    
    Test:
    ----
    mtr test added for switch user flow. Session disconnect
    is tested with debugger as DEBUG_SYNC suite will not
    work during thread disconnect.

[33mcommit 977a2fd94bebffc41761a13b64cc645584ac098c[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Tue May 26 13:57:26 2015 +0530

    BUG#20171740 - OVERRIDE DEFAULT SERVICE START/STOP
                   TIMEOUT USED BY SYSTEMD FOR MYSQL.
    
    Server startup and shutdown [1;31mtime[m is not constant
    and bounded and may take several minutes. This
    is because of InnoDB which does disk I/O operations
    that depend on InnoDB log file size, preload and dump
    of dump buffer pools, purge threads being active during
    shutdown etc. The default service start and stop [1;31mtime[mout
    specified by systemd has a small value and this triggers
    the restart logic of systemd causing another instance of
    the mysqld to be spawned.
    
    The fix is to disable the service start/stop [1;31mtime[mout logic of
    systemd by specifying TimeoutSec=0 in the service file for
    mysqld service. Also fix the comment in mysys_err.h.

[33mcommit 0fbb8a0306317a3144d6e14d5d20e3d1445dab10[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue May 26 18:03:16 2015 +0200

    Bug#17473077 EVER INCREASING MEMORY USAGE REPORTED FOR EVENT SCHEDULER
    
    Before this fix, statistics for the memory instrumentation
    were inaccurate, in the following scenario:
    
    At T1, thread A allocates some memory, of size N
    At T2, thread A gives ownership of the allocated memory to thread B
    At T3, thread B de allocates the block memory, of size N
    
    The event at T1 is instrumented with the performance schema,
    and an allocation of size N is counted against thread A.
    
    The event at T2 is not instrumented.
    The performance schema is unaware of the memory transfer.
    
    The event at T3 is instrumented with the performance schema,
    and a de allocation of size N is counted against thread B.
    
    The problem with this approach is with statistics maintained by thread.
    
    When the same code is executed many [1;31mtime[ms (in a loop with the event
    scheduler),
    - thread A appears to use an ever increasing amount of memory
    - thread B appears to consume an ever increasing amount of memory
    - the global statistics for the server diverge.
    
    To resolve this bug, two different fixes are implemented.
    
    Which fix to use depends on the nature of the instrumented memory.
    
    FIX NUMBER 1
    
    For memory instruments that are by nature measuring a global resource,
    there is no point in maintaining per thread (and per account, per user, per
    host) statistics.
    
    A typical example of such usage are shared, global structures,
    like the query cache.
    
    For these memory instruments, the performance schema now supports
    defining the instrument with PSI_GLOBAL_FLAG.
    
    For instruments defined as global, the performance schema
    only maintains the global memory statistics.
    
    In this case, nothing needs to be done for event T2,
    as instrumentation for the allocation (T1) and de allocation (T3)
    is enough to maintain everything.
    
    FIX NUMBER 2
    
    For memory instruments that are by nature measuring a local resource,
    maintaining statistics per thread is desirable.
    
    In order to do so, the performance schema needs to be told about event T2,
    which is now instrumented explicitly in the code.
    
    A new entry point in the instrumentation, PSI_MEMORY_CALL(memory_claim),
    is used to instrument the change of ownership of a block of memory.
    
    When a thread in the server allocates a data structure,
    starts a child thread, and gives ownership of the memory structure to the
    child, the code in the child thread now needs to claim ownership of the
    data.
    
    With this added instrumentation, performance schema statistics now
    reflect more closely how memory is actually used in the server.
    
    Various claim_memory_ownership() methods are implemented to support this.

[33mcommit 853b9f8a74b9ede81e6d95dc0c96a71f87de1bc0[m
Author: Terje Røsten <terje.rosten@oracle.com>
Date:   Wed May 27 15:09:30 2015 +0200

    Bug#21073014 SET LIMITNOFILE OPTION IN SYSTEMD SERVICE TO MATCH OPEN_FILES_LIMIT WANTED
    
    Default value of LimitNOFILE is often set to 1024 (by operating
    system), this is too low for common use cases.
    
    This change lift limit to 5000 by default, which match wanted value of
    open_files_limit by server.
    
    Note:
    
    When using native systemd support mysqld_safe script is not used.
    
    This have several side effects for various options and changes in
    behavior:
    
     o open-files-limit
    
    mysqld/mysqld_safe config open_files_limit set config files are
    ignored. To adjust value, systemd config files must be used. The
    corresponding systemd service option is called LimitNOFILE=
    
     o pid-file
    
    Similar issue exists for MySQL option pid-file. If changing default
    value in MySQL config files, make sure systemd service option PIDFile=
    matches. Failing to do this may cause service to fail during startup.
    
     o core-file-size
    
    MySQL config option core-file-size is replaced by systemd service
    option LimitCore=
    
     o nice
    
    MySQL config option core-file-size is replaced by systemd service
    option Nice=
    
     o [1;31mtime[mzone
    
    Using the [1;31mtime[mzone mysqld_safe option environmental variable TZ will
    be set. Use systemd Environment= option or /etc/sysconfig/mysql do the
    same: Environment="TZ=<[1;31mtime[mzone>"
    
     o malloc-lib
    
    With this option mysqld_safe can change library to use for memory
    allocation. To do this with MySQL native systemd support set
    environmental variable LD_PRELOAD to wanted library:
    Environment="LD_PRELOAD=/path/to/some/library or set LD_PRELOAD in
    /etc/sysconfig/mysql
    
     o syslog
    
    mysqld_safe have various syslog related options.  MySQL 5.7 and newer
    have native support for syslog, these options are called: log_syslog,
    log_syslog_facility, log_syslog_include_pid and log_syslog_tag. Use
    these mysqld option to control syslogging of MySQL server.
    
     o restart logic
    
    mysqld_safe tries to restart the server when an error occurs, this
    logic is replace by a similar native systemd feature.
    
    -- Adding or changing systemd options
    
    To add or change systemd options create directory called
    
     /etc/systemd/system/mysqld.service.d/
    
    and add a file with .conf suffix in this directory,
    for example named local.conf with contents:
    
    [Service]
      LimitNOFILE = <max-open_files>
      PIDFile = /path/to/pidfile
      LimitCore= <core-file-limit>
      Nice=<nice-level>
      Environment="LD_PRELOAD=/path/to/some/library"
    
    Then execute:
    
     systemctl daemon-reload
    
    and finally restart MySQL service to let changes take effect:
    
     systemctl restart mysqld
    
    If only changing TZ or memory allocator, add file /etc/sysconfig/mysql
    and set variables:
    
    TZ=<[1;31mtime[mzone>
    LD_PRELOAD=/path/to/some/library
    
    -- Passing options to mysqld without modifying config files:
    
    Contents of variable MYSQLD_OPTS is sent as option string to mysqld.
    
    Set variable with systemctl. For example like this:
    
    systemctl set-environment MYSQLD_OPTS="--general_log=1"
    
     Remember to restart server to let action take effect.
    
    Clear variable and restart server to get default settings:
    
     systemctl unset-environment MYSQLD_OPTS
     service mysqld restart

[33mcommit 302dc52191a93264864df4f44d31c6320dcc5977[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue May 26 10:41:13 2015 +0200

    Patch for bug#21109605
    
    EXCESSIVE NDB KERNEL THREAD IS STUCK IN: PERFORMING SEND; WATCHDOG TERMINATE
    
    It was possible to end up in a livelock on the send_buffer
    mutex if send buffers became a limiting resource. This could be due
    to too little send buffers configured, slow/failing
    communication network such that all send buffers are filled,
    slow receiveres which does not consume what is sent and likely other
    reasons.
    
    In this situation (all?) worker threads will fail to
    allocate send buffer memory for the signals, and
    will attempt a ::forceSend() to free up space.
    At the same [1;31mtime[m the send thread will be very busy
    trying to send to the same node(s). All these threads
    will compete for taking the send_buffer mutex, which
    result in a livelock on it. Send threads stalled due
    to hitting this livelock will be reported by the
    watchdog as 'Stuck in Send'
    
    As 'stuck' send threads also held the global send_thread mutex
    they could even block worker threads trying to grab the same
    mutex while alerting send threads as part of a do_send request.
    Thus, even the worker threads got 'Stuck in Send'
    
    This patch does two things:
    
    1) Code analysys revealed that the send thread does not
       need to hold the global send_thread mutex while grabbing
       the send_buffer mutex. By releasing this global mutex prior
       to locking the SB-mutex the *worker threads* will no
       more be 'Stuck in Send'.
    
    2) Changed the send treads locking of the send_buffer mutex
       to use a trylock. If the try-locking failed, the node to be
       sent to is re-inserted last into the list of send-nodes in
       order to be retried later. This removed the 'Stuck in Send'
       condition for the send threads as well.

[33mcommit 1b16c342db67f4bb8d4bae4910cd05af1e9b85d8[m
Author: Benny Wang <benny.wang@oracle.com>
Date:   Sat May 23 03:48:17 2015 +0200

    Fixed bug#20757211: GENERATED COLUMNS: ALTER TABLE CRASHES IN
    FIND_FIELD_IN_TABLE
    
    Block subselect in generated expression.
    
    In current wl411, the generated expression is just do a parse and doestn't
    open any table. If an subquery exists in generated expression, the tables used
    for the subquery can't be opened, which can result in such an error like bug
    reported. Before this fix, wl411 is trying to use has_subquery() to block
    subquery during fix_fields_gcol_func. However assertions might be triggered
    before this block because of the unopened table.
    
    In order to fix such a bug, we have to block the subquery before
    fix_fields_gcol_func. Moreover, we don't need has_subquery() any more after we
    block the subuqery at the early [1;31mtime[m.

[33mcommit f2300ce39b97c66dc022a4396e00b9b27ebce878[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Sun May 24 17:54:09 2015 +0100

    BUG#20980271: FIX GTID MODE TO WORK CORRECTLY IN --BOOTSTRAP/--INITIALIZE MODE
    
    Patch to enable the needed GTID infrastructure when in
    bootstrap mode.
    
    The server was not starting the necessary memory structures
    to setup GTIDs when on bootstrap mode. This patch makes the
    server generate the auto.cnf file and set up GTID sets and
    the sidmap when operating in bootstrap mode.
    
    It also changes MTR. It needs to because the bootstrap of the
    server is done once, and then the same install dir is used
    for initializing the master and slave. Thence, we need to
    remove the auto.cnf file (which holds the UUID generated
    UUID at bootstrap [1;31mtime[m), before copying the install dir for
    provisioning the new server that MTR spawns. This was not
    needed before, because mysqld --initialize would not
    setup the UUID for the server, i.e., the auto.cnf file.

[33mcommit e17a193c097fe520ede950127acdbc883bc29f75[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri May 22 14:59:11 2015 +0100

    Bug#21121042: CODE TESTING MYSQL_VERSION_ID IS BRITTLE
    
    Several places in the code we do compile-[1;31mtime[m checks of
    MYSQL_VERSION_ID without including mysql_version.h where
    MYSQL_VERSION_ID is defined.
    
    This patch makes such checks less brittle by:
    - Removing them where possible
    - Explicitly including mysql_version.h otherwise
    
    The patch also fixes a similar problem where some
    files related to MyISAM full text search use the
    HA_KEYTYPE_FLOAT symbol without including my_base.h
    where it is defined.
    
    These problems were identified by studying the output
    from the -Wundef Clang compiler warning option.

[33mcommit 3322c0c3ee7a4c72ed6bfacb0fda73085219d9d8[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed May 20 21:03:10 2015 +0200

    Bug#20665051 SQL_SHOW.CC:7764: ASSERTION `QEP_TAB->CONDITION() == QEP_TAB->CONDITION_OPTIM()
    
    The scalar subquery is uncorrelated, so it's evaluated during top
    JOIN's optimize(). This evaluation (JOIN::exec) calls JOIN::prepare_result()
    which materializes the I_S content into a tmp table then sorts this table.
    Then EXPLAIN explains the subquery: it calls JOIN::prepare_result() a
    second [1;31mtime[m, causing again materialization and sorting.
    This is inefficient, and causes an assertion failure because the
    I_S materialization expects to run before sorting (so it crashes because
    of the previous sorting).
    
    Fix: don't prepare result if already done.

[33mcommit 580179a31d2feb7eb3ee8dfaf0e3cfec49bfe9ea[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Thu May 21 14:22:39 2015 +0530

    BUG#21075892 SHUTDOWN HANG, WAITING FOR ASYNC ROLLBACK TO FINISH
    
    Problem :
    ---------
    In case of lock conflict, a high priority transaction(certified)
    attempts to rollback other transactions(optimistic). We have two
    different paths here and two distinct symptoms causing shutdown/rollback
    hang.
    
    1. The ASYNC rollback happens while handling DB_LOCK_WAIT error and
    for semi-consistent read we change the wait state to success and return
    the last committed version to SQL. If sql condition skips the row then
    we never rollback the transaction.
    
    2.If the blocking transaction is wating for some other lock, we mark the
    transaction for abort/ASYNC rollback and deadlock victim and wake it up.
    We are overwriting the DB_DEADLOCK state in following two cases
        A. When the transaction is interrupted by shutdown [DB_INTERRUPTED]
        B. When the lock wait [1;31mtime[mout is over [DB_LOCK_WAIT_TIMEOUT]
    In both cases we exit innodb without doing the rollback.
    
    3. Another independent issue was observed during debugging where the
    server crashed while printing some info for the victim thread after
    doing ASYNC rollback.
    
    Solution :
    ----------
    1.Check and kill blocking transaction for high priority transaction
    in success path.
    
    2.After wake up from a wait state check if the transaction is already
    marked as deadlock victim and if so don't overwrite the state.
    
    3.Get the victim transaction information before doing ASYNC rollback
    trx_kill_blocking.
    
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    
    RB: 8994

[33mcommit d1f2732b3da3e652227830742e845b16d2a0176b[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed May 20 11:47:20 2015 +0200

    Bug#20956599 CAN'T SET PERFORMANCE_SCHEMA=OFF IN 5.7
    
    Before this fix,
    
    For a server compiled with performance schema support,
    when starting the server with the performance schema
    disabled at startup [1;31mtime[m, as in:
      performance_schema = OFF
    
    the server would fail to create performance_schema tables,
    which in turn causes the creation of views provided by sysschema to fail.
    
    The root cause is as follows:
    - during the server startup, the code detects that
      the performance schema is not initialized.
    - the variable load_perfschema_engine stays as false as a result
    - later, when loading the performance schema plugin,
      the plugin is loaded with the option PLUGIN_OFF (aka, not loaded)
    - during the database initialization in mysql_system_tables.sql,
      the scripts looks for entries in table information_schema.engines,
      and finds none, leading the script to assume the server is not compiled
      with performance schema support.
    - performance schema tables are not created during the install.
    - installation of the sysschema views fail.
    
    The fix is to simplify and streamline the entire process,
    which is too convoluted.
    
    In particular,
    
    - a server compiled with performance schema support always
      load the performance schema engine, now mandatory,
      regardless of the run[1;31mtime[m configuration.
    
    - a server compiled with performance schema support always
      create the performance schema tables during install.
    
    The database layout of the installed product should depend only on build options
    (to compile with of without the performance schema),
    and never depend on run[1;31mtime[m configurations options
    (to start the server with or without instrumentation)
    
    When the performance schema is not used during server startup,
    no data will be collected and no memory allocated,
    so there is no performance impact: there are no changes here.
    
    The only change is to always create performance schema tables
    when the server is compiled with performance schema support.

[33mcommit bedc01828a2d21afe08d0f85eb8da64fe5da27ca[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Apr 24 09:12:24 2015 +0200

    Bug#20947871 INDEX SCAN COST IN TEST_IF_CHEAPER_ORDERING() DOES NOT USE COST CONSTANTS
    
    The cost calculation for the cost estimate of doing index scan in
    test_if_cheaper_ordering() does not use the configurable cost constant
    for the io cost of accessing a block from the table. Instead it assumes
    that all accesses to a block cost 1:
    
      const double index_scan_[1;31mtime[m= select_limit / rec_per_key *
         min<double>(rec_per_key, table_scan_[1;31mtime[m.total_cost());
    
    The the last rec_per_key in this formula corresponds to the number of
    page reads that is needed to read from the base table.
    
    This can cause unintended changes to query plans if the cost constant
    for io block read is changed. The fix for this issue is to use the
    configurable cost constant.

[33mcommit 632d24d46b5c6b5b9bc8d8b380cf135c3658c661[m
Merge: 64e4767af09 4a20b5d8c10
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue May 19 17:27:45 2015 +0100

    Merge commit mysql-5.6-cluster-7.4 into mysql-5.7-cluster-7.5
    
    Null merge of MCP_BUG20701918 into 7.5.
    
    Current intention is that users move to using new types in the
    5.6 (7.3 + 7.4) [1;31mtime[mframe.
    
    Conflicts:
            mysql-test/r/mysqld--help-notwin.result
            sql/mysqld.cc
            sql/mysqld.h
            sql/sql_yacc.yy
            sql/sys_vars.cc

[33mcommit 0214d2c072fe7aff60c26c814556003afed8444e[m
Author: Benny Wang <benny.wang@oracle.com>
Date:   Mon May 18 05:48:37 2015 +0200

    Fixed bug#20746926: GENERATED COLUMNS: INVALID READ OF THD WHEN WARNINGS
    
    This bug is caused by cached THD pointer of some Item objects.
    
    There are several kinds of Item objects which cache the THD pointer to try to
    lessen the impact of current_thd. However, generated column expression is only
    parsed and fixed once during open table first [1;31mtime[m. Moreover, the table is
    cached and shared by all sessions. If the Items which cache THD pointer as
    part of generated expression, obviously, such an generated expression can't be
    shared by all sessions. If they were, it would result in invalid read and
    memory leak.
    
    Because there are only 3 Item objects which have effect on generated
    expression. The solution is to remove to cache THD but use current_thd
    instead.

[33mcommit f2fc340ba04dd1234035ebfcdd1fc2026a98ac32[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Tue May 19 12:48:10 2015 +0600

    This patch fixes the bug#21041908 - EVENTS+RPL: VOID CLOSE_THREAD_TABLES(THD*): ASSERTION THD->GET_TRANSACTION()
    
    The issue is related to the automatic dropping of the event (since the end of
    event's life[1;31mtime[m) and the use of row based binlogging.
    
    After a row for event being dropped has been deleted from the table
    mysql.event the function binlog_log_row is called to record a new entry
    into the binlog. In case row based binlogging is on the following
    sequence of calls are happen to start a new statement-level transaction:
    binlog_log_row --> write_locked_table_maps -->
      THD::binlog_write_table_map --> binlog_start_trans_and_stmt
    
    Latter when open tables are closed the following assert
      DBUG_ASSERT(thd->get_transaction()->is_empty(Transaction_ctx::STMT) ||
                  thd->in_sub_stmt ||
                  (thd->state_flags & Open_tables_state::BACKUPS_AVAIL));
      is fired since there is a statement-level transaction.
    
    To fix the bug simple refactoring has been done to move turning off
    row binlogging inside Event_db_repository::drop_event() that is
    called both when DROP EVENT and automatic dropping are executed.

[33mcommit 72b75708175097c226436830217b3b870f6104e0[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Thu May 14 13:13:42 2015 +0400

    Bug #20720615: GEN_LEX_HASH FAILS WITH MEM.LEAK ERRORS IF COMPILED WITH CLANG-3.5 + ASAN
    
    If we use clang-3.5+ instead of gcc and enable its AddessSanitizer (ASAN),
    the gen_lex_hash utility aborts on clang's LeakSanitizer memory leak
    checks.
    
    This patch adds memory deallocation to gen_lex_hash.cc to make
    LeakSanitizer happy (normally this is superfluous, since gen_lex_has is
    a compile-[1;31mtime[m utility, and OS does the job itself).

[33mcommit a5a22244ac8237c82f85f7f96abe6b2eabc3be98[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Apr 22 15:10:23 2015 +0300

    Fix Bug#20783098 INNODB_CHECKSUM_ALGORITHM=CRC32 IS NOT BYTE ORDER AGNOSTIC
    
    The CRC32 checksum generation code interprets portions of the byte
    string to checksum as a 8-byte integer so that it can process 8 bytes
    at a [1;31mtime[m (rather than 1 byte at a [1;31mtime[m). For this, the code uses the
    native byte order of the machine:
    
      crc ^= *(ib_uint64_t*) buf;
    
    and then does numerical calculations with the result (e.g. crc >> N).
    Thus the resulting checksum depends on the byte order of the machine
    and is different on big and little endian machines. This means that
    files written to with --innodb-checksum-algorithm=crc32/strict_crc32 on
    big (little) endian machines are not readable on little (big) endian
    machines because the checksum, though valid, is not recognized.
    
    The simplest solution would be to start writing only e.g. big endian
    checksums and recognize only such ones, but this would introduce an
    unacceptable backwards incompatibility.
    
    The solution implemented is to recognize both big and little endian
    CRC32 checksums during verification, while first calculating and
    checking the little endian one.
    
    Swapping the byteorder in order to calculate "the other" CRC32 checksum
    slows down the checksum calculation by about 1-2% (e.g. recognize
    big-endian-CRC32 on little endian machines or recognize
    little-endian-CRC32 on big endian machines).
    
    When generating the checksum (when writing to disk) we now always use
    little endian byteorder (no change in little endian machines, and an
    extra step of swapping the byteorder on big-endian machines).
    
    Reviewed-by: Debarun Banerjee <debarun.banerjee@oracle.com>
    RB: 8781

[33mcommit df98fc7bf145835feae66f718415c2f962b9f824[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Tue Apr 21 14:55:05 2015 +0200

    BUG#19706455: RESET MASTER SHOULD RESET GTID STATE AND NOT ERROR OUT WHEN BINLOG IS OFF
    
    The GTID state (GTID_EXECUTED and GTID_PURGED) can be reset using
    RESET MASTER.
    
    RESET MASTER only works when the binary log is enabled. If the binary
    log is disabled, RESET MASTER fails with an error. This did not limit
    the GTID feature in 5.6, since GTIDs could only be enabled when the
    binary log was enabled.
    
    However, since WL#6559 was pushed to 5.7.5, GTIDs can be enabled even
    when the binary log is disabled. So in this case there is no way to
    reset the GTID state.
    
    Fix: Make RESET MASTER reset gtid_executed and gtid_purged even when
    binary logging is disabled.
    
    @sql/rpl_master.cc
    - Fix the server bug.
    
    @mysql-test/suite/binlog/t/binlog_anonymous_ownership.test
    - Fix unrelated test race (test failed by coincidence when testing
      this bug). The test disconnected a session that was in the middle of
      an anonymous transaction and then immediately asserted that the
      counter of anonymous transactions has decremented. However, the
      counter is decremented in thread cleanup code that runs
      asynchronously, so it might not have executed at the [1;31mtime[m the
      condition was checked.  Added a synchronization step where the test
      waits for the thread to disappear.
    
    @mysql-test/suite/sys_vars/t/gtid_purged_basic.test
    - Test that RESET MASTER works even when binary log is disabled.
    
    @mysql-test/include/rpl_init.inc
    - Make tests call RESET MASTER even if binary log is disabled.
    
    @mysql-test/suite/rpl/t/rpl_binlog_errors.test
    @mysql-test/t/disabled_replication.test
    - RESET MASTER no longer generates an error when binlog is disabled.
    
    Most other test files:
    - Many tests don't need to force a server restart now that
      rpl_init.inc executes RESET MASTER even on a binlog-less slave.
      (Removed force-restart from some unrelated cases too, where that
      was easy.)
    
    @mysql-test/suite/rpl/t/rpl_simulate_create_trunk_failure.test
    @sql/rpl_gtid_set.cc
    - Don't force restart. Also corrected typo trunk->chunk.

[33mcommit 196e28c2d8b4db994285b0fd606bff3b02609ee7[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Mon May 11 15:54:16 2015 +0200

    Silence rpl.rpl_xa_survive_crash_debug in Valgrind
    
    Test takes too much [1;31mtime[m and creates trouble for daily runs.
    
    Patch approved by Jon Olav Hauglid <jon.hauglid@oracle.com> over IM.

[33mcommit 7583d91aceb946e97a6d397630a364c3ed4ca7a1[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Mon May 11 15:55:38 2015 +0530

    Bug#21021754 - OPTION FOR MAX_STATEMENT_TIME IS MISSING
    
    Issues here is, option to set max_statement_[1;31mtime[m from
    server command line is missing.
    
    Modified code to set max_statement_[1;31mtime[m from command line.

[33mcommit c4a51b22b5992e7808ed8e6fb307fa86a2cb5d65[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Mon May 11 12:59:38 2015 +0530

    Bug#20963082 : AUTO_GENERATE_CERTS PREREQUISITE TESTING NOT CORRECT
    
    Post push fix : Fixing test [1;31mtime[mout issue.

[33mcommit e6f47a0e267e99a4847e4519add294854161cff5[m
Author: Praveenkumar.Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri May 8 11:08:17 2015 +0530

    Bug#20705648 - max_statement_[1;31mtime[m leaks memory on windows
    Bug#20705642 - max_statement_[1;31mtime[m: assertion failed: pending || thd_[1;31mtime[mr->thread_id
    
    Description:
    -------------
    Both of these issues are related to deletion of [1;31mtime[mr from the
    Timer-queue.
    
    Issue in bug20705648 is, the [1;31mtime[mr object created in windows is
    not deleted after the [1;31mtime[mr expiration. Hence memory leak is seen
    on the windows.
    
    Issue in bug20705642 is, the [1;31mtime[mr reset operation sets state of
    [1;31mtime[mr as "not expired" when the [1;31mtime[mr is already expired and the
    callback function execution of it is completed. This is resulting
    in the assert condition failure.
    
    Analysis:
    -------------
    In the current code, for the set [1;31mtime[mr operation new Timer-queue
    [1;31mtime[mr is created. For [1;31mtime[mr cancel and destroy operations, [1;31mtime[mr
    -queue [1;31mtime[mr is deleted from the Timer-queue. But when [1;31mtime[mr is
    expired, assumption was [1;31mtime[mrs of type "WT_EXECUTEONLYONCE" are
    deleted. But these [1;31mtime[mr objects are not delete from Timer-queue
    on expiration. Hence the memory leak is observed in the current
    code.
    
    The windows API to delete [1;31mtime[mr-queue [1;31mtime[mr,
    
        1 Fails when [1;31mtime[mr is expired or callback function is in
          execution.
    
        2 Succeeds when [1;31mtime[mr is *not* expired.
    
        3 Succeeds when [1;31mtime[mr is expired and callback function
          execution is completed.
    
    In the current code, case 1 and 2 are handled properly but
    case 3 is not handled. Because of which windows [1;31mtime[mr reset operation
    returns [1;31mtime[mr state as not expired. If [1;31mtime[mr is not expired then
    non zero thread_id value is expected (as notify function for [1;31mtime[mr
    expiration is not called) in reset [1;31mtime[mr code. Assert condition to
    check the same fails in debug build because of not handling
    condition 3. In non-debug build, [1;31mtime[mr object is just set for the
    reuse.
    
    Fix:
    ------
    To fix memory leak, if [1;31mtime[mr is expired then deleting it from the
    queue on next [1;31mtime[mr set or [1;31mtime[mr destroy operations.
    
    To fix assert issues, new [1;31mtime[mr_state member is introduced in the
    my_[1;31mtime[mr for windows.
    
    [1;31mtime[mr_state is set to "TIMER_SET" in [1;31mtime[mr set operation and
    in [1;31mtime[mr_callback  to "TIMER_EXPIRED".
    
    With [1;31mtime[mr_state, setting [1;31mtime[mr expired/not expired state in
    [1;31mtime[mr reset operation is handled as below,
    
    Timer reset operation (my_[1;31mtime[mr_cancel()):
      Windows API DeleteTimerQueueTimer is used to delete m
      the [1;31mtime[mr queue. This API,
    
      * Succeeds when [1;31mtime[mr is *not* expired.
          state is set to *not expired* as [1;31mtime[mr_state is TIMER_SET.
    
      * Succeeds when [1;31mtime[mr is expired and callback function
        execution is completed.
          state is set to *expired* as [1;31mtime[mr_state is TIMER_EXPIRED.
    
      *  *Fails* when [1;31mtime[mr is expired or callback function is in
         execution.
           state is set to *expired*.
           [1;31mtime[mr_state value is *not* checked in this case.
    
    So to fix assert condition failure, "[1;31mtime[mr_state" value is
    checked to set [1;31mtime[mr expired or non expired state. Now if
    [1;31mtime[mr cancellation is called after [1;31mtime[mr callback execution
    then "expired" state is returned.
    
    Testing:
    --------
    Without patch, memory leak of around 400kb/sec is observed with
    mysqlslap.exe -hlocalhost --number-of-queries=10000000000 --query="select
    max_statement_[1;31mtime[m=1 sleep(rand()) ;" --iterations=1000000
    --create-schema=test
    
    With patch no memory leak or crash is observed,
    \bin>[1;31mtime[m
    The current [1;31mtime[m is: 11:21:03.07
    Enter the new [1;31mtime[m:
    
    \bin>tasklist /FI "IMAGENAME eq mysqld.exe"
    
    Image Name                     PID Session Name        Session#    Mem Usage
    ========================= ======== ================ =========== ============
    mysqld.exe                   11304 RDP-Tcp#28                 2    119,244 K
    
    \bin>[1;31mtime[m
    The current [1;31mtime[m is: 11:28:35.59
    Enter the new [1;31mtime[m: .
    
    \bin>tasklist /FI "IMAGENAME eq mysqld.exe"
    
    Image Name                     PID Session Name        Session#    Mem Usage
    ========================= ======== ================ =========== ============
    mysqld.exe                   11304 RDP-Tcp#28                 2    119,244 K

[33mcommit 938f6a241a10a7d5f6e3309389237b6f7e508dcb[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu May 7 12:06:27 2015 -0700

    My::Memcache.pm: handle case where the last read before a [1;31mtime[mout completed the read buffer.
    Open a new memcache connection when trying to fetch server error stats.

[33mcommit 9f9602b8b5b5ce3fed3617eebdbe3387445f9733[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu May 7 20:00:20 2015 +0200

    Save the prepared key in Ndb_schema_dist_data
    
     - add placeholder in Ndb_schema_dist_data for the key prepared
       for rename
     - remove NDB_SHARE::new_key, shrinking run[1;31mtime[m footprint even more

[33mcommit 241bec17939bf228681be901a15fa78f779ed4b4[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu May 7 19:25:10 2015 +0200

    Remove NDB_SHARE::mem_root and instead use my_malloc for dynamic strings
    
     - this fixes the memory leak where the MEM_ROOT just kept growing
      for each reame/alter.
     - add NDB_SHARE::free_key() to release the memory for key
     - saves run[1;31mtime[m footprint of NDB_SHARE and reduces complexity

[33mcommit 149ba49d18c9403b09f7715e56df9b24ef4cf846[m
Author: Praveenkumar.Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Thu May 7 12:58:22 2015 +0530

    Bug#21021670 - MISLEADING WARNING WHEN PER-QUERY STATEMENT TIME IS EXCEEDED
    
    Issue here is, error reported on exceeding statement [1;31mtime[m contains
    session variable "max_statement_[1;31mtime[m" in it. This error message
    is misleading when max_statement_[1;31mtime[m is *not* set and [1;31mtime[mout
    value is set at statement level.
    
    The [1;31mtime[mout value can be set either at session level using
    max_statement_[1;31mtime[m variable or at statement level. If execution
    of statement takes more than [1;31mtime[mout value then following
    error is reported,
    
      "Query execution was interrupted, max_statement_[1;31mtime[m
      exceeded".
    
    When session variable max_statement_[1;31mtime[m is not set and [1;31mtime[mout
    value is set at statement level then error message with
    max_statement_[1;31mtime[m in it on [1;31mtime[mout is misleading.
    
    Fix:
    Modified error message so that max_statement_[1;31mtime[m is not used
    in it. Now on error following error message is reported,
    
    "Query execution was interrupted, maximum statement execution [1;31mtime[m
    exceeded"

[33mcommit f48f56d273786b7b813abd48b40894a1e7e3b4be[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Tue Apr 28 12:51:53 2015 +0300

    Bug#20507804 FAILING ASSERTION: TRX->READ_ONLY && TRX->AUTO_COMMIT && TRX->ISOLATION_LEVEL==1.
    
    Attempts to kill statements using attachable transaction (e.g.
    statements using CONVERT_TZ() function, setting [1;31mtime[m_zone variable,
    HELP statements) some[1;31mtime[ms led to assertion failures/improper
    functioning of further transactions. The same problem might have
    occurred when such statements were aborted due to max_statement_[1;31mtime[m
    [1;31mtime[mout.
    
    The problem occurred when statement was aborted after attachable
    transaction opened and called ha_innobase::external_lock() for some
    tables, but before any data was read from these tables.
    
    Attachable transaction implementation on SQL-layer relied on InnoDB
    to commit transaction implicitly when ha_innobase::external_lock(F_UNLCK)
    is called for last table in the transaction. But InnoDB code in
    ha_innobase::external_lock(F_UNLCK) treated aborted-before-any-read
    transactions as not started and didn't do anything about them.
    OTOH InnoDB transaction was still marked as registered on SQL-layer
    using trx_t::is_registered flag in this case. As result this flag
    was not reset properly when such attachable transaction ended.
    
    Further transactions which tried to reuse the same trx_t object
    (with incorrect trx_t::is_registered flag) behaved wrongly and
    eventually led to assertion failures.
    
    Note that similar problem doesn't occur for statements in auto-commit
    mode which don't use attachable transaction. This is because SQL-layer
    doesn't rely on InnoDB doing commit on last external_lock(F_UNLCK)
    for them. Instead we always call trans_commit_stmt() explicitly.
    This correctly clears trx_t::is_registered flag even though InnoDB
    might think that transaction is not started.
    
    This patch solves the problem by making attachable transactions
    implementation similar to normal statements. Now we call
    trans_commit_stmt() at the end of attachable transactions as well,
    so InnoDB's internal transaction state is reset correctly.

[33mcommit 49176a4945b4f413e974d68b4fe19a8f19e36212[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Tue May 5 15:25:15 2015 +0530

    BUG#20949314 PARTITION_HELPER::PH_RND_INIT(BOOL): ASSERTION `0' FAILED
    
    Problem :
    ---------
    During co-related subquery execution, SQL reinitialize scan in subquery
    table multiple [1;31mtime[ms without ending previous scan. This behaviour is
    consistent in previous versions. The debug ASSERTs introduced in 5.7 for
    partitioned table in sequential scan is causing the issue.
    
    Solution :
    ----------
    Close previous open scan on partition in sequential scan init and remove
    the ASSERT.
    
    Reviewed-by: Mattias Jonsson <mattias.jonsson@oracle.com>
    
    RB: 8803

[33mcommit 5b7889c6c5c6ac8ce9584da5b6252fecd20bae47[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Apr 29 23:58:43 2015 +0200

    Bug#20963147 DECOUPLE DIGEST SIZE FROM SERVER AND PFS
    
    Before this fix, the server variable 'max_digest_size'
    was used in two places.
    
    1)
    
    During query parsing, the server collects the statement digest text,
    and store the digest in a buffer of size 'max_digest_size'.
    
    Only 1 digest text per session is needed at the same [1;31mtime[m,
    which limits the memory consumption.
    
    2)
    
    In the performance schema, the digests collected in 1) are
    copied in various tables, and the memory allocated to
    keep digest historical or aggregated data also depends on 'max_digest_size'.
    
    Many digests text per session are preserved at the same [1;31mtime[m,
    so that the total memory consumption is a multiple of 'max_digest_size',
    making this parameter sensitive.
    
    The problem is that for some deployments,
    namely when using either the firewall plugin or the query rewrite plugin,
    DBA typically need to have at the same [1;31mtime[m:
    - a big value for 1), to avoid truncation
    - a lower value for 2), to limit memory consumption,
      even if truncations can occur in the recorded performance schema data.
    
    The solution is to keep variable 'max_digest_size' for part 1),
    and create a separate variable 'performance_schema_max_digest_size' for part
    2), so that sizing for the firewall/query rewrite can be independent of the
    performance schema.

[33mcommit d69af47462b72b8de7bf9aacf17d41201613e394[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Sat Apr 11 00:06:45 2015 +0300

    rb8590 Fixes to sporadically failing on PB2 rpl_xa_survive_crash_debug
    
    The test fails due to not following a valid pattern of crash-simulation
    with the following restart.
    It's fixed to follow the pattern.
    Extensive run (1600 [1;31mtime[ms) has not shown any regression now.
    
    As a side effect a non-determinism of XA ROLLBACK is worked around
    to be fixed in a separate bug fixing as commented.

[33mcommit 1f55a9909daeecb3355c867fec7c3e3c5dfe3d78[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Apr 28 12:57:12 2015 +0200

    Bug#20937654 CANNOT BUILD WITH "-DDISABLE_SHARED=ON" FOR CMAKE BECAUSE OF REWRITER PLUGIN
    
    Don't build/install anything in plugin/rewriter if DISABLE_SHARED=ON
    Don't build shared version of the standalone binlog library if DISABLE_SHARED=ON
    Also: add run[1;31mtime[m check for plugin availability to some tests.

[33mcommit 24f70310a53012c58bc24ef31ef455cdbdc86150[m
Author: Pratik Patodi <pratik.patodi@oracle.com>
Date:   Tue Apr 28 16:24:41 2015 +0530

    Added 5 autotest testcases to test node restart with following scenarios.
    1. Restarting one node at a [1;31mtime[m.
    2. killing two node of different groups and starting them
    with and without initial option.
    3. Restarting a node which doesn't belongs to node group 0, and checking
    that it is not associated with node group 0 after restart.
    4. killing four node of different groups and starting them
    with and without initial option.
    5. Killing only the master nodes one by one and starting them without
    initial option.

[33mcommit a9b5ec00bd874c79ace15ffa1377a4ab7cf97876[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Apr 27 21:58:36 2015 -0700

    My::Memcache -- longer write [1;31mtime[mot

[33mcommit 6526d517de3f72d6f6809809ec85f209ed3a5648[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Fri Apr 24 17:45:25 2015 +0530

    Bug#20782580: NDB REDO OVERLOAD: ABORT QUEUING REQUESTS IF OUT OF
    LOG SPACE
    
    If redo log writes are requested when the redo log is full, the
    writes are queued instead of being rejected. These requests have
    to wait in queue until redo space is freed. However, redo space
    is freed only when the next LCP completes, which can take a long
    [1;31mtime[m. This results in long waits and [1;31mtime[mouts.
    
    Modified redo log queuing to abort requests which are received
    when the redo log is full. The requests are aborted with the
    error code 410 'REDO log files overloaded (decrease
    TimeBetweenLocalCheckpoints or increase NoOfFragmentLogFiles)'.

[33mcommit cc364e94bc3bfa3688cc2feb6432a7ba70ca8ba9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 24 13:49:03 2015 +0200

    Fix a possible crash in AutoTest when an
    ordered scan encounter error 4008, scan [1;31mtime[mout.
    One such testcase is 'testScan -n ScanRead4880'
    
    In case scan [1;31mtime[mouts are encountered, the test attempts to reduce the
    load by changing to unordered scans. This is done by setting the
    'const NdbDictionary::Index * pIdx' argument to NULL inside
    HugoTransactions::scanReadRecords.
    
    As pIdx->getName() is later called when creating a NdbIndexScanOperation
    we will crash.
    
    The patch below fixes this issue by calling NdbScanOperation() instead
    if 'pIdx == NULL'

[33mcommit b47ac1dfa918afe851d89f4fe5646cd4438bd657[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Apr 22 17:23:48 2015 +0100

    Bug#20927239: MY_TIMER-T UNIT TEST DOES NOT WORK WITH MERGE_UNITTESTS=0
    
    The problem was that the my_[1;31mtime[mr-t unit test did not work
    properly if it was compiled separately. This was because the
    unit test code was wrapped in #if HAVE_MY_TIMER. This symbol
    is defined in my_config.h which had not been included at that
    point. The same problem existed in my_[1;31mtime[mr.h.
    
    This patch fixes the problem by removing HAVE_MY_TIMER.
    This symbol is always true for all supported platforms anyway.
    Removing this makes the unit test work as intended and
    simplifies code.

[33mcommit 810eb3a5e65424596ed9ab72d731fb6004947bbf[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Apr 17 16:00:36 2015 +0200

    Bug#20903701 FIX VALGRIND WARNINGS IN UNIT TESTS
    
    Initialize mock objects to avoid valgrind warnings.
    Also: smaller max_allowed_packet in ItemFuncExportSet, to save [1;31mtime[m.

[33mcommit c3035fed19bdc9f155775499dd23089b14e656e0[m
Author: Haixiang Li <haixiang.li@oracle.com>
Date:   Tue Apr 21 03:30:47 2015 +0200

    Bug#18486509 ASSERTION FAILED: TABLE->KEY_READ == 0 IN CLOSE_THREAD_TABLE
    
    Description:
    ------------
    - SET statement locks tables before preparing the statement, so preparation
      may some[1;31mtime[ms execute subqueries.
    
    - But remove_redundant_subquery_clauses() notices that GROUP BY clause is
      not necessary in a subquery, will remove GROUP/ORDER clause, that leads
      to a subquery in GROUP clause became "invisible", so the subquery is not
      available for cleanup when the whole query is cleaned up.
    
    Fix:
    ----
    For a SET statement with subquery, release a subquery's resources before
    cleaning up after removing the subquery from the item tree.
    
    Test case added.

[33mcommit 9ac1425053312b98fc9e6a999087efb09e60daec[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Apr 17 15:04:58 2015 +0300

    Fix Bug#20618309 ASSERT SLOT1->PAGE_LEVEL == SLOT2->PAGE_LEVEL, BTR_ESTIMATE_N_ROWS_IN_RANGE()
    
    Relax a too strict assert. If the tree is changed between both dives for
    the left boundary and the right boundary, then our markers (slot1 and
    slot2) could end up on different levels in the tree.
    
    If we detect that this has happened - then retry a few [1;31mtime[ms and if
    still unsuccessful then return an arbitrary number for an estimate.
    
    Reviewed-by:    Marko Mäkelä <marko.makela@oracle.com>
    Reviewed-by:    Jimmy Yang <jimmy.yang@oracle.com>
    RB:             8570

[33mcommit 58e2cd88730544f64ed584618ee7d6eb47cab1d0[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Apr 17 10:25:02 2015 -0700

    MTR ndb_memcache more tweaks to [1;31mtime[mout handling

[33mcommit 57d04af18dc664f726949c06ba4b6ddb8db0a7bd[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Apr 16 09:09:00 2015 -0700

    increase [1;31mtime[mouts

[33mcommit 724baefbe7bd2b8fdc2055d9372c42c08f44be67[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Apr 13 14:52:59 2015 -0700

    Attempt to avoid spurious test failures.
    Increase read [1;31mtime[mout in My::Memcache from 4 sec. to 8 sec.
    Change external_values test to use all non-logging tables.

[33mcommit 4a9842f11277e9557e903d5579aec26f20b9abab[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Apr 13 14:52:59 2015 -0700

    Attempt to avoid spurious test failures.
    Increase read [1;31mtime[mout in My::Memcache from 4 sec. to 8 sec.
    Change external_values test to use all non-logging tables.

[33mcommit 98fa3795495dfc9f786c6ec6d9fa1404f9d0de70[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Apr 13 14:52:59 2015 -0700

    Attempt to avoid spurious test failures.
    Increase read [1;31mtime[mout in My::Memcache from 4 sec. to 8 sec.
    Change external_values test to use all non-logging tables.

[33mcommit 1362730394a29b79d896551f77917a7ae4aa7ef3[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Apr 9 14:19:09 2015 +0200

    Bug#20788811 MAX_STATEMENT_TIME: ASSERTION `EXPLAIN_OTHER || UNIT->IS_OPTIMIZED()' FAILED.
    
    We open many connections sending repeatedly:
    explain select 1 from t1 where a in (select 1 union select 1)
            union select 1;
    And we have max_statement_[1;31mtime[m=1.
    Occasionally, one such query takes more than one second, and
    the KILL_TIMEOUT is given while it's optimizing:
    (select 1 union select 1) (the inside of IN());
    during that optimization, we optimize the two "select 1"
    then proceed to optimizing the fake select lex, which is when the
    [1;31mtime[mout occurs:
    Optimize_table_order::best_extension_by_limited_search()
    gets the [1;31mtime[mout:
    (gdb) p thd->killed
    $39 = THD::KILL_TIMEOUT
    so returns here:
      if (thd->killed)  // Abort
        DBUG_RETURN(true);
    That makes select_lex::optimize() fail (select_lex is fake_select_lex).
    This makes select_lex::optimize() fail (this [1;31mtime[m select_lex is the first
    query of the top union, i.e. the owner of IN()).
    So we break from the loop of unit->optimize() (where 'unit' is the top
    UNION), we still move on to optimize unit->fake_select_lex (the mistake)
    which resets 'status' to 'false'. Then it goes wrong, as
    status==false => We set unit->is_optimized() to true and return "ok".
    Later we try to explain that and crash on the non-properly-optimized
    part UNION-in-subquery.
    
    Fix: don't return "ok" if a query block had a failure; then statement
    will terminate before reaching EXPLAIN code.
    No testcase: it requires concurrency, and it's just a plain stupid
    coding mistake. I tested manually - without patch, crash in 5 minutes;
    with patch, no crash in two runs of 3 hours each.

[33mcommit ec553c2f67a7d5f56dae11bbe97b3d867316e9f9[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Fri Apr 10 06:16:41 2015 +0200

    Silence rpl_xa_survive_crash_debug in Valgrind.
    
    The test [1;31mtime[ms out after 9000 sec.
    
    Approved by Anitha Gopi <anitha.gopi@oracle.com> over IM.

[33mcommit acb2f7b15a8c2c5d95b68328bee3c330c2ca08a2[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Apr 9 15:34:57 2015 -0700

    MTR ndb_memcache : still better [1;31mtime[mout handling & more verbose reporting during test runs

[33mcommit 5d65fad6f6656cc62efcaea4906c1d4b1c15aad9[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Apr 9 08:00:21 2015 +0200

    Bug#20855853 MDL SUBSYSTEM ENCAPSULATION BROKEN
    
    WL#6936 - Implementation of server-side statement [1;31mtime[mout
    broke the mdl subsystem encapsulation by peeking into the thd state.
    
    MDL code should only access THD through the MDL_context_owner interface.
    Use MDL_context_owner::is_killed rather than THD::killed.

[33mcommit 106ff8934b559ae2c71560ce363a66019b7fd082[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Apr 8 17:17:42 2015 -0700

    Work on My::Memcache to handle server disconnects and [1;31mtime[mouts

[33mcommit efdf80773930c37023c8fc75591040d509cd893f[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Wed Apr 1 11:42:10 2015 +0200

    Fix merge error in mysql_system_tables_fix.sql
    
     -  removed duplicate "Add [1;31mtime[mstamp and expiry columns" code

[33mcommit 4cc4a01c8ea85e4aa2d1e5f47eca8aeb28d431e8[m
Author: David.Zhao <david.zhao@oracle.com>
Date:   Tue Mar 31 19:19:20 2015 +0800

    Bug#19811953 ENVELOPE() FUNCTION RETURNS INVALID POLYGON
    Bug#20196720 ENVELOPE(P) RETURNS NULL FOR A NON NULL POLYGON P
    
    Issues:
    1. Some[1;31mtime[ms the MBR of a geometry can degrade to a point or
    vertical/horizontal line segment. For example, given a point P, its MBR
    is the point P itself; given a horizontal or
    vertical linestring L, its MBR is a horizontal/vertical line segment.
    In both cases if we return a polygon in function st_envelope(), the polygon
    must be a degenerated one and many GIS functions don't accept it, preventing
    queries with nested function calls like below to be executed.
    
    SELECT ST_AREA(ST_ENVELOPE(ST_GEOMETRYFROMTEXT(geom))) from t1;
    
    2. ST_Envelope() returns NULL given an empty geometry collection. It should
    return an empty geometry(represented in the form of an empty geometry
    collection) in this case because the result isn't 'unknown', we know
    it's an empty geometry.
    
    3. ST_Envelope() returns NULL given a geometry having some points but are
    geometrically invalid. But other parts of GIS expects a valid MBR in
    this case.
    
    Fix:
    If an MBR degrades to a point/linesegment, return the point or
    line segment respectively as result of ST_ENVELOPE() instead of an
    invalid polygon;
    
    In ST_ENVELOPE return an empty geometry collection given an empty
    geometry collection.
    
    If the geometry input is geometrically invalid but has a valid WKB string,
    ST_Envelope() still calcs its MBR and returns a valid MBR.
    
    Finally, we don't accept a linestring with one point or a polygon with
    a ring of less than 4 points anywhere in MySQL GIS, this is also true
    for ST_Envelope() because such GIS data is rejected by geometry constructor
    functions such as ST_GeomFromText/WKB/GeoJson before reaching ST_Envelope.

[33mcommit 8b53b0076eddd4fd759e7fb8329e8ba17929b792[m
Author: Daogang.qu <bill.qu@oracle.com>
Date:   Sat Mar 28 17:12:23 2015 +0800

    Bug#18091217  SLAVE I/O THREAD WON'T ATTEMPT AUTO RECONNECT TO THE MASTER / ERROR-CODE 1159
    
    The slave receiver thread stops due to the 'ER_NET_READ_INTERRUPTED'
    or 'ER_NET_WRITE_INTERRUPTED' error is encountered while getting
    [1;31mtime[mstamp, server id from master, setting @master_heartbeat_period
    and so on.
    
    To fix the problem, we fix to handle the 'ER_NET_READ_INTERRUPTED'
    and 'ER_NET_WRITE_INTERRUPTED' errors as transient network errors.
    Then the slave receiver thread attempts to automatically reconnect
    to the master on these errors.

[33mcommit 1eebdb3fa41097c6491edb022e6d000b4e9b8020[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Mon Mar 23 16:18:20 2015 +0100

    Bug#20310257 Explain for insert into a view show wrong table for insert
    
    This is a problem for EXPLAIN of an INSERT ... SELECT statement into
    a multi-table view: The first table of the view was always displayed
    as the table being inserted into, even if inserting into a different
    table.
    
    This bugfix has been slightly eased by some refactoring made in WL#5275.
    EXPLAIN relied on LEX::leaf_tables_insert to be set correctly as
    the "leaf" table to insert into. But the logic in
    select_insert::prepare() was wrong: It would always assign the first
    leaf table to this.
    
    Notice that we now calculate insert_table_ref for an INSERT.
    The bug fix is to rename LEX::leaf_tables_insert to
    insert_table_leaf and copy the value from insert_table_ref.
    (The renaming was made to synchronize the name of the table_list
     pointer with insert_table, which is the name of the user-supplied
     table (base table or view) being inserted into.)
    
    We also realized that LEX::insert_table_leaf and the local variable
    insert_table_ref was duplicates, so we have removed the local variable
    wherever possible.
    
    The calculation of st_select_lex::leaf_table_count and
    st_select_lex::leaf_tables in mysql_insert_select_prepare() has also
    been simplified.
    
    There was also a problem with prepare_for_positional_update(), since
    it was based on a table object that some[1;31mtime[ms was not calculated yet.
    The simple solution was to have two calls to
    prepare_for_positional_update(), one for each place the insert table
    is determined. It is unfortunate to have it in two places, but
    the alternative is some bigger refactoring.

[33mcommit 86abcbb937b43b56b4e8293dde8e9d790b787ad5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu Mar 19 17:28:58 2015 +0200

    Bug#19229231 INNODB.LOG_FILE_NAME FAILS SPORADICALLY ON PB2
    
    Post-fix: Tolerate different ordering of messages.
    Some[1;31mtime[ms, t1.ibd is flagged before t3.ibd.
    
    Test passed with --repeat=20 --parallel=4.

[33mcommit 4de91b7015aa4a91314f2777b5a0aaeb0498867f[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 16:18:36 2015 +0200

    Post-push fix for Bug#20691930 (innodb_zip.wl5522_debug_zip failure):
    
    Do not attempt to reset the page type when importing compressed tablespaces.
    The page type can be invalid only on uncompressed tablespaces, because
    at the [1;31mtime[m when compression support was introduced, InnoDB already
    initialized FIL_PAGE_TYPE on every page.
    
    (cherry picked from commit 8823190d8898a0d02ebeae1cb4906550f788660c)

[33mcommit 518d3e437ca603d697d7d18e454fe6e56bdd17d2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a regression from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the flush-[1;31mtime[m check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    (cherry picked from commit fc1f91f396bc73fcea72f17d6c22c174ba057e9b)

[33mcommit d010b9e50a9694f0df4482c9ed209e38375b18de[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 16:18:36 2015 +0200

    Post-push fix for Bug#20691930 (innodb_zip.wl5522_debug_zip failure):
    
    Do not attempt to reset the page type when importing compressed tablespaces.
    The page type can be invalid only on uncompressed tablespaces, because
    at the [1;31mtime[m when compression support was introduced, InnoDB already
    initialized FIL_PAGE_TYPE on every page.

[33mcommit c998472c0096f2102eaa5970d4ec0cacea1946b2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a regression from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the flush-[1;31mtime[m check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>

[33mcommit 3d9e1dbd0cb7ff099c03076b33125bd71a1e2ca9[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Mon Mar 16 12:29:14 2015 +0530

    Bug#20092754:  MYSQLD CRASH (FOUND UNCOMITTED AUTOCOMMIT+RBWR
    TRANSACTION, COMMIT STATUS 3)
    
    A bulk delete operation is committed early to avoid an
    additional round-trip while returning the number of affected
    rows. This early commit fails due to a [1;31mtime[m-out error. If an
    early commit is done, this is handled at transaction commit
    [1;31mtime[m by verifying that the transaction is in the Committed
    state. This commit check only covers the case of a successful
    commit, and causes a mysqld core in the cases where the early
    commit failed.
    
    Modified the commit check to handle early-commits based on
    transaction state:
    
    1) Committed: expected status, commit succeeded
    2) Aborted: commit failed on NotStarted transaction,
                no rollback needed
    3) NeedAbort: commit failed on Started transaction,
                rollback needed
    4) Started/NotStarted: commit not attempted,
                not a valid case
    
    Added some error-handling so that the 4012 error is detected
    during transaction execution. Also fixed the row count for
    early-commit failures.

[33mcommit 455c4e8810c76430719b1a08a63ca0f69f44678a[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Mar 13 17:51:27 2015 +0100

    Bug#17668844: CRASH/ASSERT AT ITEM_TYPE_HOLDER::VAL_STR IN ITEM.C
    
    We have a predicate of the form:
    literal_row <=> (a UNION)
    
    The subquery is constant, so Item_cache objects are used for its
    SELECT list.
    In order, this happens:
    - Item_subselect::fix_fields() calls select_lex_unit::prepare,
    where we create Item_type_holder's
    (appended to unit->types list), create the tmp table (using type info
    found in unit->types), and call fill_item_list() to put the
    Item_field's of this table into unit->item_list.
    - Item_subselect::fix_length_and_dec() calls set_row() which
    makes Item_cache's of the subquery wrap the Item_type_holder's
    - When/if a first result row is found for the subquery,
    Item_cache's are re-pointed to unit->item_list
    (i.e. Item_field objects which reference the UNION's tmp table
    columns) (see call to Item_singlerow_subselect::store()).
    - In our subquery, no result row is found, so the Item_cache's
    still wrap Item_type_holder's; evaluating '<=>' reads the
    value of those, but Item_type_holder objects are not expected to be
    evaluated.
    
    Fix: instead of putting unit->types into Item_cache, and later
    replacing with unit->item_list, put unit->item_list in Item_cache from
    the start.
    There was also a bug in Item_cache_date[1;31mtime[m::val_str():
    if cache_value() says "I cached" (returns true), it might
    have cached a NULL value, in which case we must return
    a NULL pointer, instead of &str_value; note that this is
    like in other places of this function where if null_value is true
    we already return NULL.
    Without this, we had a wrong predicate value when types of the UNION
    are DATE and TIME, and UNION's result is empty and left argument is NULL.

[33mcommit 820db843d248e7755229206b298239de59ee8b3a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Mar 11 16:01:58 2015 +0100

    Bug#20685859 ENABLE STAGES WITH PROGRESS BY DEFAULT FOR EASE OF USE
    
    Before this fix, every "stage/%" instrument is disabled by default.
    
    With this fix, stage instruments that provide online progress monitoring
    are enabled and [1;31mtime[md by default.
    
    The list of stage instruments with online progress is currently:
    
    - stage/sql/copy to tmp table
    - stage/innodb/alter table (end)
    - stage/innodb/alter table (flush)
    - stage/innodb/alter table (insert)
    - stage/innodb/alter table (log apply index)
    - stage/innodb/alter table (log apply table)
    - stage/innodb/alter table (merge sort)
    - stage/innodb/alter table (read PK and internal sort)
    - stage/innodb/buffer pool load
    
    (cherry picked from commit 818f9f4660ae2d9257429e1ed025fd54dd075be1)

[33mcommit 5306426917e8b8928c29f8af2e5dc042ffa1ba88[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Mar 13 13:16:26 2015 +0100

    Bug#20691920 SOMETHING IN TEST OPT_HINTS CAUSES ALL MEMORY TO BE CONSUMED AFTER FIX 20685187
    
    Regression introduced by:
    Bug#20685187 USE -STD=C99 WHEN BUILDING WITH GCC ON SOLARIS
    
    Fix: ensure that support for [1;31mtime[mrs is compiled in for Solaris.
    Move opt_hint [1;31mtime[mout tests to separate file
    in case we have platforms that do not support [1;31mtime[mrs.
    
    Rollback the use of -std=c99
    Add extra cmake check that isinf is visible in C and C++

[33mcommit b0b58ab16d05f9f70ef7dc4af3920d4e744c5115[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Thu Mar 12 10:29:06 2015 +0100

    Disable main.opt_hints on Solaris for the [1;31mtime[m being.
    
    Approved by Bjorn Munch <bjorn.munch@oracle.com> over IM.

[33mcommit e8ffcf5e251572648725ee124220809f556bcd48[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Mar 11 16:01:58 2015 +0100

    Bug#20685859 ENABLE STAGES WITH PROGRESS BY DEFAULT FOR EASE OF USE
    
    Before this fix, every "stage/%" instrument is disabled by default.
    
    With this fix, stage instruments that provide online progress monitoring
    are enabled and [1;31mtime[md by default.
    
    The list of stage instruments with online progress is currently:
    
    - stage/sql/copy to tmp table
    - stage/innodb/alter table (end)
    - stage/innodb/alter table (flush)
    - stage/innodb/alter table (insert)
    - stage/innodb/alter table (log apply index)
    - stage/innodb/alter table (log apply table)
    - stage/innodb/alter table (merge sort)
    - stage/innodb/alter table (read PK and internal sort)
    - stage/innodb/buffer pool load

[33mcommit aee6c5647735414325446eb82c260feb127bad9f[m
Author: Daogang.qu <bill.qu@oracle.com>
Date:   Wed Mar 11 11:03:05 2015 +0800

    Bug #19451053  CRASH AFTER DIRECT INSERT INTO MYSQL.GTID_EXECUTED TABLE
    
    When gtid_mode=on and binary log is off, the server automatically
    inserts the transaction's GTID into mysql.gtid_executed within
    the transaction. But if the GTID has already been inserted into
    the table by an explicit INSERT statement, the server crashes.
    
    After the fix, the server does not crash in above case. Push a
    warning to client if user is modifying the gtid_executed table
    explicitly. Ignore the duplicate key error and log a warning
    for it when writing transaction owned GTID into gtid_executed
    table implicitly within the transaction, we did not push a
    warning to client in the case, since it is slave SQL thread
    or worker some[1;31mtime[ms.
    
    @ sql/binlog.cc
    We invoke the warn_on_modify_gtid_table(...) in two places to
    decrease the iterations to table list for improving performance.

[33mcommit 4f884b8731de9707f9498ff54c427318cb19b793[m
Author: Allen.Lai <zheng.lai@oracle.com>
Date:   Wed Mar 11 09:13:40 2015 +0800

    Bug#20547644 DATA CORRUPTION CAUSED BY SOME SIMPLE SQL
    
    if there're rows in the table.  We should block adding a new geo
    column and add spatial index on it same [1;31mtime[m.
    Since currently, we don't define any default value for geometry
    datatype.
    
    Reviewed-by: Jimmy Yang<jimmy.yang@oracle.com>
    RB: 8255

[33mcommit 0b5894ce581d3a5c82847bd6e73448821b80de8c[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Tue Mar 10 12:36:30 2015 +0400

    Bug #20087571: 5.7 EASILY HITS ERROR 1436 (HY000): THREAD STACK OVERRUN ERRORS ON MANY EXPR'S
    
    This bug is a regression of WL7200: that WL introduced an unconditional
    recursive processing of AND and OR parse tree nodes during the
    contextualization, when the original parser had some conditional
    flattening of nested AND/OR expression.
    That affected very long recursive AND/OR expression: instead of the normal
    execution the parser failed with a parse error: ER_STACK_OVERRUN_NEED_MORE.
    
    This bugfix re-introduces an optimization that flattens recursive AND/OR
    expressions at parse [1;31mtime[m (before the itemization/contextualization):
    
     (X1 AND X2) AND (Y1 AND Y2) ==> AND (X1, X2, Y1, Y2)
     (X1 AND X2) AND Y ==> AND (X1, X2, Y)
     X AND (Y1 AND Y2) ==> AND (X, Y1, Y2)
    
    and the same for OR.

[33mcommit 6f9d4323e77c0cedb71b1dd6787770400b503d70[m
Author: David.Zhao <david.zhao@oracle.com>
Date:   Mon Mar 9 11:07:49 2015 +0800

    WL#7929 GIS: Implement ST_Buffer and ST_Distance with Boost.Geometry
    
    Make test case result consistent on all platforms; Remove 2 test cases
    which in debug build, Boost.Geometry takes too much [1;31mtime[m to compute on
    Solaris 11.

[33mcommit f4c37f7aea732763947980600c6882ec908a54a0[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Mon Feb 2 12:26:28 2015 +0200

    WL#6860 Binlogging XA-prepared transaction
    
    ChangeLog
    ---------
    Fri Mar  6 17:44:42 EET 2015
    
    Resolving conflicts at merging to 5.7.
    
    Fri Mar  6 13:59:13 EET 2015
    
    - fixing innodb transaction freeing that was missed out in a case of
      1phase xa through mysqlbinlog applier.  Restructuring an if to
      comply to innodb style, as earlier requested by Marko.  Fixing an
      assert, not dealing directly with the WL per him as well.
    
    Thu Mar  5 08:50:38 EET 2015
    
    - Compilation in innodb got broken on some platforms with the last patch. Fixed now.
    
    Wed Mar  4 19:13:52 EET 2015
    
    - XA COMMIT ONE PHASE at replaying from binlog made
      binlog.binlog_xa_prepared_disconnect to cause failures in other
      tests masquerading to bug19502202. Yet the reason turned out to be
      missed re-attachment of storage engine trans context, that matches
      previous detachment.
      Fixed and the test is augmented to catch the failure alone.
      Small changes are done to innodb.cc and handler.cc.
    
    Tue Mar  3 21:35:28 EET 2015
    
    - fixing more non-deterministic cases that PB2 reveals
    
      1.  main.xa: I left disconnect to run wo/ generally needed synchronisation
                   with connection that runs XA recover. I did not like to contrain
                   the test with P_S dependency. Added warnings and todo, if the warning
                   will come true;
      2.  rpl_xa_survive_crash_debug: found a reason of result mismatch, fixed *it*, but
                                      the test fails sporadically. I suspect --error 2013;
      3.  sys_vars.innodb_support_xa_func: fixed
    
    - ending cleanup requested by Dmitry at today's review of xa.cc and sql_plugin.*
      changes. Hunks from the latter pair of files are relocated onto xa.*.
    - adding support and test for mysqlbinlog recovery for XA-COMMIT-ONE-PHASE;
    
    Tue Mar  3 16:59:14 EET 2015
    
    - determinism in binlog_xa_prepared_disconnect (gtid on/off), and
      rpl_xa_survive_crash_debug (binlog format, synchronisation)
    - fixes to Bug20608551 (LiBing's contribution).
    
    Tue Mar  3 11:59:43 EET 2015
    
    - Changes are done to support "mysqlbinlog | mysql | mysqld" recovery,
      making couple of log_event.cc static functions to turn to public into
      sql_plugin.cc,h and applier_reset_xa_trans() is defined in xa.cc.
      binlog_xa_prepared.test is extended to cover the case;
    - XA COMMIT/ROLLBACK vs GTID_NEXT testing is added;
    - Bug20616249 fixes are incorporated with the WL patch;
    - cleanup in done in xa.cc,h
    
    Fri Feb 27 18:30:28 EET 2015
    
    - Incorrect ordering of rollback to binlog and SE is fixed. XA-rollback
      is written first now. That must be the most probable reason of bug20616249;
    - set @@gtid_next and following XA-commit,rollback worked incorrectly in few ways:
      a. mysql.gtid_executed on slave missed records
      b. set @@gtid_next could not be set to another "manual" value 'cos
         of uncleared status of previous assignment done to the first logging phase
         of XA START..PREPARE
      Tests are extend to cover b.
    - Crash simulation in the middle of XA-rollback is added to prove the existing
      policy of survival of the prepared XA is not gone, as well as to prove
      correct logging ordering (see above).
    
    Tue Feb 24 12:46:33 EET 2015
    
    Updating few test result files:
     - XA basic framework main.xa complained about mismatch
       expectedly 'cos of having disconnect of a prepared transaction;
     - binlog_xa_prepared.test is slightly cleaned not to depend
       on preceeding tests that may contribute to binlog which is
       printed out to show new valid logging pattern.
    Fixing compilation on win;
    
    Todo: rpl_gtids_table_disable_binlog_on_slave seems to have caught
    mysql.gtid_executed updating by two-phased logged XA, under investigation.
    
    Mon Feb 23 21:24:48 EET 2015
    
    Fixing failed tests on hundson.
    
    i_binlog.binlog_loose_XA_trans.test  did not expect new XA-START query, corrected;
    binlog.binlog_xa_prepared_disconnect failed 'cos of a previous test, its
      correctness verification clause is refined to account that;
    
    rpl.rpl_gtids_table_disable_binlog_on_slave did not expect two-phase
      logging, GTID_NEXT handler did not let to change the value after
      XA-PREPARE which must be done each [1;31mtime[m a "manual" GTID_NEXT
      prepands XA-START.  check_super_outside_prepared_trx_outside_sf() is
      introduced to refine a former GTID_NEXT check function.  Also a
      minor bug in updating GTID_EXECUTED is fixed when XA COMMIT is
      invoked from another connection (through "recovery" interface).  The
      test was extensively cleaned from all way using (incl copy-pasting)
      integer constants.
    
    compilation on embedded is fixed.
    
    Fri Feb 20 20:06:24 EET 2015
    
    Addressing Shiv's final notes dealing with cleanup
    and undoing a result file.
    
    Fri Feb 20 13:22:11 EET 2015
    
    Cleanup is do to
     - replacing xid methods with more appropriate, adding a new m_is_binlogged
       to the XID_STATE constructor initializer list;
     - dismantling st_replace_native_trx_args;
     - restoring help--no-win results.
    
    Thu Feb 19 19:05:49 EET 2015
    
    --xa_survive_disconnect option is removed;
    changes around m_native_trx_ptr that is turned into
    THD::ha_data.ha_ptr_backup to provide one-to-many association THD to
    
    SE (multiple engines in on XA transaction) rather than 1-to-1;
    removing unneeded tests, renaming in remained;
    fixing rpl_trx_boundary_parser_warning (announced but slipped from
    yesterday patch);
    
    various consmetics: empty new lines for coding standard (announced but
    slipped from yesterday patch); renaming, few cosmentics
    
    Wed Feb 18 20:08:20 EET 2015
    
    Cosmetics requested by Luis and Dmitry: renaming, adding empty line as separator throughout
    the patch,
    rpl_trx_boundary_parser_warning references to explicit log_pos
    are elminated 'cos the test started failing due to FD's grown one byte by the fixes.
    
    todo: sort out need of --xa_survive_disconnect option, raised by Luis. Upon that I'll take
    care of tests that are new-option sensitive.
    
    Wed Feb 18 09:43:15 EET 2015
    
    A bug found out in that updating slave_relay_log_info table at XA_prepare handling
    ended up in locking the table for following transactions.
    It's fixed with refining flush_info() conditions to chose the file repo branch
    for the method invocation in this XA_prepare case.
    
    Fri Feb 13 20:25:36 EET 2015
    
    Addressed
    
     - the final Marko's notes (tests related)
     - few notes from Dmitry
     - renaming a new transaction_ctx member
    
     - cleanup in trans_xa_prepare() as well as removed open questions
       to refer to new reported bug.
       In the bug report it's suggested to correct the error code and specify details
       of prepare failure.
       We won't automatically add XA ROLLBACK in this unlikely case.
    
    Thu Feb 12 14:36:29 EET 2015
    
    xa_prepared_binlog_off is fixed in the assert part
    which could not run correctly due to log_bin is OFF in this test.
    Cosmetic changes in few tests, and picking up Marko's assert.
    
    Fri Feb  6 18:13:37 EET 2015
    
    Refining the last commit's solution to syncronize the exiting connection's
    THD::cleanup with a follower connection through SELECTing from P_S.threads.
    That makes the test available wo/ DBUG, but requirs P_S.
    
    xa.h's start_recovery_xa() is made of two arguments. The new one
    carries m_is_binlogged value.
    
    Fri Feb  6 12:06:57 EET 2015
    
    Changes in tests motivated by
    1.  It turned that there's no SQL way of learning by the killer or by
        an external connection that a prepared trx of one that is supposed
        to be gone is available.  Methods based on `show processlist' or
        SELECT from I_S do *not* work!  The exiting connect is not in the
        result list, but its THD may not have been passed through
        cleanup() method.
        I worked it around with dbug_sync. Here and in other place incl other tests.
    2.  gtid asserts added to main.xa_prepared_binlog_off
        and binlog.binlog_prepared_disconnect
    
    XID::m_is_binlogged flagging is fixed. In the previous vesion the member
    may be left uninitialized.
    
    Thu Feb  5 11:44:41 EET 2015
    
    Fixing some tests (not full addressing yet) per Shiv's notes;
    making 2nd of two ser_buf_size declarations compilable everywhere,
    the issue was spotted by Dmitry.
    da
    todo: address the rest of sticking out notes, by Marko and Shiv,
          in the following commit.
    
    Wed Feb  4 22:22:26 EET 2015
    
    opt_xa_prepared_rollback_at_disconnect is renamed to
    opt_xa_survive_disconnect;
    due to semantics got inverted, its value had to be inverted in sources;
    innodb cleanup is done;
    binlog_xa_prepared_do_and_restart.inc refactored to introduce
    another sourced include, extended;
    tests renaming is done per reviewers and the renaming above.
    
    Wed Feb  4 14:40:20 EET 2015
    
    Asserting the new and old trx properties in
    innodb_replace_trx_in_thd. It's also revealed a flaw in arguments
    passing (fixed: see changes around get_slave_trx_orig());
    fixed visibility in control_events.h per Dmitry's request, and
    compilation of static const XID_t::ser_buf_size;
    perfected binlog_xa_prepared_do_and_restart.inc to invoke
    a new
      extra/binlog_tests/binlog_xa_prepare_connection.inc
    to initiate connections with certain properties.
    
    Tue Feb  3 15:38:43 EET 2015
    
    Fixing Marko's style complains, extending tests per his suggestion.
    An dbug-sync assert was found in innodb, explained and worked around.
    Potential non-determinism of XA RECOVER in tests like
    binlog_xa_prepared_disconnect is eliminated.
    
    Mon Feb  2 12:25:28 EET 2015
    
    Cleanup, merge with trunk.
    
    Addressed remained notes by Marko.
    Futher extension to xa_prepared_binlog_off and binlog_xa_prepared_disconnect
    is done to disconnect prepared XA with the server shutdown.
    
    Thu Jan 29 15:01:48 EET 2015
    
    Replaced --sleep 1 in binlog_xa_prepared and eliminated the same race
    in rpl_xa_old_logging;
    
    Found an issue and corrected XA COMMIT ONE PHASE. It's fixed
    with making XA_prepare_log_event to record the value of ONE_PHASE option.
    It case the new bool member is true the event commits the current
    group.
    Added tests for this case (rpl_xa_old,new_logging).
    
    Fixed compilation issue due to a incomplete renaming done in innodb;
    
    Extended rpl_xa_old,new_logging with random XID generation and made
    more XA transaction to disconnect.
    
    --
    TODO:
    
    Self-review, and
    hand to reviewers to finish their work.
    
    Tue Jan 27 22:35:06 EET 2015
    
    Addressing all Marko's notes.
    Adding up 2pc deregistration into the "read-only" XA prepared
    branch of innobase_close_connection() (Marko, plz assess);
    Extending testing base with adding more tests to
    extra/binlog_tests/binlog_xa_prepared.test, a common source for either
    --skip-log-bin and --log-bin top-level test files of
    xa_prepared_binlog_off and binlog_xa_prepared_disconnect.
    
    There're already notes from Dmitry who left me them on IM,
    to be addressed in tomorrow patch.
    
    TODO:
    1. replace --sleep 1 made in binlog_xa_prepared.test
       to avoid race of connection close and XA COMMIT from another connection.
    2. eliminate the same race in rpl_xa_old_logging
    3. Address Dmitry's comments and get it fully reviewed.
    
    Tue Jan 27 10:10:44 EET 2015
    
    After --skip-log-bin support opened by the previous patch an issue was revealed
    in that "read_only" prepared XA transation cleanup was not conducted properly
    to assert in innodb
         assert ( rw_trx_list.len >= n_prepared_trx).
    The "read-only" trx must've been rolled back and get to proper state.
    This patch corrects that issue.
    
    Few review notes (by Marko) are addressed as well. These include inlines
    and white space consmetics.
    
    Fri Jan 23 14:30:43 EET 2015
    
    The final notes are cleared.
    
    Fri Jan 23 12:44:04 EET 2015
    
    Addressed Neha's documentation and cleanup related comments.
    
    Thu Jan 22 12:51:15 EET 2015
    
    Addressed all points by Neha (related to WL7440).
    
    Wed Jan 21 17:05:33 EET 2015
    
    Merging with libbinlogevent is done (uint4korr() replaced).
    
    Wed Jan 22
    
    This patch merges few WL:s pushed to the trunk
    as well addresses few notes made by reviewers.
    The external WL that caused changes are wl7440, wl7592, trx_boundary_parser.
    
    This patch does not fix two items present on RB:7755 atm:
    
    TODO/FIXME: trx_t::is_recovered life [1;31mtime[m to settle down.
    
    The WL6860 agenda
    -----------------
    
    The WL addresses long [1;31mtime[m standing limitation of XA support
    in the server. When the server runs with replication (binary log) turn on
    prepared XA transaction had to be rolled back at disconnection
    Bug#11745231 (bugs.mysql.com/12161) contains the whole story of complains.
    
    This worklog adds support for XA-transactions to make them sustaining
    connection close or server restart without any harm to replication.
    For backward compatibility a new server option/global-read-only-var is introduced:
    --xa_prepared_rollback_at_disconnect
    to have the default value to comply with the rollback "old" behavior.
    
    The Worklog patch consists of
    
    - binary logging extension to write prepared XA and its Commit or Rollback decision
      as separate group of events into the binary log.
      That makes XA-binlogging possibly interleaving yet without any harm to
      data consistency after replaying on the slave.
    
    - slave applier extension to handle XA-prepared and its termination (Commit or Rollback) event
    
    - extension to XA recovery implementation in that connection closing leaves
      a prepared XA in the transaction cache as well as specially marked in
      Innodb. Such prepared XA can be discovered and terminated as the user
      wishes, as well as by the slave applier
    
    - extension to handlerton interface to add up interface allowing
      attach and detach a SE "internal" transaction from the server level transaction
      handle
    
    - augmentment to connection close logics in Innodb as well as
      changes to maintain disconnected transaction's state sane to survive
      the server restart.
    
    Conflicts:
            sql/xa.h

[33mcommit 5186f397f48c0615fba4529f59687c33635dc1b9[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu Mar 5 14:56:04 2015 +0100

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - 5.7 outputs additional columns from EXPLAIN since EXTENDED and PARTITION
       been turned on by default.
        1) This causes the 'rows' column to shift from position 9 to 10
        2) Also it introduces the new column 'filtered' which is to be
           considered as unreliable as 'rows'. Quoting comment in existing
           test case "experience has shown that 'rows' estimates may vary
           depending on OS, compiler and .. phase of moon."
     - Fix problem by increasing the column number in masking of 'rows' and at the
       same [1;31mtime[m add masking of the 'filtered' column.

[33mcommit d90df451fea2385450314138ffc4a70232a710e6[m
Author: Marek Szymczak <marek.szymczak@oracle.com>
Date:   Thu Mar 5 11:50:35 2015 +0100

    WL#8326 Make NO_AUTO_CREATE_USER sql_mode behavior the default in 5.7
    
      * NO_AUTO_CREATE_USER mode added to the default set (compiled-in) of the sql_mode global variable.
      * Server reports missing NO_AUTO_CREATE_USER mode of the sql_mode variable during boot [1;31mtime[m.
        The initial value of the sql_mode is stored in the configuration file (*.cnf).
      * Switching on and off NO_AUTO_CREATE_USER mode of the sql_mode variable reports warning.

[33mcommit db9d035e6431e1cc9e492394c2cf24cc5cdac7f4[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed Mar 4 14:44:27 2015 +0100

    Bug#17766653 CRASH IN ADD_KEY_FIELD Bug#20558891 HANDLE_FATAL_SIGNAL (SIG=11) IN QEP_SHARED_OWNER::KEYS
    
    Problem:
    SELECT a.a FROM `t1` `a`, t1 b
    HAVING 1 NOT IN (SELECT a.a FROM `t1`);
    - We resolve the clauses of the subquery: an Item_field is created
    for 'a.a', with depended_from=select#1.
    - the subquery is inside HAVING, and as usual all columns used by
    HAVING must be Item_ref, so an Item_ref is created, with
    depended_from=select#1; this Item_ref then goes through fix_fields()
    which finds 'a.a' is also present in the SELECT list of select#1 and
    thus makes Item_ref wrap the Item_field of the SELECT list of
    select#1, which has depended_from=NULL. Note that the first Item_field
    is thus dropped.
    - So far so good.
    - in-to-exists transformation then wants to build an equality of the
    form: left_expr==right_expr; when it does so, it uses
    right_expr->real_item() for the right side of the equality; right_expr
    is the Item_ref but its real_item() is the SELECT list element of
    select#1, which has depended_from=0.
    - thus, with this real_item(), we end up with 'a.a' on the right side,
    in the WHERE clause of the subquery, with depended_from=0
    i.e. considered as a non-outer, local column, which is wrong.
    - top query is completely optimized
    - In optimization of the subquery, we then look for Keyuse-s for this
    column, which is absurd; add_key_field() reaches to reginfo.join_tab
    which is NULL (because the outer query has already been optimized, and
    WL#6042 zeroes the join_tab pointers at the end of optimization), and
    we get a problem.
    The root cause is: we use real_item().
    
    Fix:
    - I first tried to use the solution of Bug 18014565
    (substitutional_item() at line 1996 of item_subselect.cc);
    but it does not solve the problem in ps-protocol mode; indeed in that
    mode, the Item_ref being a rollbackable one (run[1;31mtime[m-created),
    substitutional_item() is equal to real_item()
    - removing real_item() isn't a solution either, for reasons explained
    in new comments in item_subselect.cc
    - So, until wl#6570 is implemented, two if()s are added, which are
    sufficient to fix all testcases of the two bug reports.
    
    (cherry picked from commit 9567b5cd4b69fae241ec16436a3c40748646fbea)

[33mcommit fc433b3ad5125c1063095ef9bbb604c028f4f2bd[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Tue Feb 24 13:59:08 2015 -0600

    Bug #20595292   INNODB: MTR TEST 16K FAILING SPORADICALLY
    
    The MTR test '16k' is failing on weekly-trunk is about 4 different ways.
    The test needs to be made more deterministic.  The actual index ID, table ID
    or space ID does not matter if there is any likelihood that it might be
    different when this test is run on a database that had previously run various
    other tests.
    
    Change the I_S queries in this test, as well as similar queries in 8k.test
    and 4k.test so that the results are predictable.
    
    In addition, it has been observed that some test fail when they query
    I_S.INNODB_SYS_DATAFILES where path like 'test' because the word
    test can some[1;31mtime[ms occur earlier in the path, depending on the system
    it runs on and the test parameters such as embedded.  So every innodb
    test that does that query is replaced with:
    --source suite/innodb/include/show_i_s_tablespaces.inc
    and when they also query SYS_TABLES, this test is also used instead:
    --source suite/innodb/include/show_i_s_tables.inc

[33mcommit 99290a77a9d9188f201d80aff8fa9c8b4cea17d6[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Tue Mar 3 18:28:58 2015 +0100

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - 5.7 outputs additional columns from EXPLAIN since EXTENDED and P
       been turned on by default.
        1) This causes the 'rows' column to shift from position 9 to 10
        2) Also it introduces the new column 'filtered' which is to be
           considered as unreliable as 'rows'. Quoting comment in existing
           test case "experience has shown that 'rows' estimates may vary
           depending on OS, compiler and .. phase of moon."
     - Fix problem by increasing the column number in masking of 'rows' and at the
       same [1;31mtime[m add masking of the 'filtered' column.

[33mcommit d9b81fb51387b8aee5b7bf6a343a4cbdd381257d[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Feb 27 12:32:06 2015 +0100

    Bug#20460208: !table || (!table->read_set || bitmap_is_set)
    
    The optimizer relies on thd->mark_used_columns to have the value
    MARK_COLUMNS_READ for columns in WHERE conditions, etc.
    If this is not the case, fields are some[1;31mtime[ms erroneously marked for
    write or not marked at all. In particular, if a failed statement of
    some "wrong" type preceded a correct statement, this problem might hit.
    In the bug report, a failed INSERT statement is followed by a DELETE
    statement.
    
    Followup patch: Make sure that we start all executions of prepared
    statements too with thd->mark_used_columns = MARK_COLUMNS_READ.

[33mcommit 2f7227db7567a8858495074e8ceb915cd0f54ddd[m
Author: Bjorn Munch <bjorn.munch@oracle.com>
Date:   Thu Feb 26 15:24:58 2015 +0100

    Adapting result file. For some reason a warning message is output later
    by mysql_upgrade than it does in mysql-trunk-wl8350. No [1;31mtime[m to find out why.

[33mcommit f14aff254d8a63625be0071ab746380823d0fcee[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Wed Feb 25 14:17:21 2015 +0100

    Bug#20600847: COMP_ERR CRASHES WHEN ADDING MORE ERROR MESSAGES
    
    The number of error messages has exceeded 1000, but comp_err is only
    prepared for 1000 messages. This causes crashes at build [1;31mtime[m on some
    platforms.
    
    Increase MAX_ROWS from 1000 to 2000 in extra/comp_err.c to allow up to
    2000 error messages. Also add a check that prints an error message and
    exits, instead of crashing, if the number of error messages exceeds
    MAX_ROWS.

[33mcommit 86ce784cfa959dbc9511abc42f9556cec66a7646[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Wed Feb 25 22:47:49 2015 +0530

    Bug #11747810: EXPLAIN EXTENDED SHOWS BOGUS VALUE FOR 'FILTERED'
                   COLUMN FOR LIMIT QUERY
    
    Problems:
    a- in EXPLAIN, "filtered" column could show values >100%.
    b- QEP_shared::m_rowcount is often a copy of
    POSITION::rows_fetched, but in rare cases it's something else; it's
    not well-defined
    c- it's only used by EXPLAIN, which itself has complex logic to decide
    what to show in "rows"/"filtered", depending on the access method.
    d- post-greedy-search optimizations (like test_if_skip_sort_order,
    adjust_access_methods and make_join_select)
    change the access method without changing rows_fetched, or without
    changing POSITION::filter_effect. Leads to displaying an obsolete
    value in EXPLAIN.
    e- test_if_skip_sort_order, when it switches to index scan, and sees
    there's a small limit, decides to show this limit as "rows" in
    EXPLAIN; it is the root cause of the values >100%.
    This assignment to QEP_shared::m_rowcount is also the only use
    of this variable in the phase before we have QEP_TABs, so it is
    forcing us to share this variable between JOIN_TAB and QEP_TAB.
    f- because for an index, table or range scan,
    rows_fetched is usually set by calculate_scan_cost (called by
    best_access_path), thus
    includes filtering by constant conditions and filter_effect
    doesn't include this filtering, make_join_readinfo() had to do
    compensation calculations so that in EXPLAIN the user sees constant
    conditions in "filtered" and not in "rows"; however, some[1;31mtime[ms
    rows_fetched was modified by (d), which didn't include constant
    condition filtering, then the compensation calculation did wrong.
    
    Fix:
    - for (b) QEP_shared::m_rowcount is removed and rows_fetched is always
    used, instead; as a consequence, EXPLAIN logic is simplified and (c)
    solved. A side-effect of this simplification is that
    semijoin-materialized tmp tables show "filtered=100" instead of
    "filtered=0", which is an improvement.
    - test_if_skip_sort_order() retains its logic, but we use rows_fetched
    instead of row_count. We also set the filter_effect to magic value
    of -2 stating that filter effect need not be calcuated for
     constant-condtion-filter. make_join_readinfo(), with more sane
     decision logic, solving (e) and (a).
    - post-greedy-search logic now sets filter_effect to a magic value
    (=-1), if it changes the access method; in make_join_readinfo() this
    triggers a complete recalculation of filter_effect, if we're
    explaining the statement. Solves (d).
    - post-greedy-search logic doesn't anymore change rows_fetched, so
    that constant-condition-filter compensation in make_join_readinfo()
    always works, as it is now sure that rows_fetched is always from
    best_access_path: the new rows_fetched is determined depending on the
    latest access method (chosen by post-greedy-search logic), and
    filter_effect is compensated. Solving (f). IF this fix is a problem
    for some reason, if post-greedy-search logic wants to set
    rows_fetched, then it is probably possible to do the filter_effect
    compensation right after greedy-search. Assuming post-greedy-search
    logic prefers to have, as input, rows_fetched without constant
    condition.
    
    Result files changes are described below; IMHO they are all correct.
    
    distinct.result
    explain SELECT distinct a from t3 order by a desc limit 2;
    rows=3 becomes 204, as test_if_cheaper_ordering calcultes the
    select limit wrongly. IMHO it is the problem that needs a fix
    in test_if_cheaper_ordering as it calculates select limit w.r.t
    the estimate from range optimizer. For this case range optimizer
    gives an estimate of 3 which is thought to be the number of qualifying
    rows after range is satisfied. But in this case it should have been
    204 as there is no range in this query.
    
    explain_for_connection*
    changes look reasonable, however some of them are difficult to
    inspect; the goal of these tests are to check equality between EXPLAIN
    and EXPLAIN CONNECTION, so if both plans change it's ok.
    Some changes are an effect that filter recalculation is done only by
    normal EXPLAIN.
    
    explain_other.result
    using LIMIT was wrong. For the last change:
    1       SIMPLE  t1      NULL    range   a       a       11      NULL    3       100.00
    ("rows" was 1), it's because we simplified the calculation of rows in
    opt_explain.cc for dynamic range; there's no best value to show, as
    during execution we switch back and forth between index scan (fanout
    3) and range scan (fanout 1).
    
    filter_single_*
    rows=LIMIT(=1) is correct.
    
    innodb_mrr*
    1       SIMPLE  t1      NULL    index   ind_parent_id   PRIMARY 4       NULL    7       57.14   Using where
    yes 57.14 is the filtering effect of IS NOT NULL (4 rows out of 7
    match this, if you look at the table's data).
    Line 732: another effect of EXPLAIN simplification, for
    ref-using-quick-with-filesort (see comment in opt_explain.cc)
    
    myisam_mrr*
    see innodb_mrr
    
    subquery_all*:
    again ref-using-quick-with-filesort
    
    subquery_sj*
    0->100 is good; for 26.53 to 19 it's again the change for "range
    checked for each record".

[33mcommit 8833752e45a13421b89c755dcbfc06f61ff481b6[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed Feb 25 13:23:50 2015 +0100

    This commit is a followup to the fix for bugs 20069617 (consider
    noofreplicas in dynamic max_failure_[1;31mtime[m calculation) and 20069624
    (allow the 120s constant part of the gcp_save gcp_stop [1;31mtime[mout to be
    configured).
    
    This commit fixes an error in the previous commit that caused a crash if
    the number of live data nodes reached zero.

[33mcommit b3e15e55f74a029b2803ad6b13149d1ea9ff9318[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Mon Feb 16 11:44:26 2015 +0000

    WL#8320: Lower default slave_net_[1;31mtime[mout in 5.7
    
    Make the following compiled-in default changes:
    slave_net_[1;31mtime[mout=60

[33mcommit b5eb4a152a8d75718feeccf61b0ecf1abe3f3ad7[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Feb 24 09:14:51 2015 +0100

    This commit is a fix for bugs 20069617 (consider noofreplicas in
    dynamic max_failure_[1;31mtime[m calculation) and 20069624 (allow the 120s
    constant part of the gcp_save gcp_stop [1;31mtime[mout to be configured).
    
    The patch makes the following changes:
    * The previously fixed 120s 'extra' [1;31mtime[m for GCP_SAVE is made configurable,
      with a minimum value of TimeBetweenEpochsTimeout.
    * The formula for calculating the maximal [1;31mtime[m to detect the failure of a
      set of nodes has been changed in two ways:
      1) The [1;31mtime[m needed to detect heartbeat failure is set at five rather than
         four heartbeat intervals, as up to five intervals may be needed.
      2) Only one round of arbitration is considered, rather than
         (no_of_nodes - 1), as there can be at most one (at least for
         NoOfReplicas<=2).
    
    The patch also adds a regression test.

[33mcommit 611f19c8a37aa05305fe7bf35551a96dfa6b6cef[m
Author: Thirunarayanan Balathandayuthapani <thirunarayanan.balathandayuth@oracle.com>
Date:   Fri Feb 20 11:10:35 2015 +0530

    Bug #20527363   TRUNCATE TEMPORARY TABLE CRASH:
                    !DICT_TF2_FLAG_IS_SET(TABLE, DICT_TF2_TEMPORARY)
    
    Problem:
    
       Server asserts while truncating the temporary tables.
    
    Reason:
    
    If temporary table object is evicted from data dictionary cache during its
    life[1;31mtime[m on reload from SYS_TABLES object is not full constructed as it was
    when it was created. This is because not all fields of temporary tables are
    persisted to SYS_TABLES and dir_path_to_temp_table is one such field which
    on reload is set to NULL. Making any decision based on such fields can result
    in inconsistent output. Truncate code was trying to do the same.
    
    Reviewed-by: Krunal Bauskar <krunal.bauskar@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    RB: 8081

[33mcommit 1b281ed15c9b30cb476a1cb6ebf873d3826ae90e[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Mon Feb 16 16:46:33 2015 +0530

    BUG#20545721 - MISSING CLOSE() CALL IN MYSQLD_DAEMON.CC
    
    PROBLEM DESCRIPTION AND FIX:
    
    close(stdinfd) is missing in mysqld::run[1;31mtime[m::mysqld_daemonize().
    
    Fix is close(stdinfd) on line 102 before fork in the if part
    as well to avoid warnings being reported by fortify.
    
    (cherry picked from commit 37dec2e644d0a1357669cd89e786740526782afb)

[33mcommit f86e196b15e5d058a134c07308b134bbdd49e504[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Mon Feb 16 15:07:46 2015 +0100

    Make some tests "no valgrind without big", to reduce turnaround [1;31mtime[m.
    
    binlog.binlog_enforce_gtid_consistency_create_select_consistent
    binlog.binlog_enforce_gtid_consistency_create_select_violation
    binlog.binlog_enforce_gtid_consistency_tmp_consistent
    binlog.binlog_enforce_gtid_consistency_tmp_violation
    binlog.binlog_enforce_gtid_consistency_trx_nontrx_consistent
    binlog.binlog_enforce_gtid_consistency_trx_nontrx_violation
    binlog.binlog_gtid_state_update_deadlock
    rpl.rpl_no_gtid_split_statements
    rpl.rpl_trx_boundary_parser_warning
    main.partition_pruning
    
    Approved by Anitha Gopi <anitha.gopi@oracle.com> over e-mail.

[33mcommit 830e0da07fbc873333d37ac667a932f9d4794d54[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Mon Feb 16 16:46:33 2015 +0530

    BUG#20545721 - MISSING CLOSE() CALL IN MYSQLD_DAEMON.CC
    
    PROBLEM DESCRIPTION AND FIX:
    
    close(stdinfd) is missing in mysqld::run[1;31mtime[m::mysqld_daemonize().
    
    Fix is close(stdinfd) on line 102 before fork in the if part
    as well to avoid warnings being reported by fortify.

[33mcommit 75445f85251e16be493a76ebe9c2a5cb72f3fb4e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Feb 11 08:23:00 2015 +0100

    Add missing failure detection for 'testBasic -n Bug54986'.
    
    The test continued on after some operation failed,
    which finaly caused it to report 'match failure' as the
    DB did not contain what the test framework expected to find.
    
    Still expect test to fail after this fix, but now it will
    hopefulle be possible to pinpont the point in [1;31mtime[m when it failed.

[33mcommit 9a0c05308ab54fa486ce4752cc5b1d4d6a40c632[m
Author: V S Murthy Sidagam <venkata.sidagam@oracle.com>
Date:   Fri Feb 6 22:43:07 2015 +0530

    Bug #20476596 HIDE UN-DOCUMENTED SYMBOLS FROM LIBMYSQLCLIENT.SO
    
    Description: Consider a requirement where there the OpenSSL
    and MySQL client library functions are both used directly
    to generate a binary. If the order of linking used is
    1. MySQL client library (i.e libmysqlclient) and then
    2. OpenSSL library (i.e libcrypto),
    then using the created binary will result in a crash.
    
    Analysis: The libmysqlclient library already has the
    built-in yaSSL library and has exported its symbols.
    The yaSSL and OpenSSL libraries share some of the same
    symbol names, so the executable which is prepared in the
    above said order, will resolve the OpenSSL symbols
    indirectly using the libmysqlclient library (yaSSL) rather
    than using the OpenSSL library directly. Hence, the binary
    will try to call functions defined in the libmysqlclient
    library instead of the desired OpenSSL function, thus
    resulting in a crash.
    
    Fix: We are maintaining two list
    1)CLIENT_API_FUNCTIONS which has documented symbols.
    2)CLIENT_API_FUNCTIONS_UNDOCUMENTED which has undocumented
      symbols.
    For [1;31mtime[m beging we will export the symbols which are part of
    above two lists.
    Symbols which are removed from this existing list are
    load_defaults, myodbc_remove_escape and mysql_embedded.
    Symbols which are added newly to the list are
    my_load_defaults, mysql_send_query
    and mysql_read_query_result.
    We need to update the document for some of these symbols.
    Rest of all symbols are hidden from libmysqlclient.so
    
    For Windows: To sync with other OS exported symbol list.
    I have auto generated the libmysql.def with the symbols
    of CLIENT_API_FUNCTIONS and
    CLIENT_API_FUNCTIONS_UNDOCUMENTED lists.
    (Added: get_tty_password, handle_options, my_load_defaults,
    mysql_session_track_get_first, mysql_session_track_get_next
    and mysql_stmt_next_result.
    Removed: load_defaults, myodbc_remove_escape and
    mysql_embedded)

[33mcommit b086fdac74f7e55259e487d383f2ebd7aa4128a1[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Feb 6 16:22:17 2015 +0100

    Bug#19678930: WRONG STACK SIZE CALCULATION LEADS TO
                  STACK OVERFLOW IN PINBOX ALLOCATOR
    
    The problem was that the allocator used by the lock-free container
    implementations misused alloca() when scanning for objects which
    could be safely deallocated. The code tried to calculate how much
    memory could safely be allocated on the stack for qsort based
    pre-sort, but these calculations were not accurate.
    
    This could manifest itself as stack overflow errors with high
    number of concurrent connections to the server. (e.g. 8000
    connections with default 256Kb stack size).
    
    This patch fixes the problem by removing the use of alloca
    and replacing qsort approach by a linear scan through all
    pointers (pins) owned by each thread. We remove each
    such pointer from the list of pointers to objects the current
    thread has marked as unused (purgatory). At the end we have
    a list of pointers to objects not in use by any thread. Since
    there are few active (non-null) pointers at any given [1;31mtime[m,
    this turns out to be quite efficent.
    
    The patch also fixes a bug in the code determining when
    scanning for unused objects should be performed. Instead
    of doing this scan only at certain intervals (for perf reasons),
    the old code did this scan almost always.
    
    Micro-benchmarks show that these two changes give a
    a clear performance improvement over the old approach.
    
    Some #include directives in the LF code was also changed
    so that they only use <> for system headers. This reduces
    the likelyhood of conflicts between system headers and
    MySQL headers with generic names (e.g. hash.h).
    
    The patch also reduces usage of thread-local storage and is a
    step towards implementing WL#6817.
    
    Based on patch written by Dmitry Lenev.

[33mcommit 1d05b689c09656529ac76f0bca546b0bc8c4ad8f[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Fri Feb 6 19:42:12 2015 +0530

    WL#6986: Make switching of index due to small limit cost based
    
    This worklog makes the optimization of queries of the form
    "SELECT .. ORDER BY LIMIT"  cost based.
    
    Problem:
    The decision to use an index that gives ordered results is not
    cost based in all cases when optimizing "order by limit" queries.
    This can result in a bad query plan for some queries.
    Analysis:
    Using the following example query from Bug#16522053
    SELECT patientId, [1;31mtime[m
    FROM t1
    WHERE patientId > 41 AND patientId < 43 AND [1;31mtime[m > 0 AND
    illness="Headache" ORDER BY [1;31mtime[m LIMIT 1;
    
    1. During the initial access method estimations 5.6 selects
    the same index and access strategy as 5.5 as the best:
    Index merge on (patientId, primary key)(note that this
    strategy will require file sort of the final result)
    
    2. During join optimization this access method is kept as
    the best access method.
    
    3. After join optimization, the optimizer sees if there
    are further opportunities for optimizing, and in this case
    there is: The query contains the following:
    order by [1;31mtime[m limit 1;
    
    Since there is a "limit 1", we check if there is an opportunity
    to avoid having to do both the complete join execution and
    the file sort. And in this case there is one such possibility:
    by using the KEY [1;31mtime[m ([1;31mtime[m).
    
    we "only" need to read entries from this index until the join
    has produced the first result record. So the optimizer decides
    to use this index for the first table of the join.
    
    In most cases this is a very good optimization since we both
    avoid the complete join and the file sort of the result.
    Unfortunately, in this case it seems like the query will produce
    zero results. As a consequence we will read the entire [1;31mtime[m index
    to its end - and for each index entry we need to look
    up the record in the clustered index.
    The difference between 5.5 and 5.6 is that in 5.5 many/most
    cases failed to select an index that could be used to avoid
    the sorting. This bug has been fixed in 5.6. And in a case
    like this one, this fix made the bad decision.
    
    Solution:
    test_if_cheaper_ordering() uses a cost based model to choose
    the index for ordering when limit is specified.
    Use the same in make_join_select() instead of the heuristic
    based decision which is present right now.
    
    Test file changes for myisam_explain_non_select_none, etc
    is because, the test requires to see that the same plan is chosen
    for DELETE and SELECT. With the previous queries, this is what is
    happening w.r.t select queries.
    Optimizer thinks that doing table scan is much cheaper
    than doing the range scan initially. But, later
    when we re-consider the plan because of small limit, even with the
    changes made to test_if_cheaper_ordering, cost is more than
    the cost for table scan.
    As a result, we do not create a range scan.
    But earlier, because cost based decision was not in picture,
    optimizer used to create a range scan because the key gives the required
    sort order.
    
    As a result, limit value had to be changed to get range scan picked
    over table scan.
    Same for single_delete_update.test.
    
    This worklog only implements the cost based handling for select queries.
    Most single table insert/delete/update statements takes a simpler path
    through the optimizer. For these queries the choice of using an
    "order by limit" supporting index is still not cost based in some cases.
    
    Existing issues that are not solved by this worklog:
    1. Cost calculation in test_if_cheaper_ordering() does not take into
    consideration the rows_evaluate_cost after the table scan/index scan
    is done. This will make a difference when the key also has a where
    condition that can filter many rows.
    2. Along with this, we also need to consider the cost for file sort, which
    is currently not taken into consideration.
    3. The current cost model in test_if_cheaper_ordering does not
    take into consideration that, when the key that gives order
    also has a range condition, then the total number of records
    that need to be scanned will be the number of rows that
    satisfy the range condition, not the number of records
    present in the table.
    This can be considered during cost calculation as, we are eventually
    going to create range scan if this is the case.
    4. select_limit is not reset after checking for an index. This can
    lead to bugs when there are multiple indexes for order by
    5. Check for a covering index is not complete. In case of innodb
    a secondary index always has access to primary key fields.

[33mcommit cada5608a0355bf0c1a36f1cc03da5a90f65e1a6[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Wed Feb 4 17:40:54 2015 +0000

    BUG#19982543 SERIOUS TPS DECLINE IF GTID IS ENABLED IN 5.7
    
    The GTIDs of transactions committed in group weren't added to
    GTID_EXECUTED in order. Because of this, some[1;31mtime[ms it created
    temporary gaps in the GTID_EXECUTED which lead to more contention
    than necessary.
    
    When this gaps happen in GTID_EXECUTED, the server would have to add
    and remove intervals from the Gtid_set, and adding and removing
    intervals requires a mutex, which would cause contention.
    
    Fix: moved the code that handled the GTIDs to a place where it will
    be added to GTID_EXECUTED in the same commit order, without gaps.

[33mcommit 57d8d00578b55990d3d4f608d5b319b681b3ea87[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Tue Nov 18 16:10:58 2014 +0100

    WL#7083 step 3.8. GTID_MODE settable: Fix tests that change GTID_MODE.
    
    Some existing tests set gtid_mode using include/rpl_set_gtid_mode.inc,
    and some set gtid_mode using include/rpl_restart_server.inc, and some
    use --gtid-mode on the command line.
    Before this patch, rpl_set_gtid_mode.inc requires a restart.
    This patch changes the following:
    - rpl_set_gtid_mode.inc does not restart.
    - Tests that call rpl_set_gtid_mode.inc because they need to use
    gtid_mode=on all the [1;31mtime[m use include/have_gtid.inc instead
    - Tests that use rpl_restart_server.inc only to change gtid_mode now use
    rpl_set_gtid_mode.inc instead.
    - Removes --gtid-mode from the command line.

[33mcommit 9a6082e5027a1752e42bb5d537f9e0f297cdb6c4[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Jan 29 22:44:54 2015 +0100

    WL#7083 step 3.1. GTID_MODE settable: Simplify checks in sys_vars.cc.
    
    This simplifies some of the check_* functions in sys_vars.cc:
    - No variable needs check_top_level_stmt or check_top_level_stmt_and_super.
    - 'top-level statement' is not a very clear term, since it some[1;31mtime[ms
      includes both stored functions and stored procedures and some[1;31mtime[ms only
      one of them. Made more explicit in function names what is checked.
    
    Doing this simplifies following patches that modify checks for
    GTID-related variables.

[33mcommit 35acaf8dfd5d4bd0325d2d9cfb05ba56352b93ac[m
Author: pedro.gomes <pedro.gomes@oracle.com>
Date:   Thu Feb 5 18:52:13 2015 +0000

    Bug #20480024: OLD VIEW CHANGE EVENTS KEEP REAPERING ON RECOVERY ROUNDS
    
    On recovery sessions, old View Change Log Events were not skipped so
    they would appear every [1;31mtime[m in a node binlog whenever it leaves and
    joins again.
    To avoid this, VCLEs are now only written on the nodes that generated
    them.

[33mcommit 2c2228ee786b2f79f0494aad6d555bf7396dc424[m
Author: Allen.Lai <zheng.lai@oracle.com>
Date:   Thu Feb 5 17:14:13 2015 +0800

    Bug#20381342 EXPIRATION TIME IGNORED
    
    This bug is caused by the expired [1;31mtime[m set to a interval value, not
    a [1;31mtime[m value.
    For example, if the expired [1;31mtime[m is set to 60 secs, which means after 60
    secs from current [1;31mtime[m, the item will be expired.
    So, the expired [1;31mtime[m of item should be current [1;31mtime[m + 60 secs, not 60
    secs.
    
    Reviewed-by: Jimmy Yang<jimmy.yang@oracle.com> on IM.

[33mcommit 6dc19dc78c35c75202669989e8baaa6894d4b6bf[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu Feb 5 14:16:43 2015 +0530

    - Bug#20422255: SOME RQG TESTS FAIL WITH TIMEKEEPER TIMEOUT (STATUS 110) SINCE 2015-01-06
    
      - After switching to use InnoDB as default SE (WL#6737) there is increase in
        debug execution [1;31mtime[m for some of the test-cases.
      - Good-part is release-execution [1;31mtime[m has been reduced (we saw 25% improvement
        for the test-case mentioned in the bug)
      - Increase in debug [1;31mtime[m came from the extra debug check that InnoDB performs
        so in short nothing wrong with the increase [1;31mtime[m.
      - But in general course given that we need to run pb2 on daily basis it would
        be good to reduce the debug-execution-[1;31mtime[m. If not for all cases at-least
        for tables created by Optimizer.
      - With that aim we evaluated the code and found that buf_validate is top-most
        costly function in the case mentioned and probably in all such cases.
        To suppress cost of this we have turned it off for temp-tablespace
        (of-course with some configurability.)
    
      Reviewed by: Sunny Bains <sunny.bains@oracle.com>
      RB: 7934

[33mcommit 836d4686e0d908c253c3e847e62762abce3087d8[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Wed Feb 4 13:52:21 2015 +0100

    WL#4807: Separate partitioning interface from handler to Partition_handler
    
    Refactoring of partitioning interface by moving partitioning specific
    functions of class handler to an own class Partition_handler.
    
    Also refactoring the generic partitioning engine ha_partition,
    by moving reusable code into a new class Partition_helper so it can
    be used by other partitioning aware engines, such as native innodb
    partitioning (wl#6035).
    
    Includes removing #ifdef WITH_PARTITION_STORAGE_ENGINE in server code,
    so that parsing etc. always support partitioning and it is engine
    support that determines if partitioning is supported or not.
    
    Refactoring ha_partition so that it can be built as a plugin,
    to show it is separated from the server code. Also moved the code
    to storage/partition directory.
    
    rb#7546
    Approved by Mikael Ronström,
    run[1;31mtime[m parts approved by Dmitry Lenev,
    ndb parts approved by Magnus Blåudd

[33mcommit b8362f8f51a90ceadc3e0e6f8063db090108f8ae[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Feb 3 10:46:07 2015 +0100

    Bug#20460208: !table || (!table->read_set || bitmap_is_set)
    
    The optimizer relies on thd->mark_used_columns to have the value
    MARK_COLUMNS_READ for columns in WHERE conditions, etc.
    If this is not the case, fields are some[1;31mtime[ms erroneously marked for
    write or not marked at all. In particular, if a failed statement of
    some "wrong" type preceded a correct statement, this problem might hit.
    In the bug report, a failed INSERT statement is followed by a DELETE
    statement.
    
    This is a quick fix that should provide safeguards for DELETE and
    UPDATE statements. It sets mark_used_columns to MARK_COLUMNS_READ
    when needed and restores the old value after use.
    
    A more thorough fix with complete safeguard will follow later.

[33mcommit 720dd1e71bd0b0fbcbbdb957a9630261bee75089[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Jan 30 14:16:39 2015 +0200

    Fix Bug#20445205 VALGRIND FAILURE AFTER WL#5889
    
    The stage object is allocated in ha_innobase::inplace_alter_table() and
    freed in ha_innobase::commit_inplace_alter_table(). The commit method
    will always be called following a call to
    ha_innobase::inplace_alter_table().
    
    But for partitioned tables ha_innobase::inplace_alter_table() is called
    multiple [1;31mtime[ms, once for each partition, while
    ha_innobase::commit_inplace_alter_table() is called just once at the
    end. This causes the calls to ha_innobase::inplace_alter_table() after
    the first to just assign to the stage object, leaking the old contents,
    left from the previous call.
    
    The solution to this is to free the stage object before assigning to it.
    
    Approved by:    Marko (on the bug report page)

[33mcommit 6ebef7dab8a1ca9153027f459e17bb7f58402461[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jan 29 18:18:49 2015 +0100

    Bug#20422145 DO ABI CHECK FOR SERVICES SEPARATELY FROM PLUGINS
    
    Post-push fix: mysql/plugin.h is used during cmake [1;31mtime[m when
    configuring windows-authentication-plugin.
    At that [1;31mtime[m "my_config.h" is not available, so dont include it.

[33mcommit 50d903d54d841f6bfb0aefa8641e47d8eba6da24[m
Author: Libing Song <libing.song@oracle.com>
Date:   Thu Jan 29 12:35:29 2015 +0800

    BUG#20400518 SQL_SLAVE_SKIP_COUNTER WITH CHANNELS CAN BRING
                  INCONSISTENCY IN RPL
    
    When setting sql_slave_skip_counter, the value is applied to all
    channels. So when starting channels, all started appliers will
    go into skip process. And the appliers will decrease the global
    sql_slave_skip_counter after skipping a event. The logic is not
    correct and it will cause problem if two or more channels are
    starting at the same [1;31mtime[m.
    
    To fix it, the be haviour of skipping event is change to:
    - Setting sql_slave_skip_counter doesn't effect any running applier.
    - Value of sql_slave_skip_counter only effects the first applier
      starting after 'SET GLOBAL sql_slave_skip_counter.
    - The applier copies the value to a local variable and just decrease
      the local variable.
    - The value of sql_slave_skip_counter will be reset to 0 after
      the applier copies it. So if users need to skip some events
      on another channel, then they they need to set sql_slave_skip_counter
      again.

[33mcommit cfc0d33b6c3162ae49b18564c9d6b98344f1b283[m
Author: Dyre Tjeldvoll <Dyre.Tjeldvoll@oracle.com>
Date:   Wed Jan 28 10:37:26 2015 +0100

    BUG#19674349: MAIN.USER_LOCK TEST IS FAILING ON SOME 32 BIT PLATFORMS
    
    Problem: On some 32-bit platforms GET_LOCK('test', -1) function immediately
    returned due to [1;31mtime[mout instead of waiting for user-level lock.
    
    In set_[1;31mtime[mspec_nsec(struct [1;31mtime[mspec *abs[1;31mtime[m, ulonglong nsec) an
    ulonglong is assigned to a [1;31mtime[m_t without checking for overflow. In
    GET_LOCK('test', -1) this function is called with a value that results
    in a seconds component which exceeds what can be represented by a 4
    byte signed [1;31mtime[m_t. Consequently, we get negative value in
    [1;31mtime[mspec::tv_sec which causes immediate return instead of expected
    "infinite" wait.
    
    Solution: For platforms where SIZEOF_TIME_T is less than
    SIZEOF_LONG_LONG truncate the number of seconds so that it fits in a
    [1;31mtime[m_t.

[33mcommit 43dceffd5d641909fdbaebba7160abc01680008b[m
Author: David.Zhao <david.zhao@oracle.com>
Date:   Wed Jan 28 17:07:17 2015 +0800

    WL#7224 : Spatial Relation Check Functions Depending on Boost Geometry Extensions
    
    Implement the rest of parameter type combinations of each spatial relation check
    function using Boost.Geometry. Such combinations had to rely on old GIS algrithm
    because the older version of Boost.Geometry lacked support for them.
    
    We did so by calling Boost.Geometry algrithm functions directly if available,
    or indirectly by implementing the check algorithms using other BG functions,
    and we used boost.geometry rtree index as necessary for better efficiency.
    
    Empty geometry collections are well supported as arguments for any
    spatial relation check functions. Normal geometry collections are supported in
    all spatial relation check functions except ST_Touches, ST_Overlaps and ST_Crosses.
    
    Now no MySQL GIS relation check functions rely on old GIS algorithms anymore.
    
    Implemented the following relation checks with an algorithm of O(NlogM) [1;31mtime[m complexity,
    where N and M are the number of components of the multipoint and the other multi-geometry
    respectively.
    
            multipoint disjoint multipolygon
            multipoint disjoint multilinestring
            multipoint within multipolygon
            multipoint within geometry-collection

[33mcommit f6479084648fae1a33e6c2a5ee87e03c2a1b1468[m
Author: Robert Golebiowski <robert.golebiowski@oracle.com>
Date:   Fri Jan 23 09:38:18 2015 +0100

    Bug #20292712 HEAP-USE-AFTER-FREE WHEN CONCURRENTLY INCLUDING/EXCLUDING
    ACCOUNTS
    
    Analysis:
    Crash when concurrently setting audit_log_include_accounts and
    audit_log_exclude_accounts. This crash was caused by freeing
    audit_log_exclude_accounts variable twice. Once it was freed in the
    check function when audit_log_include_accounts was being set (setting
    this variable causes audit_log_exclude_accounts to be freed). The second
    [1;31mtime[m it was freed when setting audit_log_exclude_accounts itself (the
    old variable after setting is freed). This only could happen in specific
    concurrent case.
    Fix:
    I moved setting audit_log_exclude_accounts into update function of
    audit_log_include_accounts. This makes sure that this variable cannot be
    updated concurrently as this pice of code is guarded by mutex. Also I
    have moved all code which modifies hashes from check functions to the
    update functions.

[33mcommit a58f1807e53633b4a5033763d54687c1ee20a180[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Thu Jan 22 17:46:23 2015 +0530

    BUG#19667566 :
    PARTITION BY WITH COMMENT LEADS TO MYSQL SERVER REBOOT IN MYSQL CLUSTER
    
    If the alter table query has comments and partition clause, it throws
    segfault and crashes the mysqld server.
    
    The server calls the handler's can_switch_engines() function to check if
    switching engines is allowed when there is an alter table switch engine
    or alter partition query. But NDB has native partitioning, so the handler
    function has to read into server data(esp create_info) to make decision. But
    the value of create_info is some[1;31mtime[ms null since the server assigns value to
    a local copy and the handler tries to read from the original copy in lex
    which is still null. The bug is fixed by adding a null check to create_info.
    Changes :
      - added null check for create_info in ha_ndbcluster::can_switch_engines()
        and removed few redunant checks
      - added test cases to test the fix

[33mcommit 29950a09537a821b5bd424aa4ffa73fb6daf6550[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 22 09:34:49 2015 +0100

    Bug#17579998 TEST 'TESTNODERESTART -N GCP -L 1 T1' FAILS DUE TO MYSQLD ABORTED
    
    When a data node fails or being restarted, the other nodes in the
    same nodegroup will resend data to subscribers that they "believe"
    that the failed node have not already sent.
    
    Normally when a data node (suma block) have sent all data for an epoch
    it is responsible for, it will send a SUB_GCP_COMPLETE_REP with a count
    to all subscribers.  When the subscribers receives SUB_GCP_COMPLETE_REP
    it reply with a SUB_GCP_COMPLETE_ACK.  Then Suma have received the ack
    from all subscribers it will distribute it to the other nodes in same
    nodegroup so they will know that that data need no resend in case the
    node fail.  If a node fail between all subscribers sent ack but before
    all other nodes in nodegroup received the ack from failing node, data
    for some epoch may be sent twice and reported complete twice.  In this
    cases a crash as in bug may occur.
    
    This patch add a list of identifiers for which buckets that are reported
    in SUB_GCP_COMPLETE_ACK in addition to the count.  The receiver can now
    use this list of identifiers to keep track of which buckets are completed
    and detect when an already completed bucket are reported a second [1;31mtime[m, and
    can ignore that signal.
    
    This patch also introduces the concept of Subscriptions Data Streams to hide
    the details of Suma buckets in ndbapi.

[33mcommit 5edc95ddcc3242dfea24163ec8945bd7be89ab14[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Jan 21 23:26:17 2015 +0000

    ndb_rpl_slave_binlog_index.test
    
    Reduce Binlog size further in attempt to avoid
    Valgrind target [1;31mtime[mouts.

[33mcommit 2bebdad2be65ff6649842b2d8c51c0f6828c15c9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:39:21 2015 +0100

    Fix regression introduced by fix for bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    [1;31mtime[md out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 regression, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 60e1b37d9c904fb5cd24c7275e565cc41a6ffb99[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:37:53 2015 +0100

    Fix regression introduced by fix for bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    [1;31mtime[md out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 regression, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 0c47952f2a467ce1a32ac8c6043424c5b26cba69[m
Merge: bd4a76f3fa3 90126a75b39
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:21:54 2015 +0100

    Fix regression introduced by fix for bug#bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to simply remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Don't copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    Intend to push this 7.1 -> even if commiting to 7.2 now.
    
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    [1;31mtime[md out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                 -> TransporterFacade::start_poll -
                    > lock TransporterFacade mutex (a global mutex)
    
      - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                 -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
      Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                    neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 regression, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 90126a75b3948e7faee257dd5ac2c2665b73a6ad[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:16:07 2015 +0100

    Fix regression introduced by fix for bug#bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    [1;31mtime[md out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 regression, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 704231728c055b98adaf200c249f8f56c636e5c4[m
Author: Mithun C Y <mithun.c.y@oracle.com>
Date:   Tue Jan 20 18:40:59 2015 +0530

    Bug #20201465: FIX DISABLED TEST INNODB.INNODB_MYSQL AND RE-ENABLE IT.
    
    Changes:
    1. Corrected the multi update queries, which changes results
       based on join order.
    2. Queries which fail because of new default sql_mode
       (STRICT_TRANS_TABLES, ONLY_FULL_GROUP_BY). Are now
       executed under older default sql_mode.
    3. Corrected ever waiting statement by adding minimal wait [1;31mtime[m
       out.
    4. Corrected with new better condition filter stats in explain.

[33mcommit 488f9aabe5c98e5e95cad9ad95f0013c15946baf[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Jan 15 10:12:04 2015 +0100

    WL#7592 step 14. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event. Fix Multi-threaded slave.
    
    Multi-threaded slave has its own logic to parse transaction
    boundaries.  Before WL#7592, there was a special case that it handles
    in a way that is not compatible with WL#7592.
    
    Before WL#7592, DML is wrapped in BEGIN...COMMIT and DDL normally
    consists of a single Query_log_event.  However, there is one
    exception: if CREATE TABLE ... SELECT is logged in statement format,
    and the SELECT uses a user variable, or calls RAND() or uses
    INSERT_ID/LAST_INSERT_ID, then CREATE ... SELECT will result in one or
    more User_var_log_event/Rand_log_event/Intvar_log_event, followed by a
    Query_log_event.
    
    Before WL#7592, when GTID_MODE=OFF, Multi-threaded slave stored
    logical [1;31mtime[mstamps in the Query_log_event at the beginning of the
    transaction.  In the case of CREATE ... SELECT, where the transaction
    started with another event, a specialized logic was used.  The master
    wrote the Query_log_event with a logical [1;31mtime[mstamp of 0, since it was
    not the first event of the transaction.  The slave then used a
    dedicated worker thread (called worker 0) to process any transaction
    having [1;31mtime[mstamp 0, and ensured that all such transactions were
    executed in isolation.
    
    The logic of multi-threaded slave, before WL#7592, required that a
    User_var_log_event that appeared outside of BEGIN...COMMIT was treated
    as a transaction of its own.  This was just a technicality in the code
    and did not cause a problem, since the User_var_event and the
    following Query_log_event would both be assigned to worker 0.
    
    After WL#7592, there is an Anonymous_gtid_log_event before any CREATE
    ... SELECT statement, and this event carries the correct logical
    [1;31mtime[mstamps.
    
    Problem:
    
    After WL#7592, the slave may process a sequence of events like:
    
      Anonymous_gtid_log_event
      User_var_log_event
      Query_log_event
    
    The Anonymous_gtid_log_event has nonzero logical [1;31mtime[mstamps and
    therefore it may be executed by a worker other than worker 0.  The
    User_var_log_event will be executed by the same worker.  However,
    after WL#7592 and before this patch, the User_var_log_event will
    terminate the transaction.  Then the Query_log_event will be
    scheduled.  Since it has logical [1;31mtime[mstamp 0 it will be scheduled to
    worker 0, where the User_var_event has not taken effect.  So the wrong
    value will be inserted into the table.
    
    In the code, the problem is located to two places:
    
    - In Log_event::get_slave_worker, logic to terminate the transaction
      was invoked for events that appear outside BEGIN...COMMIT and which
      are not COMMIT or Xid.  This includes User_var_log_event appearing
      outside of BEGIN...COMMIT.
    
    - The function slave_worker_exec_job_group iterates over the events of
      a transaction in a while loop.  There was logic to break out of the
      loop that was invoked for events other than
      Gtid_log_event/Anonymous_gtid_log_event which appeared outside
      BEGIN...COMMIT, for the logical clock scheduler.  This includes
      User_var_event appearing outside BEGIN...COMMIT.
    
    Fix:
    
    - In Log_event::get_slave_worker, add a condition so that the logic is
      only invoked for events other than Query_log_event, if a
      Gtid_log_event or Anonymous_gtid_log_event has been seen.
    
    - In slave_worker_exec_job_group, add a condition so that the logic is
      only invoked if there has not been a Gtid_log_event or
      Anonymous_gtid_log_event.

[33mcommit 2ebff72bb226b69c578cf0820112c25b05677bd9[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Mon Nov 17 10:47:47 2014 +0100

    WL#7592 step 13. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix binlogging of strange SQL statements.
    
    A few SQL statements have strange semantics that causes trouble for GTIDs. This includes DROP TABLE with multiple tables, CREATE TABLE ... SELECT, DROP DATABASE that fails on rmdir, OPTIMIZE/REPAIR/ANALYZE/CHECKSUM TABLE, CREATE TEMPORARY/DROP TEMPORARY, DROP TEMPORARY generated by client disconnect, and statements/transactions that mix both transactional and non-transactional updates.
    
     1. Background:
        When DROP TABLE is used with multiple tables, and the tables are of
        different types (transactional/non-transactional or
        temporary/non-temporary), tables of the same type get grouped
        together and each group is logged as a separate statement. For
        example:
          DROP TABLE temporary, non_temporary
        gets logged as
          DROP TABLE temporary; DROP TABLE non_temporary.
    
        When GTID_MODE = ON, each such statement is assigned its own GTID.
        In order to generate the GTID, mysql_rm_table_no_locks calls
        mysql_bin_log.commit for each group of tables.
    
        Problem:
    
         1. mysql_bin_log.commit is only called when gtid_mode !=
            OFF. When gtid_mode == OFF, all the statements are written to
            the binary log in one operation. So after WL#7592 there is
            only one Anonymous_gtids_log_event, instead of one for each
            DROP TABLE.
    
         2. If GTID_NEXT='ANONYMOUS', Gtid_state::update_on_commit
            releases anonymous ownership. But the statement should hold
            anonymous ownership until it completes.
    
        Fix:
    
         1. Call mysql_bin_log.commit unconditionally.
    
            Note: inside a transaction, if only temporary tables are
            dropped, we should not call mysql_bin_log.commit, since the
            transactional context must remain open in this case.
    
         2. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
     2. Background:
        Prior to this patch, when binlog_format=row, CREATE...SELECT gets
        written to the binary log as
          BEGIN
          CREATE
          row events
          COMMIT
        CREATE...SELECT is not allowed when gtid_mode=on (in fact, not when
        enforce_gtid_consistency=1).
    
        Problem:
    
         1. Although CREATE without SELECT has an implicit commit, it
            appears in the middle of a transaction on the slave. Thus,
            after this worklog and prior to this patch, it gets logged as:
    
              Anonymous_gtids_log_event
              BEGIN
              CREATE
              row events
              COMMIT
    
            This causes problems on an MTS slave.
    
         2. If GTID_NEXT='ANONYMOUS', Gtid_state::update_on_commit
            releases anonymous ownership. But the statement should hold
            anonymous ownership until it completes.
    
        Fix:
    
         1. Call mysql_bin_log.commit after writing the CREATE statement.
            This causes CREATE...SELECT to be logged like:
    
            Anonymous_gtids_log_event
            CREATE
            Anonymous_gtids_log_event
            BEGIN
            row events
            COMMIT
    
         2. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
        A side-effect of this is that a CREATE...SELECT statement that
        fails in the SELECT part is already logged when the error happens,
        but the error causes the statement to be rolled back.  To make
        this case work correctly, we log a compensatory DROP statement if
        the SELECT part fails.
    
     3. Background:
        If DROP DATABASE fails after dropping some tables (e.g., if there
        are extra files in the database directory), then it writes a
        DROP TABLE statement that lists all the tables that it dropped.
        If there are many tables, this statement gets long. In this case,
        the server splits the statement into multiple DROP TABLE statements.
    
        Problem:
    
         1. If the statement fails when GTID_NEXT='UUID:NUMBER', then
            there is no way to log this correctly. So we must generate an
            error, log nothing, and not add GTID to GTID_EXECUTED.
    
         2. If the statement fails and generates multiple transactions
            when GTID_NEXT='AUTOMATIC' or 'ANONYMOUS', then we must
            generate multiple transactions, and each should have its own
            Gtid_log_event or Anonymous_gtid_log_event.
    
         3. If GTID_NEXT='ANONYMOUS', we must hold anonymous ownership
            until all transactions have been written to the binary log.
    
        Fix:
    
         1. Introduce a new error code,
            ER_CANNOT_LOG_PARTIAL_DROP_DATABASE_WITH_GTID, and generate
            the error if GTID_NEXT='UUID:NUMBER' and DROP DATABASE fails.
    
         2. Call mysql_bin_log.commit after writing DROP TABLE statements.
    
         3. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
     4. Background:
        OPTIMIZE/REPAIR/ANALYZE/CHECKSUM TABLE are written to the binary
        log even if they fail, after having called trans_rollback.
    
        Problem:
        trans_rollback calls gtid_state::update_on_rollback, which normally
        releases GTID ownership. But we must not release ownership before
        writing to the binary log.
    
        Fix:
        This was already fixed for the case gtid_mode=on; for that case we
        set a special flag in the THD object which tells
        gtid_state::update_on_rollback to not release ownership. Now we need
        to fix the case gtid_mode=off, so we set the flag in this case too.
    
     5. Background:
        CREATE TEMPORARY and DROP TEMPORARY behave very strange. If executed
        outside transactional context, they behave as DDL: they get logged
        without BEGIN...COMMIT and cannot be rolled back. If executed in
        transactional context, they behave as non-transactional DML: they
        get logged inside BEGIN...COMMIT, leave the transactional context
        open, but cannot be rolled back.
        Before this patch, CREATE TEMPORARY and DROP TEMPORARY call
        gtid_end_transaction unconditionally.
    
        Problem:
        gtid_end_transaction ends the transactional context and releases
        ownership. This was not a problem before WL#7592 since
        gtid_end_transaction could only be called when gtid_mode=on, and
        when gtid_mode=on we disallow CREATE TEMPORARY and DROP TEMPORARY
        inside transactional context. However, after WL#7592, we call
        gtid_end_transaction also when gtid_mode=off, and
        gtid_end_transaction releases anonymous ownership.
    
        Fix:
        Do not call gtid_end_transaction for CREATE TEMPORARY and
        DROP TEMPORARY inside transaction context.
    
     6. Background:
        When a client that has open temporary tables disconnects, the
        temporary tables are dropped and DROP TEMPORARY is written to the
        binary log.
    
        Problem:
        After WL#7592 and before this patch, if a client disconnects when
        GTID_NEXT='ANONYMOUS', the client would not hold anonymous ownership
        when writing to the binary log, which would trigger an assertion in
        write_gtid.
    
        There was no problem when GTID_NEXT='UUID:NUMBER', since this case
        was taken care of already before WL#7592. In this case, we set
        GTID_NEXT='AUTOMATIC' before dropping any tables.
    
        Fix:
        Set GTID_NEXT='AUTOMATIC' regardless of GTID_MODE.
    
     7. Background:
        Anything that is written to the binary log is first written to a
        thread-specific IO_CACHE. There are two such IO_CACHES: the
        transaction cache and the statement cache. Transactional updates are
        written to the transaction cache and non-transactional updates are
        written to the statement cache. (Under certain conditions, such as
        when a non-transactional update appears after a transactional update
        within the same transaction and
        binlog_direct_non_transactional_updates=0, non-transactional updates
        are written to the transaction cache, but that is not relevant to
        the current discussion.)
    
        The statement cache is flushed when the statement ends and the
        transaction cache is flushed when the transaction ends.
    
        It is possible that both caches are non-empty if a single
        transaction or a single statement updates both transactional and
        non-transactional tables. This is not allowed when GTID_MODE=ON.
        If both caches are non-empty, an autocommitted transaction flushes
        first the statement cache and then the transaction cache (see
        binlog.cc:binlog_cache_mngr::flush). Both happen in the BGC flush
        stage.
    
        Problems:
         7.1. When flushing both caches in an autocommitted transaction,
              and GTID_NEXT=AUTOMATIC, Gtid_state::generate_automatic_gtid
              is called once for each cache.
              Gtid_state::generate_automatic_gtid acquires anonymous
              ownership, but it also assumes (and asserts) that no ownership
              is held when entering the function. Thus, before this patch
              the assert would be raised when flushing the transaction
              cache, since the previous flush of the statement cache did
              acquire ownership.
    
         7.2. When the statement cache is flushed in the middle of a
              transaction (due to a non-transactional update happening
              in the middle of the transaction), and GTID_NEXT=ANONYMOUS,
              before this patch ownership was released. This breaks the
              protocol that anonymous ownership must be held for the
              duration of the transaction when GTID_NEXT=ANONYMOUS.
    
         7.3. Any statement calls gtid_reacquire_ownership_if_anonymous
              before it executes (mysql_execute_statement calls
              gtid_pre_statement_checks which calls
              gtid_reacquire_ownership_if_anonymous). This is important
              because when GTID_NEXT=ANONYMOUS, anonymous ownership must
              be held from when the transaction begins to execute.
              Therefore, when the transaction commits it also asserts that
              if GTID_NEXT=ANONYMOUS, it holds anonymous ownership.
    
              For the same reason, Rows_log_event::do_apply_event calls
              gtid_pre_statement_checks which calls
              gtid_reacquire_ownership_if_anonymous.
    
              However, before this patch the call to
              gtid_reacquire_ownership_if_anonymous was missing in
              Xid_log_event::do_apply_event.
    
              There is no existing case when GTID_NEXT=ANONYMOUS and
              anonymous ownership is not held when
              Xid_log_event::do_apply_event is called. However, it could
              potentially happen if a future version has a bug that sends
              a lonely Xid_log_event to the slave, or if the relay log is
              generated by something else than mysql, which has a bug. So
              we should call gtid_reacquire_ownership_if_anonymous at the
              beginning of Xid_log_event::do_apply_event too (just like
              we would do in a Query_log_event that contains a COMMIT
              query).
    
        Fix:
         7.1. When both caches are non-empty, and GTID_NEXT='AUTOMATIC',
              call thd->clear_owned_gtids between the two cache
              flushes. This releases anonymous ownership so that the
              call to Gtid_cache::generate_automatic_gtid for the second
              flush is done without holding any ownership.
    
         7.2. In Gtid_state::update_gtids_impl, do not release ownership
              if the transaction cache is nonempty.
    
         7.3. Call gtid_reacquire_ownership_if_anonymous from
              Xid_log_event::do_apply_event.
    
    @mysql-test/extra/binlog_tests/drop_tables_logical_[1;31mtime[mstamp.inc
    - Remove this file. See binlog_mts_logical_clock.test for explanation.
    
    @mysql-test/extra/binlog_tests/logical_[1;31mtime[mstamping.inc
    - Use assert_logical_[1;31mtime[mstamps.inc instead of grep_pattern.inc See
      binlog_mts_logical_clock.test for explanation.
    
    @mysql-test/extra/rpl_tests/rpl_gtid_drop_multiple_tables.inc
    - New auxiliary file used by
      rpl_gtid_drop_multiple_tables_in_multiple_ways.inc.
    
    @mysql-test/extra/rpl_tests/
    rpl_gtid_drop_multiple_tables_in_multiple_ways.inc
    - New auxiliary test file used by rpl_split_statements.test.
    
    @mysql-test/extra/rpl_tests/rpl_split_statements.test
    - New test case.
    
    @mysql-test/include/assert_binlog_events.inc
    - New test utility. This is needed by rpl_split_statements.inc.
    
    @mysql-test/include/assert_grep.inc
    - New test utility. This is needed by assert_logical_[1;31mtime[mstamps.inc.
    
    @mysql-test/include/assert_logical_[1;31mtime[mstamps.inc
    - New auxiliary test utility used by binlog_mts_logical_clock.test.
    
    @mysql-test/include/grep_pattern.inc
    - Add comment suggesting to use assert_grep.inc instead.
    
    @mysql-test/include/gtid_step_assert.inc
    - Add $gtid_step_gtid_mode_agnostic, needed by rpl_split_statements.test
    - Update test utility due to changes in gtid_utilities.inc.
    
    @mysql-test/include/gtid_utils.inc
    - Add new utility functions.
    
    @mysql-test/include/gtid_utils_end.inc
    - Drop the new functions introduced in gtid_utilities.inc.
    
    @mysql-test/include/rpl_connect.inc
    - Make it easier to map mysqltest connections to thread numbers in
      server debug traces.
    
    @mysql-test/include/rpl_connection.inc
    - Add $rpl_connection_silent_if_same.
    
    @mysql-test/include/rpl_get_end_of_relay_log.inc
    - New auxiliary test utility. This is needed by
      rpl_skip_to_end_of_relay_log.inc.
    
    @mysql-test/include/rpl_init.inc
    - Set some more mtr variables for convenience.
    
    @mysql-test/include/rpl_skip_to_end_of_relay_log.inc
    - New auxiliary test utility. This is needed by
      rpl_gtid_split_statements.test.
    
    @mysql-test/include/rpl_stop_slaves.inc
    - Correct a typo.
    
    @mysql-test/include/save_binlog_position.inc
    - New auxiliary test script, useful in combination with
      include/assert_binlog_events.inc.
    
    @mysql-test/include/set_gtid_next_gtid_mode_agnostic.inc
    - New auxiliary test utility. This is needed by
      rpl_split_statements.test.
    
    @mysql-test/include/wait_for_status_var.inc
    - Fix broken implementation of $status_fail_query.
    
    @mysql-test/suite/binlog/r/binlog_gtid_utils.result
    - Update result file for modified test.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock.result
    - Update result file due to change in test.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock_gtid.result
    - Update result file due to change in test.
    
    @mysql-test/suite/binlog/t/binlog_gtid_utils.test
    - Add tests for new utility function.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock.test
    - This test started failing because DROP TABLE is now logged
      differently.
      The failure was in 'grep_pattern.inc' executed just after
      drop_tables_logical_[1;31mtime[mstamp.inc. This was fixed by changing the test
      assertion.
    - The test assertion following drop_tables_logical_[1;31mtime[mstamp.inc belongs
      inside drop_tables_logical_[1;31mtime[mstamp.inc. Moved the test assertion
      there.
    - The tests in drop_tables_logical_[1;31mtime[mstamp.inc were located in this
      file because they differed between gtid_mode=on and gtid_mode=off. But
      because of the change in logging of DROP TABLE, the test works in the
      same way regardless of gtid_mode. Therefore, the contents of
      drop_tables_logical_[1;31mtime[mstamp.inc was moved into
      logical_[1;31mtime[mstamping.inc and drop_tables_logical_[1;31mtime[mstamp.inc as
      removed.
    - This file used grep_pattern.inc to check assertions. This made the
      test very verbose and hard to read and understand. Also, it was
      imprecise since any future server bug that introduces a logical
      [1;31mtime[mstamp that does not match the given pattern would go unnoticed by
      the test. Therefore, replaced all grep_pattern.inc by
      assert_logical_[1;31mtime[mstamps.inc.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock_gtid.test
    - See binlog_mts_logical_clock.test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_create_select.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_disconnect_drop_temporary_table.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_split_statements.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_no_gtid_split_statements.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_semi_sync.result
    - When semisync is enabled, there will be one ack for each transaction.
      Since we now have more 'transactions' due to the extra
      Anonymous_log_event preceding each DROP statement, this changes the
      result file.
    
    @mysql-test/suite/rpl/t/rpl_drop_db.test
    - This test can now run regardless of GTID_MODE.
    
    @mysql-test/suite/rpl/t/rpl_gtid_create_select.test
    - New test to verify that CREATE ... SELECT is logged as expected.
    
    @mysql-test/suite/rpl/t/rpl_gtid_disconnect_drop_temporary_table.test
    - New test file to verify that DROP TEMPORARY generated by client
      disconnect is logged correctly.
    
    @mysql-test/suite/rpl/t/rpl_gtid_split_statements.test
    - New test file.
    
    @mysql-test/suite/rpl/t/rpl_invoked_features.test
    - Explain why not_gtid_enabled is used.
    
    @mysql-test/suite/rpl/t/rpl_mixed_drop_create_temp_table.test
    - Explain why not_gtid_enabled is used.
    
    @mysql-test/suite/rpl/t/rpl_no_gtid_split_statements.test
    - New test file.
    
    @mysql-test/suite/rpl/t/rpl_stop_slave.test
    - Explain why not_gtid_enabled is used.
    
    @sql/rpl_gtid.h
    - Add need_lock parameter to Sid_map::sidno_to_sid,
      Gtid::to_string, Gtid::dbug_print, Gtid_specification::to_string, and
      Gtid_specification::dbug_print, to simplify the use of these
      functions.
    - Simplify the definition of Gtid_specification::MAX_TEXT_LENGTH.
    - Make gtid_reacquire_ownership_if_anonymous extern so that it can be
      called from log_event.cc.
    
    @sql/rpl_gtid_execution.cc
    - Make gtid_reacquire_ownership_if_anonymous extern so that it can be
      called from log_event.cc.
    - Move parts of gtid_pre_statement_checks into an own function,
      gtid_reacquire_ownership_if_anonymous. This is just to make
      the logic more easy to follow.
    - Make gtid_reacquire_ownership acquire anonymous ownership in case
      GTID_NEXT='ANONYMOUS'. This is needed for cases like:
        SET AUTOCOMMIT=1;
        SET GTID_NEXT='ANONYMOUS';
        INSERT;
        BEGIN;
      The first INSERT will commit the transaction and therefore release
      anonymous ownership. Then the BEGIN has to re-acquire anonymous
      ownership so that it can execute correctly.
    
      (The logic to reacquire ownership in this case is needed because we
      allow user to set GTID_NEXT='ANONYMOUS' and then execute multiple
      transactions. This differs from the case of GTID_NEXT='UUID:NUMBER',
      where we generate an error if user tries to execute a second
      transaction without setting GTID_NEXT again. The reason we want to
      generate an error in this case is that otherwise the GTID autoskip
      feature would silently skip the transaction and user would not notice
      it.)
    
    - Move the call to gtid_reacquire_ownership_if_anonymous
      earlier in gtid_pre_statemen_checks. This is needed for the
      statements BEGIN/ROLLBACK/COMMIT in two cases.
    
      Case 1:
        User executes:
          SET GTID_NEXT='ANONYMOUS';
          BEGIN;
          INSERT;
          COMMIT;
          BEGIN;
      Here the second BEGIN needs to acquire anonymous ownership so that
      anonymous ownership is held during the entire transaction.
    
      Case 2:
        The relay log comes from an old master that does not generate
        Anonymous_gtids_log_event, so it begins with:
          Format_desc
          Previous_gtids
          BEGIN;
        Then the Format_desc will set GTID_NEXT=NOT_YET_DETERMINED, and the
        BEGIN needs to acquire anonyous ownership.
    
      The only case where we should not acquire anonymous ownership is for a
      SET statement that does not invoke a stored function. This is
      important in case a client processes output from mysqlbinlog, for a
      binary log generated with GTID_MODE=ON. Then, the server will first
      execute a Format_description_log_event, which will set
      GTID_NEXT='NOT_YET_DETERMINED', followed by a
      SET GTID_NEXT='UUID:NUMBER' statement. If it would try to reacquire
      anonymous ownership in this case, and gtid_mode=on, an error would be
      generated since anonymous transactions are not allowed when
      gtid_mode=on.
    
    @sql/rpl_gtid_misc.cc
    - Add need_lock parameter to Gtid::to_string, to simplify the use of
      this function.
    - Allow Gtid::to_string to take sid_map==NULL in debug mode, so that
      debug printouts can show the sidno.
    
    @sql/rpl_gtid_specification.cc
    - Add need_lock to Gtid_specification::to_string.
    
    @sql/rpl_gtid_state.cc
    - Print any change of thd->owned_gtid to the debug trace.
    - Implement thd->is_commit_in_middle_of_statement.
    - Return early from update_gtids_impl to avoid releasing ownership, if
      the transaction cache is nonempty and GTID_NEXT=ANONYMOUS.
    
    @sql/share/errmsg-utf8.txt
    - New error message.
    
    @sql/binlog.cc
    - Release anonymous ownership between flushing the statement cache
      and flushing the transaction cache, if GTID_NEXT=AUTOMATIC.
    
    @sql/log_event.cc
    - Call gtid_reacquire_ownership_if_anonymous in
      Xid_log_event::do_apply_event.
    
    @sql/sql_admin.cc
    - Administrational statements (OPTIMIZE TABLE, REPAIR TABLE,
      CHECKSUM TABLE, ANALYZE TABLE) behave strange if they fail: they first
      call trans_rollback and then write the statement to the binary log.
      This causes problems for GTIDs, since ha_rollback eventually calls
      gtid_state::update_on_rollback, which releases GTID ownership. Then
      no GTID is owned when it comes to writing the statements to the binary
      log.
    
      This was handled when GTID_NEXT='UUID:NUMBER' by setting the flag
      thd->skip_gtid_rollback before calling ha_rollback.
      gtid_state::update_on_rollback checks the flag and if the flag is set,
      it does not release ownership.
    
      After WL#7592 we need to hold anonymous ownership in case
      GTID_NEXT='ANONYMOUS'. Therefore we set the flag also in this case.
    
    @sql/sql_base.cc
    - When a client which has open tables disconnects, DROP TEMPORARY TABLE
      is written to the binary log. Before doing that, we must set
      GTID_NEXT='AUTOMATIC', so that it generates new GTIDs correctly.
    
      This was done only in the case of GTID_MODE=ON. Now we need to do it
      regardless of GTID_MODE.
    
      Moreover, in case a GTID is owned already, we must release ownership
      before setting GTID_NEXT='AUTOMATIC'. Thus we call
      gtid_state->update_on_rollback() first.
    
    @sql/sql_class.cc
    - Print any change of thd->owned_gtid to the debug trace.
    - Initialize new THD member variable.
    
    @sql/sql_class.h
    - Clarify life cycle of THD::owned_gtid.
    - Print any change of thd->owned_gtid to the debug trace.
    - Add THD::is_commit_in_middle_of_statement.
    
    @sql/sql_db.cc
    - If DROP TABLE fails, so that it generates DROP TABLE statements in the
      binary log, then we need to:
      - Call mysql_bin_log.commit after each statement. If we did not do
        this, all DROP TABLE statements would be written to the same
        statement cache, so there would just be one
        Anonymous_gtid_log_event. We need each DROP TABLE to have its own
      - If GTID_NEXT='UUID:NUMBER', and multiple DROP TABLE are needed, then
        there is no way to log this correctly. Thus, we generate
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS. This is
        a new error code.
        To handle this case correctly, we must also postpone the generation
        of ER_DB_DROP_RMDIR. If we would not move this error until later,
        then ER_DB_DROP_RMDIR would be generated before
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS, so then
        the user would never see
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS.
      - If GTID_NEXT='ANONYMOUS', it is not a problem that we generate
        multiple transactions. However, we must set
        thd->is_commit_in_middle_of_statement in order to hold
        anonymous ownership for the duration of the statement.
    
    @sql/sql_insert.cc
    - For CREATE ... SELECT, write the CREATE as a non-transactional
      statement, directly to the binlog.
      This prevents BEGIN and COMMIT statements from being generated around
      the CREATE statement.
    - Call mysql_bin_log.commit after writing the CREATE statement. This
      causes the row events to be written as a separate transaction, so they
      will be preceded by an Anonymous_gtid_log_event.
    - Do not release anonymous ownership in the middle of the statement.
    - Log a compensatory DROP TABLE statement if the CREATE...SELECT fails
      on the SELECT part.
    
    @sql/sql_parse.cc
    - CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE are very strange
      statements. When executed in transactional context, they behave like
      MyISAM: they leave the transaction context open and get logged between
      BEGIN and COMMIT, but cannot be rolled back. When executed outside
      transactional context, they get logged as DDL, without BEGIN/COMMIT.
    
      If we would call gtid_end_transaction without ending the transaction,
      then ownership would be released too early. This would cause an
      assertion in write_gtid, where it is expected that the thread holds
      ownership of whatever GTID_NEXT is set to (unless
      GTID_NEXT='AUTOMATIC').
    
      This was not a concern before WL#7592, since gtid_end_transaction was
      only called if gtid_mode=ON, and CREATE TEMPORARY/DROP TEMPORARY are
      disallowed in transactional context when gtid_mode=ON.
    
      Now that we can call gtid_end_transaction also when gtid_mode=OFF,
      CREATE TEMPORARY/DROP TEMPORARY must only invoke gtid_end_transaction
      if executed outside transaction context.
    
    @sql/sql_table.cc
    - Call mysql_bin_log.commit regardless of gtid_mode, only avoid it
      between temporary tables inside a transaction.
      The code has three blocks:
       1. nontransactional temporary tables
       2. transactional temporary tables
       3. non-temporary tables
      Before, the commit was at the end of block 1 and 2. We moved it
      to the beginning of block 2 and 3 instead, to simplify the condition.
      This does not change the logic for writing to the log: in both
      cases the point is that we do the commit *between* two calls to
      thd->binlog_query.
    - Fix some comments and re-wrap some long lines.
    - In parameters to mysql_bin_log.commit, use true/false instead of
      TRUE/FALSE, and add comments to clarify the meaning of the parameters.
    
    @mysql-test/suite/binlog/r/binlog_row_binlog.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/binlog/r/binlog_row_insert_select.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/binlog/r/binlog_row_mix_innodb_myisam.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/r/rpl_non_direct_row_mixing_engines.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/r/rpl_row_mixing_engines.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/t/rpl_stm_start_stop_slave.test
    - Suppress a warning, which is generated from its included file
      rpl_start_stop_slave.test
    
    @mysql-test/suite/rpl/r/rpl_stm_start_stop_slave.result
    - Update result file to remove a redundent suppression warning.

[33mcommit 8ef56039b73fc4b8d38ea0a9e41dc55d849b5fa9[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Nov 13 13:20:31 2014 +0100

    WL#7592 step 12. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix GTIDs in P_S tables.
    
    In this patch we correct the GTID shown in the GTID columns of
    performance_schema.events_transactions_current and
    performance_schema.events_transactions_history.
    
    Background:
    The GTID is changed during the life[1;31mtime[m of a transaction as follows:
    
    - On a master, the transaction is "AUTOMATIC" while executing. Only when
      it commits is it assigned a GTID of the form UUID:NUMBER.
    
    - On a slave, the transaction is assigned a GTID before it starts to
      execute.
    
    - When GTID_MODE = OFF, transactions use the special GTID "ANONYMOUS"
      rather than "UUID:NUMBER".
    
    This was not reflected correctly in the GTID column of these
    performance_schema tables. Among other things, the history table could
    contain 'AUTOMATIC'. This should never happen since it makes it
    impossible to identify the transaction.
    
    This issue is not directly related to WL#7592. However, it showed up as
    a test failure in perfschema.transaction after a refactoring that was
    part of WL#7592. Therefore, we fix it in order to make tests pass for
    WL#7592.
    
    @mysql-test/include/execute_at_sync_point.inc
    - New auxiliary test file.
    
    @mysql-test/suite/perfschema/include/reset_transaction_gtid.inc
    - Auxiliary test file used by the new test.
    
    @mysql-test/suite/perfschema/include/show_transaction_gtid.inc
    - Auxiliary test file used by the new test.
    
    @mysql-test/suite/perfschema/r/transaction.result
    - Update result file because test file changed.
    
    @mysql-test/suite/perfschema/r/transaction_gtid.result
    - Result file for new test case.
    
    @mysql-test/suite/perfschema/t/transaction.test
    - Since we need a more elaborate testing of what GTID value shows up in
      the P_S table, moved it to a different file.
    
    @mysql-test/suite/perfschema/t/transaction_gtid.test
    - New test file to verify the new behavior.
    
    @sql/binlog.cc
    - Move the code for setting GTID for P_S away from this place and into
      generate_automatic_gtid.
    - Introduce a debug_sync point used by the new transaction_gtid.test.
    
    @sql/handler.cc
    - Set the GTID for P_S when the transaction starts.
    
    @sql/rpl_gtid.h
    - Forward declaration of new function.
    
    @sql/rpl_gtid_execution.cc
    - Set the GTID for P_S also for anonymous transactions. This is for the
      case when the GTID is specified by SET GTID_NEXT or
      Gtid_log_event::do_apply_event.
    - Add new auxiliary function gtid_set_performance_schema_values to call
      the P_S macro MYSQL_SET_TRANSACTION_GTID with the correct arguments.
    
    @sql/rpl_gtid_state.cc
    - Set the GTID for P_S when generating automatic GTID.
    
    @sql/transaction.cc
    - Set the GTID for P_S when the transaction starts.
    
    @storage/perfschema/pfs.cc
    - Remove pfs->m_gtid_set.
    
    @storage/perfschema/pfs_events_transactions.h
    - Remove pfs->m_gtid_set, since it is not needed.
    
    @storage/perfschema/table_events_transactions.cc
    - Remove pfs->m_gtid_set, since it is not needed.
    - Always rely on Gtid_specification::to_string to generate a correct
      string.
    - Update/clarify a comment.

[33mcommit 2cac07b9f221b84f1258ee86aee1c9225c0813a6[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Nov 13 13:18:56 2014 +0100

    WL#7592 step 11. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Remove dead code.
    
    This patch removes some code that died due to this worklog.
    This does not change behavior of the server, so it is not possible
    to have a test case for this.
    
    The following changes were made:
    - Remove code to store logical [1;31mtime[mstamps in Query_log_event. This
      is now done only in Gtid_log_event.
    - Remove class Group_cache. This is not used now since the
      Gtid_log_event is not written to the transaction or statement cache.
    - Remove the G_COMMIT_TS field. This was a single byte that had the
      constant value 1, and was stored in the binary log for
      Gtid_log_events. It did not serve a purpose, so we remove it.
    - Remove IO_CACHE::commit_seq_no and IO_CACHE::commit_seq_offset. These
      were clearly misplaced in the first place, and are now not needed any
      more since the [1;31mtime[mstamp is generated in the Gtid_log_event
      constructor.
    - Remove enumeration value INVALID_GROUP from enum_group_type. This was
      used in two places. The first one was Group_cache, which is now
      removed. The second was Gtid_specification::get_type. But get_type was
      only used in Gtid_specification::is_valid. We can merge get_type into
      is_valid (this is a simplification), and the result is that we don't
      need INVALID_GROUP.
    - Make Gtid_state::get_automatic_gno private, since it is not needed
      outside the class anymore (before it was needed by Group_cache).
    - Remove Gtid_log_event::commit_flag. This was unused even before
      WL#7592.
    
    @include/my_sys.h
    - Remove IO_CACHE::commit_seq_offset.
    
    @libbinlogevents/include/control_events.h
    - Remove Gtid_log_event::commit_flag.
    
    @libbinlogevents/src/control_events.cpp
    - Remove Gtid_log_event::commit_flag.
    
    @libmysqld/CMakeLists.txt
    - Remove rpl_gtid_cache.cc
    
    @sql/CMakeLists.txt
    - Remove rpl_gtid_cache.cc
    
    @sql/binlog.cc
    - Remove Group_cache.
    - Remove IO_CACHE::commit_seq_offset.
    - Remove binlog_trx_cache_data::reset_commit_seq_offset. This was only
      needed because we wrote Gtid_log_event to the cache before the
      logical [1;31mtime[mstamp was known.
    - Remove unused write_commit_seq_no.
    
    @sql/log_event.cc
    - Remove Q_COMMIT_TS, Q_COMMIT_TS2, and the logical [1;31mtime[mstamps from
      Query_log_event.
    - Improve a comment in Gtid_log_event constructor.
    - Update symbolic names for logical [1;31mtime[mstamps, since the term
      "commit_seq" was only used by the pre-5.7.5 MTS.
    - Remove Gtid_log_event::commit_flag.
    
    @sql/log_event.h
    - Move COMMIT_SEQ_LEN into Gtid_log_event, sinc it is not needed in
      Query_log_event any more.
    - Remove unused COMMIT_SEQ_LEN_OLD.
    - Remove Q_COMMIT_TS and Q_COMMIT_TS2.
    - Move G_COMMIT_TS to Gtid_log_event::LOGICAL_CLOCK_TYPECODE.
    - Remove Query_log_event::last_committed and
      Query_log_event::sequence_number.
    - Rename constants using 'commit_seq', since this term was only used by
      the pre-5.7.5 MTS.
    - Remove Gtid_log_event::commit_flag.
    
    @sql/rpl_gtid.h
    - Make get_automatic_gno private. Before, it was used by Group_cache,
      but now it is only used by Gtid_state::generate_automatic_gtid.
    - Remove INVALID_GROUP.
    - Remove Gtid_specification::get_type and move implementation of
      Gtid_specification::is_valid into rpl_gtid_specification.cc (it reuses
      most of the old implementation of Gtid_specification::get_type).
    - Remove Group_cache.
    
    @sql/rpl_gtid_cache.cc
    - Remove Group_cache.
    
    @sql/rpl_gtid_specification.cc
    - Merge the implementation of Gtid_specification::get_type into
      Gtid_specification::is_valid, so that we can get rid of enumeration
      value INVALID_GROUP from enum_group_type.
    - Remove INVALID_GROUP case.
    
    @sql/rpl_mts_submode.cc
    - Remove Query_log_event::sequence_number and
      Query_log_event::last_committed.
    
    @sql/rpl_rli_pdb.cc
    - Remove Query_log_event::last_committed.
    
    fixup cleanup - rmeove commit_flag

[33mcommit 9ad6b77365292af944226ff03bbb1e63189e5c6f[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Nov 13 13:17:02 2014 +0100

    WL#7592 step 10. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix failing tests.
    
    This patch fixes all test cases that fails due to the previous two
    patches. In addition it fixes two code bugs that were exposed/introduced
    due to the previous two patches, and which caused some of the test
    failures.
    
    - Fix bug in START SLAVE UNTIL MASTER_LOG_POS logic.
      The problem was: some Rotate_log_events in the relay log are generated
      on the slave, not on the master. Thus, their end_log_pos field is
      relative to the slave relay log. Since MASTER_LOS_POS is relative to
      the master binary log, we must not evaluate the MASTER_LOS_POS
      condition for such slave-generated Rotate_log_events. But the logic to
      skip the until check for slave-generated events was missing, and this
      caused tests to fail.
    
      The fix is to avoid evaluating the until condition for slave-generated
      events. This is easy because such events are easily distinguishable
      since their server_id is zero. So we check if the server_id==0, and in
      that case we don't evaluate the until condition.
    
      This did not cause any tests to fail before this worklog, because the
      events appeared so early in the relay log that their positions would
      be smaller than the position specified by MASTER_LOG_POS. However,
      after this patch, the events appear after Previous_gtids_log_event,
      which moves the position forward so much that it causes the slave
      thread to stop before the rotate event, which causes the test to fail.
    
      This was also not triggered by running the suite with gtid_mode=on,
      because the test was using include/not_gtid_enabled.inc.
    
    - Fix bug in sql_slave_skip_counter with GTIDs.
      sql_slave_skip_counter did not compute transaction boundaries
      correctly in the presence of Gtid_log_events. This did not cause any
      problems before this patch since sql_slave_skip_counter is not allowed
      when gtid_mode=on.
    
      sql_slave_skip_counter is supposed to decrease for every event
      processed, except it should not decrease down to 0 in the middle of a
      group. This ensures that the applier thread does not stop in the
      middle of a transaction. However, the applier thread did not consider
      Gtid_log_event to be part of a group, and therefore it could stop
      after the Gtid_log_event.
    
      The problem was that Gtid_log_event implemented a specialized
      do_shall_skip function. This caused it to decrease the counter down to
      zero. The fix is to implement Gtid_log_event::do_shall_skip and make
      it call continue_group.
    
    - Fix simplified-binlog-recovery.
      Writing Previous_gtids_log_event always broke the logic for
      simplified-binlog-recovery.
      Background:
      Before this patch, simplified-binlog-recovery would avoid
      iterating over multiple binary logs only in the case that the
      binlog lacks a Previous_gtids_log_event.
      Problem:
      Since we now generate Previous_gtids_log_event always,
      recovery would iterate over all binary logs even when
      simplified-binlog-recovery was enabled.
      Fix:
      Make it so that simplified-binlog-recovery skips the rest
      of the binary logs also in the case that the first binary log
      contains a Previous_gtids_log_event and no Gtid_log_event.
    
    @mysql-test/extra/binlog_tests/binlog.test
    - Use show_binlog_events.inc instead of SHOW BINLOG EVENTS, so that
      Gtid/Anonymous events gets masked appropriately.
    - This particular test requires that columns 1, 2, and 5 are masked out
      (so that server_id is not masked out). However, show_binlog_events.inc
      only masks columns 2, 4, 5. Changed show_binlog_events.inc so that it
      allows user to specify the set of columns to be masked.
    
    @mysql-test/extra/binlog_tests/binlog_mysqlbinlog_row.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/ctype_ucs_binlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/drop_tables_logical_[1;31mtime[mstamp.inc
    - Update test because grep_pattern.inc was altered.
    
    @mysql-test/extra/binlog_tests/logical_[1;31mtime[mstamping.inc
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/extra/binlog_tests/mix_innodb_myisam_binlog.test
    - Test was failing because it was expecting an exact number of events in
      the binlog. Fixed by adding an auxiliary test script
      include/get_row_count.inc that computes the number of events in the
      binlog.
    - While I was here, also changed to use assertion.
    
    @mysql-test/extra/binlog_tests/mysqlbinlog_row_engine.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/mysqlbinlog_start_stop_2.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/rpl_tests/check_type.inc
    - Provide more debug info if $rpl_debug is set.
    
    @mysql-test/extra/rpl_tests/create_recursive_construct.inc
    - The test expected that an empty binlog would contain 2 events. Changed
      this to 3.
    - The test expected that the third event of a binlog containing only one
      DML transaction in row format would be a Table_map. Changed third to
      fifth.
    - Simplify code to use assertions, to make it more readable and produce
      more debug output on failure.
    
    @mysql-test/extra/rpl_tests/rpl_implicit_commit_binlog.test
    - Fix test failure:
      This test executed some statements and then asserted that there was a
      COMMIT after a specific number of events in the binary log. Since
      we now write more events to the binary log, we have to increase this
      number in order for the test to succeed.
    - Use assert.inc instead of manual 'if' statements.
    - Rename variable $ok to $check_position_of_commit_event, since that
      explains better what the variable does.
    - Clarify purpose of the test.
    
    @mysql-test/extra/rpl_tests/rpl_insert_ignore.test
    - Test failure fix:
      The test asserted that there was a query_log_event after a specific
      number of events in the binary log. Since the number of events has
      changed, this number has to be updated.
    - The test case used to have a special case for gtid_mode=on, handled
      by extra/rpl_tests/rpl_insert_ignore_gtid_on.inc
      Since there is now no difference in event count between gtid_mode=on
      and gtid_mode=off, we can hardcode the number again and do not need
      the include file. Removed the include file.
    
    @mysql-test/extra/rpl_tests/rpl_insert_ignore_gtid_on.inc
    - Remove this file as it is not needed any more.
      See changeset comment for
      mysql-test/extra/rpl_tests/rpl_insert_ignore.test
    
    @mysql-test/extra/rpl_tests/rpl_log.test
    - Update the LIMIT clause for SHOW BINLOG EVENTS because
      the number of events in the binlog has changed. Now we can
      unify this instead of having different cases for GTID_MODE=ON
      and GTID_MODE=OFF.
    
    @mysql-test/extra/rpl_tests/rpl_row_show_relaylog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_show_binlog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_show_log_events_with_varying_options.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_sp.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/rpl_tests/rpl_start_stop_slave.test
    - Disable a part of the test that crashes MTS, which will be fixed in a
      separate bug.
    
    @mysql-test/extra/rpl_tests/rpl_stm_mix_show_relaylog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/type_conversions.test
    - Provide more debug info if $rpl_debug is set, but do not output lots
      of junk to the result file if $rpl_debug is unset.
    
    @mysql-test/include/assert_grep.inc
    - New auxiliary test script to be used instead of
      include/grep_pattern.inc
    
    @mysql-test/include/assert_gtid_mode_on.inc
    - New auxiliary file that causes the test to fail if gtid_mode is not
      ON. This was added to avoid similar bugs to the one found in
      rpl_incompatible_gtids_in_relay_log.test (see commit comments for that
      file for details). This file is sourced from the auxiliary files
      include/sync_*.inc
    
    @mysql-test/include/filter_file.inc
    - Add parameter to allow masking a given set of columns. This is needed
      to implement the new $show_binlog_events_mask_columns parameter of
      show_binlog_events.inc.
    
    @mysql-test/include/get_row_count.inc
    - New auxiliary file used by
      mysql-test/extra/binlog_tests/mix_innodb_myisam_binlog.test.
    
    @mysql-test/include/grep_pattern.inc
    - Add $grep_output parameter, to allow different modes of output. This
      was needed in order to fix a bug in
      mysql-test/extra/rpl_test/rpl_large_serverid.inc
      (see changeset comment for that file for details).
    - Move the file to mysql-test/include. This is a generic utility, and
      as such it belongs to mysql-test/include. mysql-test/extra/... is
      generally used for test-specific includes.
    - Use mtr variables instead of environment variables for parameters.
    - Remove extra newline that was printed after each output row.
    - Improve the output: s/Occurrences of the $pattern/Occurrences of
      '$pattern'/
    - Suggest using include/assert_grep.inc
    
    @mysql-test/include/mysqlbinlog.inc
    - New test framework file to filter out nondeterministic output.
      (Before, similar filter regexes were repeated in lots of places all
      over the test suite.)
    
    @mysql-test/include/rpl_change_topology_helper.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    - Provide more debug info.
    
    @mysql-test/include/rpl_init.inc
    - Allow user to override $use_gtids=1 when gtid_mode=on.
    
    @mysql-test/include/save_io_thread_pos.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/save_master_pos.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/show_binlog_events.inc
    - Add missing documentation for existing parameter
      $mask_binlog_commit_events.
    - Add documentation for new parameters $show_binlog_events_verbose and
      $show_binlog_events_mask_columns (see show_events.inc for details).
    
    @mysql-test/include/show_events.inc
    - Change SQL commands to UPPERCASE.
    - Add $show_binlog_events_verbose parameter that will print statement
      with positions and filenames masked.
    - Add output of full statement if $rpl_debug is set.
    - Add parameter to allow masking out a given set of columns. This
      is needed in order to fix and simplify
      mysql-test/extra/binlog_tests/binlog.test
    - Mask out Anonymous_Gtid so that output is the same whether GTID_MODE
      is ON or OFF.
    - Correct a variable name, s/sidno/gno/.
    
    @mysql-test/include/show_rpl_debug_info.inc
    - Select from P_S tables.
    - Add $rpl_topology to output.
    
    @mysql-test/include/sync_slave_io.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/sync_slave_sql.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    - Fix typo to produce correct output on [1;31mtime[mout.
    
    @mysql-test/include/wait_for_query_to_succeed.inc
    - Document the file.
    - Write debug info when it fails.
    
    @mysql-test/include/write_result_to_file.inc
    - Print perl's error text ($!) on error.
    - Print $stmt on error.
    
    @mysql-test/r/flush_block_commit_notembedded.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/r/mysqlbinlog.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/r/user_var-binlog.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row_innodb.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row_myisam.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_start_stop.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_row_ctype_ucs.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_stm_ctype_ucs.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_hexdump.result
    - Update result file because output of mysqlbinlog has changed.
    
    @mysql-test/suite/binlog/r/binlog_implicit_commit.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_innodb.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock_gtid.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_innodb.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_myisam.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_trans.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_start_stop.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-
    test/suite/binlog/r/binlog_mysqlbinlog_start_stop_slave_server_id.result
    - Update result file since the test uses include/mysqlbinlog.inc instead
      of $MYSQL_BINLOG.
    
    @mysql-test/suite/binlog/r/binlog_rewrite_suppress_use.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_row_binlog.result
    - Update result file since we now use show_binlog_events.inc instead of
      SHOW BINLOG EVENTS. show_binlog_events.inc filters out
      Format_description_log_events.
    
    @mysql-test/suite/binlog/r/binlog_row_ctype_ucs.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_row_insert_select.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_row_mix_innodb_myisam.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_stm_binlog.result
    - Update result file because of changes in test.
    
    @mysql-test/suite/binlog/r/binlog_stm_ctype_ucs.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_stm_insert_select.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_stm_mix_innodb_myisam.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    - Also an assert was added.
    
    @mysql-test/suite/binlog/r/binlog_unsafe.result
    - Update result file because assert was added.
    
    @mysql-test/suite/binlog/t/binlog_killed.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    - Removed useless replace_result.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock_gtid.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_mysqlbinlog_concat.test
    - This test started failing because of this worklog. The test generates
      three binary logs: one with GTID_MODE=ON, one with GTID_MODE=OFF, and
      one with GTID_MODE=ON. Then it runs mysqlbinlog and executes the
      output from mysqlbinlog on a server that uses GTID_MODE=ON.
      Before this worklog, this did not cause any problems, because the
      binary log generated with GTID_MODE=OFF did not contain any GTIDs.
      Now, it contains Anonymous_gtid_log_event, which is not allowed when
      GTID_MODE=ON.
      To fix this, we use the --skip-gtids with mysqlbinlog when processing
      the second binary log file.
    
    @mysql-test/suite/binlog/t/binlog_mysqlbinlog_row_trans.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/binlog/t/binlog_rewrite_suppress_use.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_row_query_log_events.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/binlog/t/binlog_server_id.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/rpl/r/rpl_begin_commit_rollback.result
    - Updated result file because of changes in test file.
    
    @mysql-test/suite/rpl/r/rpl_do_db_filter.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_filter_warnings.result
    - Update result because output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_gtid_binlog_errors.result.THIS
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/r/rpl_gtid_mode.result
    - Update result file because of changes in test file and in output
      from SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/r/rpl_gtid_row_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_gtid_stm_mix_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_ignore_db_filter.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_loaddata_s.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_mixed_implicit_commit_binlog.result
    - Update result file.
    
    @mysql-test/suite/rpl/r/rpl_mts_logical_clock_[1;31mtime[mstamping.result
    - Update result since output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_mts_logical_clock_wrong_start_pos.result
    - Update result because of change in test.
    
    @mysql-test/suite/rpl/r/rpl_mysqlbinlog_gtid_on.result
    - Update result.
    
    @mysql-test/suite/rpl/t/rpl_recovery_replicate_same_server_id.result
    - Update result file because of clarifications in the main test file.
    
    @mysql-test/suite/rpl/r/rpl_replicate_same_server_id.result
    - Update result file because of clarifications in the main test file.
    - Rename the file since this really tests replicate-same-server-id.
    
    @mysql-test/suite/rpl/r/rpl_replication_observers_example_plugin.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-
    test/suite/rpl/r/rpl_replication_observers_example_plugin_io.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/rpl/r/rpl_row_event_max_size.result
    - Update result file because binlog now contains
      Anonymous_gtids_log_event.
    
    @mysql-test/suite/rpl/r/rpl_row_ignorable_event.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_row_implicit_commit_binlog.result
    - Update result file because assert was added.
    
    @mysql-test/suite/rpl/r/rpl_row_mts_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_row_mysqlbinlog.result
    - Update result because test uses the new mysqlbinlog.inc file.
    
    @mysql-test/suite/rpl/r/rpl_row_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_row_until.result
    - Update result file because of simplification in main file.
    
    @mysql-test/suite/rpl/r/rpl_server_id.result
    - Renamed this file. No need to have a numeric suffix.
    
    @mysql-test/suite/rpl/r/rpl_server_uuid.result
    - Update result file because test file was changed.
    
    @mysql-test/suite/rpl/r/rpl_show_relaylog_events.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_skip_slave_err_warnings.result
    - Update result since output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_sp_innodb.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_sp_myisam.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_stm_implicit_commit_binlog.result
    - Update result file because assert was added in the test file.
    
    @mysql-test/suite/rpl/r/rpl_stm_mix_mts_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_stm_mix_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_typeconv.result
    - Update result file because the test file was simplified.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback-master.opt
    - Rename database to make test more readable.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback-slave.opt
    - Rename database to make test more readable.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
    - Test failure fix:
      The test did a START SLAVE UNTIL MASTER_LOG_POS, where
      MASTER_LOG_POS was one byte into a transaction. With no
      Gtid_log_event, that will round the position up so that the entire
      transaction is exeucted on slave. However, if the position is one byte
      into a Gtid_log_event, it will instead stop before the transaction has
      been committed. This caused the test to fail. The fix is to set
      MASTER_LOG_POS to one byte into the BEGIN query_log_event instead.
    - Cosmetic fix:
      added comments to explain what the test does
    - Improve debugging:
      use assertions instead of 'eval SELECT $result as 'Must be 0'
    - Cleanup:
      The test did:
        echo [on master]
        SET SESSION AUTOCOMMIT=0;
      Since there was no semicolon after the echo, the SET statement
      was never executed, only echoed. This was very confusing, and
      the test would fail if we actually set autocommit=0 here. So
      removed this.
    - Cosmetic fix:
      removed redundant DROP DATABASE IF EXISTS at the beginning of the test
    - Cosmetic fix:
      renamed databases to make test more readable:
      db1 -> replicate_do_db
      db2 -> binlog_ignore_db
    - Cosmetic fix:
      replaced
        connection master;
        echo [on master];
      by
        --source include/rpl_connection_master.inc
    - Cosmetic fix:
      use uppercase for SQL in some cases
    
    @mysql-test/suite/rpl/t/rpl_binlog_errors.test
    - Change LIMIT clause of SHOW BINLOG EVENTS to compensate for adding two
      more events.
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_binlog_errors.test
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_dual_pos_advance.test
    - Fix failing test:
      because the number of events has changed, we change the LIMIT options
      for SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/t/rpl_filter_warnings.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_grant_plugin.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/rpl/t/rpl_gtid_binlog_errors-master.opt
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_gtid_binlog_errors.test
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_gtid_drop_table.test
    - Fix typos in comment.
    - Simplify test assertion.
    
    @mysql-test/suite/rpl/t/rpl_gtid_mode.test
    - Fix failing test:
      because the number of events has changed, we change the LIMIT options
      for SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/t/rpl_gtid_row_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_gtid_stm_mix_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_mts_logical_clock_[1;31mtime[mstamping.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_mts_logical_clock_wrong_start_pos.test
    - Mask out positions from result file.
    
    @mysql-test/suite/rpl/t/rpl_mysqlbinlog_gtid_on.test
    - Add comments explaining the purpose of the test.
    - Remove last part of the test. The tested behavior is not intended, and
      this part of the test failed.
    
    @mysql-test/suite/rpl/t/rpl_recovery_replicate_same_server_id.test
    - Use include/assert_grep.inc instead of grep_pattern.inc since
      grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id-master.opt
    - Rename this file to better reflect what is being tested.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id-slave.opt
    - Rename this file to better reflect what is being tested.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id.test
    - Rename this file to better reflect what is being tested.
    - Add comments to explain what is being tested.
    - Upgrade to our current coding standards.
    - Remove a possible race.
    
    @mysql-test/suite/rpl/t/rpl_replication_observers_example_plugin.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/rpl/t/rpl_replication_observers_example_plugin_io.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/rpl/t/rpl_row_ignorable_event.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/rpl/t/rpl_row_mts_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_row_mysqlbinlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/rpl/t/rpl_row_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_row_until.test
    - Read master position using SHOW MASTER STATUS instead of
      SHOW BINLOG EVENTS, because SHOW BINLOG EVENTS requires that you
      give the exact number of events.
    - Use replace_result to filter out exact positions, instead of echoing
      the filtered statement and disabling the query log while executing the
      statement.
    
    @mysql-test/suite/rpl/t/rpl_server_id.test
    - Improve and clarify the comment.
    - Change the name of the test.
    
    @mysql-test/suite/rpl/t/rpl_server_uuid.test
    - Improvements in readability, used when debugging the test. (Eventually
      the fix was elsewhere but we may as well keep the improvements.)
    
    @mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    - Simplify the test suite:
      This family of tests contained a lot of duplications and
      confusions:
      - The following tests existed in the suite:
          rpl.rpl_stm_mix_show_relaylog_events
          rpl.rpl_stm_mix_gtid_show_relaylog_events
          rpl.rpl_stm_mix_mts_show_relaylog_events
          rpl.rpl_row_show_relaylog_events
          rpl.rpl_row_gtid_show_relaylog_events
          rpl.rpl_row_mts_show_relaylog_events
      - The *_mts_* tests were not specific to mts, they were specific to
        innodb. The only difference was that their result files differed on
        the commit events, which were Xid_log_events for MTS and
        query_log_event(COMMIT) events for non-MTS.  There is a flag in
        show_binlog_events.inc to mask Xid events so that they look like
        query_log_events, so we can merge them to one file.
      - The *_gtid_* tests were only different because there was a
        Gtid_log_event when GTID_MODE=ON and no event when GTID_MODE=OFF.
        Now we always have an event, so we can merge the files together.
      - The *_stm_mix_* and *_row_* tests were only different because there
        was DML in the binlogs.  There is no need for DML in this test case,
        it's enough to test with DDL, so we can merge these files together
        too.
      - So now that the output has been unified, we only need one test file
        and one result file.
      - The test used four levels of include files. This was redundant
        and made it very difficult to follow the logic. The test now
        invokes include/show_relaylog_events.inc directly, which
        makes it easier to understand what is going on.
      - The test used show_binlog_events.inc. This was unnecesary since
        the purpose is to test SHOW RELAYLOG EVENTS. Other tests test
        SHOW BINLOG EVENTS. So I have removed SHOW BINLOG EVENTS.
    - Fix test failure:
      The tests were failing because the output of SHOW RELAYLOG EVENTS
      has changed because Gtid_log_event is now generated.
    
    @mysql-test/suite/rpl/t/rpl_skip_slave_err_warnings.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_stm_mix_mts_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_stm_mix_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/t/mysqlbinlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/t/sp_trans_log.test
    - Change LIMIT clause of SHOW BINLOG EVENTS since binlog now
      contains two new events.
    
    @mysql-test/t/user_var-binlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @sql/binlog.cc
    - Make simplified-gtid-recovery stop iteration over binary logs in the
      case that the first binary log contains a Previous_gtids_log_event.
    - Improve English in a comment.
    
    @sql/log_event.cc
    - Document logic for sql_slave_skip_counter.
    - Remove printing of logical [1;31mtime[mstamps from Query_log_event. The code
      for these [1;31mtime[mstamps will be removed in the next patch, but we remove
      the output already in this patch so that the necessary upates of
      tests get included in this patch.
    - Fix bug in Gtid_log_event for sql_slave_skip_counter, by implementing
      Gtid_log_event::do_shall_skip and make it call continue_group.
    
    @sql/log_event.h
    - Implement Gtid_log_event::do_shall_skip.
    
    @sql/rpl_rli.cc
    - Fix bug in START SLAVE UNTIL MASTER_LOG_POS.
    
    @sql/rpl_rli.h
    - Document logic and purpose of is_in_group.
    
    @mysql-test/suite/engines/funcs/t/disabled.def
    - Disable rpl_row_until due to BUG#20365935 (which is unrelated to this worklog).

[33mcommit aaabe8c5ce88fe3b56b46f07a6cfa32cc8b84bd6[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:12:46 2014 +0100

    WL#7592 step 8. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Generate Gtid_log_event_always.
    
    After all the previous preparation steps, we can now generate
    Gtid_log_event always. This does change server behavior. Moreover, it
    breaks many existing test cases that assume there is no Gtid_log_event.
    However, since we have another big change to make (generate
    Previous_gtids_log_event always), we do not touch tests in this patch.
    Later patches will fix the tests.
    
    This patch has three primary goals:
    
     1. Remove Gtid_log_event from the statement/transaction cache.
     2. Generate Gtid_log_event just before flushing the
        statement/transaction cache. Store the event in a separate buffer.
     3. Generate Gtid_log_event even when GTID_MODE=OFF.
    
    In order to make this work, we have to do the following additional
    tasks:
    
     4. Previously, MTS logical [1;31mtime[mstmaps were generated when flushing the
        cache. Since Gtid_log_event is not part of the cache now, we
        generate them in Gtid_log_event.
     5. Since Anonymous_log_events now exist, a case must be added to
        MTS code so that it reads the logical [1;31mtime[mstamps from the
        ANONYMOUS_GTID_LOG_EVENT just like it reads from the GTID_LOG_EVENT.
    
    In addition we do the following cleanup tasks:
    
     6. Remove binlog.cc:gtid_before_write_cache. This functionality is now
        in the new function MYSQL_BIN_LOG::write_gtid. In addition, we move
        the part of gtid_before_write_cache that deals with generating the
        GTID into the new function Gtid_state::generate_automatic_gtid.
     7. Avoid taking a lock in Gtid_log_event constructor.
     8. Remove the '[commit=yes/no]' output when mysqlbinlog prints a
        Gtid_log_event. This was only relevant for a pre-GA version of
        the GTID feature.
    
    @sql/binlog.cc
    - Fix 4: Remove generation of commit_seq_no from Binlog_event_writer.
      This means we do not need the member variables
      Binlog_event_writer::thread_cache or
      Binlog_event_writer::is_first_event any more so we remove them too.
    - Fix 1: Remove generation of Gtid_log_event from
      binlog_cache_data::write_event.
    - Fix 6: Implement MYSQL_BIN_LOG::write_gtid, which does the following
      things:
      - Generate the GTID by calling Gtid_state::generate_automatic_gtid.
        generate_automatic_gtid replaces logic that was previously located
        in gtid_before_write_cache and Group_cache::generate_automatic_gno.
      - Generate Gtid_log_event, which was previously done in
        binlog_cache_data::write_event.
      - Generate the logical [1;31mtime[mstamps for MTS, and pass them to the
        Gtid_log_event constructor. The logic for generating commit_seq_no
        was previously located in binlog_cache_data::finalize.
    - Fix 2,3: Replace call to gtid_before_write_cache by call to
      MYSQL_BIN_LOG::write_gtid.
    - Fix 4: Remove generation of MTS logical [1;31mtime[mstamps from
      binlog_cache_data::finalize.
    
    @sql/binlog.h
    - Add member function write_gtid.
    
    @sql/log_event.cc
    - Fix 7: Change Gtid_log_event::Gtid_log_event(THD*...) constructor to
      avoid taking lock, to rely on what is owned by the thread rather than
      a given Gtid_specification.
    - Fix 4: Change Gtid_log_event::Gtid_log_event(THD*...) to take the MTS
      logical [1;31mtime[mstamps as parameters.
    - Fix 8: Remove "[commit=yes/no]" output when mysqlbinlog prints a
      Gtid_log_event.
    - Fix 4: In Gtid_log_event::write_data_header, do not store
      commit_seq_offset in IO_CACHE::commit_seq_offset. Also, instead of
      storing placeholders for the logical [1;31mtime[mstamps, store the correct
      values directly. (Previously, the logical [1;31mtime[mstamps were not known at
      the [1;31mtime[m Gtid_log_event::write_data_header_to_memory was called. Now
      that we moved the writing of the Gtid_log_event to the end of the
      transaction it is known). Also remove the IO_CACHE* parameter.
    
    @sql/log_event.h
    - Clarify a comment.
    - Fix 4: Update the prototype for Gtid_log_event constructor.
    - Fix 4: Remove the IO_CACHE* parameter to write_to_memory. This was
      only needed because the logical [1;31mtime[mstamps were stored in the
      IO_CACHE; since we do not store thm in the IO_CACHE any more, this
      parameter is not needed now.
    - Fix 2: Define the constant Gtid_log_event::MAX_EVENT_LENGTH, which is
      used in MYSQL_BIN_LOG::write_gtid for the size of the buffer that
      holds the Gtid_log_event.
    
    @sql/rpl_gtid.h
    - Fix 6: Declare the new function generate_automatic_gtid.
    
    @sql/rpl_gtid_state.cc
    - Fix 6: Implement the new function generate_automatic_gtid. This was
      previously done in binlog.cc:gtid_before_write_cache, but it is
      better to have it here since the function is not dependent on the
      binary log.
    
    @sql/rpl_mts_submode.cc
    - Fix 5: Now that ANONYMOUS_GTID_LOG_EVENT is generated, MTS has
      to be able to read the commit_seq_no from it.

[33mcommit 08e3bd904b9185c464cd9b249e32a744ba12cf86[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:05:38 2014 +0100

    WL#7592 step 3. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Clean up binlog.cc:gtid_empty_group_log_and_cleanup.
    
    This only refactors and cleans up the function, to make it easier to
    understand what it does. This does not change the server behavior, so it
    is not possible to have a test case for this.
    
    Address multiple issues in this function:
    - Add comments.
    - The function was only called under the condition that
      thd->owned_gtid.sidno != 0 &&
      thd->variables.gtid_next.type==GTID_GROUP &&
      opt_log_bin && opt_log_slave_updates. These checks were repeated in
      all places where the function was called, so moved the checks into the
      function.
      The check that gtid_next.type==GTID_GROUP was redundant, since
      owned_gtid.sidno is only != 0 if gtid_next.type==GTID_GROUP, so
      changed that into an assertion in the function.
    - This function was called from sql_class.cc. Here, next to the call,
      there was some logic for inserting GTIDs into table in case the binary
      log is off.
      Moved this logic into the function, because (1) this is not a
      replication file, so better to minimize replication logic in it; (2)
      this handles the case of empty transactions, so it seems natural to do
      it inside the function.
    - Rename the function and move it into the MYSQL_BIN_LOG class.
    - Remove the call to gtid_before_write_cache(): this is done in any
      case, at a later point in [1;31mtime[m, since gtid_empty_group_log_and_cleanup
      calls
        mysql_bin_log.commit(THD*,bool), which calls
        MYSQL_BIN_LOG::ordered_commit(THD*,bool,bool), which calls
        process_flush_stage_queue(my_off_t*,bool*,THD**), which calls
        flush_thread_caches(THD*), which calls
        binlog_cache_mngr::flush(THD*,my_off_t*,bool*), which calls
        binlog_cache_data::flush(THD*,my_off_t*,bool*), which calls
        gtid_before_write_cache().
    
    @sql/binlog.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    
    @sql/binlog.h
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Comment the function.
    
    @sql/log_event.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Move the checks for thd->variables.gtid_next.type,
      thd->owned_gtid.sidno, opt_bin_log, and opt_log_slave_updates
      into gtid_end_transaction.
    
    @sql/sql_parse.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Move the checks for thd->variables.gtid_next.type,
      thd->owned_gtid.sidno, opt_bin_log, and opt_log_slave_updates
      into gtid_end_transaction.
    - Move the code for storing GTIDs into table into gtid_end_transaction.

[33mcommit 5ff65df0595094fda32eb4fdc32a39a8d7bd8433[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:02:16 2014 +0100

    WL#7592 step 1. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Small simplifications.
    
    In this patch we just make a few very small simplifications to improve
    DBUG output, add assertions, etc. These are not directly related to the
    worklog, but were improvements made as part of debugging the feature.
    
    This does not change behavior of the server, so it is not possible to
    have a test case for this.
    
    The following changes have been done:
    - Assert that Gtid.set() is only used with a valid sidno and gno.
    - Assert that gtid_acquire_ownership is only called when gtid_next is
      set correctly.
    - Add DBUG_ENTER to some functions.
    - Improve DBUG_PRINT output.
    - Encapsulate all places where THD::owned_gtid is modified.
    - Make Gtid.set() assert that sidno and gno are valid.
    - Gtid.is_null() and Gtid.empty() seem to have been added concurrently
      by different patches. Both have the same purpose, so we unify them
      into Gtid.is_empty(). Also added assert ensure that gno>0 if and only
      if sidno>0; this is consistent with the protocol for THD::owned_gtid
      and with all other places that use this struct.
    - Move forward declarations for functions defined in rpl_rli_pdb.cc from
      rpl_slave.cc to rpl_rli_pdb.h.
    - Make it possible to use rpl_reconnect.inc also when
      rpl_default_connections.inc has not been used previously.
    - Make rpl_init.inc autodetect the number of servers correctly also in
      the case when
      $rpl_topology=='none'.
    - Improve some comments.
    - Remove Xid_log_event::is_valid, which was left behind by mistake
      after WL#7440.
    
    @mysql-test/include/rpl_default_connections.inc
    - Make it possible for mysql-test/include/rpl_reconnect.inc to know
      whether this script has been called.
    
    @mysql-test/include/rpl_init.inc
    - Make rpl_init.inc autodetect that only one server is needed if
      $rpl_topology=='none'
    
    @mysql-test/include/rpl_reconnect.inc
    - Make this script work also if rpl_default_connections.inc has not been
      used.
    
    @mysql-test/include/show_rpl_debug_info.inc
    - Add GTID_MODE.
    
    @sql/binlog.cc
    - Rename Gtid::empty to Gtid::is_empty.
    - Improve DBUG output.
    - Add comment to clarify where FLUSH LOGS is implemented.
    
    @sql/handler.cc
    - Rename Gtid::is_null to Gtid::is_empty.
    
    @sql/log_event.cc
    - Remove strange multiplication by worker id in debug sleep.
    - Add DBUG_ENTER in User_var_log_event::do_apply_event.
    - Fix some comments.
    - Remove Xid_log_event::is_valid, which was left behind by mistake
      after WL#7440.
    
    @sql/log_event.h
    - Use builtin true/false instead of custom defined ones.
    
    @sql/rpl_binlog_sender.cc
    - Improve DBUG output.
    - Add a comment.
    
    @sql/rpl_context.cc
    - Use THD::owned_gtid_set only inside #ifdef HAVE_GTID_NEXT_LIST.
    
    @sql/rpl_gtid.h
    - Make Gtid.set() assert that sidno and gno are valid.
    - Replace Gtid::is_null() and Gtid::empty() by new function
    Gtid::is_empty().
    - Correct comments for return status for from Gtid_set::Gtid_set and
      Gtid_set::add_gtid_encoding.
    - Correct comment for Gtid_state::get_automatic_gno.
    
    @sql/rpl_gtid_execution.cc
    - Add assertions in gtid_acquire_ownership_single.
    - Remove useless return value from skip_statement().
    - Move call to skip_statement() out from DBUG_RETURN(), since function
      calls inside DBUG_RETURN will make the debug trace look wrong if the
      called function uses DBUG_ENTER/DBUG_RETURN too.
    
    @sql/rpl_gtid_state.cc
    - Change group to gtid in debug printout.
    - Add missing semicolon after ifdef-ed out line.
    - Use THD::clear_owned_gtid, instead of duplicating the code inside
      clear_owned_gtid.
    - Rename Gtid::is_null to Gtid::is_empty.
    
    @sql/rpl_rli_pdb.cc
    - Correct DBUG_ENTER text.
    - Use int64 instead of longlong for logical [1;31mtime[mstamps (it is int64
      everywhere else).
    - Use existing symbolic constant instead of hard-coded number.
    
    @sql/rpl_rli_pdb.h
    - Moved these forward declarations to the file where they belong.
    
    @sql/rpl_slave.cc
    - Move two forward declarations to the appropriate header file.
    - Rename Gtid::empty to Gtid::is_empty.
    - Remove Gtid_specification::clear. Now using set_automatic instead.
    
    @sql/sql_base.cc
    - Do not call decide_logging_format from DBUG_RETURN. Doing that causes
      the debug trace to look like decide_logging_format was called after
      the return statement, i.e., one level up in the call stack from where
      it was actually called.
    
    @sql/sql_class.cc
    - Compile out owned_gtid_set since it is only used by the
      compiled out code for gtid_next_list.
    - Use Gtid::clear instead of clearing the values manually.
    
    @sql/sql_class.h
    - Compile out owned_gtid_set since it is only used by the
      compiled out code for gtid_next_list.
    - Use Gtid::clear instead of setting the value directly.
    
    @sql/sys_vars.cc
    - Add DBUG_ENTER/DBUG_RETURN to update_gtid_next.
    
    @storage/perfschema/pfs.cc
    - Remove Gtid_specification::clear. Now using set_automatic instead.
    
    @storage/perfschema/table_replication_execute_status_by_worker.cc
    - Make worker->currently_executing_gtid use type==AUTOMATIC_GROUP to
      indicate that no GTID is executing, rather than type==GTID_GROUP,
      sidno=0, gno=0.
    
    @libbinlogevents/include/control_events.h
    - Remove comment that was completely wrong.
    
    @libbinlogevents/src/control_events.cpp
    - Remove explanation of what GTIDs are (better do that in the manual
      and/or where GTIDs are defined).

[33mcommit 589577345819e77a90b0f6b92f1d9f781449986e[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Mon Jan 19 14:05:21 2015 +0000

    BUG#20380487: RPL_PERFSCHEMA_NO_MASTER_UUID FAILS SPORADICALLY
    
    The test fails due to poor synchronization. The slave thread may
    be slow reading the uuid from the master. It starts, connects and
    then reads the UUID. The user thread (the one from the test case)
    knows nothing about this, it just waits until the slave thread
    starts and connects to the master. Thence, it may already be
    checking that the UUID exported on the P_S table is the one
    expected, and consequently assert too soon - i.e., before the
    slave thread actually retrieves the UUID from the master.
    
    The fix is to make sure that the user thread keep checking until
    it gets the expected UUID or bails out after a certain amount of
    [1;31mtime[m. The latter will make the test case fail, thence acts as an
    implicit assertion.
    
    This patch also cleans up the test case a bit.

[33mcommit fca0b189345ad1fbca0280c93a31d19ae75cfe29[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Fri Jan 16 18:25:13 2015 +0000

    Improve ndb_rpl_slave_binlog_index.test behaviour under Valgrind.
    
    Seems to [1;31mtime[mout waiting for the slave to apply ~16MB of Binlog.
    
    Hopefully reduce / remove failures by reducing replicated data size
    by ~ 100x.

[33mcommit 604f30606bd92ed55af78df798354111850cd5ec[m
Author: Bjorn Munch <bjorn.munch@oracle.com>
Date:   Wed Jan 7 14:10:45 2015 +0100

    Bug #20316320 - DEBUG BUILD ON WINDOWS SHOULD REPLACE /OB0 WITH /OB1 TO SPEED UP TEST RUNS
    
    Replace /Ob0 which CMake automatically adds for debug builds, with /Ob1
    
    This caused linking error due to some functions in my_[1;31mtime[m.c which
    were declared inline there but not in my_[1;31mtime[m.h. This makes little
    sense, so I removed the inline.
    
    Added new CMake option WIN_DEBUG_NO_INLINE which is default OFF.
    If this is turned ON, inlining will again be disabled as before.

[33mcommit 5736ea5b38ef2492471f87389a13bd83f10a7e9c[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Tue Jan 13 16:18:13 2015 +0530

      Bug#20083612 INNODB_UNDO.TRUNCATE_RECOVER FAILS DUE TO "SERVER FAILED TO
      DISSAPEAR"
    
      Tried to reproduce it on Solaris-Sparc machine by running it for 200 [1;31mtime[ms.
      It never failed even once.
      From error message it is evident that it has to do with [1;31mtime[mout.
      We are waiting for purge action to kick in which on normal loaded
      server will happen within the set [1;31mtime[mout (100 secs) but on heavily
      loaded server using Solaris-Sparc may take [1;31mtime[m.
      That's the reason test doesn't fail consistently too.
    
      Also, test is failing on weekly trunk only where-in the machines are
      heavily loaded to use full power to run weekly suite.
    
      I would increase the [1;31mtime[mout from 100 to 240 and monitor it test-case.
      It is diffcult to predict a good [1;31mtime[mout but that's what we can do.

[33mcommit 59e7407392bd10321adbecea03424e915bd6ef8b[m
Merge: 3f4abb52fbf 521789e9f05
Author: sneha modi <sneha.modi@oracle.com>
Date:   Tue Jan 13 08:25:02 2015 +0100

    Adding the base [1;31mtime[ms in the failure message for startup [1;31mtime[m and shutdown [1;31mtime[m.
    
    Merge branch 'mysql-trunk' of myrepo.no.oracle.com:mysql into mysql-trunk

[33mcommit 6ca08005772ef90253e2e5c70bc1237f053d6dec[m
Merge: bfd9bcd0f2b 91a746848ba
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Jan 12 13:04:21 2015 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      WL#8190: Refactor low-level thread handling
      Bug #20043707 RENAME FTS COMMON TABLE DURING ALTER RENAME FAILURE.
      Daily runs are taking too much [1;31mtime[m. It takes almost 16 hours on solaris and windows. This is not acceptable for daily run. Investigations showed that most of the overhead is coming from use of debug binaries. The same suite was tested to finish in 4 hours on windows with non-debug builds. Changing daily runs to use non debug builds. PerPush and weekly will continue to run on debug binaries.
      rpl.rpl_server_uuid was disabled with Bug#19245622  which is in closed state. Replaced with open bug#20341933

[33mcommit 60809a3a804729efda9cb43381c308bbd914e532[m
Author: Anitha <anitha.gopi@oracle.com>
Date:   Mon Jan 12 07:53:27 2015 +0100

    Daily runs are taking too much [1;31mtime[m. It takes almost 16 hours on solaris and
    windows. This is not acceptable for daily run. Investigations showed that most
    of the overhead is coming from use of debug binaries. The same suite was tested
    to finish in 4 hours on windows with non-debug builds. Changing daily runs to
    use non debug builds. PerPush and weekly will continue to run on debug binaries.

[33mcommit ca2dad98b5ca6753328def97843f2a71653af205[m
Merge: 111ce628a73 8bb1d18c714
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sun Jan 11 11:00:25 2015 -0800

    merge 7.3 -> 7.4
    
    Improve garbage collection of clusterj artifacts
    
    A user object obtained via query or session.newInstance is a proxy that
    holds a strong reference to an NdbRecordSmartValueHandlerImpl, which
    in turn holds a strong reference to an NdbRecordOperationImpl, which
    holds strong references to ByteBuffers which wrap native byte arrays.
    
    When the user object goes out of scope, it is garbage collected but
    the objects that are now unreferenced take a long [1;31mtime[m to be collected.
    
    This patch immediately releases objects that are referenced by user objects
    when the user object is collected. This can improve the ability of the
    garbage collector to release the space used by the objects.
    
    DynamicObject.java:
      implement finalize method for dynamically created objects
    
    DynamicObjectDelegate.java:
      add finalize to the methods to be implemented by the dynamic object
    
    DomainTypeHandlerImpl.java:
      require that the proxy (user object) implement finalize
    
    InvocationHandlerImpl.java:
      implement release
    
    KeyValueHandlerImpl.java:
      implement release
    
    NdbRecordBlobImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordOperationImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordSmartValueHandlerImpl.java:
      implement finalize() which forwards to release() to release unneeded objects
    
    NdbOpenJPAValueHandler.java:
      implement release
    
    NoPublicConstructorClusterConnectionService.java:
      fix bug in test

[33mcommit 8bb1d18c7149453fb2341cbb921d4bd345848f04[m
Merge: c4142621122 ffdfdb56f8d
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sun Jan 11 10:07:00 2015 -0800

    merge 7.2 -> 7.3
    
    Improve garbage collection of clusterj artifacts
    
    A user object obtained via query or session.newInstance is a proxy that
    holds a strong reference to an NdbRecordSmartValueHandlerImpl, which
    in turn holds a strong reference to an NdbRecordOperationImpl, which
    holds strong references to ByteBuffers which wrap native byte arrays.
    
    When the user object goes out of scope, it is garbage collected but
    the objects that are now unreferenced take a long [1;31mtime[m to be collected.
    
    This patch immediately releases objects that are referenced by user objects
    when the user object is collected. This can improve the ability of the
    garbage collector to release the space used by the objects.
    
    DynamicObject.java:
      implement finalize method for dynamically created objects
    
    DynamicObjectDelegate.java:
      add finalize to the methods to be implemented by the dynamic object
    
    DomainTypeHandlerImpl.java:
      require that the proxy (user object) implement finalize
    
    InvocationHandlerImpl.java:
      implement release
    
    KeyValueHandlerImpl.java:
      implement release
    
    NdbRecordBlobImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordOperationImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordSmartValueHandlerImpl.java:
      implement finalize() which forwards to release() to release unneeded objects
    
    NdbOpenJPAValueHandler.java:
      implement release
    
    NoPublicConstructorClusterConnectionService.java:
      fix bug in test

[33mcommit ffdfdb56f8d0ccdcceeb71e55fe4390d2a30172e[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sun Jan 11 09:48:44 2015 -0800

    Improve garbage collection of clusterj artifacts
    
    A user object obtained via query or session.newInstance is a proxy that
    holds a strong reference to an NdbRecordSmartValueHandlerImpl, which
    in turn holds a strong reference to an NdbRecordOperationImpl, which
    holds strong references to ByteBuffers which wrap native byte arrays.
    
    When the user object goes out of scope, it is garbage collected but
    the objects that are now unreferenced take a long [1;31mtime[m to be collected.
    
    This patch immediately releases objects that are referenced by user objects
    when the user object is collected. This can improve the ability of the
    garbage collector to release the space used by the objects.
    
    DynamicObject.java:
      implement finalize method for dynamically created objects
    
    DynamicObjectDelegate.java:
      add finalize to the methods to be implemented by the dynamic object
    
    DomainTypeHandlerImpl.java:
      require that the proxy (user object) implement finalize
    
    InvocationHandlerImpl.java:
      implement release
    
    KeyValueHandlerImpl.java:
      implement release
    
    NdbRecordBlobImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordOperationImpl.java:
      implement release() to release unneeded objects
    
    NdbRecordSmartValueHandlerImpl.java:
      implement finalize() which forwards to release() to release unneeded objects
    
    NdbOpenJPAValueHandler.java:
      implement release
    
    NoPublicConstructorClusterConnectionService.java:
      fix bug in test

[33mcommit ea44e85d6b2d5f2e2563eac2f9ceb97037cbd429[m
Merge: ecee931373c 2a4cddebc92
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jan 7 14:11:49 2015 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      Bug#20223893 TIMEOUT, SET IN CMAKE/BOOST.CMAKE IS TOO SMALL FOR FLAKY CONNECTIONS
      Description: According to the manual, the maximum value for group_concat_max_len in 64bit mysqld is 18446744073709547520. This works fine when set dynamically in the mysql command prompt. However, if the value is set in my.cnf, mysqld will refuse to honor and will not start up.
      Bug#20108908 CMAKE DOESN\'T PRINT WHICH BOOST VERSION IS REQUIRED
      Bug#20308296 CLEANUP #INCLUDE OF PLUGIN CODE IN M_STRING.H
      Bug#20083691 DBUG_SUICIDE SOMETIMES EXIT WITH STATUS 0 FOR MYSQLD_CMD
      Bug #19817663 PASSWORD HASH SHOW FOR PROXY USER.
      Bug#18758002:RPL.RPL_FILTER_WARNINGS FAILED WITH RESULT MISMATCH
      Remove trailing space in include file used by multiple tests. Trailing spaces are detected by the CHTEST tool used for vetting new MTR tests in WLs, and we try to avoid trailing spaces in new files. When new tests include wait_until_connected_again.inc they will fail CHTEST runs regardless of the quaility of the new test itself. This change avoids such issues.
      Modifying the variable to be used for comparing the server startup [1;31mtime[m to base server startup [1;31mtime[m.
      Bug#20310730 - N TEST WORKLOAD: DO NOT USE ZIP COMPRESSION FOR PAGE SIZE > 16K It is not supported by InnoDB which leads to false alarm test failures.
      Bug#20083691 DBUG_SUICIDE SOMETIMES EXIT WITH STATUS 0 FOR MYSQLD_CMD

[33mcommit 2a4cddebc92b946d5007a83ff6ee81d1ee4f8cbf[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jan 7 10:58:55 2015 +0100

    Bug#20223893 TIMEOUT, SET IN CMAKE/BOOST.CMAKE IS TOO SMALL FOR FLAKY CONNECTIONS
    
    Add configurable [1;31mtime[mout for download of boost.

[33mcommit ece3652d5226807ca2c92d80e2f84c67b76734c7[m
Author: sneha modi <sneha.modi@oracle.com>
Date:   Tue Jan 6 19:42:17 2015 +0100

    Modifying the variable to be used for comparing the server startup [1;31mtime[m to base server startup [1;31mtime[m.

[33mcommit 627fc718140c845a561dc042979c65c4f626554c[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Jan 5 17:37:23 2015 +0200

    Bug#20303205 INNODB FAILS TO UPDATE UPDATE_TIME AFTER XA COMMIT
    
    trx_resurrect_table_locks(): Add each table to trx->mod_tables for
    transactions that are in XA PREPARE state, so that the update_[1;31mtime[m
    will always be updated on XA COMMIT.
    
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>

[33mcommit 927600620b896a156a0183600611a8d93b590747[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Tue Dec 30 16:16:04 2014 +0100

    Bug#17238670 MAIN.FUNC_GCONCAT SPORADICALLY FAILING ON PB2 WITH RESULT MISMATCH
    
    The func_gconcat test fails some[1;31mtime[ms due to an off-by-one difference
    in the rows estimate in the output from explain.
    
    To stabilize the explain output, ANALYZE TABLE is run on the table used
    by the test.

[33mcommit 8af8eae07a59d91440814cd4fec8e439d4235683[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Dec 31 14:24:10 2014 +0200

    Correct a stale comment that was missed in
    
    revno: 3434
    revision-id: sunny.bains@oracle.com-20110905210427-gj3jdiysqkyz5me3
    parent: tor.didriksen@oracle.com-20110905141327-tl6lc4tcfcbheoec
    committer: Sunny Bains <Sunny.Bains@Oracle.Com>
    branch nick: trunk
    [1;31mtime[mstamp: Tue 2011-09-06 07:04:27 +1000
    message:
      Non-functional change. Change trx_sys->lock to trx_sys->mutex
      in the comments.

[33mcommit 5a473a621d4149b4163c396998557ee759f8313f[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Nov 28 14:57:54 2014 +0100

    Bug#19505175 REGRESSION IN Q21 OF DBT3 TEST FOR WL7339
    
    WL#7339 started to use more correct records per key estimates from
    InnoDB. This caused the cost estimate for the original query plan
    to become four [1;31mtime[ms higher than previous, which caused another
    query plan to be selected.
    
    The new selected query plan does a table scan on the orders table.
    This table has 1.5 million records. On this table we have the
    following condition:
    
      orders.o_orderstatus = 'F'
    
    This is used for calculating the condition filter effect for this
    table. With the current guestimate for equality conditions of 0.005,
    this caused the estimated number of partial rows to be produced from
    this table to be only 7500. In reality, 730.000 rows were produced.
    
    The cause for this very wrong estimate is that the o_orderstatus
    column only contains three distinct values and almost half of the
    records have the value 'F'.
    
    To make the condition filter produce a better (more conservative)
    estimate for cases like this and to reduce the likelihood of
    similar regressions, the fix for this problem is to increase the
    condition filter constant for equality estimates to 0.1. With this
    we estimate that 1 of 10 records will pass the filter (instead of
    only 1 out of 200).
    
    Changes in tests:
    
    @ mysql-test/r/compress.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/derived.result
       Mostly changes to filtered estimates. One change in query plan.
       This query now gets the same plan as it had before WL#6635.
    @ mysql-test/r/ds_mrr-big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/eq_range_idx_stat.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/explain_for_connection_rqg_json.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_for_connection_rqg_trad.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_other.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_small.result
       Mostly changes to filtered estimates. One query has
       changes to its query plan: changes from using FirstMatch
       to do materialization.
    @ mysql-test/t/filter_single_col_idx_small.test
       Added test to verify that the filter estimate for basic
       filter constants is not less than one row
    @ mysql-test/r/greedy_optimizer.result
       Mostly changes to filtered and cost estimates. Eight queries
       has changes in query plans. Four of these changes back to
       the query plan they had before WL#6335 was pushed. The four
       last had similar changes in query plans as for when WL#6635 was
       pushed.
    @ mysql-test/r/greedy_search.result
       Mostly changes to filtered estimates. Two queries has
       changes to its query plan. One of these are returned to
       what it was before WL#6635 was pushed. The other has
       similar changes to as what was introduced by WL#6335.
       The number of partial query plans for some queries are
       increased. With the exception of one of these, all
       new numbers are below what it was before WL#6335.
       The one the has a very high increas in the number of
       partial query plans, changes from 22201 to 5799004.
       Before WL#6635 this query considered 735518 partial
       query plans.
    @ mysql-test/r/group_by.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap_hash.result
       One change in query plan. The new plan is more similar
       to what the query plan was before WL#6635 was pushed. It has
       the same join order but uses table scan on the second
       table instead of ref access as it did before WL#6635.
    @ mysql-test/r/index_merge_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_intersect_dml.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_myisam.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_none.result
       Changes in filtered, rows and estimates in explain output.
    @ mysql-test/r/innodb_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bkaunique.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_nojb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_none.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/named_pipe.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/negation_elimination.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/order_by_all.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_icp_mrr.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_none.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/partition_locking.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/partition_pruning.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr_cost.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/shm.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl_compress.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_sj_all.result
      Mostly changes to filtered, rows and cost estimates. Three queries
      has plan changes. Two of these returns to what the plan was before
      WL#6635. The last has changes that makes the plan look more like
      it was before WL#6635 but not identical.
    @ mysql-test/r/subquery_sj_all_bka.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Five of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_mat.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Six of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_nosj.result
      Changes to filtered and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka_nixbnl.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bkaunique.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subselect_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/type_temporal_fractional.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/wl6711_heap_to_disk.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/innodb/r/innodb_lock_wait_[1;31mtime[mout_1.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/general2_no_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/general2_ps_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_no_prot.result
      Changes in filtered and rows estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_ps_prot.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_no_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_myisam.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/perfschema/r/batch_table_io_func.result
      Changes in filtered estimate in explain output.
    @ sql/item.h
      Change the value for the condition filter constant COND_FILTER_EQUALITY
      from 0.005 to 0.1.
    @ unittest/gunit/item_filter-t.cc
      Change in unit test for condition filter for IN lists:
      Reduced from having six values in the IN list to four
      values. The reason is that with six values in the
      IN list the calculated condition filter will be larger
      than 0.5 and then rounded down to 0.5.

[33mcommit 0e9fb6d353436a01c4745ecd58f132d9a08d4ae1[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Thu Dec 25 17:35:07 2014 +0600

    This is follow-up for bug#19792203 - FAILING ASSERTION: TRX->IS_DD_TRX == FALSE WHEN KILLING QUERY WITH CONVERT_TZ
    
    After a patch for the bug has been pushed into trunk the test main.[1;31mtime[mzone2
    starts failed. The reason for test failure is that the pushed patch contains
    a test that sets debug variable before calling some sql-statement and reset it
    after statement executed. Since debug variable is present in the mysql server
    compiled with debug option this test fails when runs against non-debug mysql
    server. To fix this issue the test was moved into separate file that has
    directive include/have_debug.inc to force run only against server compiled
    in debug mode.

[33mcommit 19c53adf14ae5a1d69047dbb4f810d1eb9878c58[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Wed Dec 24 17:47:03 2014 +0600

    Bug#19792203 - FAILING ASSERTION: TRX->IS_DD_TRX == FALSE WHEN KILLING QUERY WITH CONVERT_TZ
    
    When the SQL-function CONVERT_TZ is executed the function my_tz_find
    is called to get [1;31mtime[mzone information. This function calls
    the function open_trans_system_tables_for_read() to open the tables
    [1;31mtime[m_zone_name, [1;31mtime[m_zone, [1;31mtime[m_zone_transition_type, [1;31mtime[m_zone_transition.
    This function starts attachable data dictionary transaction and calls the
    function open_tables() to iterate along the list of tables mentioned above
    and call the method open() for every table's handlerton.
    Handlerton for InnoDB tables sets the flag trx->is_dd_trx on opening the table.
    After all tables are opened SUCCESSFULLY the function lock_tables() is called
    in order to lock the tables. As a side effect this function sets the THD::lock
    to point to a lock structure. Later on closing system tables the value of
    THD::lock is checked to determine whether the table's lock has to be released.
    When THD::lock is not NULL the method ha_innodbase::external_lock() is called
    to release the InnoDB table lock. Implementaion of ha_innodbase::external_lock()
    makes transaction commit and resets the flag trx->is_dd_trx.
    The reason for the bug is that if an attempt to open the second or further
    table is failed the function lock_tables() is never called and the transaction
    never be closed. As side effect the flag trx->is_dd_trx is never be reset that
    leads to crash inside the function trx_free().
    
    To fix the crash setting of the flag trx->is_dd_trx moved into the method
    ha_innobase::external_lock().

[33mcommit 1141b2df98859c548a9110f398cd9cca6a86a310[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Wed Dec 24 17:13:25 2014 +0600

    This is a fix for Bug#11745851 - MYSQL_TZINFO_TO_SQL DOES NOT WORK WITH STRICT_ALL_TABLES
    
    On some unix platforms there are [1;31mtime[mzone information files
    with junk content. Such files begin with correct magic string
    'TZif' but almost all of other fields in such files have zero values
    and [1;31mtime[mzone abbreviation string in the file has the value
    "Local [1;31mtime[m zone must be set--see zic manual page".
    
    Loading data from such file leads to generating of the
    following INSERT statement:
    
    INSERT INTO [1;31mtime[m_zone_transition_type (Time_zone_id, Transition_type_id,
    Offset, Is_DST, Abbreviation) VALUES
    (@[1;31mtime[m_zone_id, 0, 0, 0, 'Local [1;31mtime[m zone must be set--see zic manual page')
    
    The column [1;31mtime[m_zone_transition_type.Abbreviation is declared
    as CHAR(8) so execution of the statement above is failed since
    column constraint is violated.
    
    To fix this bug [1;31mtime[mzone information files that contains
    the magic string 'Local [1;31mtime[m zone must be set--see zic manual page'
    as the value of [1;31mtime[mzone abbreviation string will be either skipped or
    an error will be returned depends on arguments specified to
    the program mysql_tzinfo_to_sql. In case when directory is specified
    as its argument such file will be skipped, for other case an error will
    be returned.
    
    This patch also changes the function print_tz_as_sql() a bit.
    Since the column [1;31mtime[m_zone_transition_type.Abbreviation is declared
    as CHAR(8) the width of value for the column abbreviation is limited
    by 8 chars when values for the statement INSERT INTO [1;31mtime[m_zone_transition_type
    is generated.

[33mcommit 6b81826a3beb9466dd67d722446dce248971a217[m
Author: Dmitry Shulga <dmitry.shulga@oracle.com>
Date:   Wed Dec 24 16:19:37 2014 +0600

    WL#7160 - Move plugin and servers tables from MyISAM to transactional storage.
    
      The mysql.plugin and mysql.servers system tables were stored in MyISAM.
    This WL makes them stored in InnoDB, so that these tables can take benefit of
    transactional and crash-safe SE.
    
    Technically, the following changes have been done within the scope of this WL:
      * the function innobase_is_supported_system_table() has been modified
        to return true for the system tables plugin and servers. This function
        is assigned to the function pointer is_supported_system_table of Innodb
        handlerton and is called every [1;31mtime[m when a system table is created to check
        whether it’s allowed to create this one with the InnoDB engine type;
      * INSTALL/UNINSTALL PLUGIN statements are DDL by implication, but
        nevertheless the flag CF_AUTO_COMMIT_TRANS isn't set for these statements.
        This patch fixes this issue;
      * Implementation of the statements INSTALL/UNINSTALL PLUGIN moved to classes
        derived from Sql_cmd;
      * Engine type for the tables plugin and servers has been changed to InnoDB
        in the sql- scripts called during a server installation/server upgrade;
      * The function plugin_init() is modified in order to enable InnoDB plugin
        before loading data from the system table plugin.

[33mcommit 57b4da607c1b829fb9ce721f4b3589b41ce5c95b[m
Merge: 6e6d8c1423a d2f276138e8
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Dec 22 11:10:53 2014 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      Bug #19815702 TIS620: CRASH WITH MULTI TABLE DELETE
      Bug#20246211 - IMPLEMENT --CREATE_OPTION IN IBTEST_CTD_W.PL It allows passing arbitrary clauses for CREATE TABLE.
      Bug#20221262: MEMORY LEAK IN MYSQLTEST AFTER UNCLEAN SHUTDOWN OF THE SERVER
      Bug#20221262: MEMORY LEAK IN MYSQLTEST AFTER UNCLEAN SHUTDOWN OF THE SERVER
      Bug#19940297 UPGRADE TO BOOST 1.57
      Revert "WL#7706 : SSL cert and key generation for MySQL Community"
      bug1984591 (post push) valgrind failure fixing.
      WL#7706 : SSL cert and key generation for MySQL Community
      Bug#20111105 CRASH IN BTR_CUR_LATCH_LEAVES
      WL#5757 :  InnoDB: Support Page Sizes 32k and 64k
      WL#7440: Moving binlog event decoding into separate package.
      Follow-up fix to Bug#19694618 DEFINE A DATA TYPE WRAPPER FOR PRETTY-PRINTING TABLE NAMES
      Bug #20065517 MEMORY LEAK OF 8160 BYTES IN MYSQL_STMT_PREPARE() API
      WL#7440: Moving binlog event decoding into separate package
      Bug #19845913 MTS: WITH MSR, SLAVE CRASHES ON SHUTDOWN COMMAND AT RPL_MTS_SUBMODE.H:35
      BUG#19704710: TWO DIFFERENT REPLICATION CHANNELS WITH SAME (HOST, PORT) ARE POSSIBLE
      Bug#20225524 TESTS SHOULD USE AT LEAST 60 SEC TIMEOUT FOR SHUTDOWN_SERVER COMMAND
      BUG#20029625 - HANDLE_FATAL_SIGNAL (SIG=11) IN DICT_MEM_TABLE_COL_RENAME_LOW
      Bug #20065461 MEMORY LEAK IN MYSQL_LIST_FIELDS() API
      WL#8216: Deprecate and remove the sync_frm sysvar
      Fix result content mismatch after merge from mysql-5.6
      BUG#20080942 - ADAPTIVE_HASH_SEARCHES_BTREE NOT UPDATED
      BUG#19665003: RPL.RPL_MULTI_SOURCE_BASIC UNSTABLE ON PB2
      BUG#20080942 - ADAPTIVE_HASH_SEARCHES_BTREE NOT UPDATED
      WL#7440: Moving binlog event decoding into separate package.
      Fix test failures like blob_redo on pb2
      wl5757: Skipping the testcases running in parallel
      Empty commit for gcov
      renamed:  libbinlogevents/include/binary_log_funcs.h ->  libbinlogevents/export/binary_log_funcs.h
      Simply max record size check when optimistic update
      wl5757: Reolved an issue of build and install locations
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      wl5757# Test: adding a new inc file, reolved windows problem and experiment with parallel run on PB2
      wl5757# Test:Windows platform issue was resolved ,experiment with parallel run on PB2.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding
      WL#7440
      Fix innodb.innodb_update_[1;31mtime[m failure on pb2
      Force pb2 rebuild
      Fix pb2 failures with 64k page size: innodb.blob_redo innodb.innodb-wl5522-1 i_innodb.innodb_bug14529666 innodb.innodb-import-partition innodb.innodb_update_[1;31mtime[m
      WL#7440: Moving binlog event decoding into separate package
      perl code added for platform independent for removeal of files
      Modify default.push to run mtr test on 32k and 64k page sizes
      Fix innodb_zip.16k failure on pb2
      Fix pb2 failure of innodb.innodb, innodb_zip.16k and innodb.innodb_trx_weight
      WL#7440: Moving binlog event decoding into separate package.
      wl5757:2014-11-11:Fix fb compatible problem with UNIV_EXTERN_STORAGE_FIELD
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440:Moving binlog event decoding into a separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package.
      WL7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package.
      Wl#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding to a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7546: Moving binlog event decoding into a separate package
      WL#7546: Moving binlog event encoding into a separate package
      WL#7440: Moving binlog decoding to a separate package.
      WL#7440: Moving binlog decoding into a separate package.
      WL#7440: Moving binlog decoding into a separate package.
      WL#7440: Moving binlog decoding to a separate package
      WL#7440: Moving binlog decoding into separate package
      WL#7440: Moving binlog events decoding to a separate package.
      Wl#7440: Moving binlog decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Fixing Windows compilation error
      WL#7440: Fix for Windows build failures
      Fixing compilation error on Solaris:
      Error: The function "strndup" must have a prototype. Error: The function "memcpy" must have a prototype.
      Add dependency on GenError, to avoid race conditions.
      When building with -DWITHOUT_SERVER=ON and SunStudio, we need <stdlib.h> rather than <cstdlib> to find the definition of size_t.
      Add -m64 to link command when building libbinlogstandalone.so
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Addressing Comments from reviewers
      WL#7440: Post-push fix
      Wl#7440: Fixing valgrind error:        Cause:          The variable server_version in class Format_description_event is initialized          either by a global variable or through the parameter server_ver passed          to the Format_description_log_event constructor.          In the cases when it is initialized by server_ver, some part of the          variable server_version remains uinitialized.
      Wl#7440: Moving decoding into a separate package (post-push fix)
      -Adding comments for all the binlog event class constructors. -Removed the check for finding the definition of macros min and max in the library  libmysqlclient and undefining them, as they are not required. -Added files decoder.cpp & decoder.h
      WL#7440: Addressing comments from Reviewers.
      Addressing review comments from Mats and Shubhangi on the patch for Removing the use of global variable server_version from libbinlogevents
      WL#7440: Addressing comments from reviewers
      Addressing review comments on the patch for Moving the code for advancing the event buffer to point to the beginning of post-header from all event constructors.
      Addressing review comments on the patch for removing the is_valid virtual method.
      WL#7466: Post push fix, resolving build issues on windows-32 bit OS
      WL#7440 : Fixing Linker errors on Windows-32bit OS
      - The bapi_memdup method shouldn't have the dest pointer as const - And the dest pointer should be passed to memcpy and not its address.
      WL#7440: Updating Query_event and Query_log_event
      WL7466: Renaming the directory libbinlogapi to libbinglogevents
      WL#7466: Renaming the directory libbinlogapi to libbinlogevents
      Addressing review comments from Mats about changing the comment.
      Fixed failing test case, i_binlog.binlog_large_row
      WL#7440: Post push fix: Addressing comments from reviewers
      - Enabling compilation of free standing version of the library libbinlogevents   - The library name is updated from libbinlogevent to libbinlogevents   - Added comments and fixed indentation   - Added getters for Rows_event, Table_map_event and Incident_event members
      Fixing failing test rpl_known_bugs_detection
      ISSUE: Earlier is_valid was a pure virtual method declared inside the class Log_event and implemented by all the event classes.
      Issue: Fixing valgrind errors   ------
      ISSUE ===== The previous implementation for fetching the type_code for an event was: get_type_code() was a pure virtual function in the class Log_event, which was implemented by all the event subclasses, and returned the enum value, Log_event_type. To fetch the type_code, a call to the method was made using a Log_event pointer.
      Addressed review comments: -Addition of one more structure Uuid_parent in binlog_event.h which is the parent structure for Uuid struct in rpl_gtid.h -Two elements are moved from struct gtid_info to the struct Uuid_parent, namely   -bytes_to_copy   -uuid_buf
      Summary: --------    Changing the header guards to the format XXX_INCLUDED for libbinlogevent
      Fixing failing test cases on Solaris platform
      Fixing the Solaris build failure and test failures:
      - Removed the following:  Binary_log.cpp : Contained the error messages which not generated from                   libbinlogevent anymore. It is, therefore, removed.
      Addressed review comments.
      1. Fixing build errors on Solaris
      - Changed the name of the library generated from libbinlogapi to libbinlogevent  - Removed dependency on dbug library, when the library is compiled independently  - Removed the files protocol.h and protocol.cpp, for they are not a part of the    decoder  - Removed the files resultset_iterator.h and resultset_iterator.cpp, for they    are a part of the bindings and not the Decoder
      Resolving build issues on Windows-64bit
      Description: ------------
      Fixing Valgrind Errors
      Removing the content handlers from the Decoder code.
      - Adding header file for LOAD_DATA_INFILE events - Fixing failing test cases
      The patch addresses the following:
      - Adding documentation for User_var_event and Rand_event - Changed the name of method get_version_product to get_product_version as it make more sense that way. - Fixing the Solaris compiliation failure
      Fixing the tests failures arised after pushing Ignorable_event and Rows_query_event
      Address comments from reviewers
      Addressing review comments
      Addressing review comments.
      7763d4167ec47828853cf2f36e48cd2e0f032d23
      Description:  ============  - Fixing build errors on Solaris-x86, 64 bit machine  - Adding zlib include paths
      Post Push Fix: Resolving Valgrind errors on WL branch
      Post Push Fix: Fixing failing tests on Windows
      Post push fix
      BAPI 82: Linking binlog event processing library to the MySQL server
      BAPI 85: Moving Decoding of binary log events in a separate package
      Fixing the valgrind error on pb2:
      Fixed failing tests on solaris
      Fixed failing test cases, and valgrind errors also
      Fixed failing tests
      -Fixing test failures -Removed the file access_method_factory.h as it is not needed
      -Fixed the failing test cases on pb2 -Missed to add the call for ctor of Binary_log_event through Gtid_event ctor,  added now
      Post Push Fix: Resolving linker errors on Windows in WL branch
      Replaced uint*korr with le*toh
      Fixing compilation error when compiled with GCC 4.7+
      BAPI 85: Moving Incident_log_event
      BAPI 85: Moving Gtid_log_event to libbinlogapi
      BAPI 85 : Moving Previous_gtids_log_event
      BAPI 85: Refactoring Heartbeat_log_event
      Fixing the tests failures arised after pushing Ignorable_event and Rows_query_event
      - Fixing valgrind errors on WL branch    Error:      conditional jump or move depends on uninitialized value
      - Fixing valgrind errors on WL branch for Rows_log_event::do_add_row_data   - Adding Documentation   - Addressed Review comments   - Addressing a few TODO's
      BAPI 85  :Moving Ignorable_log_event and Rows_query_low_event
      BAPI 85: Moving Decoding of Rows event into libbinlogapi
      Removing the usuage of global variable server_version from libbinlogapi
      - Reverting changing uint*korr methods with le16toh, le32toh and le64toh   as they are not working as expected. - Uint*korr methods will be removed in subsequent patches once the cause of   the above failure is detected
      Fixing compilation error on solaris,
      - Added documentation for Format_description_event and Start_event_v3 - Removed use of uint*korr methods. - Fixing the Solaris compiliation failure
      BAPI 85:     Refactoring Start_log_event_v3 and Format_description_log_event
      BAPI-100 : Adding the HAVE_MYSYS flag
      Post-push Fix
      BAPI 85:  Refactoring Rand_log_event and User_var_log_event
      BAPI 85: Refactoring Intvar_log_event
      Post push Fix
      Post Push Fix
      BAPI 85: Moving Decoding logic of QUERY_EVENT and EXECUTE_LOAD_QUERY_EVENT            into binlogapi library
      BAPI 85: Moving the decoding logic of Xid_log_event into binlogapi library          Xid_log_event has been made the subclass of Xid_event
      Fixing failing test cases:
      -Added one more ctor for Binary_log_event   - Binary_log_event(const char **buf, const Format_description_event->binlog_version)     This will create an object of Log_event_header and initialize m_header - And added one more ctor for Log_event also   - Log_event(const char *buf, const Format_description_event)   - this will be called by the events ctor which are not yet implemented -Removed the creation of Log_event_header object from log_event.cc, rpl_slave.cc,  rpl_binlog_sender.cc as it will be done in the above ctor of Binary_log_event
      Post push fix: Resolving build errors on Windows
      BAPI 85: Moving decoding logic of the events representing LOAD_DAT_INFILE            SQL query into binlogapi library
      BAPI 85: Moving the decoding logic of Rotate_event and Stop_event into binlogapi library
      post push fix
      pot push fix for solaris
      post push fix
      post push fix
      post push fix
      post push fix
      Post push fix
      Fixing issues on solari and rhelx86_64
      reverting last 5 pushes
      Post push checks To be reverted
      post push fix to be reverted
      display dbug messages
      post push fix
      post push fix
      post push fix.  Note: we need to revert this patch later
      Fixing undefined reference errors to functions defined in endian.h
      Post push fix
      post push fix
      Post push fix for windows+solaris
      post push fix
      post push fix
      Post-push fix
      BAPI 85: rb2997: Moving event header from class Log_event into a separate class
      BAPI 84: rb2984: Moving methods and variables used for event checksum tests            into libbinlogapi
      BAPI 84: Linking libbinlogapi to mysql-trunk
      Initial import of binlog-api

[33mcommit d2060c3d50b4ab944f636564434ac78eb39a3631[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Tue Dec 16 21:49:23 2014 +0200

    Bug #19845913 MTS: WITH MSR, SLAVE CRASHES ON SHUTDOWN COMMAND AT RPL_MTS_SUBMODE.H:35
    
    The crashes were caused by an attempt to access already deleted
    info by Slave SQL thread or Worker threads.
    WL6314 design [1;31mtime[m deallocation of rli->current_submode turned to be wrong.
    It actually was against existing pattern of how to do that properly.
    In more details the shutdown thread may break waiting for slave threads
    explicit exit through [1;31mtime[mout option.
    And when it choses that it carries on to destoy the above object
    which can be still in use. That causes crashing.
    
    Proper deletion of dyn objects related to MTS, RLI::current_submode
    incl, must be done inside or near slave_stop_workers().
    And that's what this patch has to correct.
    
    I could not create an mtr testcase because testing with a mysqltest
    client connection that does return ack before the server gets down
    is something that mtr does not like:
    Attempt to execute
    
      --send STOP SLAVE
    
      let $restart_file= $MYSQLTEST_VARDIR/tmp/mysqld.1.expect;
      --exec echo "wait" > $restart_file
      --shutdown_server 10
      ...
    
    results in
      mysqltest failed but provided no output
    
    Therefor the patch is tested manually.

[33mcommit 5505f669979a0504988c36fa1e13b42daeda1b6e[m
Author: Anitha <anitha.gopi@oracle.com>
Date:   Thu Dec 18 09:30:01 2014 +0100

    Bug#20225524 TESTS SHOULD USE AT LEAST 60 SEC TIMEOUT FOR SHUTDOWN_SERVER
    COMMAND
    
    Removed [1;31mtime[mouts less than the default from all shutdown_server commands in the
    .tets and .inc files

[33mcommit b52a531b4da50993ad158592fab77a787096626f[m
Author: Robert Golebiowski <robert.golebiowski@oracle.com>
Date:   Mon Dec 15 15:55:19 2014 +0100

    Bug #19309652 MAGICALLY REAPPEARING GRANT OPTION FOR PROXY
    
    Description:
    -----------
    The GRANT PROXY syntax is overloaded to accept a WITH GRANT OPTION
    modifier,
    which allows the specified user to issue GRANT PROXY privileges for the
    proxied user account.  When GRANT PROXY is issued against an existing
    proxy/proxied user combination, the operation appears to succeed, but
    apparently only affects the in-memory privileges.  Consequentially,
    while the
    WITH GRANT OPTION appears to have been honored, it is reverted the next
    [1;31mtime[m
    FLUSH PRIVILEGES is issued, or the server is restarted
    
    Fix:
    ------------
    Option with_grant was not saved to the disk  when the proxy grant was
    updated. This is why it was as if it had not changed at all after
    flushing the privilages. The fix makes sure that with_grant gets stored
    on the disc.
    
    Testing:
    ------------
    Added new test: i_main.grant-bug19309652

[33mcommit 7660b6b1675ad29882c2c77e4f54b5b48c9d9374[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Mon Dec 15 11:04:04 2014 +0100

    Bug#20174289: REFACTOR STRUCTS.H
    
    Part 1: Remove dead code from structs.h and move code out from
    structs.h to sql_[1;31mtime[m.h and table.h where it more naturally
    belongs (higher cohesion, lower coupling).

[33mcommit 0474bbfefc60a9385cedb32498d7c539b3569795[m
Author: Staale Deraas <staale.deraas@oracle.com>
Date:   Tue Dec 9 16:20:03 2014 +0100

    Bug #20108866: --LOG_TIMESTAMPS=UTC NEEDS --LOG-BIN
    
    The problem was that the --log_[1;31mtime[mstamp startup option introduced by WL#6661 by mistake
    required --log_bin in order to be accepted.
    
    This fix removed the copy/paste of OPT_BINLOG_FORMAT as a required argument from the
    command line parsing. Before this fix you would get the folowing warning printed, if you
    tried to set --log_[1;31mtime[mstamps=UTC and log-bin was not used:
    
    "You need to use --log-bin to make --binlog-format work."

[33mcommit 2f11308e7ae876880302b5024018918011003c56[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Dec 9 14:23:53 2014 +0000

    Bug #19858151   MAKING DATA NODE SELF-EXCLUDE MECHANISM MORE ROBUST
    
    The LCP Scan Frag watchdog and GCP Monitor can both decide to
    exclude a node if it is too slow when participating in these
    protocols.
    
    Currently the exclusion is implemented by asking the failing node
    to shutdown.
    
    This allows it to first log some debugging information, and
    shutdown with a clear failure cause.
    
    However in some situations it may be slow to shutdown, and prolong
    the duration of GCP/LCP stall for the other unaffected nodes.
    
    To minimise this [1;31mtime[m, this fix adds an isolation mechanism which
    causes the other live nodes to forcibly disconnect the failing
    node after some delay.
    
    This gives the failing node the chance to shutdown with debugging
    info and a good message if possible, but limits the [1;31mtime[m the others
    must wait for this to occur.
    
    Once the live nodes have processed the disconnection of the failing
    nodes, they can commence failure handling and restart the protocol(s).
    
    Even if the failed node takes a long [1;31mtime[m to shutdown, the others
    can proceed with processing.
    
    The GcpMonitor and the Lcp Scan Fragment watchdog are enhanced to
    make use of this mechanism.
    
    Three new testcases are added :
     1.  GcpStop
         Testing of GcpStop handling in normal cases
     2.  GcpStopIsolation
         Testing of GcpStop self-shutdown failure so that Isolation is
         required
     3.  LcpScanFragWatchdogIsolation
         Testing of Lcp Scan Fragment Watchdog where Isolation is
         required.
    
    These are added to the daily-devel test suite.
    
    Additionally :
    
    Bug #20128256   NDB : GCP STOP MONITOR HAS ONLY ONE BULLET
    
    This bug was discovered while testing (GcpStop testcase).
    
    The Gcp Monitor did not continue operation after detecting a Gcp stop.
    
    This is fixed so that it does continue operation after detecting
    a Gcp stop, and this is tested by both the GcpStop and GcpStopIsolation
    testcases (where the Master node is not a victim and must detect and handle
    multiple separate GCP stop events).

[33mcommit 8abc0fee02ee6b3cbb74280f620c4d6c71b58c01[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Thu Dec 4 15:22:49 2014 +0530

    Bug#20083349: RPL.RPL_PERFSCHEMA_APPLIER_STATUS_BY_COORDINATOR FAILING
                  IN TEST ASSERTION
    
    The objective of the test was to test that there are no coordinators
    on the master. Number of coordinators should be zero. But the assertion
    fails some[1;31mtime[ms with a value 1.
    
    This test failed sporadically on PB2 but it always failed if it is executed
    immmediately after rpl.rpl_dual_pos_advance.test; In the latter
    test the configuration is 1->2->1. This means the master is  also
    a slave which had it's own SQL thread(coordinator). Because
    of this the assertion failed correctly.
    
    The fix was to properly cleanup the rpl_dual_pos_advance.test by
    using force_restart.inc

[33mcommit 29c2cb26e4f315f3a44e0fc22e44aea25d77682e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Dec 8 10:36:42 2014 +0100

    Update to recent fix where we failed to close scan cursors
    at the end of their life[1;31mtime[m.
    
    As we are not interested in these NdbScanOperation objects
    anymore at all, we could as well tell ::close() to
    even release the NdbScanOperations at this point.

[33mcommit f24d63e37257b9a202c63e920a55cbc926114580[m
Author: david <david.zhao@oracle.com>
Date:   Mon Dec 1 17:38:48 2014 +0800

    Bug#20073459 GIS PERFORMANCE ISSUE: TOO MANY REALLOCS
    
    Geometry data in practical use cases often are at least hundreds of bytes, so
    we should use more aggressive allocation increments i.e. this version
    
             int String::reserve(size_t space_needed, size_t grow_by)
    
    to reserve memory space for GIS data.
    This function is dedicated for GIS data, it allocates memory by bigger
    chunks, should be called by GIS code. Now when calling this function, we grow
    by 512 bytes normally, and only grow by smaller steps if we know for sure we
    are reaching the end of the geometry data.
    
    We should not use the int String::reserve(size_t space_needed) version as some
    of the GIS code does currently, otherwise we would be reallocating the geometry
    memory buffer multiple [1;31mtime[ms incrementally for a single geometry when
    generating/producing the geometry data piece by piece, which is a lot of
    overhead.
    
    Since this is a performance enhancement, no test cases are added. Existing
    GIS tests can verify the correctness of this change.

[33mcommit 35e14302c38ac390b8e233fe60169d5e2de63a5d[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Dec 3 14:52:03 2014 +0100

    Bug#20136739 NEW WARNINGS AFTER MERGE FROM SERVER 5.5.41 AND 5.6.22 INTO MYSQL CLUSTER
    
    Fix for new warnings in memcache backported from mysql-trunk-cluster:
    
    ------------------------------------------------------------
    revno: 3663
    revision-id: magnus.blaudd@oracle.com-20140605095217-cmp3odmco9z7n8w5
    parent: magnus.blaudd@oracle.com-20140605094150-o8bkqi2l442wcmya
    committer: magnus.blaudd@oracle.com
    branch nick: trunk-cluster
    [1;31mtime[mstamp: Thu 2014-06-05 11:52:17 +0200
    message:
      WL#6815 Adapt MySQL Cluster to 5.7
       - The -Wdeclaration-after-statement has been made default
         for build of C files in 5.7. This flag turns on the warning
         "ISO C90 forbids mixed declarations and code" when code and
         declarations are mixed in a function. Most likely this intends
         to help with keeping the C code portable to compilers which
         does not yet support such constructs  - thus we should adapt
         all our code not to cause that warning.
       - Fix by changing the code in ndb_engine.c which uses
         said construct.

[33mcommit 0ccd4f473cf223c369ac6cff8d96c4e33dd949a1[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Wed Dec 3 10:37:59 2014 +0530

    Bug#19664931: rpl.rpl_parallel_change_master is unstable on pb2
    
     rpl_parallel_change_master when run stand alone, the test always passes.
     However, if rpl.rpl_multi_source_channel_name_relay_log is executed
     apriori, then rpl.rpl_parallel_change_master always fails.
     On pb2, since the ordering of the tests is not defined,
     rpl.rpl_parallel_change_master fails some[1;31mtime[ms and becomes unstable
     if the test is run on the server left by
     rpl.rpl_multi_source_channel_name_relay_log.test
    
     The actual cause of the problem is rpl_multi_source_channel_name_relay_log
     sets --relay-log and --relay-log-index options and does not restore their
     values to "slave-relay-bin".
    
     In the test rpl.rpl_parallel_change_master, when the server is restarted,
     the values of --relay-log and --relay-log-index are defaulted to
     "slave-relay-bin". This change in values after server restart caused
     inconsistencies with the previous run of the server options inherited from
     rpl_multi_source_channe_name_relay_log.test
    
     Fixed by properly cleaning up the rpl_multi_source_channel_name_relay_log.test
     using force_restart.inc

[33mcommit c16980eb3f9a7d6bf3875586e39299d3cdc6d5d5[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Dec 2 21:07:34 2014 +0100

    WL#6815  Adapt MySQL Cluster to 5.7
    
     - usage of the "dummy"  date[1;31mtime[m '0000-00-00 00:00:00' is not allowed in strict mode
     - three tests fail becuase they use the "dummy" date[1;31mtime[m as default value. However no
      insert are done where the default value is used. Remove the usage of "dummy"  date[1;31mtime[m
     - add test that "dummy" date[1;31mtime[m is still allowed(also for NDB tables) without strict mode.

[33mcommit a81490e1c1773a01c4b5097738af529efdfb90c2[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Tue Dec 2 18:58:50 2014 +0000

    BUG#19665003: RPL.RPL_MULTI_SOURCE_BASIC UNSTABLE ON PB2
    
    The test case is a multi-source test case, where server 2 replicates
    from server 1 and 3.
    
    There was a race, causing the slave to concurrently execute two CREATE
    TABLEs with the same table definition coming from server 1 and 3. One
    of the CREATE TABLE statements had an IF EXISTS clause, which would
    make the test pass some[1;31mtime[ms. However, if that statement was replayed
    first on the slave, then next CREATE TABLE would trigger an error
    (table already exists).
    
    We fix this by making sure that the CREATE TABLE is always executed
    first. We allow the second one to execute as well and be put in the
    binary log, since it will not hurt correctness.

[33mcommit bf7a132bff1adde5359c9ef3b3b773ea29c269c4[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:40:28 2014 +0100

    Cherry-picked from mysql-5.6.22-release
    
    ------------------------------------------------------------
    revno: 6230
    revision-id: venkatesh.duggirala@oracle.com-20141030170343-4pfljkuwt48gkerg
    parent: marko.makela@oracle.com-20141030065549-onni0r95h2uc4s8w
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Thu 2014-10-30 22:33:43 +0530
    message:
      Bug#19704825 TEMPORARY SLAVE TYPE CONVERSION TABLES RELEASED TO EARLY
    
      Problem: The memory used in preparing slave type conversion temporary table
      is getting released early and causing unexpected results
    
      Analysis: As part of bug#18770469 fix, We introduced an event
      m_event_mem_root (a special mem root), added to Log_event
      class. While server is creating the temporary table, the memory needed
      is allocated from this special mem root which will be freed in ~Log_event()
      i.e., the scope of this memroot is one event. But it could happen
      that in some cases, server might need to access this
      conversion temporary table for next following event.
      For eg: A nested row event (insert is causing insert in a trigger)
      In this situation, the memory is getting delayed too early
      and causing issues when the server is trying to access the temporary
      table inside the trigger.
    
      Fix: We cannot use a mem_root whose scope is limited to an event
      execution in this situation. With some further analysis, found out
      that clearing a thd->mem_root at the end of statement (upon applying
      an event which has STMT_END_F flag) will solve out of memory problem
      while running a long transactions (bug#18770469) and will also
      make this reported problem (memory is getting released early) to go away.

[33mcommit 743379549b7cc4e6d1ac52c951c076bbc10ca487[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:35:34 2014 +0100

    Cherry-pick from mysql-5.6.22-release
    
    ------------------------------------------------------------
    revno: 6216 [merge]
    revision-id: balasubramanian.kandasamy@oracle.com-20141023045941-2v1eniiofwf83t4w
    parent: balasubramanian.kandasamy@oracle.com-20141021142230-66l9sqp68jjk91f1
    parent: balasubramanian.kandasamy@oracle.com-20141023045647-vdg0wswxf99ka31j
    committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Thu 2014-10-23 06:59:41 +0200
    message:
      Merge 5.5 => 5.6 Rpm 4.9 and newer is more strict regarding weekday in %changelog
        ------------------------------------------------------------
        revno: 2875.475.9
        revision-id: balasubramanian.kandasamy@oracle.com-20141023045647-vdg0wswxf99ka31j
        parent: balasubramanian.kandasamy@oracle.com-20141021141947-6s5z6rglet5z9huk
        committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
        branch nick: mysql-5.5
        [1;31mtime[mstamp: Thu 2014-10-23 06:56:47 +0200
        message:
          Rpm 4.9 and newer is more strict regarding weekday in %changelog

[33mcommit dc033c35bc5e3626de7d3ee5956f5410789c7da3[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:33:56 2014 +0100

    Cherry-picked from mysql-5.6.22-release
    
    ------------------------------------------------------------
    revno: 6215 [merge]
    revision-id: balasubramanian.kandasamy@oracle.com-20141021142230-66l9sqp68jjk91f1
    parent: balasubramanian.kandasamy@oracle.com-20141021130408-6wrsyrsn0q2g9h13
    parent: balasubramanian.kandasamy@oracle.com-20141021141947-6s5z6rglet5z9huk
    committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Tue 2014-10-21 16:22:30 +0200
    message:
      Merge 5.5 => 5.6 Fix changelog entries build failure
        ------------------------------------------------------------
        revno: 2875.475.8
        revision-id: balasubramanian.kandasamy@oracle.com-20141021141947-6s5z6rglet5z9huk
        parent: balasubramanian.kandasamy@oracle.com-20141021124858-1bd81lznq6qo24f6
        committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
        branch nick: mysql-5.5
        [1;31mtime[mstamp: Tue 2014-10-21 16:19:47 +0200
        message:
          Fix changelog entries build failure

[33mcommit 30a284b3eb441b2091336ddb74857cd4426d2c9c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:32:40 2014 +0100

    Cherry-picked from mysql-5.6.22-release
    
    ------------------------------------------------------------
    revno: 6187 [merge]
    revision-id: balasubramanian.kandasamy@oracle.com-20141006113040-cbrx6pbrzmmrr8yo
    parent: jon.hauglid@oracle.com-20141006105632-v80eiee7hfnazb4z
    parent: balasubramanian.kandasamy@oracle.com-20141006111941-zoqvgxbtqy0v659h
    committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Mon 2014-10-06 13:30:40 +0200
    message:
      Merge 5.5 => 5.6 Add license info in each subpackage
        ------------------------------------------------------------
        revno: 2875.468.120
        revision-id: balasubramanian.kandasamy@oracle.com-20141006111941-zoqvgxbtqy0v659h
        parent: jon.hauglid@oracle.com-20141006105453-3a4g8db44bzux741
        committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
        branch nick: mysql-5.5
        [1;31mtime[mstamp: Mon 2014-10-06 13:19:41 +0200
        message:
           Add license info in each subpackage

[33mcommit d3695c50bc36743ac2c6374d9fa8429d1b3d7190[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:29:03 2014 +0100

    Cherry-picked from mysql-5.6.22-release
    
    ------------------------------------------------------------
    revno: 6184 [merge]
    revision-id: magnus.blaudd@oracle.com-20141002141200-bimdxhvfjabkbpkt
    parent: thirunarayanan.balathandayuth@oracle.com-20141001063950-avm93e3egdsaoxv3
    parent: magnus.blaudd@oracle.com-20141002135802-00q4w0jl8n4utndq
    committer: magnus.blaudd@oracle.com
    branch nick: 5.6
    [1;31mtime[mstamp: Thu 2014-10-02 16:12:00 +0200
    message:
      Merge 5.5 -> 5.6
        ------------------------------------------------------------
        revno: 2875.468.118
        revision-id: magnus.blaudd@oracle.com-20141002135802-00q4w0jl8n4utndq
        parent: kristofer.pettersson@oracle.com-20140929081738-k7bfyw7eul1lby5r
        committer: magnus.blaudd@oracle.com
        branch nick: 5.5
        [1;31mtime[mstamp: Thu 2014-10-02 15:58:02 +0200
        message:
          Bug#19553099 EXPIRE_LOGS_DAYS=1 CRASH MYSQLD DURING RESTART, DBUG_SYNC(NULL, ...)
    
           - Restarting mysqld with --expire-log-days=1 triggers 'log_in_use()' to be called while current_thd is NULL.
           - Check current_thd before calling DEBUG_SYNC() to avoid passing NULL pointer to DEBUG_SYNC()
           - Wrap debug code construct inside #ifndef DBUG_OFF like in other parts of the file

[33mcommit 7108dd886480392f2e382ae8d17ec65922031b45[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:15:07 2014 +0100

    Cherry-picked from mysql-5.5.41-release
    
    ------------------------------------------------------------
    revno: 4731
    revision-id: balasubramanian.kandasamy@oracle.com-20141023045647-vdg0wswxf99ka31j
    parent: balasubramanian.kandasamy@oracle.com-20141021141947-6s5z6rglet5z9huk
    committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
    branch nick: mysql-5.5
    [1;31mtime[mstamp: Thu 2014-10-23 06:56:47 +0200
    message:
      Rpm 4.9 and newer is more strict regarding weekday in %changelog

[33mcommit 9a296ad68ad9ee29838aa5ddf5879f947f3d5a35[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:12:35 2014 +0100

    Cherry-picked from mysql-5.5.41-release
    
    ------------------------------------------------------------
    revno: 4730
    revision-id: balasubramanian.kandasamy@oracle.com-20141021141947-6s5z6rglet5z9huk
    parent: balasubramanian.kandasamy@oracle.com-20141021124858-1bd81lznq6qo24f6
    committer: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
    branch nick: mysql-5.5
    [1;31mtime[mstamp: Tue 2014-10-21 16:19:47 +0200
    message:
      Fix changelog entries build failure

[33mcommit d4186f171e02e93de165b9d17de8ffac748d8933[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Dec 2 13:02:20 2014 +0100

    Cherry-pick from mysql-5.5.41-release
    
    ------------------------------------------------------------
    revno: 4721
    revision-id: magnus.blaudd@oracle.com-20141002135802-00q4w0jl8n4utndq
    parent: kristofer.pettersson@oracle.com-20140929081738-k7bfyw7eul1lby5r
    committer: magnus.blaudd@oracle.com
    branch nick: 5.5
    [1;31mtime[mstamp: Thu 2014-10-02 15:58:02 +0200
    message:
      Bug#19553099 EXPIRE_LOGS_DAYS=1 CRASH MYSQLD DURING RESTART, DBUG_SYNC(NULL, ...)
    
       - Restarting mysqld with --expire-log-days=1 triggers 'log_in_use()' to be called while current_thd is NULL.
       - Check current_thd before calling DEBUG_SYNC() to avoid passing NULL pointer to DEBUG_SYNC()
       - Wrap debug code construct inside #ifndef DBUG_OFF like in other parts of the file

[33mcommit c97e1fa9f31dc8d750ebb67749fcd6319547711c[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Nov 20 14:30:17 2014 +0100

    Bug#20031708 ASSERT ON GROUP_CHECK::IS_FD_ON_SOURCE
    
    Problem: a too strong assertion.
    It is best understood by looking at this query:
    
    CREATE VIEW v2 AS
     SELECT t1.pk, t2.col_int_key+1 as c, t1.pk+t2.col_int_key as p
     FROM t1, t2;
    
    # FDs will be discovered in this order: {}->v2.pk, v2.pk->v2.c,
    # v2.c->v2.p
    SELECT COUNT(*), v2.p
    FROM v2
    WHERE v2.c=v2.p and v2.c=v2.pk AND v2.pk = 4;
    
    The query has COUNT(*) so must be validated by
    only_full_group_by.
    Everything happens in the loop of is_fd_on_source().
    - 1st iteration:
    Because there is no GROUP BY list, there is no "determined" column at
    the start. So the block searching for "unique keys among already
    determined columns" finds nothing.
    Then find_fd_in_cond(WHERE) scans the AND parts in order and finds that
    v2.pk is determined (=4).
    - 2nd iteration:
    this [1;31mtime[m we find an already determined column (v2.pk) which is a
    unique key (it's Item_view_ref wrapping t1.pk, primary key of t1)
    so we add its table's map bit (t1's i.e. '1') to whole_tables_fd (all
    columns of t1 are indeed determined). Then we scan WHERE  and
    find that v2.c=v2.pk makes v2.c be determined.
    - 3rd iteration:
    it starts with last_whole_tables_fd=1 (content of whole_tables_fd so
    far), finds no new key, scans WHERE and finds that v2.c=v2.p makes
    v2.p be determined; then it computes map_of_new_eq_fds ("new" = those
    discovered in this iteration), it's v2.p->used_tables() i.e. 3, and
    the assertion "this map should not intersect last_whole_tables_fd"
    fires.
    
    The assertion is wrong today and has to be removed.
    This assertion is a leftover of the prototype stage of WL#2489, where
    FD logic was limited to columns of base tables and to inner joins, so
    all relevant items were simply Item_field having only one
    "used_tables()" bit (its table's bit); it made sense to
    require that any addition to the 'fd' list would not be made for
    columns of tables which are already in whole_tables_fd (the goal was
    efficiency and non-redundancy). With views and their expression-based
    multi-table columns, as we see, this is not valid anymore. It is
    entirely possible to first add t1 to whole_tables_fd, then, after more
    discovery loops, to add v2.p which involves t1 columns but not only
    them.
    
    Fix: relax the assertion; also removed "eq" from the name map_of_new_eq_fds
    because if views are involved, when find_fd_in_cond() scans an equality
    it may also discovers FDs in an underlying view and they are not always
    equality-based.
    
    Added tests:
    - the reporter's testcase
    - a variant
    - the query used in the description above
    - a query with COALESCE
    The reporter's testcase was a conjunction of view and outer join which
    forms another legal case where the assertion was excessive. It is
    strongly related to the subtely shown by the query with COALESCE.

[33mcommit 75b61ee49bd67a915dbb4303c2999fa39848ac97[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Dec 2 11:25:52 2014 +0800

    Fix innodb.innodb_update_[1;31mtime[m failure on pb2

[33mcommit 0dc80dc62be015d861aeb2d3e43f43bd98927d62[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Fri Nov 28 12:35:20 2014 +0200

    bug#19930381 WL1697: START SLAVE CRASHES POST RESTART, BLOCKING GTID_MODE (ON/OFF) TESTING
    
    The actual reason of the bug is that MSR removes unrelated to a being reset channel
    worker records out of mysql.slave_worker_info.
    When previously the table may be totally demolished by RESET SLAVE, now
    MSR has to remove affected workers delicately.
    
    That is done with refactoring basic methods of the slave info storage to
    chose the key prefix scan instead of former data scanning.
    
    This patch does not address few more lingering issues that are spotted
    in MSR+MTS recovery.
    To brief, the channel workers are not initialized properly when the worker info
    table is read at mts_recovery_groups() [1;31mtime[m.
    Therefore these workers can not take part in recovery.

[33mcommit 19855664de0245ff24e0753dc82723fc4e2fb7a5[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Fri Nov 28 19:15:23 2014 +0800

    Fix pb2 failures with 64k page size: innodb.blob_redo innodb.innodb-wl5522-1
    i_innodb.innodb_bug14529666 innodb.innodb-import-partition innodb.innodb_update_[1;31mtime[m

[33mcommit a13dedd53f4744f6758c78f4cc067781821245dc[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Nov 26 16:33:40 2014 +0100

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - The lex_start() function uses the global cost model
       variables. Those variables are created at the end of
       init_server_components()
      - When the ndbcluster_init() function starts
      the ndb_util_thread and ndb_binlog_thread those
      global variables haven't yet been created. The handlerton
     are initialized earlier in init_server_components.
     - Fix by removing these early calls to lex_start(), it
      is a function that should be called 'before every query'.
     - Calling the function just after creation of THD is probably
      just copy&paste or leftover from the [1;31mtime[ms when ha_ndbcluster called
      mysql_query() direct, nowadays we use the Ed_connection class
     which handles running queries against the local mysqld for us.

[33mcommit 4e054381b673cae1a6265fc48571a87a2fe161d6[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Wed Nov 26 13:31:29 2014 +0100

    BUG#75007: Ensure that we drop LCP_FRAG_REP and LCP_COMPLETE_REP that originates from a dead node. The LCP could potentially be done before this arrives due to node failure handling, these signals are some[1;31mtime[ms delayed and thus we need to take care that we can still receive them.

[33mcommit 76bc26636cfc069e3447ab790391e397e7825756[m
Author: BennyWang <benny.wang@oracle.com>
Date:   Wed Nov 26 07:25:10 2014 +0100

     WL#411: Generated Columns
      Fixed bug#20022189: WL411:DEBUG ASSERT AT FIELD_LONG::VAL_INT IN SQL/FIELD.CC
      For the issued query
      SELECT alias1 . `col_int` AS field3 FROM E AS alias1 ORDER BY field3;
      because field3 is a stored GC, it can be got from table E directly without
      evaluating its' generated expression,which means the base columns used to
      evaluate value of field3 don't exist in READ_SET.
      However due to ORDER-BY, field3 is needed to be written. During written, its'
      generated expression is required to be evaluated. But because the READ_SET
      doesn't include base columns, the evaluation has a problem.
    
      There are two ways to solve such a problem:
      1) Short term, add the base columns related with field3 into READ_SET during
      preparing for FILESORT.
      2) Long term, separate INSERT/UPDATE/REPLACE operation with other operations
      on stored GC, which should not be evaluated again.
    
      Now, I chose 1) because of [1;31mtime[m limitation.

[33mcommit ed2032587f2ecdd6eed6f5aaf577a8f08eff2ab6[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Wed Nov 26 00:32:40 2014 +0100

    BUG#74594: Assumption that all alive nodes have participated in LCP at close down [1;31mtime[m caused havoc since we can now be alive and still not have participated in the LCP. So had to move the send of the last LCP_COMPLETE_REP backwards a bit to be able to reuse the m_participatingDIH bitmap to send to.

[33mcommit b510550a6e96ef127ffe0c2c11dc3ca4fb45b986[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Tue Nov 25 08:38:30 2014 +0530

    - Bug#20083612: INNODB_UNDO.TRUNCATE_RECOVER FAILS DUE TO "SERVER FAILED TO
      DISSAPEAR"
    
      Truncate of undo tablespace is an asynchronous activity that happens in
      background as part of purge-action. There is no hard guarantee that the
      action will be completed within the stipulated [1;31mtime[m.
      Designing a test-case that relies on such an action is big
      challenge but at same [1;31mtime[m we can make some assumption that even on
      heavily loaded system with some [1;31mtime[mout we should hit the action.
      Timeout that was configured currently was not enough for weekly-trunk
      load on all machines so increasing it such that it can cover up
      for slower and faster machine.

[33mcommit 31f37333dcdf339d8b24d7765fdcb4e67bc2ad60[m
Merge: 9065d8efaaf 2dc42680207
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Nov 24 20:55:45 2014 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      BUG#11747548: DETECT ORPHAN TEMP-POOL FILES, AND HANDLE GRACEFULLY
      Bug #19183565   CREATE DYNAMIC INNODB_TMPDIR VARIABLE TO CONTROL              WHERE INNODB WRITES TEMP FILES
      BUG#19363386: BINLOG.BINLOG_STM_DROP_TMP_TBL FAILS WITH RESULT CONTENTS MISMATCH
      Bug#10063897 - COM_EXECUTE potential problem.
      Bug#19786861: HANDLE_FATAL_SIGNAL AT MARK_TRX_READ_WRITE IN HANDLER.CC
      Bug#19786861: HANDLE_FATAL_SIGNAL AT MARK_TRX_READ_WRITE IN HANDLER.CC
      Bug #19880569: CIRCULAR INCLUDE DEPENDENCY BETWEEN SQL_LEX.H AND SQL_CLASS.H
      Bug#20048354 UNDO TABLESPACE FLAGS MISMATCH AT SERVER STARTUP
      inserted copyright. Modified test runs to have an own var dir.
      Revert "Bug #18607971 : 5.5 TO 5.6 REGRESSION WITH A SUBQUERY IN THE FROM CLAUSE."
      Bug #18607971 : 5.5 TO 5.6 REGRESSION WITH A SUBQUERY IN THE FROM CLAUSE.
      Bug #20031243 CREATE TABLE FAILS TO CHECK IF FOREIGN KEY COLUMN NULL/NOT NULL MISMATCH
      Bug #18607971 : 5.5 TO 5.6 REGRESSION WITH A SUBQUERY IN THE FROM CLAUSE.
      Bug #20029439: default_password_life[1;31mtime[m is volatile, doesn't need to be-   protected by mutex

[33mcommit ab740f6b6b9015561e90b2790fc912e7ab4e84f1[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Wed Nov 19 13:21:31 2014 +0100

    BUG#74594: Node failure in inopportune [1;31mtime[m caused crash

[33mcommit 8344dcc9ea2df81fbf9b3e862bcf87b2592ae431[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Tue Nov 18 17:18:53 2014 +0200

    Bug #20029439: default_password_life[1;31mtime[m is volatile, doesn't need to be-
      protected by mutex
    
     Based on an external contribution by Stweart Smith:
    
         removed the volatile qualifier
         added a lock when reading the value to ensure proper handling using
           the PolyLock classes
         created a dedicated lock for the variable

[33mcommit 8e41c195191919ed7e070b963c7fd8d8476a5890[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Mon Nov 17 17:37:18 2014 +0100

    BUG#74594: Missed setting lcp id in COPY_TABREQ, added ints of LCP variables on replica and fragment records, removed init of lcpStatus in INCL_NODEREQ which depended on old fact that we included when LCP was idle, now this is no longer necessary, missed init of m_lcp_start_[1;31mtime[m in including node in LCP, added a number of log prints

[33mcommit f8ca5fdf0a74525672e53d1fb05192cfecb97868[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 17 12:17:01 2014 +0100

    Bug#19676350 NDB_RPL_CONFLICT_EPOCH SHOWS SCHEMA DIST TIME FAST FORWARD DUE TO EINVAL
    
     - On OSX pthread_cond_[1;31mtime[mdwait() returns EINVAL when same condition variable is used with
       different mutexes simultaneously.
     - rewrite the two places which uses pthread_cond_[1;31mtime[mdwait(&injector_cond) with other
       mutexes to first unlock he other mutex before waiting on the condition. This causes
       a small chance that wakeup from pthread_cond_signal() is lost. That's however
       acceptable since the wait is done with such short [1;31mtime[mout that wakeup will soon be
       detected anyway.

[33mcommit 1c9dd529d55ba9d83389c5a76edf5b57878f4930[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Sat Nov 15 08:45:03 2014 +0100

    BUG#19795152: Fix a placement new that overwrote the node recovery [1;31mtime[mrs in debug mode, added a bit more debug code and arranged the parameters in some common functions a bit more logically

[33mcommit 7b14b067763682f21a5da70b84de9f6f606c412a[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Sat Nov 15 01:36:36 2014 +0100

    BUG#74594: Remove need for long wait for LCP to copy meta data to make restart [1;31mtime[ms more predictable and faster

[33mcommit 3fcde47d60eb9d66828ea1cb08547f04e932b507[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Wed Nov 12 15:34:04 2014 +0100

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - The lex_start() function uses the global cost model
       variables. Those variables are created at the end of
       init_server_components()
     - When the ndbcluster_init() function starts
       the ndb_util_thread and ndb_binlog_thread those
       global variables haven't yet been created. The handlerton
       are initialized earlier in init_server_components.
     - Fix by removing these early calls to lex_start(), it
       is a function that should be called 'before every query'.
     - Calling the function just after creation of THD is probably
       just copy&paste or leftover from the [1;31mtime[ms when ha_ndbcluster called
       mysql_query() direct, nowadays we use the Ed_connection class
       which handles running queries against the local mysqld for us.

[33mcommit e669ad6dcf132e7998e480184e8676cec7ded6f7[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Nov 11 16:50:41 2014 +0800

    Bug#19424075    WRITE/SYNC REDO LOG BEFORE FLUSH THREAD CACHE TO BINLOG
    
    Removing system variable 'binlog_max_flush_queue_[1;31mtime[m' broke
    sys_vars.session_track_system_variables_basic in some platforms.
    
    Post-fix: Restore the system variable 'binlog_max_flush_queue_[1;31mtime[m'.

[33mcommit c443ca7e306fce6c1856d69063e1999908d4937e[m
Merge: 0b5c5bfe76b 0d35c6771c1
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Nov 7 13:11:43 2014 -0800

    Merge 7.3 -> 7.4
    bug#19913569
    
    Add [1;31mtime[mout to clusterj for initial connection to mgm
      This allows clusterj to report errors due to shutting down mgm nodes
      The default [1;31mtime[mout is too long for some production use
    
    Constants.java:
      Add property name for connection property PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
      Add default value for DEFAULT_PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
    
    SessionFactoryImpl.java:
      Add [1;31mtime[mout property CLUSTER_CONNECT_TIMEOUT_MGM
      Use [1;31mtime[mout value to connect to cluster connection service
    
    ClusterConnectionService.java:
      Change signature of create method to include [1;31mtime[mout value
    
    ClusterConnectionImpl.java:
      Use [1;31mtime[mout value to set [1;31mtime[mout after create but before connect
    
    ClusterConnectionServiceImpl.java:
      Use [1;31mtime[mout with create method
    
    Bundle.properties:
      Add new error message for failure of set_[1;31mtime[mout
      Change log message for connect to include [1;31mtime[mout value

[33mcommit 0d35c6771c1c96baaa3fdffd56790b5d83158f60[m
Merge: 4f1953af7fa db365b4da88
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Nov 7 13:03:56 2014 -0800

    Merge 7.2 -> 7.3
    bug#19913569
    
    Add [1;31mtime[mout to clusterj for initial connection to mgm
      This allows clusterj to report errors due to shutting down mgm nodes
      The default [1;31mtime[mout is too long for some production use
    
    Constants.java:
      Add property name for connection property PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
      Add default value for DEFAULT_PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
    
    SessionFactoryImpl.java:
      Add [1;31mtime[mout property CLUSTER_CONNECT_TIMEOUT_MGM
      Use [1;31mtime[mout value to connect to cluster connection service
    
    ClusterConnectionService.java:
      Change signature of create method to include [1;31mtime[mout value
    
    ClusterConnectionImpl.java:
      Use [1;31mtime[mout value to set [1;31mtime[mout after create but before connect
    
    ClusterConnectionServiceImpl.java:
      Use [1;31mtime[mout with create method
    
    Bundle.properties:
      Add new error message for failure of set_[1;31mtime[mout
      Change log message for connect to include [1;31mtime[mout value

[33mcommit db365b4da88c4ea68061c798b41de017c3ed71fb[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Nov 7 13:00:42 2014 -0800

    bug#19913569
    
    Add [1;31mtime[mout to clusterj for initial connection to mgm
      This allows clusterj to report errors due to shutting down mgm nodes
      The default [1;31mtime[mout is too long for some production use
    
    Constants.java:
      Add property name for connection property PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
      Add default value for DEFAULT_PROPERTY_CLUSTER_CONNECT_TIMEOUT_MGM
    
    SessionFactoryImpl.java:
      Add [1;31mtime[mout property CLUSTER_CONNECT_TIMEOUT_MGM
      Use [1;31mtime[mout value to connect to cluster connection service
    
    ClusterConnectionService.java:
      Change signature of create method to include [1;31mtime[mout value
    
    ClusterConnectionImpl.java:
      Use [1;31mtime[mout value to set [1;31mtime[mout after create but before connect
    
    ClusterConnectionServiceImpl.java:
      Use [1;31mtime[mout with create method
    
    Bundle.properties:
      Add new error message for failure of set_[1;31mtime[mout
      Change log message for connect to include [1;31mtime[mout value

[33mcommit fc3c271adf2cac75fd3cc545047cf70b42c71788[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Wed Nov 5 12:34:42 2014 +0100

    Bug#19931126: VALGRIND REPORTS USE OF UNINITIALIZED VALUE IN
                  MY_WILDCMP_BIN_IMPL
    
    The LIKE operator accepts ESCAPE clauses that contain expressions that
    are constant at execution [1;31mtime[m. However, it only evaluates the
    expression in the ESCAPE clause if its value is known at resolve [1;31mtime[m.
    If the ESCAPE clause contains an expression that is constant at
    execution [1;31mtime[m, but unknown at resolve [1;31mtime[m, the escape character will
    be uninitialized, and the LIKE operator will produce unreliable
    results. This is a regression in 5.6.
    
    The fix factors out the code that initializes the escape character
    from Item_func_like::fix_fields() into a new function. If the escape
    expression is constant and known at resolve [1;31mtime[m, it is initialized by
    fix_fields() as before. If its value is not known at resolve [1;31mtime[m,
    Item_func_like::val_int() will now initialize the escape character the
    first [1;31mtime[m it is called.
    
    The fix also makes the range optimizer skip the optimization with LIKE
    if the escape character is unknown at resolve [1;31mtime[m. Otherwise, it
    would use the uninitialized value for the escape character and produce
    unreliable results.

[33mcommit 9cdea16ab19b5d305e09305a2acd5922fa496914[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Sat Nov 1 12:38:48 2014 +0530

    Bug#19605472 -  max_statement_[1;31mtime[m (memory/sql/thd_[1;31mtime[mr) leaks
                    memory
    
    Description:
    ------------
    Issue here is, while cancelling the windows [1;31mtime[mr the state of the
    [1;31mtime[mr is set to "signalled" in function "my_[1;31mtime[mr_cancel".
    Because of this in "sql_[1;31mtime[mr.cc" layer, [1;31mtime[mr is set as
    non-reachable and responsibility of freeing memory allocated
    for "THD_[1;31mtime[mr_info" is left to the "[1;31mtime[mr_callback". But
    "[1;31mtime[mr_callback" function is not called when [1;31mtime[mr is cancelled.
    Hence memory leak is found on Windows platform.
    
    Fix:
    ------------
    Changed code of "my_[1;31mtime[mr_cancel" function for  Windows so that,
    state of the [1;31mtime[mr is set to "signalled" when [1;31mtime[mr is expired
    (or signalled). Otherwise [1;31mtime[mr state is set to "non-signalled".
    With this, when [1;31mtime[mr state is non-signalled, "THD_[1;31mtime[mr_info"
    object is reused. On next "set" operation of [1;31mtime[mr, new windows
    [1;31mtime[mr is created for it in "my_[1;31mtime[mr_set" function. Memory
    allocated for it is freed in "thd_[1;31mtime[mr_destroy" operation.
    When [1;31mtime[mr state is "signalled", "THD_[1;31mtime[mr_info" is freed in
    "[1;31mtime[mr_callback" function. On next "set" operation of [1;31mtime[mr,
    new "THD_[1;31mtime[mr_info" object and windows [1;31mtime[mr is created.

[33mcommit 3e4368d04fe8fc1a998cfc874c9eb10d95f900da[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Fri Oct 31 10:28:35 2014 +0200

    Bug#19895222 REFACTOR SOME DIAGNOSTIC OUTPUT THAT GOES DIRECTLY TO STDERR
    
    Some diagnostic code in InnoDB is outputting to stderr directly.
    The way how it is done is creating some extra line breaks and lines
    without a preceding [1;31mtime[mstamp. Also, the output of a single message
    from multiple threads can easily become interleaved, as the messages
    are being output in several nonatomic steps.
    
    dict_index_name_print(): Remove.
    
    rec_index_print, rec_offsets_print: Wrapper for outputting a record to
    a stream.
    
    Implement operator<<(std::ostream&, const dtuple_t&).
    
    We still have some references to dtuple_print(), including some that
    output to stderr. These should be removed in a follow-up fix.
    
    rb#7189 approved by Jimmy Yang

[33mcommit a9840ba26e9f2af18c5f16ee79d0b008f98f22d0[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Oct 27 11:49:59 2014 +0530

    Bug#19583563: RPL.RPL_SEMI_SYNC_SHUTDOWN_HANG FAILS
    SPORADICALLY ON PB2
    
    Problem:
    ========
    CURRENT_TEST: rpl.rpl_semi_sync_shutdown_hang
    mysqltest: At line 52: query 'reap' failed: 2013: Lost
    connection to MySQL server during query
    
    Analysis:
    ========
    As part of test script with semisync enabled we stop the
    slave's IO thread and execute a CREATE TABLE on master. This
    statement will hang waiting for acknowledgement from slave
    and IO thread has gone away. This CREATE TABLE statement is
    kept in hang mode using --send command. Then master server
    goes down because of shutdown causing the CREATE TABLE
    statement to end successfully. When ever --send is used in
    MTR it expects a --reap command to be used to capture the
    return status of the statement that used --send. Because of
    timing issue some [1;31mtime[ms --reap command is able to get the
    successful return status of --send command and some [1;31mtime[ms
    the --reap command is executed post the server has actually
    shutdown. Because of this the reap command complains that it
    has lost connection to mysql server during query. --reap
    commands return code is not important for the actual bugfix
    verification. The main aim of test script is shutdown should
    not hang.
    
    Fix:
    ===
    As part of fix --error 0,2013 is added above the --reap
    command.

[33mcommit de723bba93bda7e72f8ae3b1c71ce933f4764b07[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 22 13:31:25 2014 +0300

    Bug#19729855 RACE CONDITION WHEN READING FIL_SPACE_T::MAX_LSN
    
    When reading or writing the fil_space_t::max_lsn field,
    we must always hold log_sys->mutex.
    
    This was not the case in fil_space_detach() and fil_space_free(),
    which were checking space->max_lsn != 0 without holding any mutex.
    It could happen that by the [1;31mtime[m when log_mutex_enter() completed,
    space->max_lsn was already reset to 0 and the object had already
    been removed from fil_system->named_spaces by fil_names_clear()
    during a log checkpoint that was executed by another thread.
    
    This error was introduced in the fix of
    Bug#18645050 WL#7142 CAUSED PERFORMANCE REGRESSION (rb#5918).
    
    Also, fil_delete_tablespace() was acquiring space->latch for no reason.
    The reason went away with an earlier fix,
    Bug#19149177 Remove space_id lookups (rb#5919).
    
    rb#7121 approved by Sunny Bains and Jimmy Yang

[33mcommit 1b3ec001e2480f45ddaaba60a729c7c5523e54ef[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Oct 21 12:56:37 2014 +0200

    Fix for bug#19712569
    
        NODE RESTART BLOCKED BY DICT LOCK HELD BY SCHEMA TRANSACTION
    
    The dictionary API code may incorrectly interpret a TRANS_END_REPORT
    belonging to a previous TRANS_BEGIN_REQ as a failure of the latest
    schema transaction. This caused the API to assume that the transaction
    failed and its locks was released, while the transaction was actually
    started. This transaction is then effectively lost, and its locks
    prevents other schema transactions to be started. As node restarts also
    need the DICT lock, they are also prevented from getting past start phase 2.
    
    This fix ignores TRANS_END_REP signals which belongs to unknown Txn,
    or to transactions not yet started. To correctly track the Txn state,
    we now also defer setting Txn-state to 'Started' from sending
    the TRANS_BEGIN_REQ until TRANS_BEGIN_CONF is received.
    
    Furthermore, this patch also changes the expected outcome of starting a
    schema transaction during a master node failure: If a master node now fails
    before a TRANS_START_CONF has been sent, ::dictSignal() will resend
    the TRANS_START_REQ signal to the new master, and it will
    likely succeed the next [1;31mtime[m. (Which it actually also did before,
    but the API incorrectly assumed that it failed)

[33mcommit 712d8950b596ae45ba6b0f72ca6291a52c669505[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Tue Oct 21 08:34:49 2014 +0300

    Bug#19856411 REMOVE HAVE_ATOMIC_BUILTINS FROM INNODB SOURCE CODE
    
    Since a recent change in MySQL 5.7.5
    WL#7655 Remove MY_ATOMIC_MODE_RWLOCKS
    the MySQL build depends on atomic memory access primitives being present on
    the target platform.
    
    Because there is no benefit from keeping the HAVE_ATOMIC_BUILTINS
    in the InnoDB source code, we will remove it and thus simplify the code.
    
    With this cleanup, InnoDB will depend on Microsoft atomics on Windows,
    and on GCC-style atomics on any other platform.
    
    Solaris atomics had been disabled for some [1;31mtime[m, probably ever since
    the Solaris build environment was changed to use GCC, in
    WL#7385 Switching from Sun Studio to GCC on Solaris.
    
    rb#7099 approved by Vasil Dimov and Sunny Bains

[33mcommit d92eae0beb39a6fb5833ea2724af7c9ec3dd176b[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Oct 20 09:53:21 2014 +0200

    Fixes restart race causing test [1;31mtime[mout in AutoTest 'testDict -n schemaTrans'
    
    The test cases v1,v2, and x1,x2 restarted a node without the
    'NoStart' option, and then waited for the node to reach the
    'NO START' state. However, this opens the possibility of
    the node to restart, and reach the 'STARTING' state without the
    test client ever seing the 'NO START' state, Thus the test
    would finaly [1;31mtime[m out (Fail) after 120s.

[33mcommit c61f434680d6e612febe26bc6515183130f7bd4c[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Thu Oct 16 15:49:18 2014 +0530

    Bug#19803939 -  SIMPLIFY THE LOGIC TO CHOOSE NATIVE AIO ON WINDOWS
    
    Before fix:
    -----------
    1.
    The code turned srv_use_native_io to false if the OS was VERY old
    (OS_WIN95, OS_WIN31, OS_WINNT). For not-so-old OS versions (OS_WIN2000 &
    OS_WINXP) srv_use_native_aio is ON and srv_use_native_conditions is OFF.
    
    2. Run[1;31mtime[m Loading of Windows Condition Variable functions (for example:
    InitializeConditionVariableProc, SleepConditionVariableCSProc, etc)
    
    After fix:
    ----------
    This handling of different Windows OS is not required as the minimum
    supported Windows version for 5.7 is Windows 7
    
    1.
    Now, for all windows versions supported by 5.7, srv_use_native_aio = ON
    & we will always use native Condition Variables APIs
    
    2.
    Run[1;31mtime[m loading of Condition Variable functions are replaced with their
    compile [1;31mtime[m variants
    
    Approved by Marko, Kevin, rb#7013

[33mcommit 26807c6be445ccfd4a13a87747be5914471cab14[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Mon Oct 13 21:39:57 2014 +0200

    BUG#19724313: Handle multiple threads crashing at the same [1;31mtime[m properly

[33mcommit 670530b8028bdc422e89ba895f1282a78f074775[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Oct 13 13:36:37 2014 +0200

    Bug #19582925 ENHANCED DETECTION AND DUMP OF CORRUPT OR UNSUPPORTED MESSAGES
    
    Add more checks while unpacking received signals over tcp.
    
    Byte order must be the native byte order.
    Compressed flag may not be used.
    Check also byte order, compressed flags, and message length for
    next message if in receive buffer.
    
    If either current unpacked message or next fails in check, current
    message are assumed bad.
    
    If message is bad, the transporter are marked having bad data and
    no more unpacking of messages will occur until reconnected.
    
    When bad message is detected a [1;31mtime[mstamped log with error and
    hexdump of message and if next message was bad also a hexdump of
    first part of that and the six preceding words.

[33mcommit 2c84d31daf1450fbef30891535baf5916aa1ee79[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Fri Oct 10 11:06:16 2014 +0300

    Follow-up fix for Bug#19710564 REMOVE TABLESPACE_VERSION
    
    This fixes crash recovery failures when tablespace flags
    are not 0 (when innodb_page_size is not 16k).
    
    In buf_read_recv_pages(), the tablespace may need to be opened.
    Initially, both space->flags and space->size will be 0,
    until the files are opened for the first [1;31mtime[m.
    
    fil_space_open_if_needed(): Utility function, to open tablespace
    files during recovery if needed.
    
    fil_space_acquire(): Remove the parameter "verbose", and use
    fil_space_get() in recovery instead.
    
    fil_space_get(): Fix a debug assertion when space==NULL,
    and update the function comment to say that it is safe to
    call during single-threaded crash recovery.
    
    rb#6863r3 approved by Vasil Dimov

[33mcommit 1a49d4c4791d3daaeec9f95be59b89b121ff0654[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Oct 9 15:04:27 2014 +0200

    Cherrypicked
    
    revision-id: mauritz.sundell@oracle.com-20141009124636-dg0th9bzvr27r1i7
    parent: mauritz.sundell@oracle.com-20140930122950-gn1rl2yigc4s7ucu
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-7.1
    [1;31mtime[mstamp: Thu 2014-10-09 14:46:36 +0200
    message:
        Bug #19582807     MAKE SIGNAL DUMP IN TRACE FILES ALWAYS START FROM LATEST SIGNAL
    
        Removes a regression introduced with patch for the above bug.
    
        During crash dumps there could be a segmentation fault or most recent
        signals could be dump as old signals or not at all.

[33mcommit 7eaa176fc34ddee697ecad6690cbc9d3df52344b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Oct 9 15:02:54 2014 +0200

    Cherrypicked
    
    revision-id: mauritz.sundell@oracle.com-20141009124636-dg0th9bzvr27r1i7
    parent: mauritz.sundell@oracle.com-20140930122950-gn1rl2yigc4s7ucu
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-7.1
    [1;31mtime[mstamp: Thu 2014-10-09 14:46:36 +0200
    message:
        Bug #19582807     MAKE SIGNAL DUMP IN TRACE FILES ALWAYS START FROM LATEST SIGNAL
    
        Removes a regression introduced with patch for the above bug.
    
        During crash dumps there could be a segmentation fault or most recent
        signals could be dump as old signals or not at all.

[33mcommit 8384cb66183e06eac2993eee0bf4d0f77b2342c5[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 8 11:08:46 2014 +0200

    WL#6813: Intra-schema MTS: ordered commits (sequential consistency)
    
    Post-push fix: Remove rpl_slave_commit_order_manager.cc from
    libsql and libbinlog. It is enough to have it in libslave. This avoids
    compiling the same file three [1;31mtime[ms.

[33mcommit 4b4fddfd999ec103a5cafa85b9975689edd6494d[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Oct 7 13:19:03 2014 +0200

    Bug#19646643 REPEAT() WASTES CPU TIME ON LARGE INPUT VALUES THAT GAURANTEE EMPTY RESULT
    
    In Item_func_repeat::val_str(): don't wast [1;31mtime[m concatenating the empty string.

[33mcommit c5f50f4a8981cecdbfa6d8e17ff25e572d9a6cd1[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Oct 3 09:21:09 2014 -0500

    WL#8109 Follow-up
    
    The previous patch was a non-functional change extracted from wl#6205.
    But it did one thing which exposed problems in page_size=4k & 8k runs.
    That is, it made a requirement that page_size_t objects always use
    non-zero page sizes. This is a much better approach becasue zero was
    getting interpreted by the code that uses page_size_t, some[1;31mtime[ms to
    UNIV_PAGE_SIZE (which can be 4k,8k,16k,etc) and some[1;31mtime[ms to
    UNIV_PAGE_SIZE_ORIG (16k). The original patch had a bug which caused
    all 4k runs to fail which was fixed quickly by Marko. That left only
    innodb_double_write.test failing which this patch fixes.
    
    The innodb_double_write test writes all zeros to the header page of the
    system tablespace, hoping that the double-write recovery can fix it.
    
    This patch (a) changes 2 places where a page_size_t is initialized with
    zero to use non-zero numbers. (b) It stops initializing system tablespaces
    from the contents of the first page, which may be corrupted, like it is in
    innodb-double-write.test. It uses the space_id and flags that system
    tablesapces are known to use. (c) It moves the assert that physical() is
    not zero when UNIV_PAGE_SIZE is not 16k to the function that puts the page
    size into an FSP flags.  This means we can have an invalid page_size_t and
    an FSP flags read from a corrupted file, but we are not going to build an
    invalid FSP flags.
    
    approved by Marko in rb#6897

[33mcommit 4b9807ce21a2d497e12b0310dba6b482b72e3abc[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Oct 3 10:48:27 2014 +0200

    Bug#19656296 FIND_ALL_KEYS SPAMS SERVER ERROR LOG WITH DEADLOCK/LOCK WAIT ERRORS
    
    The server error log gets littered with client-side error messages:
    Deadlock found when trying to get lock; try restarting transaction
    
    A deadlock or lock wait [1;31mtime[mout is NOT an error,
    but a normal occurrence in a transactional database.

[33mcommit 179f4e4823c027377fd4e66d0da1893c9ba91d5e[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Fri Oct 3 11:40:57 2014 +0300

    Introduce include files for skipping recovery tests
    if a log checkpoint occurred during the critical window.
    
    We cannot prevent a checkpoint from occurring, but we can detect it.
    Some InnoDB recovery tests are testing the redo log apply since the
    latest checkpoint. There must not be extra checkpoints in the
    [1;31mtime[m frame where we are writing the interesting redo log records.
    
    Approved by Vasil Dimov.

[33mcommit 30545d133b8924facdb69707c0437399bb952446[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 1 14:12:31 2014 +0300

    Bug#19694618 DEFINE A DATA TYPE WRAPPER FOR PRETTY-PRINTING TABLE NAMES
    
    InnoDB is displaying table names in some diagnostic messages. Some[1;31mtime[ms they
    are translated, some[1;31mtime[ms they are displayed directly in the internal
    representation, which currently is "databasename/tablename" in the
    filename-safe encoding. This is inconsistent.
    
    table_name_t: Wrapper struct for pretty-printing table names.
    
    operator<<(std::ostream&, const table_name_t&): Display table names.
    
    innobase_convert_identifier(), add_identifier(): Use '`' instead of '"'
    as the default quote character.
    
    Clean up some tests, to allow them to be run with embedded server.
    
    rb#6846 approved by Kevin Lewis and Mattias Jonsson

[33mcommit 98ad584284848a99101d18a153242108a0236502[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 1 10:17:14 2014 +0300

    Bug#19693488 REMOVE INDEX NAME LOOKUPS DURING DML
    
    For fulltext indexes, there is a special index name FTS_DOC_ID_INDEX that is
    being looked up during DML operations. A pointer to this special index could
    be cached at DDL [1;31mtime[m to avoid the costly lookups.
    
    dict_table_t::fts_doc_id_index: Pointer to the FTS_DOC_ID_INDEX.
    
    dict_load_indexes(), ha_innobase::create(), prepare_inplace_alter_table_dict(),
    commit_cache_norebuild(): Initialize the flag.
    
    Many places: Replace dict_table_get_index_on_name(table, ...)
    with table->fts_doc_id_index.
    
    rb#6845 approved by Shaohua Wang and Kevin Lewis

[33mcommit 22548182d5b8cdca2b3c35bd7f3669cfc58c60fe[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Fri Sep 26 11:50:32 2014 +0200

    Bug #17257842 EXCESSIVE RETRY FREQUENCY FOR API NODES RECONNECT CAUSES HIGH CPU ON MGM NODES.
    
    For system that have lot of API nodes and also have added data node
    entries in configuration without starting any process for them the
    API nodes will continously try to connect to the missing data nodes
    several [1;31mtime[ms per second.  This put extra load on MGM nodes and
    network in general.
    
    
    
    This patch introduces logic to make an API node wait longer and
    longer between attempts to connect to data nodes that fail to
    respond.
    
    The logic is controlled by two new parameters for api nodes:
    StartingConnectingBackoffMaxTime and ConnectingBackoffMaxTime.
    
    Both are given in milliseconds with approximately 100ms resolution.
    Note that the [1;31mtime[m controls [1;31mtime[m *between* attempts, during the
    [1;31mtime[m a connect attempt occur no [1;31mtime[m float with respect to the
    backoff logic.
    
    Between first and second attempt one waits 100ms (as before), but
    for the following attempts the wait is doubled until the configures
    maximal [1;31mtime[m are reached or ~100s whichever is smallest.
    
    As long as the API node is not connected to any data node the
    StartingConnectingBackoffMaxTime parameter is used, otherwise
    ConnectingBackoffMaxTime.
    
    Note, as soon as the API node is connected to a data node and that
    node reports, in heartbeat message, that it has connected to some
    other data nodes, connection attempts to those data nodes will not
    be affected by the backoff logic but will be attempted every 100ms
    until connected.
    
    In worst case it can take HeartbeatIntervalDbApi (default 1,5s)
    until the API node get notified that a restarting data node is up.
    
    But since data nodes connects to each other during start phase 2,
    and API node connections are allowed from start phase 8, that
    should normally not be an actual delay since the [1;31mtime[m between start
    phase 2 and 8 probably are at least several seconds.
    
    For systems with many unstarted data nodes the new parameters
    should be raised to avoid extreme traffic either to MGM nodes or
    connection attempts to non existing datanodes.

[33mcommit 7108b660559ae839ac120f2f67ff3a07ecda8888[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Mon Oct 20 15:16:46 2014 +0300

    Bug#18799371 MEMORY LEAK IN MTS LC: KILLED COORDINATOR LEAVES EVENT UNDELETED
    
    In case Coordinator threads sees itself killed at [1;31mtime[m it is scheduling an event to yet undefined Worker, it won't delete the event.
    The case became evident at valgrind testing, in a specific to Logical Clock MTS execution path, involving Mts_submode_logical_clock::get_least_occupied_worker
    where Coordinator may find itself killed.
    
    It is fixed with making the negative get_least_occupied() branch of Log_event::get_slave_worker() to add up the being scheduled event to the Coordinator's deferred array of events.
    The array is necessarily processed at Coordinator thread exit.
    
    The patch has been tested extensively with --valgrind-mysqld against reported tests to
    prove its effectiveness.

[33mcommit fd34446bfb16871b696e838d88dd60db6d8eb240[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Fri Sep 26 10:29:33 2014 +0530

    Bug #19550875: SESSION STATE NOT SENT AS PART OF RESULT SETS WHEN
                         QUERY CACHE IS ON
    Problem:
    In case a session state exists do not cache the SELECT stmt. If we
    cache SELECT statment when session state information exists, then
    the result sets of this SELECT are cached which contains changed
    session information. Next [1;31mtime[m when same query is executed when there
    is no change in session state, then result sets are picked from cache
    which is wrong as the result sets picked from cache have changed
    state information.
    Fix:
    Fix is to not cache the SELECT query when there exists a session state.

[33mcommit 3a0f0628f6a338df326b52a2b2291bb5d378bcdd[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 25 13:54:10 2014 +0200

    Patch 2 for WL#7953 Transporter handshake retry
    
    Change the server side of Transporter/TransporterRegistry to request the client to close() the
    socket first when server is not yet ready to accept the connection. This moves the TIME_WAIT
    state for the socket to the client and thus frees resources  from the network stack on the server.
    
    Requesting client to close is done by sending garbage to the client. Although we send the nice text
    string BYE to the client it will be interpreted as garbage and it will close the connection. After
    sending the BYE string wait for the EOF on the socket by reading 0 bytes from socket. ie. select
    says there is something to read but you only read 0 -> EOF.  Since connections are handskaed serially
    wait for EOF with fairly short [1;31mtime[mout in case there should be a connected client which does not
    behave as expected. After the new BYE and wait for EOF code the server side socket is closed as
    usual and the TIME_WAIT TCB shold now end up on the client where the previous "use last port" patch
    will eliminate it.
    
    Also change so that socket is closed with a reset(RST) when the handshake is rejected
    due to invalid parameters or other error. This effectively eliminates TIME_WAIT TCB from both
    server and client side for the non graceful case(s).

[33mcommit 54d32ff512e8fa3578bff138d76625e24c67dc06[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 25 13:25:41 2014 +0200

    Patch 1 for WL#7953 Transporter handshake retry
    
    Change SocketClient bind to last used port
    
    Client connections uses a new ephemeral port for each new connection. This
    leads to rapid buildup of TIME_WAIT sockets when those connections
    are short lived. Most significant buildup occurs during node restart
    of a data node. The data node will for a (long) [1;31mtime[m accept connections
    which are closed more or less immediately because connections from api
    nodes are not yet accepted.
    
    Almost all client connections are done using the SocketClient class and the
    same SocketClient instance is normally used for subsequent connects. Thus it's
    possible to remember which port was used for last successful connect and try to
    use the same port on next attempt. This effectively reduces the number of
    TIME_WAIT TCB's from either server or client side.
    
    Changes:
    - Try to bind to the same port as last successful connect instead of any ephemeral port.
     - Retry bind with any epehemeral port if last used port fails
     - Only remember last used port when connect is successful

[33mcommit eaa3b8ad8ec9cb02fe2674deab79bb3510188f58[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Thu Sep 25 15:50:59 2014 +0800

    Fix Bug#19319085 INNODB_GIS.INNODB_GIS_RTREE_ROLLBACK1 CRASHES ON
    WEEKLY-TRUNK
    Basiclly, the crash is caused by the testcase [1;31mtime[mout when page size
    is 4k or 8k. Since for 4k or 8k, rtree splited to more pages and
    will take more [1;31mtime[m to rollback. So, we change it to only test 16k
    page size.

[33mcommit f0660c30d8d5a222540f93dd7b613f5febda177e[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Sep 24 14:59:55 2014 +0300

    Remove sleeps from --suite=innodb_undo tests.
    
    Instead of sleeping for a "long enough" [1;31mtime[m, rely on server shutdown.
    This will reduce the test run-[1;31mtime[m from roughly 2+2+5 minutes to 1+1+2
    on my system.
    
    Also, move some common definitions to an include file.
    
    Approved by Krunal Bauskar.

[33mcommit 784e3bdb50cb7b80e7d45cffc2086dece3f4fb46[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Tue Sep 23 18:48:45 2014 +0300

    This is post_push patch for WL#7165. Recommitting cause by a loose assertion,
    in a new function, to have been corrected now.
    
    The patch addresses few cases that were discovered by analysis of failing
    tests on PB trunk.
    
    A. failed but still loggable DML on a non-transactional table.
    In such case "@@global" max committed transaction left not
    incremented.
    That causes sequentially following transaction not to regard it
    as its parent. Failure to do so is caught by rpl_view and
    rpl_stm_EE_err2.
    
    B. Duplicate sequence_number:s can appear as result of logging from
    two caches. A traditional use case for that is mixed engine transaction.
    Logging therefore is refined to [1;31mtime[mstamp separately either cache content,
    and to force the 1nd (the statement cache part) be the commit parent to the 2nd.
    It is important to serialize the two such way 'cos of use cases where
    LOAD DATA is found out to be potentially two-cache loggable
    (see rpl_parallel_load_data as example).
    
    C. gaps in sequence_number that make the scheduler to hang
    in waiting for non-existent transaction commit.
    Gaps appear in circular replication and can show up in GTID
    autoposition-based recovery (it all seems there's no dedicated
    test for that, otherwise it would fail as well).
    It's fixed with allowing "positive" (skipped groups) gaps. Negative
    (as jump back in past) are made to be errored out exclusively.
    A transaction that is found to succeed a gap waits for all
    preceding to finish (fallback to sequential).
    And at that point its dependency with *any* commit parent is satisfied.
    rpl_circular_for_4_hosts verifies this solution to [ pass ].
    
    D.There was a sporadic failure (assert) in rpl_mts_submode_switch which
    turns out to be a result of sort of a race when while it's proven
    all transactions are committed some Workers can still have
    undeleted yet events in their private queues.
    This case is specific to exceptional handling of CREATE..SELECT @user_var,
    there're 2 hunks in rpl_mts_submode_switch.cc for it.
    
    rpl_mts_slave_preserve_commit_order_error.test is blocked out from
    execution with Logical-clock. It should be refined, suggested
    as a separate task (todo: report a task to list all tests that
    are incompatible to LC).
    
    Finally, suppression of an expected warning in
    rpl_user_var_zero_length_name is done.
    
    There's a separate issue exposed by rpl_wl6292 as results mismatch.
    In fact it's a failure to adapt Worker THD context to requirements
    carried by the master version.
    Although this issue is not witnessed by PB elsewhere, not least thanks
    to the test had been until recently blocked by running with Logical_clock,
    Database type MTS is prone to that too.
    See it as Bug #19630322 MTS: WORKERS MAY NOT ADAPT TO THE MASTER VERSION
    
    
    All changes to parts A,B are located in binlog.cc. Diff file is annotated
    more specifically to separate the two.
    All changes to part C are located in rpl_mts_submode.cc.

[33mcommit 6d56efaf9dc27c382fdd8ce8f2c0fa9b814b2c23[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Tue Sep 23 14:00:00 2014 +0530

    Bug #19077239   TESTS FOR CONFIG_EDITOR AND SECURE_INSTALLATION WITH SSL ARE UNSTABLE
    
    Changing the .opt file for mysql_secure_installation_ssl.test for proper ssl keys
    Increasing the [1;31mtime[mout value for perl Expect in mysql_secure_installation.test to run the test
    successfully on slower machines.
    Skipping mysql_config_editor for windows.

[33mcommit 296c465403de4e87aecece11a3718c243e2a374f[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu Sep 18 23:17:32 2014 +0100

    Bug #19600834   DOWNGRADE FROM 7.4 TO 7.3 IS FAILING (CRASH) ...
    
    Missing upgrade code in increased LCP parallelism feature.
    
    Prior to 7.4
     - DIH master asked LQH to LCP 2 fragments at a [1;31mtime[m, and queued
    2 internally to ensure LQH is kept fed with fragments to lcp.
     - LQH checkpointed 1 fragment, and queued the other.
     - The LQH queue length was 1.
    
    In 7.4
     - DIH master asks LQH to LCP 32 fragments at the same [1;31mtime[m, and
    queues 32 internally
     - LQH can checkpoint up to 1 fragment per instance at the
    same [1;31mtime[m, and queues the others.
    
    DIH master and the LQH blocks involved can be on different nodes.
    
    Mastership stays with 'older' nodes, so during a normal upgrade,
    the master node is generally the last to upgrade.  e.g. for 7.3->7.4
    the master will normally stay on 7.3 until the last minute.
    
    However during a downgrade, the master will be on 7.4 while one or
    more LQHs move to 7.3.
    
    This can result in 7.4 DIH master asking 7.3 LQHs to checkpoint
    32 fragments in parallel, causing a failed ndbrequire on the
    3rd fragment.
    
    Fix
    
    Have 7.4 DIH master check how much parallelism a destination node
    can handle based on its version rather than using a hard coded
    limit.

[33mcommit a2f065a9c7c62e1cdb3df0724460ff35a72bd756[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Sep 18 09:02:45 2014 +0530

    - Bug #19632964: WL#6965: TRY TO TRUNCATE AS MUCH AS YOU CAN
    
      Currently purge try to truncate only one tablespace at any given [1;31mtime[m.
      Next tablespace will be truncated on next invocation of purge. (of-course
      if tablespace qualifies for truncate).
    
      This needs to be accelerated by following greedy approach.
      Truncate as much as you can. This approach will help especially when
      server is idle to complete most of the work.
    
      Approved by: Sunny (rb#6722)

[33mcommit 8ca572a0ea0d1304c4c11ad52e5164dab6c7e6ec[m
Author: kevin.lewis@oracle.com <>
Date:   Mon Sep 15 13:04:00 2014 -0500

    Bug#19611755 - MYSQLTEST.CC NEEDS ENHANCEMENT TO IDENTIFY
        IBD FILES IN DATADIR ON WINDOWS
    
    This small patch is a test case cleanup that was missed in the main patch
    for this bug;
    
    revno: 8832
    revision-id: kevin.lewis@oracle.com-20140912145708-3zm7gnjr33pu7w1y
    [1;31mtime[mstamp: Fri 2014-09-12 09:57:08 -0500
    
    The test case failed on Windows and requires --big-test.
    The regexp in the test is no longer needed since the fix for this bug now identifies
    windows paths by the existence of ".ibd"

[33mcommit 4f81455952a67b8fc35c9481a0e4ae7db5b49fd6[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Sep 15 14:37:31 2014 +0200

    Repush of WL#7544 after 7.4.1 clone tag.
    
    ------------------------------------------------------------
    revno: 4458
    revision-id: mikael.ronstrom@oracle.com-20140915090314-4feki0sr3fcmsflr
    parent: mikael.ronstrom@oracle.com-20140912185859-6417bvvey4ih4blu
    committer: Mikael Ronstrom <mikael.ronstrom@oracle.com>
    branch nick: push_wl7544
    [1;31mtime[mstamp: Mon 2014-09-15 11:03:14 +0200
    message:
      WL#7544: Optimisations of the NDB API receiver thread in various manners. 1) Optimising method to unpack field data from TRANSID_AI signal, 2) optimised handling of trp_client locking check, 3) reorganised code in NDB API receiver thread for better performance and more readability, 4) Optimisation signal receive handler

[33mcommit 37aa9209c763db48a59ab6b9468b619d935513d6[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Fri Sep 12 19:32:32 2014 +0100

    WL#6128, WL#6972
    
    Follow up patch to fix a bug discovered when merging main into the
    local tree.
    
    Due to the fact that we keep an internal mapping between a number
    (sidno) and UUIDs of GTIDs, there was an bug that was preventing, in
    some cases, to map the correct UUID into a GTID. This was happening
    because the Gtid_set, used to collect GTIDs, has its own (local) map
    which is not in sync with the global map. Actually, we do not want it
    to be, otherwise we be facing a synchronization penalty. The offending
    behavior is that when switching the tracking mode from OFF to
    OWN_GTID, and when GTIDs had already executed before this step - thus
    having sidnos in the global map - we could end up with different
    sidnos for the same transaction on the local map when compared to the
    global map.
    
    We fix this by using only sidnos local to the tracker when filling in
    the map. At the same [1;31mtime[m, we avoid incurring into synchronization
    overhead with the global map, as we are not really using it.
    
    Added also a test case to validate this fix.
    
    The side effect of this change is that we need to clear the local
    sid_map every[1;31mtime[m we start a new transaction, since the next one may
    be tracked with ALL_GTIDS, which copies the entire executed_gtids into
    the local set. This again would lead to a mismatch if the sid_map was
    not pristine when this operation takes place. A consequence is that an
    assertion in ensure_sidno had to be removed since it is a bit
    ambiguous when sets with sid_maps that have been cleared are used.

[33mcommit 9857bb26177eff59154925ee28097da5a6e5db60[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Sep 12 10:24:30 2014 -0500

    This is a change related to a problem found when running innodb.log_file
    test on Windows.  This test starts a second mysqld with various startup
    mis-configurations while the main one is still running.
    
    I found on Windows that it would quite often hang waiting for the
    mis-configured mysqld to quit, but it was stuck trying to delete ibtmp1
    in innobase_space_shutdown(), calling srv_tmp_space.delete_files();,
    which calls os_file_delete_if_exists_func().  This function can loop
    2000 [1;31mtime[ms trying to delete the file every 0.5 seconds.  The delete
    fails because the file has an open handle.
    
    The mysqld startup routine innobase_start_or_create_for_mysql() contains
    about 36 calls to srv_init_abort().  Depending on which one of these
    startup failures is called, ibtmp1 may still be open.
    
    The fix for this has several parts.
    
    1) Enhance srv_init_abort() so that in debug mode, it will print the
       line number of where it was called from as well as the error that
       occurred.
    2) In innobase_space_shutdown(), make sure we close the file handles
       found in the fil_node_t object.
    3) In srv_open_tmp_tablespace(), make sure the srv_tmp_space is
       re-opened through the fil_system after it has been validated and
       closed.  This looks like it was a mistake in the oringal coding
       since the comment before the call to this function says;
       "/* Open temp-tablespace and keep it open until shutdown. */".
       It was not being left open until shutdown.  But now it is.
    4) Added new functions fil_space_open() and fil_space_close() to
       accomplish 2 and 3 above.
    5) fil0fil.cc functions that worked on a fil_node_t were being sent
       pointers to the global fil_system variable and the related fil_space_t.
       They can be determined without putting these on the function call.
    6) Cleaned up related code.
    
    Approved by Vasil in RB#6638

[33mcommit bafef71e79e25444419320ec850d9cdf03b25bd0[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Fri Sep 12 14:28:31 2014 +0530

    Bug#17793901 MYSQL_SECURE_INSTALLATION* FAILS ON PB2.
    Increasing the value of [1;31mtime[mout for Expect as the problem seems to be due to a slow machine.

[33mcommit 06981086fea7cc61b168e0000635a5cdc6260dbb[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Fri Sep 12 11:17:10 2014 +0300

    WL#7165 Optimizing MTS scheduling by increasing the parallelization window on master
    
    The patch implements the WL agenda. That includes logical [1;31mtime[mstamping
    transactions on the master side with two [1;31mtime[mstamps one of which
    enumerates the transaction in the binary log, and the other identifies
    an earlier transaction on which the current one may depend on.
    
    The slave applier tracks dependencies to schedule a transaction only
    when the dependency parent transaction is committed.
    To activate the logical clock scheduler one needs to specify
    
      mysql> set @@global.slave_parallel_type=logical_clock;
    
    Values of the two [1;31mtime[mstamps aka `last_committed' and
    `sequence_number' can be seen in binlog events through
    
      shell> mysqlbinlog binlog_file | grep 'last_committed\|sequence_number'.
    
    The [1;31mtime[mstamp values are valid within a single binlong. They "rotate"
    along with the binlog files.
    
    A sane (recommended) mumber of workers is that that slightly exceed
    the size of a parallelization window as defined as an average of
    a subtract of
    
       sequence_number - last_committed
    
    across transactions in the binlog.

[33mcommit 2b22d9909690b58227ed20486037f5b1c70c97a9[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Sep 11 19:03:46 2014 +0300

    Bug#17332603 REMOVE BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH OLD
    INNODB DATA FILES
    
    The field block->check_index_page_at_flush was needed in the bad old
    days when InnoDB did not initialize FIL_PAGE_TYPE when creating a
    page. It would only initialize this field to FIL_PAGE_INDEX when it
    was about to flush a B-tree page. For other page types, InnoDB would
    leave garbage in the field. That garbage could also be FIL_PAGE_INDEX
    by chance. There was no problem with this in the past, because we
    would only invoke the B-tree validation functions when
    block->check_index_page_at_flush had been set by some B-tree operation
    while the page was in the buffer pool.
    
    Since MySQL 5.1, InnoDB initializes the FIL_PAGE_TYPE for every
    page. But, there could be some old data files that contain garbage
    FIL_PAGE_TYPE values in some pages. This garbage must be reset when
    flushing the page.
    
    buf_block_t: Remove check_index_page_at_flush.
    
    buf_dblwr_check_block(): Reset unknown FIL_PAGE_TYPE values to
    FIL_PAGE_TYPE_UNKNOWN if the tablespace flags are 0.
    For newer tablespaces, the flags never are 0, and we will complain
    about a corrupted page.
    
    FIL_PAGE_TYPE_UNKNOWN: New constant (13) for FIL_PAGE_TYPE.
    
    i_s_page_type[]: Map I_S_PAGE_TYPE_UNKNOWN to FIL_PAGE_TYPE_UNKNOWN.
    Both will be reported as "UNKNOWN".
    
    I_S_PAGE_TYPE_BITS: A new macro, to make compile-[1;31mtime[m checks more localized.
    
    With this fix, it is possible that some non-index page that happens
    to be tagged as FIL_PAGE_INDEX or FIL_PAGE_RTREE will start to trigger
    failures in page_simple_validate_new() or page_simple_validate_old().
    This situation can be handled by manually patching the data file to replace
    the FIL_PAGE_TYPE on that page with FIL_PAGE_UNKNOWN. The error
    message will identify the tablespace ID and page number.
    
    rb#6604 approved by Vasil Dimov

[33mcommit 3454f9a36df2c03c3ea095ee6e2b0cabc2164ae2[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 10 13:13:53 2014 +0200

    Fix for Bug#19552283 :
    
      Transporters receiveBuffers might be reset while
      another thread read them (race)
    
    There is a potential race between a thread
    receiving data with TransporterRegistry::performReceive,
    while another thread (async) initiate disconnect of the
    transporter by calling Transporter::doDisconnect(). The
    later method will call TCP_Transporter::disconnectImpl()
    which clears the receive buffer.
    
    A typical case where this happens is ::doDisconnect() being
    called from the thread running TransporterRegistry::start_clients_thread().
    Simultaneously another thread might 'do_poll()' which performReceive()
    on the same transporter.
    
    This fix moves 'receiveBuffer.clear()' out of ::disconnectImpl()
    and into a new method Transporter::resetBuffers() which is now
    called from ::do_connect(). At this point The transporter has
    been trough a report_disconnect() synched with performReceive().
    Furthermore, it is known to be in a DISCONNECTED state, which
    implies that it can't be seen as connected again by
    ::performReceive() as this also has to be synched.
    
    This also introduce a relaxing of in which states buffered
    data can be read & unpacked from the transporters: Any received
    data will now be available in the receivebuffers as long as
    the transporter as available in 'm_has_data_transporters'.
    The patch takes advantage of this by simplifying the check
    of connection states in ::performReceive().
    
    The patch also removes the 'virtual' decl. of
    Transporter::doDisconnect(). This was a leftover from the
    [1;31mtime[m before (the virtual) Transporter::disconnectImpl()
    was introduced.
    
    Lots of comments are added in order to better explain
    the concurrency controll and restrictions between
    ::pollReceive(), ::performReceive() and ::update_connections().
    ******
    Fix for bug#19559313, call getPerformStateString() with a nodeId arg instead of a state argument

[33mcommit 2eec5f3e7febf99fb7f718cf9243c93dd5db3615[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Sep 5 18:43:31 2014 +0530

    WL#6936 - Implementation of server-side statement [1;31mtime[mout
    
    Followup patch for WL#6936:
    Missing HAVE_MY_TIMER in my_[1;31mtime[mr.h. Adding it to avoid
    compilation errors.

[33mcommit 7fea0fa89befc3bdb422c4cb67933c8c4f9192e0[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Sep 5 15:32:04 2014 +0530

    WL#6936 - Implementation of server-side statement [1;31mtime[mout
    
    Followup patch for WL#6936:
    Resetting of THD::[1;31mtime[mr and THD::[1;31mtime[mr_cache to NULL was not
    in #ifdef HAVE_MY_TIMER guard. Adding #ifdef for it.

[33mcommit aa1ec204a804ab650e8adbff96c64a9c882d9714[m
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Wed Sep 3 16:31:30 2014 +0530

    Bug#18593479: ON MASTER_INFO_REPOSITORY CHANGE,
      IGNORE_SERVER_IDS DOUBLES WITH DUP VALUES
    
    The bug caused the ignore_server_list to double the set of
    values every[1;31mtime[m the repository type changed.
    This was seen when changing from FILE repository to TABLE
    repository or vice-versa.
    
    For example, given repository type= FILE
     ignore_server_ids= (5, 10, 15) followed by
     SET GLOBAL master_info_repository='TABLE';
     would make ignore_server_ids= (5, 10, 15, 5, 10, 15).
    
    Also a SET GLOBAL master_info_repository= FILE again would
    make ignore_server_ids= (5, 10, 15, 5, 10, 15, 5, 10, 15, 5, 10, 15).
    
    The bug was fixed by the patch for Bug#18920203.
    The current patch only adds a test to verify the fix.

[33mcommit d2609a70cd10534d11f8ea1ad4807c0c31e55dc2[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Sep 2 09:56:28 2014 +0200

    Revert the following revisions:
    
    ------------------------------------------------------------
    revno: 8734
    committer: bin.x.su@oracle.com
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Fri 2014-08-29 10:15:45 +0800
    message:
      Commit the missing test case result file for WL#6835.
    
    ------------------------------------------------------------
    revno: 8732 [merge]
    committer: Sunny Bains <Sunny.Bains@Oracle.Com>
    branch nick: trunk
    [1;31mtime[mstamp: Fri 2014-08-29 10:24:18 +1000
    message:
      WL#6835 - InnoDB: GCS Replication: Deterministic Deadlock Handling (High Prio Transactions in InnoDB)
    
      Introduce transaction priority. Transactions with a higher priority cannot
      be rolled back by transactions with a lower priority. A higher priority
      transaction will jump the lock wait queue and grab the record lock instead
      of waiting.
    
      This code is not currently visible to the users. However, there are debug
      tests that can exercise the code. It will probably require some additional
      work once it is used by GCS.
    
      rb#6036 Approved by Jimmy Yang.

[33mcommit 5d7c2e2a33023fa4d3d42d8f66c94cc0dac33964[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Aug 29 09:18:04 2014 +0300

    Work around for Bug#19442959 DO NOT CALL EXIT() FROM INNODB
    
    The test innodb.log_file_name is creating an empty *.ibd file,
    prompting InnoDB to call exit() from an I/O handler thread.
    This will some[1;31mtime[ms cause other threads to catch ut_ad(!m_freed)
    in ib0mutex.h in a mutex operation.
    
    We work around the problem by introducing a debug flag that will be set
    before calling exit().
    
    Also, remove some unreachable code and an obsolete comment.
    Approved by Sunny Bains on IM.

[33mcommit bc228fd3033ee176970aea6d6447a0c4ef155044[m
Merge: 508e3e0ea04 f8c28227be9
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Aug 28 18:08:38 2014 +0100

    WL#7793: Add native support for syslog on unixoid platforms
    
    mysqld did not have native support for traditional unix-style
    syslog; it instead used a shell-script wrapper that would pipe
    the daemon's output to the syslog.  This implied certain limitations:
    - it was impossible to see the syslog status at run[1;31mtime[m
    - it was impossible to change the syslog status at run[1;31mtime[m
    - the shell script could interfere with log rotation
    - the setup would often result in log lines containing two [1;31mtime[mstamps
    
    a) add native syslog support for unixoid OSs.
    
       The following can be set a start-up as well as viewed
       and set at run[1;31mtime[m via system variables (requiring the
       SUPER privilege).
    
       --log_syslog[=0|1]
         on/off switch, defaults to off
    
       --log_syslog_include_pid[=0|1]
         Include mysqld's process ID on each syslog line?
         Defaults to on.  (UNIX only)
    
       --log_syslog_facility=local5
         choose the syslog "facility". Well-known facilities
         may be selected by name.
         Defaults to "daemon".  (UNIX only)
    
       --log_syslog-tag=<string>
         If empty and by default, an ident of "mysqld" is used.
         If e.g. tag "abcd" is supplied, "mysqld-abcd" will be used.
         (The hyphen will be used automatically, the user need not
         supply it.)
    
    b) refactoring: mysys/my_syslog.cc and sql/log.cc had
       some duplication of effort with regard to the
       Windows EventLog.  This patch attempts to eliminate
       the duplicates from log.cc, and merge the remaining
       useful code into my_syslog.cc so that other binaries
       may have a chance to benefit from it.
    
    c) Support features of a) on Windows where possible.
    
    d) Update mysqld_safe to use features of a) instead of
       trying to emulate them in the wrapper script where
       possible, with as little change in syntax as possible.
    
    journalctl -f SYSLOG_IDENTIFIER=mysqld-meow -o json|
    jq '.SYSLOG_IDENTIFIER + " " + .SYSLOG_FACILITY + ": " + .MESSAGE' 2>/dev/null

[33mcommit 3f332c12bd54faa029caf03c7ecff00ddedccc7c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Aug 28 13:44:16 2014 +0900

    Bug#17554489 : UNNECESSARY OVERHEAD FROM PERSISTENT ADAPTIVE HASH INDEX LATCHES
    
    If INNODB_RW_LOCKS_USE_ATOMICS is enabled, rw_lock implementation is fast enough.
    So no need to keep btr_search_latch also when over 10000 [1;31mtime[ms AHI searches per 1 transaction,
    because just might block the other AHI updates.
    
    Approved by Sunny in rb#6257

[33mcommit 1e5b6f96ee9a06fabb2b87413250539914e4d4f8[m
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Tue Aug 26 12:04:33 2014 +0530

    Bug#18192817: IGNORE_SERVER_IDS IS SHOWING DUPLICATE
                  SERVER ID FOR LAST VALUE
    
    Problem:
    
    The list of insert_id is stored in an array.
    Before inserting in the array, there is a binary
    search to find if the same server_id already
    exists in the list. The issue is that the list
    is not sorted. Note that binary search can ONLY
    be applied on a sorted list.
    
    As a result the list can some[1;31mtime[ms contain a value
    but not be spotted by the binary search logic.
    In such a case, the value is again added to the list.
    
    Fix:
    
    The bug was fixed by the patch for Bug#18920203.
    The patch for this bug implemented insertion-sort
    in Prealloced_array, and used insert_unique() to
    keep the array sorted at all [1;31mtime[ms.
    
    The current patch only adds a test to verify that
    we dont have duplicate values in the list at any [1;31mtime[m.

[33mcommit f1530936a7972673b759c1af6c62d06560f943f5[m
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Mon Aug 25 12:13:15 2014 +0530

    BUG#19216843: WL1697: NONBLOCKING OPTION WITH SSS
                  COMMAND SHOULD NOT BE ALLOWED
    
    WL#6402 addded NONBLOCKING flag to SHOW SLAVE STATUS
    command to prevent SHOW SLAVE STATUS from getting blocked.
    This was done as a use case for replication monitoring
    tools to decrease the wait [1;31mtime[m while STOP SLAVE
    is holding LOCK_active_mi.
    
    The main idea in this patch was to not take
    LOCK_active_mi before executing SHOW SLAVE STATUS.
    Multi-source replication changes the LOCK_active_mi to
    LOCK_msr_map which protects the map containing all the
    channel information. But we can not allow SHOW SLAVE STAUS
    to execute without taking LOCK_msr_map for if someone
    deleted a channel while SHOW SLAVE STATUS was executing
    we would end up referencing strange memory locations.
    This means we would need to take LOCK_msr_map before
    SHOW SLAVE STATUS NONBLOCKING which will again make
    it equivalent to the regular SHOW SLAVE STATUS
    command (without NONBLOCKING flag).
    
    Hence, we remove SHOW SLAVE STATUS NONBLOCKING
    for now and investigate alternate ways of doing the same.

[33mcommit 8ca07aee5bed4d95a47bc9fc03c6e138ec102ac3[m
Author: Neha Kumari <neha.n.kumari@oracle.com>
Date:   Sun Aug 24 20:00:41 2014 +0530

    Wl#7440: Moving binlog decoding into separate package.
    
    Addressing comments from Tor and Sven.
    
    - Added the method is_valid(not virtual though) to class Log_event
    - Added the method get_type_code(Not virtual though) to class Log_event
    - Removed the method get_event_name from libbinlogevent as it is not required there.
    - Added the method get_type_str in the server code(in class Log_event).
    - Moved the methods used for calculating binary_length of [1;31mtime[m, date[1;31mtime[m, and [1;31mtime[mstamp
      from server to libbinlogevents.
    - Code beautification.

[33mcommit 617ab590fae6b709cdce91dad427c9d23888c57d[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Sun Aug 24 12:59:53 2014 +0200

    WL#2489: Better ONLY_FULL_GROUP_BY mode
    
    Complete patch. See sql/aggregate_check.h or the WL for design.
    
    Moreover, this contains changes (not written in the WL page):
    
    1) Item_ident::alias_name_used used to convey the two pieces of
       information below; it's removed and replaced with:
    
       * m_alias_of_expr tells if this is a reference, through an alias, to a
         SELECTed expression which has "AS alias"
       * to know if this is a reference to a column of an aliased table, we
         consult table->alias_name_used
    
    2) only_full_group_by checks are now done after simplify_joins();
       the latter function may convert outer joins to inner joins, and
       inner joins allow more functional dependency detection;
       so with this change we can allow more queries.
    
    3) moved JOIN::outer_join to SELECT_LEX::outer_join; indeed, this is
       constant through executions, and we now need it for FD detection so
       computing this variable at optimization [1;31mtime[m was too late.
    
    4) in trunk we eliminate DISTINCT if all group expressions are in the
       select list, but we do it at optimization stage; in this WL we do
       it at preparation stage. This makes only_full_group_by
       distinct-related checks to allow more queries, like
    
         SELECT distinct a, min(b) FROM t1 group by a order by count(*)-count(*);
    
       (which otherwise would yield the new error
       ER_AGGREGATE_IN_ORDER_NOT_SELECT): and it allows the optimizer in
       general to choose better plans (see the change in group_min_max.result).
    
    5) a new class Bool3 for holding true/false/unknown values.

[33mcommit 4fd89ce6ce3b807b348db910611c787889bc1ee1[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Aug 22 13:01:54 2014 +0200

    Bug#18404381: REMOVE UNNEEDE CMAKE CHECKS AND #IFDEFS IN 5.7.5
    
    Patch #5: This patch does the following:
    - Removes HAVE_CRYPT_H, HAVE_CXXABI_H, HAVE_SYNCH_H, HAVE_SYS_MALLOC_H,
      HAVE_SYS_PRCTL_H, HAVE_ASM_TERMBITS_H, HAVE_TERMCAP_H, HAVE_UTIME_H,
      HAVE_SYS_UTIME_H. The header they check for is not needed.
    - Removes the inclusion of several system header files that is not needed.
    - Moves inclusion of serveral system header files from header files to impl files.
    - Removes serveral unneeded extern "C" blocks.
    - Uses HAVE_xxx checks consistently when including system headers.
    - Removes NEED_EXPLICITY_SYNC_DIR - only used in one file. Replaced with __linux__
    - Changes many [1;31mtime[mspec macros to inline functions, removes dead code.
    - Adds a missing header guard.

[33mcommit 2467c1d97470a6e6e26d213ce57be54d06da6fcf[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Wed Aug 20 20:22:39 2014 +0200

    bug#18900198: SHUTDOWN_SERVER FAILS WITH ERROR -2 IN SOME TESTS
    
    Fix for innodb.innodb_bug60196 which had a too small [1;31mtime[mout,
    which failed some[1;31mtime[ms on slow platforms.
    
    Fixed by removing the hardcoded 10 sec shutdown [1;31mtime[mout.
    (The default is 60 s)
    
    Missed the second shutdown_server :(

[33mcommit a0fc855ec110269940a365f2e33c4cd9568b45f7[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Aug 20 15:50:06 2014 +0300

    Correct incorrect comments abuot innobase_flush_logs.
    
    The function has never made an InnoDB redo log checkpoint,
    which would involve flushing data pages from the buffer pool
    to the file system. It has only ever flushed the redo log buffer
    to the redo log files.
    
    The actual InnoDB function call has been changed a few [1;31mtime[ms
    since the introduction of InnoDB to MySQL in 2000, but the semantics
    never changed.
    
    Approved by Vasil Dimov

[33mcommit 6db0922b0fc21d43511dbf56a8a21aafd44ae9f2[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Aug 20 16:30:57 2014 +0530

    - WL#6965: Truncate UNDO logs.
      Follow-up checkin. Increasing the [1;31mtime[mout so that TCs passes on all
      machine (including slow one)

[33mcommit 76fe3ef40a22a3e09a7b26c273da878b7bc3dd90[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Wed Aug 27 14:22:48 2014 +0400

    WL#7159: Move [1;31mtime[m zone tables and help tables from MyISAM to transactional
    storage.
    
    Pushed on behalf of Dmitry Shulga.
    
    The following system tables are migrated to InnoDB by this patch:
      - help_category
      - help_keyword
      - help_relation
      - help_topic
      - [1;31mtime[m_zone
      - [1;31mtime[m_zone_leap_second
      - [1;31mtime[m_zone_name
      - [1;31mtime[m_zone_transition,
      - [1;31mtime[m_zone_transition_type
    
    Attachable transaction provided by previously pushed WLs (7828 and 8003) is
    started to be used by this patch.

[33mcommit 317548d2bcebd66f74d4e61477cb19e072d44dad[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Tue Aug 26 18:54:39 2014 +0400

    WL#7159: Move [1;31mtime[m zone tables and help tables from MyISAM
    to transactional storage.
    
    Prerequisite refactoring patch: in-line open_and_lock_tables() call
    in open_nontrans_system_tables_for_read().
    
    The reasons for that are:
    
      - open_and_lock_tables() should now be used for all but
        the system tables. That allows better component layering and
        will be of use later.
    
      - open_and_lock_tables() code can be significantly simplified
        for opening system tables only (no handling of derived tables,
        no debug code).
    
      - it should be possible to assert that open_and_lock_tables()
        is used only for user tables;
    
      - it should be possible to assert that the system tables are
        opened used open_nontrans_system_tables_for_read().
    
    NOTE:
    
      - open_nontrans_system_tables_for_read() will be paired with
        open_trans_system_tables_for_read() to open system tables in InnoDB.
    
      - open_nontrans_system_tables_for_read() will eventually be gone
        (as soon as all system tables reside in InnoDB).

[33mcommit 61cf00e9dd0097d4fd088c33bdb5e11b3eaf3cc6[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Tue Aug 26 18:52:06 2014 +0400

    WL#7159: Move [1;31mtime[m zone tables and help tables from MyISAM
    to transactional storage.
    
    Prerequisite refactoring patch:
      - rename open_system_tables_for_read() to
        open_nontrans_system_tables_for_read();
      - rename close_system_tables() to close_nontrans_system_tables().

[33mcommit 20336e1b00bfe9bbf04c435422537a58113ea6d0[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Tue Aug 19 10:34:58 2014 +0800

    Bug #18127749 OPTIMIZER SHOULD THROW AN ERROR WHEN USER USES COMPARISION OPERATORS ON GIS DATA
    In arithmetic and comparison operators, as well as math functions and
    aggregate functions that operate on numerical data, don't allow operands
    being geometries. At prepare [1;31mtime[m, check whether the operand's field type
    is MYSQL_TYPE_GEOMETRY, if so, report error.
    Doing so isn't sufficient if all operands are variables, because we don't
    store a variable's value's specific field type.

[33mcommit b3429861bbd7d241d9266ee8e502eb1e498c6a1f[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Mon Aug 18 06:29:24 2014 +0100

    Bug#11762739: MYSQLD GENERAL LOG OUTPUT TO SYSLOG
    WL#7793: Add native support for syslog on unixoid platforms
    
    mysqld did not have native support for traditional unix-style
    syslog; it instead used a shell-script wrapper that would pipe
    the daemon's output to the syslog.  This implied certain limitations:
    - it was impossible to see the syslog status at run[1;31mtime[m
    - it was impossible to change the syslog status at run[1;31mtime[m
    - the shell script could interfere with log rotation
    - the setup would often result in log lines containing two [1;31mtime[mstamps
    
    a) add native syslog support for unixoid OSs.
    
       The following can be set a start-up as well as viewed
       and set at run[1;31mtime[m via system variables (requiring the
       SUPER privilege).
    
       --log_syslog[=0|1]
         on/off switch, defaults to off
    
       --log_syslog_include_pid[=0|1]
         Include mysqld's process ID on each syslog line?
         Defaults to on.  (UNIX only)
    
       --log_syslog_facility=local5
         choose the syslog "facility". Well-known facilities
         may be selected by name.
         Defaults to "daemon".  (UNIX only)
    
       --log_syslog-tag=<string>
         If empty and by default, an ident of "mysqld" is used.
         If e.g. tag "abcd" is supplied, "mysqld-abcd" will be used.
         (The hyphen will be used automatically, the user need not
         supply it.)
    
    b) refactoring: mysys/my_syslog.cc and sql/log.cc had
       some duplication of effort with regard to the
       Windows EventLog.  This patch attempts to eliminate
       the duplicates from log.cc, and merge the remaining
       useful code into my_syslog.cc so that other binaries
       may have a chance to benefit from it.
    
    c) Support features of a) on Windows where possible.
    
    d) Update mysqld_safe to use features of a) instead of
       trying to emulate them in the wrapper script where
       possible, with as little change in syntax as possible.
    
    
    Now incorporates following additions after review:
    
    - mysql-test-run.pl disables syslog/EventLog
    - mysqld--help-win.result updated to reflect the above
    - my_syslog.c: EventID temporarily wasn't used after refactoring
    - mysqld.cc: syslog activated earlier in start-up, for more information
    
    Syslog query on systemd platforms:
    journalctl -f SYSLOG_IDENTIFIER=mysqld-mytag -o json|jq '.SYSLOG_IDENTIFIER + " " + .SYSLOG_FACILITY + ": " + .MESSAGE' 2>/dev/null

[33mcommit 2d62bf27fd819f55dafdbe1d756c8303415c0a61[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Thu Aug 14 11:41:12 2014 +0200

    bug#18900198: SHUTDOWN_SERVER FAILS WITH ERROR -2 IN SOME TESTS
    
    Fix for innodb.innodb_bug60196 which had a too small [1;31mtime[mout,
    which failed some[1;31mtime[ms on slow platforms.
    
    Fixed by removing the hardcoded 10 sec shutdown [1;31mtime[mout.
    (The default is 60 s)
    
    Approved by Marko on IM.

[33mcommit e37a84d33c7029ec1c1486a23d7039ced8e43d0f[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Aug 14 09:02:54 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Don't fill up bg stats' recalc_pool with zeroes (recalc_pool_t::value_type())
    when creating it because that will be treated later like valid table_ids and
    will be processed, wasting lots of [1;31mtime[m.
    
    Instead use the reserve() method for pre-allocating some entries,
    like before.

[33mcommit 3c493fc53fdd296a6d3cdea1ea67447817ca9e3d[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Aug 13 15:21:16 2014 +0200

    Bug#19078013: AVOID USING PUSH_WARNING_PRINTF() WITH NO PRINTF ARGUMENTS
    
    This patch adds __attribute__((format(printf... to the push_warning_printf()
    declaration so that any use of this function without any printf argments
    is detected at compile [1;31mtime[m. In such cases, push_warning() should be
    used instead.
    
    Note that since the format string is not known at compile [1;31mtime[m, we cannot
    detect wrong number or type of arguments.
    
    The patch also changes a number of push_warning_printf() calls to
    push_warning() where this possibility is now detected by the compiler.

[33mcommit fdca8c6cf2e650469a8b3f4d65e5788d61d7801c[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Aug 12 13:48:18 2014 +0300

    Bug#19233510 [ERROR] INNODB: IGNORING THE REDO LOG DUE TO MISSING
    MLOG_CHECKPOINT
    
    InnoDB crash recovery expects the redo log to be empty after the
    latest checkpoint, or to contain a MLOG_CHECKPOINT marker.
    
    Some[1;31mtime[ms, a MLOG_CHECKPOINT marker was incorrectly being omitted when
    performing a log checkpoint.
    
    We must only omit the marker when performing a shutdown, to avoid
    hitting an infinite loop.
    
    This bug used to mostly occur in tests that invoke extra checkpoints,
    such as those related to WL#6501 (TRUNCATE TABLE) and WL#7277
    (bulk load for index creation).
    
    rb#6330 approved by Jimmy Yang

[33mcommit 5855c7c77c10bb1b3c0efff3598a88e05a8f65b2[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Mon Aug 11 11:23:16 2014 +0400

    WL#7976: Deprecate skip-innodb in 5.6, remove in 5.7.
    
    Follow-up patch to remove rpl_myisam_recovery.test.
    
    The point of rpl_myisam_recovery.test was to check that even if the
    procedure to the binary log and at the same [1;31mtime[m recover prepared
    transaction was engaged when total_ha_2pc <= 1 there would not be any
    assert raised inside ha_recover.
    
    Since WL#7976 there is no way to start the server without InnoDB. Thus,
    there can be no case when total_ha_2pc <= 1.
    
    This removal has been discussed with RPL & InnoDB teams.

[33mcommit 78122b159ec6d2d7331c8ccbc17a7ebf13aade15[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Aug 8 04:12:32 2014 +0100

    Bug#11762739: MYSQLD GENERAL LOG OUTPUT TO SYSLOG
    WL#7793: Add native support for syslog on unixoid platforms
    
    mysqld did not have native support for traditional unix-style
    syslog; it instead used a shell-script wrapper that would pipe
    the daemon's output to the syslog.  This implied certain limitations:
    - it was impossible to see the syslog status at run[1;31mtime[m
    - it was impossible to change the syslog status at run[1;31mtime[m
    - the shell script could interfere with log rotation
    - the setup would often result in log lines containing two [1;31mtime[mstamps
    
    a) add native syslog support for unixoid OSs.
    
       The following can be set a start-up as well as viewed
       and set at run[1;31mtime[m via system variables (requiring the
       SUPER privilege).
    
       --log_syslog[=0|1]
         on/off switch, defaults to off
    
       --log_syslog_include_pid[=0|1]
         Include mysqld's process ID on each syslog line?
         Defaults to on.  (UNIX only)
    
       --log_syslog_facility=local5
         choose the syslog "facility". Well-known facilities
         may be selected by name.
         Defaults to "daemon".  (UNIX only)
    
       --log_syslog-tag=<string>
         If empty and by default, an ident of "mysqld" is used.
         If e.g. tag "abcd" is supplied, "mysqld-abcd" will be used.
         (The hyphen will be used automatically, the user need not
         supply it.)
    
    b) refactoring: mysys/my_syslog.cc and sql/log.cc had
       some duplication of effort with regard to the
       Windows EventLog.  This patch attempts to eliminate
       the duplicates from log.cc, and merge the remaining
       useful code into my_syslog.cc so that other binaries
       may have a chance to benefit from it.
    
    c) Support features of a) on Windows where possible.
    
    d) Update mysqld_safe to use features of a) instead of
       trying to emulate them in the wrapper script where
       possible, with as little change in syntax as possible.

[33mcommit 230d18f7b49f1a552dacbdeef429f14db912b3a8[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Aug 5 18:05:08 2014 +0200

    Revert fix merged in from 5.5.39 that does not work as is in MySQL Cluster
    Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS, SHOW PROCESSLIST, SHOW BINLOGS
    
    ndb_binlog.ndb_binlog_purge fail with debug.
    
    Reverted revisions are:
    
    ------------------------------------------------------------
    revno: 3069.220.13
    revision-id: venkatesh.duggirala@oracle.com-20140509042215-kukwf135nfyy9yy2
    parent: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.5
    [1;31mtime[mstamp: Fri 2014-05-09 09:52:15 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Fixing post push test failure (MTR does not like giving
      127.0.0.1 for localhost incase of --embedded run, it thinks
      it is an external ip address)
    ------------------------------------------------------------
    revno: 3069.220.12
    revision-id: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
    parent: mithun.c.y@oracle.com-20140508091953-qafkm6r1dhycdjyh
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.5
    [1;31mtime[mstamp: Thu 2014-05-08 18:13:01 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Problem:  A deadlock was occurring when 4 threads were
      involved in acquiring locks in the following way
      Thread 1: Dump thread ( Slave is reconnecting, so on
                    Master, a new dump thread is trying kill
                    zombie dump threads. It acquired thread's
                    LOCK_thd_data and it is about to acquire
                    mysys_var->current_mutex ( which LOCK_log)
      Thread 2: Application thread is executing show binlogs and
                     acquired LOCK_log and it is about to acquire
                     LOCK_index.
      Thread 3: Application thread is executing Purge binary logs
                     and acquired LOCK_index and it is about to
                     acquire LOCK_thread_count.
      Thread 4: Application thread is executing show processlist
                     and acquired LOCK_thread_count and it is
                     about to acquire zombie dump thread's
                     LOCK_thd_data.
      Deadlock Cycle:
           Thread 1 -> Thread 2 -> Thread 3-> Thread 4 ->Thread 1
    
      The same above deadlock was observed even when thread 4 is
      executing 'SELECT * FROM information_schema.processlist' command and
      acquired LOCK_thread_count and it is about to acquire zombie
      dump thread's LOCK_thd_data.
    
      Analysis:
      There are four locks involved in the deadlock.  LOCK_log,
      LOCK_thread_count, LOCK_index and LOCK_thd_data.
      LOCK_log, LOCK_thread_count, LOCK_index are global mutexes
      where as LOCK_thd_data is local to a thread.
      We can divide these four locks in two groups.
      Group 1 consists of LOCK_log and LOCK_index and the order
      should be LOCK_log followed by LOCK_index.
      Group 2 consists of other two mutexes
      LOCK_thread_count, LOCK_thd_data and the order should
      be LOCK_thread_count followed by LOCK_thd_data.
      Unfortunately, there is no specific predefined lock order defined
      to follow in the MySQL system when it comes to locks across these
      two groups. In the above problematic example,
      there is no problem in the way we are acquiring the locks
      if you see each thread individually.
      But If you combine all 4 threads, they end up in a deadlock.
    
      Fix:
      Since everything seems to be fine in the way threads are taking locks,
      In this patch We are changing the duration of the locks in Thread 4
      to break the deadlock. i.e., before the patch, Thread 4
      ('show processlist' command) mysqld_list_processes()
      function acquires LOCK_thread_count for the complete duration
      of the function and it also acquires/releases
      each thread's LOCK_thd_data.
    
      LOCK_thread_count is used to protect addition and
      deletion of threads in global threads list. While show
      process list is looping through all the existing threads,
      it will be a problem if a thread is exited but there is no problem
      if a new thread is added to the system. Hence a new mutex is
      introduced "LOCK_thd_remove" which will protect deletion
      of a thread from global threads list. All threads which are
      getting exited should acquire LOCK_thd_remove
      followed by LOCK_thread_count. (It should take LOCK_thread_count
      also because other places of the code still thinks that exit thread
      is protected with LOCK_thread_count. In this fix, we are changing
      only 'show process list' query logic )
      (Eg: unlink_thd logic will be protected with
      LOCK_thd_remove).
    
      Logic of mysqld_list_processes(or file_schema_processlist)
      will now be protected with 'LOCK_thd_remove' instead of
      'LOCK_thread_count'.
    
      Now the new locking order after this patch is:
      LOCK_thd_remove -> LOCK_thd_data -> LOCK_log ->
      LOCK_index -> LOCK_thread_count

[33mcommit 18d3540818f5965966ab9dff9db35480c40131a5[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Aug 5 16:29:07 2014 +0200

    Revert fix merged in from 5.6.20 that does not work as is in MySQL Cluster
    Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS, SHOW PROCESSLIST, SHOW BINLOGS
    
    Reverted revisions are:
    
    ------------------------------------------------------------
    revno: 2876.552.146
    revision-id: venkatesh.duggirala@oracle.com-20140718110044-ivyb4xygx4q59puv
    parent: ashish.y.agarwal@oracle.com-20140717135640-3m6s2n57s1th3vom
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6.20-release
    [1;31mtime[mstamp: Fri 2014-07-18 16:30:44 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Post push fix (removing try-catch)
    ------------------------------------------------------------
    revno: 2876.552.16 [merge]
    revision-id: venkatesh.duggirala@oracle.com-20140509042350-ni6xx9khsej94hl9
    parent: venkatesh.duggirala@oracle.com-20140508124746-8de4t4utydx15x2k
    parent: venkatesh.duggirala@oracle.com-20140509042215-kukwf135nfyy9yy2
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Fri 2014-05-09 09:53:50 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Fixing post push test failure (MTR does not like giving
      127.0.0.1 for localhost incase of --embedded run, it thinks
      it is an external ip address)
        ------------------------------------------------------------
        revno: 2875.596.13
        revision-id: venkatesh.duggirala@oracle.com-20140509042215-kukwf135nfyy9yy2
        parent: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
        committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
        branch nick: mysql-5.5
        [1;31mtime[mstamp: Fri 2014-05-09 09:52:15 +0530
        message:
          Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
          SHOW PROCESSLIST, SHOW BINLOGS
    
          Fixing post push test failure (MTR does not like giving
          127.0.0.1 for localhost incase of --embedded run, it thinks
          it is an external ip address)
    ------------------------------------------------------------
    revno: 2876.552.15 [merge]
    revision-id: venkatesh.duggirala@oracle.com-20140508124746-8de4t4utydx15x2k
    parent: mithun.c.y@oracle.com-20140508092217-2vyrjxxtzf18r0ho
    parent: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6
    [1;31mtime[mstamp: Thu 2014-05-08 18:17:46 +0530
    message:
            Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
            SHOW PROCESSLIST, SHOW BINLOGS
    
            Merge from mysql-5.5 ( Fix is changed from
            mysql-5.5's patch)
    
            Problem:  A deadlock was occurring when 4 threads were
            involved in acquiring locks in the following way
            Thread 1: Dump thread ( Slave is reconnecting, so on
                          Master, a new dump thread is trying kill
                          zombie dump threads. It acquired thread's
                          LOCK_thd_data and it is about to acquire
                          mysys_var->current_mutex ( which LOCK_log)
            Thread 2: Application thread is executing show binlogs and
                           acquired LOCK_log and it is about to acquire
                           LOCK_index.
            Thread 3: Application thread is executing Purge binary logs
                           and acquired LOCK_index and it is about to
                           acquire LOCK_thread_count.
            Thread 4: Application thread is executing show processlist
                           and acquired LOCK_thread_count and it is
                           about to acquire zombie dump thread's
                           LOCK_thd_data.
            Deadlock Cycle:
                 Thread 1 -> Thread 2 -> Thread 3-> Thread 4 ->Thread 1
    
            The same above deadlock was observed even when thread 4 is
            executing 'SELECT * FROM information_schema.processlist' command and
            acquired LOCK_thread_count and it is about to acquire zombie
            dump thread's LOCK_thd_data.
    
            Analysis:
            There are four locks involved in the deadlock.  LOCK_log,
            LOCK_thread_count, LOCK_index and LOCK_thd_data.
            LOCK_log, LOCK_thread_count, LOCK_index are global mutexes
            where as LOCK_thd_data is local to a thread.
            We can divide these four locks in two groups.
            Group 1 consists of LOCK_log and LOCK_index and the order
            should be LOCK_log followed by LOCK_index.
            Group 2 consists of other two mutexes
            LOCK_thread_count, LOCK_thd_data and the order should
            be LOCK_thread_count followed by LOCK_thd_data.
            Unfortunately, there is no specific predefined lock order defined
            to follow in the MySQL system when it comes to locks across these
            two groups. In the above problematic example,
            there is no problem in the way we are acquiring the locks
            if you see each thread individually.
            But If you combine all 4 threads, they end up in a deadlock.
    
            Fix:
            Since everything seems to be fine in the way threads are taking locks,
            In this patch We are changing the duration of the locks in Thread 4
            to break the deadlock. i.e., before the patch, Thread 4
            ('show processlist' command) mysqld_list_processes()
            function acquires LOCK_thread_count for the complete duration
            of the function and it also acquires/releases
            each thread's LOCK_thd_data. Instead of it, Now it will take
            a copy of THDs from global_thread_list and perform traversal
            (on copied THDs) only after releasing  LOCK on LOCK_thread_count.
            During traversal(on copied THDs), removal from global_thread_list is
            blocked using another mutex LOCK_thd_remove such that
            THD copied are valid during traversal(otherwise remove destroys THD).
    
            Now the new locking order after this patch is:
            LOCK_thd_remove -> LOCK_thd_data -> LOCK_log ->
            LOCK_index -> LOCK_thread_count
        ------------------------------------------------------------
        revno: 2875.596.12
        revision-id: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
        parent: mithun.c.y@oracle.com-20140508091953-qafkm6r1dhycdjyh
        committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
        branch nick: mysql-5.5
        [1;31mtime[mstamp: Thu 2014-05-08 18:13:01 +0530
        message:
          Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
          SHOW PROCESSLIST, SHOW BINLOGS
    
          Problem:  A deadlock was occurring when 4 threads were
          involved in acquiring locks in the following way
          Thread 1: Dump thread ( Slave is reconnecting, so on
                        Master, a new dump thread is trying kill
                        zombie dump threads. It acquired thread's
                        LOCK_thd_data and it is about to acquire
                        mysys_var->current_mutex ( which LOCK_log)
          Thread 2: Application thread is executing show binlogs and
                         acquired LOCK_log and it is about to acquire
                         LOCK_index.
          Thread 3: Application thread is executing Purge binary logs
                         and acquired LOCK_index and it is about to
                         acquire LOCK_thread_count.
          Thread 4: Application thread is executing show processlist
                         and acquired LOCK_thread_count and it is
                         about to acquire zombie dump thread's
                         LOCK_thd_data.
          Deadlock Cycle:
               Thread 1 -> Thread 2 -> Thread 3-> Thread 4 ->Thread 1
    
          The same above deadlock was observed even when thread 4 is
          executing 'SELECT * FROM information_schema.processlist' command and
          acquired LOCK_thread_count and it is about to acquire zombie
          dump thread's LOCK_thd_data.
    
          Analysis:
          There are four locks involved in the deadlock.  LOCK_log,
          LOCK_thread_count, LOCK_index and LOCK_thd_data.
          LOCK_log, LOCK_thread_count, LOCK_index are global mutexes
          where as LOCK_thd_data is local to a thread.
          We can divide these four locks in two groups.
          Group 1 consists of LOCK_log and LOCK_index and the order
          should be LOCK_log followed by LOCK_index.
          Group 2 consists of other two mutexes
          LOCK_thread_count, LOCK_thd_data and the order should
          be LOCK_thread_count followed by LOCK_thd_data.
          Unfortunately, there is no specific predefined lock order defined
          to follow in the MySQL system when it comes to locks across these
          two groups. In the above problematic example,
          there is no problem in the way we are acquiring the locks
          if you see each thread individually.
          But If you combine all 4 threads, they end up in a deadlock.
    
          Fix:
          Since everything seems to be fine in the way threads are taking locks,
          In this patch We are changing the duration of the locks in Thread 4
          to break the deadlock. i.e., before the patch, Thread 4
          ('show processlist' command) mysqld_list_processes()
          function acquires LOCK_thread_count for the complete duration
          of the function and it also acquires/releases
          each thread's LOCK_thd_data.
    
          LOCK_thread_count is used to protect addition and
          deletion of threads in global threads list. While show
          process list is looping through all the existing threads,
          it will be a problem if a thread is exited but there is no problem
          if a new thread is added to the system. Hence a new mutex is
          introduced "LOCK_thd_remove" which will protect deletion
          of a thread from global threads list. All threads which are
          getting exited should acquire LOCK_thd_remove
          followed by LOCK_thread_count. (It should take LOCK_thread_count
          also because other places of the code still thinks that exit thread
          is protected with LOCK_thread_count. In this fix, we are changing
          only 'show process list' query logic )
          (Eg: unlink_thd logic will be protected with
          LOCK_thd_remove).
    
          Logic of mysqld_list_processes(or file_schema_processlist)
          will now be protected with 'LOCK_thd_remove' instead of
          'LOCK_thread_count'.
    
          Now the new locking order after this patch is:
          LOCK_thd_remove -> LOCK_thd_data -> LOCK_log ->
          LOCK_index -> LOCK_thread_count

[33mcommit 1a8d1735bb5d3ccf506788f6002cb043f6ae2599[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Aug 5 10:18:24 2014 +0200

    Revert quick fix for dtrace problems in release tree.
    New fix in CMake files will be merged in from 5.6.20.
    
    ------------------------------------------------------------
    revno: 4383 [merge]
    revision-id: sowmya.dass@oracle.com-20140712180534-710x44w88idoew4v
    parent: magnus.blaudd@oracle.com-20140711175006-p4n3l9hnltnfupmd
    parent: bjorn.munch@oracle.com-20140702083103-ajqdteouwcv8i0ej
    author: sowmya.dass@oracle.com
    committer: Sowmya Dass <sowmya.dass@oracle.com>
    branch nick: mysql-5.6-cluster-7.3
    [1;31mtime[mstamp: Sat 2014-07-12 20:05:34 +0200
    message:
      Merge from mysql-cluster-7.3.6-release
        ------------------------------------------------------------
        revno: 4372.1.2
        tags: mysql-cluster-7.3.6
        revision-id: bjorn.munch@oracle.com-20140702083103-ajqdteouwcv8i0ej
        parent: bjorn.munch@oracle.com-20140701131010-65gq2vq91m8ia0t2
        committer: Bjorn Munch <bjorn.munch@oracle.com>
        branch nick: clu-736
        [1;31mtime[mstamp: Wed 2014-07-02 10:31:03 +0200
        message:
          Unconditionally disable dtrace for rpm, barfs on Oracle dtrace
        ------------------------------------------------------------
        revno: 4372.1.1
        revision-id: bjorn.munch@oracle.com-20140701131010-65gq2vq91m8ia0t2
        parent: ole.john.aske@oracle.com-20140626140938-6zl4x8i49u36hb81
        committer: Bjorn Munch <bjorn.munch@oracle.com>
        branch nick: clu-736
        [1;31mtime[mstamp: Tue 2014-07-01 15:10:10 +0200
        message:
          Unconditionally disable dtrace for rpm-oel, barfs on Oracle dtrace

[33mcommit febe1236e040784dde0263e16d3481f2fb1724fa[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sun Aug 3 11:08:23 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    Avoids to generate mysqld-debug.dmp during the test.
    IO thread can exit always when SRV_SHUTDOWN_EXIT_THREADS, if page cleaner already exited.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for performance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      optimal performance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit ff659570dd24733c162fd260d52dc23eee56cbdf[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jul 31 11:18:11 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    There are race between io threads and page cleaners to exit only when abort initialize InnoDB.
    Especially, Windows native AIO is weak for the race.
    IO threads should not exit during page cleaners active, in accurate.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for performance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      optimal performance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit db7dbc509e273420a805955aa9a0ed6ab0df1ca3[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jul 30 13:25:40 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Followup to vasil.dimov@oracle.com-20140725165803-3g8u629t2ya3pc9p
    remove an assert that key is not PSI_NOT_INSTRUMENTED because when
    PFS is disabled at run[1;31mtime[m then all keys are PSI_NOT_INSTRUMENTED.

[33mcommit 4a4f248e7593400fe2f4223678eaf42a7bf6ba89[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Jul 25 10:02:57 2014 +0530

    Bug#19129559 -  pb2 main.max_statement_[1;31mtime[m.test crashing the
                    server in weekly trunk.
    
    Analysis:
    ---------
    This test is failing because few reasons,
    
    1. Issue mentioned in the bug report: (Started failing on Window
       from some [1;31mtime[m 21 June)
    
      qld-debug.exe![1;31mtime[mr_notify_thread_func()[win_[1;31mtime[mrs.c:78]
      7ff74fe2b42e    mysqld-debug.exe!pfs_spawn_thread()[pfs.cc:2074]
      7ff74f92d306    mysqld-debug.exe!pthread_start()[my_winthread.c:73]
      7ff74ff964f5    mysqld-debug.exe!_callthreadstartex()[threadex.c:376]
      7ff74ff96747    mysqld-debug.exe!_threadstartex()[threadex.c:359]
      7ff86d0015dd    KERNEL32.DLL!BaseThreadInitThunk()
      7ff86f6343d1    ntdll.dll!RtlUserThreadStart()
    
      => This issue is seen on Windows 64 bit machines. In function
         "[1;31mtime[mr_notify_thread_func" pointer value of [1;31mtime[mr object is
         stored in ULONG variable then type casted to pointer of
         my_[1;31mtime[mr_t. This works well in 32 bit machine and 64 bit machine
         until address crosses 2^32 value.
         Windows 64 is of LLP64 model. So ULONG variable is of 32 bit
         and pointer is of 64 bit. Because of this pointer value was
         getting corrupted when 64 bit value is stored in 32 bit ULONG
         type variable. Accessing this memory was causing an issue on
         Windows 64.
    
         To fix this issue, changed type of variable from ULONG to
         ULONG_PTR.
    
    2.  (Sporadic failure)
        mysqltest: At line 472: query 'SELECT CONVERT(VARIABLE_VALUE, UNSIGNED) INTO @qc_hits
                                       FROM INFORMATION_SCHEMA.GLOBAL_STATUS
                                       WHERE VARIABLE_NAME = 'Qcache_hits''
        failed: 1909: Query execution was interrupted, max_statement_[1;31mtime[m
                      exceeded
    
       => Issue here is, max_statement_[1;31mtime[m is set 50 milliseconds
          and assumption was "SELECT CONVERT(VARIABLE_NAME,...."
          returns withing 50 millisecond. But on some slow platforms
          it took more than that so query terminated.
    
          To fix this issues, Changed session max_statement_[1;31mtime[m to 0.
    
    
    3.  (Sporadic failure)
       mysqltest: At line 119: query 'SELECT * from t1' succeeded -
                 should have failed with errno 1909...
    
       =>Issue here is, max_statement_[1;31mtime[m at session level is set
         to 2 milliseconds and query "SELECT * FROM..." is executed.
         Here, expectation is SELECT will take more than 2 milliseconds
         but on some platforms it returned before 2 millisecond.
         So it is not [1;31mtime[md out.
    
         To fix this issues, changed query to select *, sleep(0.5) FROM t1;
         Now sleep(0.5) is executed for each row. This will make query
         execution [1;31mtime[m greater than 2 microsecond.
    
    4.  (Sporadic failure)
        SET @@SESSION.max_statement_[1;31mtime[m = 950;
        SELECT SLEEP(1);
         SLEEP(1)
         -1
         +0
    
       => Test case 10, checks [1;31mtime[mr precision. In this test case,
          query calls sleep(1) function and for that [1;31mtime[mr with
          different [1;31mtime[mout values are set. And expectation is, if
          [1;31mtime[mr is set for less than 1 sec then sleep should be
          interrupted.
    
          But, max_statement_[1;31mtime[m is a soft hint for queries, on loaded
          machines or slow machines [1;31mtime[m taken notify thread after
          [1;31mtime[mr expiration would be little bit more. Because of this
          sleep() is not interrupted.
    
          We can reduce statement execution [1;31mtime[mout value, but test
          case to check [1;31mtime[mr precision will be incorrect. Not
          changing test case will result in sporadic test case
          failure. Removed this test case to avoid sporadic failure.
    
    
    5. (sporadic failure)
       @@ -151,7 +151,7 @@
       ERROR HY000: Query execution was interrupted, max_statement_[1;31mtime[m exceeded
       SELECT @a;
        @a
        -1
       +NULL
    
       => Here we set sessions level max_statement_[1;31mtime[m to 2 and
          execute stored function having "SELECT SLEEP(2) INTO @a".
          After SELECT query interruption value of variable @a is
          printed. But some [1;31mtime[ms, the SELECT is interrupted before
          populating variable.
    
          Actually test case is verified by checking error reported.
          We need not have to print variable value here. To avoid this
          failure, removed "SELECT @a" statement.

[33mcommit c41c376d6502f767cd4c738b9cf050c43e69ac31[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Thu Jul 24 13:27:53 2014 +0530

    Problem:
    ========
    The trx returned from the trx pool is not in clean state.
    This is verified by adding assert ut_ad(trx->will_lock ==0)
    in trx create function.
    
    1) trx->will_lock is incremented after trx_commit
    
    i)   ha_commit_trans
    ii)  close_thread_tables->mysql_unlock_tables->unlock_external->
         ha_external_lock(.., F_UNLCK)->ha_innobase::external_lock(.. F_UNLCK)
    iii) in ha_innobase::external_lock() : this increments trx->will_lock
            if (!trx_is_started(trx)
                && (prebuilt->select_lock_type != LOCK_NONE
                    || prebuilt->stored_select_lock_type != LOCK_NONE)) {
    
                    ++trx->will_lock;
            }
    
    2)  in Functions like ha_innobase::create(), ha_innobase::delete_table(),
        etc, trx->will_lock is incremented but
        the decrement may or may not happen.
    
       i)   If functions called by ha_innobase::create() do a
            trx_commit()/rollback, trx->will_lock is reset, there is
            no problem in this case
       ii)  Some[1;31mtime[ms(incase of failure) the trx is not started by
            ha_innobase::create()/rollback(), trx state remains as
            TRX_STATE_NOT_STARTED.
       iii) The following trx commit or rollback will not reset
            trx->will_lock because the trx state is TRX_STATE_NOT_STARTED
    
    Fix:
    ====
    1) This is solved by not incrementing the trx->will_lock counter when we
       are unlocking.
    2) This is solved by resetting trx->will_lock counter when the
       trx->state remains as TRX_STATE_NOT_STARTED
    
    Approved by Sunny. rb#6068

[33mcommit ab077196f632677b343c5a8b85e488ae022c55b4[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jul 22 22:18:03 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Do not use UT_NEW() and UT_DELETE() for managing handler_ctx
    because that object may some[1;31mtime[ms be freed by the upper layer using
    Sql_alloc's "delete", which is different from UT_DELETE().

[33mcommit b3775846f8e52763c739adc36d907af826c97bf0[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Fri Jul 18 13:31:23 2014 +0530

    Increasing the [1;31mtime[mout value for Expect .

[33mcommit 579a88e3636ecfec26bd8f70586c3f1e4a87932b[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Jul 16 15:24:04 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - increase [1;31mtime[mout considering windows slower run.

[33mcommit 66b8dfdb8b29321466600ff60a86aa3688238cb9[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Tue Jul 15 21:49:23 2014 +0530

    BUG#18675237- NOW() : MISSING FRACTIONAL SECONDS FROM A VIEW
    
    Analysis:
    --------
    
    Creating a view using the [1;31mtime[m function NOW(6) registers
    the view query definition as NOW() in the FRM file for
    the view. Thus the fractional seconds part is missing
    in the view definition.
    
    CREATE VIEW v1 AS SELECT NOW(6) as fld1;
    
    Is transformed as:
    CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost`
    SQL SECURITY DEFINER VIEW `v1` AS select now() AS `fld1`
    
    While registering the query definition in the FRM file,
    the 'decimals'(fractional seconds) was ignored. Hence NOW(6)
    was printed as NOW() in the FRM file, thus missing the
    fractional seconds.
    
    Fix:
    ---
    While registering the view query definition in the FRM file
    which uses temporal functions like NOW(), CURTIME() and
    SYSDATE(), print the 'decimals'(fractional seconds part)
    value, if it is set.

[33mcommit 7b88f8c4adf99f94a276f6c381cf7b92a5e13227[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Mon Jul 14 15:42:00 2014 +0100

    Bug #19193927   TC TAKEOVER TAKES TOO LONG CAUSING MANY PROBLEMS
    
    The fix for (18069334 POOR MULTI-TC-INSTANCE + MULTI NODE FAILURE HANDLING LEADS TO CLUSTER CRASH)
    introduced a new bug where the number of TC instances in the failed
    node is set to an excessively high value.  This results in many rounds
    of the TC failover protocol running, which takes a long [1;31mtime[m, and
    impacts on the processing of other node failures, and GCP handling with
    various side-effects.
    
    This patch fixes the problem.

[33mcommit 9f5b1692cdcac3b34a18a2e03836b61f18329e90[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Jul 11 18:39:06 2014 +0200

    Bug#19193756 MTR GENERATES TOO LONG SOCKET PATHS
    
     - the socket path generated by ConfigFactory.pm was some[1;31mtime[ms longer
       then the max tested socket path.
     - fix by generating a shorter path based on the generated port number, which
       is unique i.e mysqld-<port>.sock

[33mcommit 6c78a0f7bfb1718ff72b2350efad6bcc8c2f4d20[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Thu Jul 10 12:44:21 2014 +0100

    BUG#16409385: SEVERAL RPL TESTS FAIL ON SOLARIS WITH TIMEOUT FOR 'CHECK WARNING'
    
    The check-testcase stage is timing out for these two test cases
    on PB2. The tests output too many spurious warnings related
    to restarting the slave threads.
    
    To fix this, we suppress warnings from the error log for
    rpl_typeconv and rpl_show_slave_status_nonblocking. These
    warnings are not the focus of the test cases anyway. This reduces
    the size of the error log and makes the tests less vulnerable to
    fail on the check-testcase stage. Note that, error entries are
    still logged.
    
    Also, increased the [1;31mtime[mout for rpl_typeconv as well.

[33mcommit 7d8ac9a74ee02d697a218bc927dd5bd50bb4c451[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Wed Jul 9 17:30:15 2014 +0530

    Bug#18497612 - INCORRECT VIEW BEHAVIOR WHEN SEVERAL CHARACTER SETS
                   ARE USED
    
    Description:
    
    There are two issues as follows
    a)main.ddl_i18n_utf8.test and main.ddl_i18n_koi8r.test are skipped
      when tried with mtr.
    b)As test cases were skipped, 5.7 has got regression with collations.
    
    Analysis:
    
    a) Due to change in 'show collation' command behavior, new records
       returned by 'SHOW COLLATIONS' started to mismatch what's stored
       within r/have_xxx.require. This caused the tests to skip.
    
    b) Regression was developed by the fix of the Bug#13520710.
       Fix made sure that while writing view statement, client character
       set is used. Regression arised in the case the view statement has
       different character set than client character set.
    
    FIX :
    
    a) Changed r/have_xxx.require files with the latest result of
       'SHOW COLLATIONS' statement. Test cases will not be skipped now.
       As the test case was disabled for a long [1;31mtime[m, result file got
       outdated. Updated result file.
    b) Fixed code to use the character set if provided in the statement.

[33mcommit 16827fcee42fdc668523375b42194a0cfb10b1bd[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Jul 9 15:33:20 2014 +0900

    Revert the following change, because the regression was innocent about the Bug#11755438:Bug#47213 fix now.
    
    ------------------------------------------------------------
    revno: 8279 [merge]
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Wed 2014-06-25 05:40:00 +0200
    message:
      Disable memory barrier only for Intel CPU, because performance regression was observed at some conditions for Intel CPU.
      follow up for Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL

[33mcommit ce720cea7f254633a64f30fa68250669f64db3f0[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Wed Jul 9 05:19:51 2014 +0200

    WL#6711: This file is for big test. If --valgrind is used for regression
    test, the test will last a long [1;31mtime[m which will result in Timeout for
    PB2 test. So I skip such a file test if in --valgrind mode.

[33mcommit fad82cd80b23a22885ecd1fb9d0470f1c508d170[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Jul 4 06:01:03 2014 +0300

    Bug#19061440 CMAKE -DINNODB_PAGE_ATOMIC_REF_COUNT:BOOL=OFF IS BROKEN
    
    The configuration option
    cmake -DINNODB_PAGE_ATOMIC_REF_COUNT:BOOL=OFF
    
    is broken ever since WL#7682 was merged.
    
    It is also useless since WL#7655, because MySQL 5.7 can no longer
    be built on systems that do not support atomic memory access primitives.
    
    This patch removes the option. It should be a non-functional change,
    because the undocumented build-[1;31mtime[m option was ON by default.
    
    rb#5844 approved by Yasufumi Kinoshita

[33mcommit 3986ca65bb90b80ae10dbc0da8a021579b487c16[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Thu Jul 3 10:35:38 2014 +0100

    BUG#18844897:  ASSERT AT INFO.DRY_RUN
    
    A test case configured with --skip-innodb and that checks whether the
    slave server is able to restart and replication is able to either
    restart or step into a degraded mode when using slave_master_info and
    slave_relay_log_info tables on InnoDB, was failing with two
    crashes. One during the shutdown and another during recovery on server
    restart. The former, even though not entirely related to replication,
    was triggering the latter. Since the former crash was sporadic, so was
    the subsequent one. This was noticed in that test, because the test
    stops and restarts the slave server several [1;31mtime[ms.
    
    The core of the problem was that the shutdown crash was causing the
    server not to close the binary log properly. Then, on server restart,
    while opening the the binary log, the server would find that it had
    not been closed properly. Therefore, it would engage the log for
    recovery and roll forward or roll back the pending prepared
    transactions in the engine. This results in the server scanning the
    binary log, collect xids - in this case none, since the tests was
    running with MyISAM (remeber --skip-innodb?) - and call ha_recover.
    But, inside, this function would run into an assert that checks
    whether there are sufficient parties to resume the pending prepared
    phases of the 2PC. Since the engine involved was MyISAM and no xid had
    been collected and ha_recover was called regardless, this assert would
    be triggered.
    
    This patch fixes this issue by checking if there are enough 2PC parties
    involved before calling into ha_recover.

[33mcommit 84fca39d685db285fea4b88f3b11a496fdea40b1[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Thu Jul 3 13:13:48 2014 +0530

    WL#1697: Multisource Replication.
    
     This patch implements the Multisource feature for MySQL Replication.
     Through this feature, a slave could connect to multiple masters at
     the same [1;31mtime[m and start replication.
     The present patch contains only the code for the feature.
    
     Important information for this feature. (See WL#1697, for full description)
     ==========================================================================
     1. MSR is supported only for TABLE type slave repositories.
     2. Having a FOR CHANNEL to the replication command would
        act on that CHANNEL only.
     3. If FOR CHANNEL clause is not present, then by default it
        would act on all channels. START SLAVE, STOP SLAVE, SHOW SLAVE STATUS,
        FLUSH RELAYLOGS, RESET SLAVE etc unless where the command doesn't make sense.
     4. CHANGE MASTER, SHOW_RELAYLOG_EVENTS, master_pos_wait(), gtid_set_wait()
        error out if a channel name is not specified.
     5. The folloing patch implements all the commands for MSR
     6. replication performance schema tables are also supported for MSR
     7. All SHOW STATUS VARIABLES act on default channel only (which is "",
        this always exists, not settable and deleted by the user)

[33mcommit 010811852fb66d5bccd42ebd84c263aba0157f9c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Wed Jul 2 22:07:50 2014 +0400

    WL#1159 "Allow multiple locks in GET_LOCK()".
    
    This patch is based on contribution from Konstantin Osipov.
    
    The goal of this patch is to allow user to acquire multiple
    user-level locks in the same connection via serie of calls to
    GET_LOCK() function. Also it makes possible to take the same
    lock twice in connection (i.e. locks became recursive).
    GET_LOCK() no longer implicitly releases the previous lock held
    by the connection. To release all locks in connection one can
    use new RELEASE_ALL_LOCKS() function.
    
    The above is achieved by deleting all old code which implements
    user-level locks, and forwarding user-level lock requests to the
    metadata locking subsystem. A new metadata type was introduced
    for user-level locks - "user". Instances of "USER" locks are
    mutually exclusive.
    
    Possible deadlocks between multiple locks taken in different order,
    as well as between user-level locks and metadata/waits for table
    flushes are detected and resolved using the MDL deadlock detector.
    Waits for user-level locks are preferred as victim over waits
    for locks typically acquired by DDL, but waits for locks typically
    acquired by DML are preferred over waits for user-level locks.
    
    Wait for user-level lock which is aborted due to deadlock is
    reported as ER_USER_LOCK_DEADLOCK error. No transaction rollback
    happens in this case.
    
    MDL subsystem was extended to support the old behavior when waits
    for user-level locks are automagically aborted if connection which
    requested them disconnects. However now this situation is handled
    similarly to KILL statement rather than to lock wait [1;31mtime[mout.
    
    Since IS_USED_LOCK() function returns id of connection which owns
    the lock the MDL API had to be extended with capability to query
    lock owner.
    
    Note that with old implementation when a wait for user-level lock was
    aborted due to query or connection being killed GET_LOCK() function
    always returned NULL and statement using this function might have
    succeeded or failed with ER_QUERY_INTERRUPTED error, depending if
    statement tried to do anything else after calling GET_LOCK().
    With new implementation statement which called GET_LOCK() and was
    killed will always fail with ER_QUERY_INTERRUPTED due to slightly
    different KILL error handling by MDL subsystem.
    
    Since MDL subsystem imposes limits on the length of key used to
    identify objects new implementation introduces limit on user-level
    lock name length. New limit is 64 characters.
    ER_USER_LOCK_WRONG_NAME error is emitted when one of functions
    accepting user-level lock name as argument gets name which is
    longer than 64 character. Also behavior is changed to return the
    same error for NULL or empty ('') lock name.
    Also new implementation will always convert lock name to utf8 from
    its original charset and perform case-insensitive comparison.
    
    Existing test coverage for user-level lock functionality has been
    extended and moved from main.func_misc to main.user_lock test.
    
    New rpl.rpl_user_lock test covers replication of statements using
    user-level lock functions.
    
    Some other existing tests had to be adjusted to the new user-lock
    behavior including rpl.rpl_err_ignoredtable test. The latter was
    additionally fixed to really test what it was intended to test.
    
    Tests in perfschema test suite which used mutex used in user-level
    lock implementation to test aggregation of wait events were changed
    to use different mutex. Tests which used user-level locks to test
    aggregation for memory events we adjusted to new implementation
    memory usage. Coverage for new MDL namespace was added to
    perfschema.mdl_func test.
    
    Unit tests covering MDL API/code changes were added.

[33mcommit 75f9fa74ede1909c9d1694ac39b06881137f5d74[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Jul 1 15:28:42 2014 +0200

    Bug#18404381: REMOVE UNNEEDE CMAKE CHECKS AND #IFDEFS IN 5.7.5
    
    Patch #2: This patch does the following:
    - Enforces Visual Studio 2013 as the minimum version of Visual Studio,
      in accordance with our supported platform list for 5.7
    
    - Removes the unused CYBOZU and BACKUP_TEST CMake options
    - Removes the following CMake checks which were always false for
      our supported platforms: HAVE_TERMBITS_H, HAVE_NDIR_H, HAVE_SELECT_H,
      HAVE_SYS_NDIR_H, HAVE_SYSENT_H, HAVE_RDTSCLL
    - Removes the following CMake checks which were always true for
      our supported platforms: HAVE_SYS_TYPES_H, HAVE_FCNTL_H, HAVE_FENV_H,
      HAVE_INTTYPES_H, HAVE_STDINT_H, HAVE_SYS_STAT_H, HAVE_RINT,
      HAVE_FESETROUND
    - Removes the following CMake checks where the results are not needed:
      HAVE_SYS_DIR_H, HAVE_DLERROR, HAVE_ASM_MSR_H, HAVE_DECL_MADVISE
    
    - Moves the defintion of NOGDI from my_global.h to the Windows CMake
      file so its defintion no longer depends on including my_global.h
    - Moves the defintion of __EXTENSIONS, _POSIX_THREAD_SEMANTICS and
      _REENTRANT from my_global.h to the Solaris CMake file so their
      defintion no longer depends on including my_global.h
    - Moves the HAVE_SOLARIS_STYLE_GETHOST CMake check to the Solaris
      CMake file so that it is not run on all platforms
    
    - Removes use of #_lint
    - Removes use of TARGET_OS_LINUX, use __linux__ instead
    - Removes use of HAVE_LONG_LONG, always true
    - Removes various __GNUC__ version checks which are no longer relevant
    - Changes remaining use of DBUG_ON to !DBUG_OFF
    
    - Removes setting of _GNU_SOURCE from my_global.h, already set by CMake
    - Removes setting of _THREAD_SAFE from my_global.h, not needed
    - Removes declaration of madvise from my_global.h, not used
    - Removes duplicate declaration of closesocket from my_global.h
    - Removes STACK_DIRECTION check from my_global.h, already done by CMake
    - Removes various defintions from my_global, use limits.h instead
    
    - Removes many unused declarations from my_sys.h and my_static.h
    - Removes various definitions of __attribute__, use my_compiler.h instead
    
    - Removes the following files with only dead code: include/t_ctype.h,
      mysys/my_getncpus.c
    - Removes dead code from mysys/my_getsys[1;31mtime[m.c
    - Removes mysys/mf_sort.c and moves used code to bounded_queue unit test
    
    - Removes st_my_thread_var::cmp_length, not used
    - Removes st_my_thread_var::pthread_self, use pthread_self() instead

[33mcommit b9478eec801de5fab45cf38d61ce249dd1a00161[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Jun 27 15:57:35 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - Disable run in valgrind as the test-case relies on output that is [1;31mtime[md.

[33mcommit 1671a8232a7b72df0a492376c291bfabaee3fc14[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Jun 26 10:29:44 2014 +0200

    Decrease some AutoTest max-[1;31mtime[m(out) values back to theirs
    original settings from 1-2 months back.
    
    A lots of [1;31mtime[mout were increased back then as there were
    several testfailures due to a too low [1;31mtime[mout value.
    
    However, some of these tests also [1;31mtime[mouts due to restart
    failures which still has to be analyzed / fixed. So these test
    will [1;31mtime[mout independent of how high we set the [1;31mtime[mout.
    
    As the original [1;31mtime[mout then seems to be sufficient, we
    decrease it back in order to shorten the AutoTest run length.

[33mcommit 889a21db2f00e1d13feb65c1b813964d3fb88fa4[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jun 25 15:01:32 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Replace ut_malloc(), ut_zalloc() and ut_free() with macros that
    call the corresponding ut_allocator methods.
    
    Fix a malloc()/ut_free() inconsistency introduced in
    marko.makela@oracle.com-20140402200417-qe02djjtf4ro9s15 that uses
    strdup(3) (which uses malloc(3)) to allocate a string in the
    constructor of Datafile but uses ut_free() to destroy the string
    in the destructor.
    
    Replace all ::ut_free() occurences with ut_free() because when PFS is
    disabled at compile [1;31mtime[m, then our ut_free() macro will expand to ::free().
    
    Replace all usages of strdup(3) with mem_strdup() and use ut_free() to free
    the string.

[33mcommit 12421fdcfcae6534d1df3f531588d286d67363b0[m
Merge: 45875a3c913 518f1d0cb7d
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Tue Jun 24 14:25:42 2014 +0200

    Bug #19048563 : PERFORMANCE REGRESSION IN UPDATE OPS CAUSED BY TRUNK REVNO 8242 (BUG#11755438)
    
    Some of the added memory barrier (internal of spin loops of mutex/rw_lock) was too expensive. Removed.
    This is partial reverting of the fix for Bug#11755438.
    
    ------------------------------------------------------------
    revno: 8242 [merge]
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Thu 2014-06-19 07:36:42 +0200
    message:
      Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL
    
      Because of difference about memory ordering, some critical flags of mutex/rw_lock might be missed to read on non-Intel CPUs.
      Even for Intel-CPUs, the explicit memory barrier instruction might cause positive effects for performance.
    
      Approved by Kevin Lewis in rb#5466
    ------------------------------------------------------------

[33mcommit 8f8fd5a92eece70004987d2d6977c6ae4a4c94bc[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Jun 23 16:37:00 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - On truncate we set the file size to restore the default file size
        by filling all zero. If file is open in ASYNC MODE then it would not
        be compatible with os_file_set_size and so we need to re-open the same
        file in compatible mode for os_file_set_size. This instance is closed
        immediately. In mean[1;31mtime[m all new ops on the file are anyway barred
        as file is being truncated.

[33mcommit b3fcdf6bceb71e17669a862a24d5f3bc7894226f[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 23 10:01:22 2014 +0200

    Increased max-[1;31mtime[m for the AutoTest 'testDict -n SchemaTrans -l 1'
    as it failed due to [1;31mtime[mout on some platforms / servers (ndb07)

[33mcommit 2bd0fd129b39859002131f069a1cd0a3e7e0ee6f[m
Merge: 3249e5735c0 9737298df9f
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Jun 19 17:30:09 2014 +0200

    WL#7436: Deprecate and remove [1;31mtime[md_mutexes system variable
    
    This is the 5.7 version of the patch.
    
    Remove the [1;31mtime[md_mutexes system variable.

[33mcommit b2c1c14845160153dc1e21af035b5bc8bde3bfdc[m
Author: Narendra Chauhan <narendra.chauhan@oracle.com>
Date:   Thu Jun 19 12:38:37 2014 +0530

    Bug#18128323 - RPL.RPL_OPTIMIZE FAILED WITH TIMEOUT 900 IN DAILY-TRUNK.
    
    With InnoDB engine on SLOW Solaris/WINT machines and debug builds
    test is failing due to [1;31mtime[mout. So, we will be using MyISAM engine
    as default until that issue is sorted out.

[33mcommit f94b3791056445d54d9c3a629bf9242b837a27ac[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Jun 19 09:30:48 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - Fixed an issue where-in rseg that is being truncated is added to purge
        queue. Before starting truncate we should remove all such rseg instances
        from purge queue.
      - Ignore processing of these rseg is other option but if same rseg get
        re-added immediately after truncate then invalid rseg instance will
        be projected with wrong information.
    
        let's understand this with [1;31mtime[mline graph
        t1 -> rseg-1 ... rseg-2
        t2 -> rseg-2 undo-tablespace gets truncated.
        t3 -> rseg-2 is now being processed but last_page_no = FIL_NULL.
        Issue. Solved by removing rseg-2 from the list before truncate.

[33mcommit bd6b3a160473eeea4dd9ad67bc8acb466d211998[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Jun 18 12:52:13 2014 +0200

    Bug#11751331: CONCURRENT DML AND LOCK TABLE ... READ FOR INNODB
                  TABLE CAUSE WARNINGS IN ERRLOG
    
    The original problem was that concurrent execution of
    LOCK TABLES ... READ statement and DML statements affecting
    the same InnoDB table on debug builds of MySQL server might lead
    to "Found lock of type 6 that is write and read locked" warnings
    appearing in error log.
    
    This could happen since one connection might  acquire TL_READ_NO_INSERT
    on a table while another connection held TL_WRITE_ALLOW_WRITE lock on
    the same table. At the same [1;31mtime[m, the locking code assumes that that
    such locks are incompatible.
    
    After WL#6671 this issue is no longer repeatable since InnoDB no
    longer uses thr_lock.c locks. It is also not repeatable using other
    in-house engines since they only use TL_READ_NO_INSERT for
    LOCK TABLES READ and this statement is now handled by the MDL
    subsystem.
    
    This patch removes the MTR suppressions that were added to
    suppress the warnings generated and adjust a code comment.

[33mcommit f8b61d9286d1528e170ee2dec1323d621e1e57fb[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Jun 16 15:09:21 2014 +0200

    Bug#18920203 GET RID OF DYNAMIC_ARRAY IN SERVER_IDS::DYNAMIC_I
    
    Our array abstractions should be removed, and substituted with modern data
    structures.
    
    Templatized vectors are:
     - type safe
     - easier to read/maintain
     - faster
    
    Also: remove some dead code.
    The class Database_ids is unused, which means we can remove it,
    and the base class Dynamic_ids.
    The member function Server_ids::do_search_id is also unused,
    and can be removed.
    
    The macro sort_dynamic() is no longer in use, and can be removed.
    
    There was a bug in change_receive_options():
    If we are inserting multiple ids into mi->ignore_server_ids
    then we need to re-sort before doing a binary_search.
    The old code worked "by accident", bsearch() found an element,
    even though the array was no longer sorted after an insert_dynamic()
    
    The fix for the sort/bsearch() bug: implement insertion-sort in Prealloced_array,
    and use insert_unique() to keep the array sorted at all [1;31mtime[ms.
    This is a short array of ints, with pre-allocation, so insert-sort should be fairly efficient.

[33mcommit 6073c2319c6fc655c010a31e422aa54498f4ca70[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Thu Jun 12 18:28:31 2014 +0530

    Bug #18806829 OPENING INNODB TABLES WITH MANY FOREIGN KEY REFERENCES IS
    SLOW/CRASHES SEMAPHORE
    
    Problem:
    
    There are 2 lakh tables - fk_000001, fk_000002 ... fk_200000.  All of them
    are related to the same parent_table through a foreign key constraint.
    When the parent_table is loaded into the dictionary cache, all the child table
    will also be loaded.  This is taking lot of [1;31mtime[m.  Since this operation happens
    when the dictionary latch is taken, the scenario leads to "long semaphore wait"
    situation and the server gets killed.
    
    Analysis:
    
    A simple performance analysis showed that the slowness is because of the
    dict_foreign_find() function.  It does a linear search on two linked list
    table->foreign_list and table->referenced_list, looking for a particular
    foreign key object based on foreign->id as the key.  This is called two
    [1;31mtime[ms for each foreign key object.
    
    Solution:
    
    Change the linked lists table->foreign_list and table_referenced_list to
    std::set structures table->foreign_set and table->referenced_set.
    
    rb#5673 approved by Vasil.

[33mcommit 95dae6426a49276dc8fb938ec9781dab3213d1f6[m
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Fri Jun 6 16:57:48 2014 +0530

    BUG#18791604 SERVER HITS ER 1200 ON START SLAVE CMD, STILL HEARTBEAT_PERIOD SETS TO DEFAULT,
    BUG#18777899 HEARTBEAT_PERIOD AUTOMATICALLY SETS TO DEFAULT POST RESET SLAVE COMMAND,
    BUG#18778485 SSL_VERIFY_SERVER_CERT AUTOMATICALLY SETS TO NO POST RESET SLAVE COMMAND)
    
    Problem:
    =======
    
    There are three problems we address in this patch.
    The cause of these is more or less the same problem.
    But this exhibits different symptoms as described in
    the three bugs listed above, namely:
    1) reset slave automatically sets heartbeat_period to
       default.
    2) reset slave automatically sets ssl_verify_server_cert
       to default.
    3) When a server is not configured as a slave
       (no CHANGE MASTER done yet), START SLAVE will fail
       and that is fine. But this resets the heartbeat_period
       to default which is strange and un-intuitive.
    
    Analysis:
    ========
    
    The function init_master_log_pos() reset heartbeat_period
    to default and ssl_verify_server_cert= 0. This was called
    from reset slave as:
     reset_slave() => clear_in_memory_info() => init_master_log_info
    
    Fix:
    ===
    
    1) relocated code to reset heartbeat period to default to
       change_receive_options() and
       removed ssl_verify_server_cert= 0 in init_master_log_pos().
       init_master_log_pos() is a different thing altogether,
       it shouldn't care about heartbeat or ssl certificate at
       all.
    2) As a side-effect of (1), on a CHANGE MASTER if host and
       port are given but heartbeat_period is not specified,
       it remains 0 which means heartbeats are disabled.
       While adding/switching master, we want to keep heartbeat
       enabled by default. So in such a situation we force
       heartbeat_period to default heartbeat_count= 0 and
       last_heartbeat_[1;31mtime[mstamp= 0 unless the user specifically
       chooses to disable heartbeats.

[33mcommit e5fec519e0a25b856109a2856463a76c2a287952[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri Jun 6 15:16:31 2014 +0530

    BUG#16664462 - MYSQL_TZINFO_TO_SQL CREATES BAD DATA
    Problem Description And Fix:
    mysql_tzinfo_to_sql utility creates bad data. In tz_load
    function, the string length for abbreviation list (read
    from [1;31mtime[mzone information file) doesn't account for the
    the null character that is appended to the end of the
    char array. This possibly can result in a buffer overflow
    as indicated by the valgrind warning (shown on bug
    http://bugs.mysql.com/bug.php?id=20545) based on type of
    data.

[33mcommit 61858e678802a722d3df16fdfd771d722a9cc612[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Jun 5 13:21:58 2014 -0500

    Bug #18684389-SPAM ERROR 'FILE NOT FOUND' MSG WHEN 'DATA DIRECTORY'
    CLAUSE IN 'CREATE TABLE'
    
    The patch for WL7142 caused this regression in which an error message
    is written to the log at startup for every remote tablespace that was
    created with DATA DIRECTORY.  A 'strict' parameter was added to
    Datafile::open_read_only(strict) in order to determine if the message
    would be logged.  The function fil_open_single_table_tablespace() tries
    to find the file in three possible locations and it always looks at the
    default location.  If the file is not there, this strict parameter causes
    the spam message.
    
    But by the [1;31mtime[m the default-location filepath is attempted,
    fil_open_single_table_tablespace() has already tried the other two
    locations.  So the fix is to print the message only if the file is not
    found elsewhere.
    
    Approved by marko in RB#5536

[33mcommit 72379b88ba1d48735a987ac2461e5effa3ba6d08[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 12:15:12 2014 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
    - 5.7 outputs additional columns from EXPLAIN since EXTENDED and PARTITIONS have
       been turned on by default.
       1) This causes the 'rows' column to shift from position 9 to 10
       2) Also it introduces the new column 'filtered' which is to be
          considered as unreliable as 'rows'. Quoting comment in existing
          test case "experience has shown that 'rows' estimates may vary
          depending on OS, compiler and .. phase of moon."
     - Fix problem by increasing the column number in masking of 'rows' and at the
       same [1;31mtime[m add masking of the 'filtered' column.
     - NOTE! Should have done this the first [1;31mtime[m when EXPLAIN output was recorded
       for 5.7 but the problem went unnoticed at that [1;31mtime[m.

[33mcommit e53e81cd315f83bc97c79059651b5c504eaac0ab[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 12:11:49 2014 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
    - Usage of -Wextra produces another 3 "ordered comparison of pointer with integer zero [-Wextra]"
       warnings. This [1;31mtime[m comparing the abort() function with > 0. Presumably abort
       has been a local variable in the function which rename/remove went undetected.
     - Intention seem to be to only check the "should trans be aborted" condition when
       the abortPercent variable is set.
     - Fix by replacing the usage of abort to abortPercent

[33mcommit 887aebeb4ac41c78bbc590cb9ee110a6e2452440[m
Author: kevin.lewis@oracle.com <>
Date:   Wed Jun 4 15:14:17 2014 -0500

    Bug#17713871-ASSERT ERROR == DB_SUCCESS, COMMIT_CACHE_REBUILD(),
    ALTER TABLE, PRBLEM RENAMING
    
    In order to add even more uniqueness to the temporary filename, this patch replaces the LSN in the filename with a more unique another number.  It is just a static global number that is initialized to a random distributed 32-bit number using ut_[1;31mtime[m() and ut_crc32().  It is then incremented atomically for each temporary file name assigned.  This should not cause performance regression since an atomic_increment is nothing compared to a OS file rename.
    
    Approved by Marco in RB#5541

[33mcommit 1007d851b8e2a0e1ff05e0f63f1927623ecae54a[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Wed Jun 4 08:16:09 2014 +0200

    1. Added big-test flag to tests that take more than a minute on the fastest platform OEL6 on PB2. This will help reduce [1;31mtime[m taken for per push runs
    2. Disabled rpl.rpl_stm_mixed_mts_rec_crash_safe_small. This is supposed to be shorter version of .rpl_stm_mixed_mts_rec_crash_safe suitable for PerPush runs. This is taking about 2 minutes and hence needs to be tuned down
    3. Moved rpl.rpl_optimize from experimental to disabled list. It is timing out on windows leading to very long running [1;31mtime[m for windows in daily.

[33mcommit 2a1c1a2e9326954ef8d56aa5eb6ca3aaa90704fc[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Mon Jun 2 16:13:36 2014 +0100

    WL#7742: BGC: waiting more transactions entering bgc queues
    
    Adds two new options to introduce an artificial delay to make the
    binary log group commit procedure wait. This gives a chance that
    more transactions are flushed and synced together to disk, thus
    reducing the overall [1;31mtime[m spent to commit a group of transactions
    (the bigger the groups the less number of sync operations).
    
    These options are named: binlog-group-commit-sync-delay and
    binlog-group-commit-sync-no-delay-count. The former takes as
    input the number of microseconds to wait. The latter, takes as
    input the number of  transactions that the server waits for,
    before deciding to abort the waiting.

[33mcommit dfaa566c6118e089c0e1d0dc244790291462dd23[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Jun 2 13:22:55 2014 +0200

    Cherrypick from mysql-5.6
    
      revno: 5960
      revision-id: tor.didriksen@oracle.com-20140523105024-zidi66prp5p27kbg
      parent: venkatesh.duggirala@oracle.com-20140523051255-xfzbokhi1hdwiqvl
      committer: Tor Didriksen <tor.didriksen@oracle.com>
      branch nick: 5.6-cmake
      [1;31mtime[mstamp: Fri 2014-05-23 12:50:24 +0200
      message:
        Bug#18605389 PROVIDE OPTION TO LINK AGAINST LIBCSTD INSTEAD OF STLPORT4 ON SOLARIS 10+
        Bug#72352    Provide option to link against libCstd instead of stlport4 on Solaris 10+
    
        Patch for 5.6
        This patch enables the use of Cstd when building with SunStudio.
        It only works for client code, as the server depends on C++98
        Usage:
        cmake -DWITHOUT_SERVER=1 -DSUNPRO_CXX_LIBRARY=Cstd
    
        Also backport:
        Bug#14367046: PROBLEM BUILDING CLIENT ONLY

[33mcommit 098a2a4eba583205d84afbe42d33c20deb1dae37[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jun 2 10:08:35 2014 +0200

    'testSystemRestart -n SR_FULLDB T6' is really slow on
    some test rigs (ndb07), and need even long [1;31mtime[mout
    to be able to complete.
    
    Increase from 2.5h to 4h

[33mcommit 33a60b44afcefa172fff7da3f5ac89c4085cd116[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Fri May 30 14:46:28 2014 +0800

    WL7220 Spatial Relation Check Functions
    WL7221 Geometry Set Operations
    WL7280 WKB geometry container
    Add more files, this [1;31mtime[m all files added, sorry for the mess.

[33mcommit 3a0cbeab64ed988b142e474373eb38cc562d874b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed May 28 23:34:32 2014 +0200

    ndb_suma_handover.test
    
    For a restarting node in start phase 101 SUMA waits for all
    subscribing nodes to connect.
    
    This test discards one CONNECT_REP from an API node during restart
    of one node.
    
    Without preceding fix the test will fail ([1;31mtime[mout).

[33mcommit 43901d722a3dfe7b6d44d77ef4dd7a91f5030063[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed May 28 23:33:08 2014 +0200

    Bug #18599198 NDB : LIMIT TIME SPENT BY SUMA WAITING FOR API CONNECTIONS IN PHASE 101
    
    This patch adds a limit for how long Suma will wait for all
    subscribers to connect before continuing with suma handover during
    start phase 101.
    
    The root couse to why some subscribers does not connect to
    restarted data nodes is not found nor fixed.
    
    A new configuration parameter RestartSubscriberConnectTimeout for
    data nodes are added.  This [1;31mtime[mout specifies how long Suma will
    wait in phase 101 for all subscribing API nodes to connect.
    
    Those API nodes that have not connected in [1;31mtime[m will be reported
    failed.
    
    And eventually when all those API nodes have failed, SUMA will
    continue handover.

[33mcommit a54e64c2347a48208066fb3fdd11531bdb4b91b7[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed May 28 11:05:26 2014 +0200

    There are still some AutoTest failures due to [1;31mtime[mout values
    set too low (Increased several TO-values ~week ago)

[33mcommit 213ca14a499d0f9d2b9a4d8143071151a21d1c0b[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue May 20 20:53:00 2014 +0200

    Don't run rpl_parallel_seconds_behind_master in valgrind, it [1;31mtime[ms out.

[33mcommit 8bafea1478b6c210ea491ac1446b208fc5c685c7[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue May 20 15:31:45 2014 +0200

    Fix for Bug #18770217   DIFFERENT RESULT WITH SP ON 2ND EXECUTION OF QUERY WITH STRAIGHT_JOIN IN NOT IN
    We have this subquery predicate:
    NOT IN (
    SELECT sq1_alias1.col_varchar_nokey
    FROM t3 AS sq1_alias1
    JOIN ( t1 AS sq1_alias2
    STRAIGHT_JOIN view_d AS sq1_alias3 )
    WHERE (
    SELECT col_int_key
    FROM t1 AS c_sq1_alias1
    WHERE c_sq1_alias1.col_varchar_key = sq1_alias3.col_varchar_nokey
    )
    )
    which is handled with subquery materialization.
    It contains a WHERE clause involving a scalar subquery which returns
    two rows. Thus, it is expected that when we fill the temporary table,
    we hit and report an error.
    However, in the second execution of a stored procedure it is not the
    case. A necessary ingredient is the view column of view_d, used as
    outer reference in the scalar subquery.
    First execution:
    - view_d is merged
    - the view's column is resolved by fix_outer_field():
              prev_subselect_item->used_tables_cache|=
                (*reference)->used_tables();     // 1
              prev_subselect_item->const_item_cache&=
                (*reference)->const_item();
              mark_as_dependent(thd, last_checked_context->select_lex, // 2
    
    *reference is Item_direct_view_ref. Its depended_from is NULL at
    (1). So its used_tables() returns the map of t2.col_varchar_nokey
    (==4). This goes into used_tables_cache of the scalar subquery item,
    which is correct - from the POV of the materialized subquery which
    owns the scalar subquery, this scalar subquery depends on view_d, the
    table of map 4 in the materialized subquery.
    At (2), depended_from becomes "select#2" i.e. the materialized
    subquery. It is correct: the item_direct_view_ref has the scalar
    subquery as context, so it is an outer reference.
    
    Second execution:
    - in find_field_in_tables() we go through code specific of any not-first
    execution:
      if (item->cached_table) <<<< this is true on non-first execution
      {
      ...
            /*
              If the field was an outer referencee, mark all selects using this
              sub query as dependent on the outer query
            */
            if (current_sel != last_select)
              mark_select_range_as_dependent(thd, last_select, current_sel,
                                             found, *ref, item);
    This sets depended_from of the item_direct_view_ref to "select#2"
    (correct).
    - then we go into the same fix_outer_field() as in the first
    execution, but note that this [1;31mtime[m we enter it with
    depended_from!=NULL, so:
              prev_subselect_item->used_tables_cache|=
                (*reference)->used_tables();     // 1
    adds OUTER_REF_TABLE_BIT to used_tables_cache of the scalar
    subquery. That is wrong.
    - then, this scalar subquery gets lost by make_join_select() because
    this function (as it is operating for a materialized subquery)
    excludes conditions using OUTER_REF_TABLE_BIT (see
    join->allow_outer_refs).
    - the condition being lost, it won't throw the expected "more than one
    row" error.
    
    Fix: use resolved_used_tables(). Indeed, prev_subselect_item is owned
    by the qualifying query of the item. A second similar line is changed too.

[33mcommit 553623e50744a83ebe77d523a478aa0abd5da104[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue May 20 14:25:46 2014 +0200

    Increase [1;31mtime[mout for several Autotest which has 'FAILED(TO)'
    and where the expected run[1;31mtime[m is too close to max-[1;31mtime[m

[33mcommit e4f0cd8bce87dfc1f1e3f8c2bf1e54e741320722[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon May 19 10:52:40 2014 +0200

    Fix for autotest failure in: 'testNdbApi -n MaxOperations'
    
    This test has been failing for *every day* since 2008 with the following
    problems:
    
    1)
      Got error '1217 Out of operation records in local data manager ..'
      instead of the expected error:
      '233 Out of operation records in transaction coordinator'
    
      This was caused by only generating pkReadRecord-ops for
      row #1 in the test, thus all LQHKEYREQs ended up on the same
      datanode and thus exhausted the 'MaxNoOfLocalOperations'
      in that node.
    
      Fixed by pseudo randomly creating rowNo in the range 1..256
    
    2)
      As all pkReadRecords were executed in a giant execute batch,
      the Transporter got overloaded for some of the larger tables.
      (ERROR: 1218 Send Buffers overloaded in NDB kernel)
    
      Fixed by sending an 'execute_NoCommit()' after every 1000'th row.
      Note: The no-commit keeps the operation records in TC which the
      test try to exhaust.
    
      As previously, a execute_Commit() is still done after the last
      row.
    
    An optimization is also introduced in HugoOperations.cpp, where we
    use the getValue() variant taking a 'Column*' arg. instead of the
    'char* name' argument of the same column. Debugging with this testcase
    revealed that the test client spent significant [1;31mtime[m doing strcmp
    in getValue.

[33mcommit 16d6485a41de4f7199d4d9551f8823b2e3cebbaf[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Mon May 19 08:48:22 2014 +0200

    Move 42 tests to "no_valgrind_without_big" to reduce PB2 per-push running [1;31mtime[m.
    
    Approved by Anitha over IM.

[33mcommit 6cfde51100e3210ade8b760d14d8413cf47999a8[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun May 11 07:51:21 2014 +0800

    Bug#18363515 MULTIPLE THREADED SLAVE MAY BE CRASHED BY
                 STOP SLAVE SQL
    
    Worker error will crash MTS if coordinator is waiting for a
    free worker to dispatch next event. The crash happens because
    the caller of Mts_submode_logical_clock::get_least_occupied_worker()
    supposes it always return a valid worker. But that is not true.
    It may turn NULL when coordinator is aborted at the same [1;31mtime[m.
    
    This patch makes the caller of get_least_occupied_worker() to
    check if the function returns a valid value.

[33mcommit e261a31e7d2238bfd080966b63ed02311e83872a[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Wed May 7 10:35:42 2014 +0200

    post-push fix for test [1;31mtime[mout related failures from bug#18330694.

[33mcommit 570413be71702a7231820e1629b86387ed8be0ea[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed May 7 10:17:31 2014 +0200

    Reduce rqg_spj.test from running 60mins to 30mins.
    Also reduce the [1;31mtime[mout from 3h to 1h.
    
    (Will [1;31mtime[mout in the very rare case that an 'incomputable'
    join was created randomly, or in case of error)

[33mcommit b3e655fc0c6ab75e4037409e1783122eddcc736e[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Mon May 5 19:03:38 2014 +0200

    Bug#18330694: INNODB TESTCASES RUN TOO SLOW ON WINDOWS
    
    This fix contains 3 different parts
    
    1) Reason for mysqltest command shutdown server always waited the full
    [1;31mtime[mout:
    During shutdown of the server there are still handles open to the process
    (by mtr?),
    resulting in that it is possible to retrieve a new handle to the process,
    which leads to the current my_kill(pid, 0) windows implementation thinks
    the process is still alive, when it infact already exited.
    
    So it needs to either check with WaitForSingleObject or GetExitCodeProcess
    to verify if the process handle really has exited or not.
    
    2) Reason for spurious test failures when doing copy_file/remove_file/
    move_file:
    Even if the process signaled that it exited, Windows may not yet closed
    all the process files, resulting in failures (ERROR_SHARING_VIOLATION)
    during move/delete/copy. So to be more tolerant I added on retry in such
    cases.
    
    3) Killing the server if the shutdown takes too long [1;31mtime[m.
    Changing shutdown_server to:
    If [1;31mtime[mout is non-zero, send shutdown to the server and wait until the
    server terminates or [1;31mtime[mout is exceeded. If [1;31mtime[mout is exceeded return
    error.
    If [1;31mtime[mout is zero, then kill/terminate the server (kill -9 on Unix,
    TerminateProcess on Windows).
    This since killing the server if [1;31mtime[mout is exceeded can have side-effects
    that is hard to predict. Perhaps we should never kill in shutdown_server
    (not allowing zero as [1;31mtime[mout) and introduce kill_server
    (current shutdown_server 0 behavior)? I have not done this since it
    would need to update several test files.
    
    I also included a test change that I used during testing.

[33mcommit c79fc6ceb09de75b6dabc1bcc7d5dc59ba7bf8dd[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Mon May 5 16:28:59 2014 +0800

    Fixed bug#18644435 ASSERT 0 IN RTR_PAGE_SPLIT_AND_INSERT(), GIS
    
    This assertion should be removed, since we have re-split now.
    This patch also diabled valgrind test on some of gis test cases,
    since it will take a long [1;31mtime[m.
    
    Approved by Jimmy on IM.

[33mcommit 1ddd96337e99d7a8c6e0bb0c965d8fb58704bcaa[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Mon May 5 11:06:33 2014 +0700

    This is a patch for Bug#18068253 - XA START WITH THE EXIST XID, AND THEN
    XA COMMIT WITH THE XID WILL FAILED.
    
    When a xa transaction is prepared and the server crashed, then after restart the
    MySQL server,  first, do XA START with the same xid, says:" XAER_DUPID: The
    XID already exists", that's OK, but do XA COMMIT with this xid, says that:
    XAER_RMFAIL: The command cannot be executed when global transaction is
    in the  NON-EXISTING state. So, the prepared transaction can't be committed.
    
    The reason for the bug was that the method eq() of class xid didn't take into
    account the value of xid::format during comparison. First [1;31mtime[m when XA  START
    is executed the server found prepared transaction and return error but for all
    of that the value of thd->transaction.xid_state.xid is reset assigning the
    value -1 to the the attribute xid.format. Later when the statement XA COMMIT
    is executed comparison of  thd->transaction.xid_state.xid with xid value
    specified in the statement is made without taking into account
    the value of format.

[33mcommit 591b441304a576e1698d855fa5dfbf585e6b403f[m
Author: Prabeen Pradhan <prabeen.pradhan@oracle.com>
Date:   Fri May 2 10:41:32 2014 +0530

    Added "--source include/not_valgrind.inc" so that the tests skip on valgrind. These tests take too much [1;31mtime[m on valgrind.

[33mcommit a334f8f2d96a908b38fce68270f411cb668def5f[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Apr 28 17:05:21 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - If table has auto-increment column and there are multiple insert
        operating on it parallely then predicting the value of auto-increment
        column is bit difficult. This is solved by taking an AUTOINC lock
        on table that is valid for statement life[1;31mtime[m.
      - As intrinsic table are not shared there is no need to take such lock.

[33mcommit d477b01e706e06fabab3e8d59fcdfdee8279f753[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Apr 25 23:41:35 2014 -0700

    Delay after (rather than before) the first test iteration.
    This allows V8 to compile all the JavaScript code before dtrace starts running.
    High dtrace profiling rates caused the first iteration to run very slowly
    and skewed the profile towards compile [1;31mtime[ms rather than run [1;31mtime[ms.

[33mcommit 4cdf9feb106f669f210387c206a64c3b2356c818[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 25 15:50:48 2014 +0200

    As part of preparing the MTR test suite for running with 4 LDM threads,
    the size of the ts_1 tablespace in ndb_addnode.test was increased
    from 4M to 12M. This was tested, and wound sufficient on Linux and
    x86 Solaris.
    
    However, in Sparc Solaris, we observe that we get a 't2 table is full'
    even with this setting.
    
    This fix increase the tablespace size to 16M, which is 4 [1;31mtime[ms
    the original size used with a single LDM thread.

[33mcommit c061ed8468efdb3c557427891d19c9b5a6a19765[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Fri Apr 25 14:23:30 2014 +0200

    * Fixed: Don't expire password if password cred comes from .login.cnf
    * Fixed: Allow for --admin-user and --admin-host options to override --login-path credentials.
    * Fixed: System some[1;31mtime[ms crashed if login-path wasn't a qualified path.

[33mcommit 2cfe8b28322329d5f32c54398a39c11a67fa910a[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Apr 16 14:50:23 2014 +0300

    WL#7806 InnoDB: Log-based discovery of built-in tablespaces
    
    This is follow-up to
    WL#7142 InnoDB: Simplify tablespace discovery during crash recovery
    
    We will write MLOG_FILE_NAME for all persistent tablespaces, not only
    for *.ibd files. Currently, this includes the following:
    
    * The InnoDB system tablespace (ibdata*)
    * The InnoDB undo log tablespaces (undo*)
    
    On startup, the InnoDB system tablespace and the InnoDB redo log will be opened.
    If there are redo log records to be applied since the latest checkpoint,
    any tablespaces requiring cleanup will be opened and recovered based on
    MLOG_FILE_NAME records in the redo log.
    
    If the MLOG_FILE_NAME records for the system tablespace disagree with
    the server configuration affecting the data file names for the system
    tablespace, recovery will be aborted with an error message, before
    applying any redo log.
    
    After recovery, any undo log tablespaces for which no redo log records were
    applied will be opened based on existing mechanism. The system tablespace will
    remain open at all [1;31mtime[ms.
    
    is_predefined_tablespace(): Remove. All redo-logged tablespaces will
    be treated in the same way.
    
    mtr_t::m_undo_space, mtr_t::set_undo_space(): New field and method, to
    associate an undo tablespace associate with the mini-transaction.
    
    mtr_t::m_modifies_sys_space, mtr_t::set_sys_modified(): New field and
    method, to note that the mini-transaction is modifying the system
    tablespace.
    
    mtr_t::set_spaces(): A kind of copy constructor that copies the
    information on modified tablespaces from another mini-transaction.
    
    mtr_t::is_undo_space(): A debug method to ensure that set_undo_space()
    has been called.
    
    mtr_t::Command::prepare_write(): Invoke fil_spaces_lookup() before
    log_mutex_enter(), to look up all tablespaces that were flagged as
    modified by the mini-transaction, and optimistically invoke
    fil_names_write() for the system tablespace. (This is equivalent to
    old behaviour introduced in WL#7142, with the exception that we may
    look up an undo tablespace and the system tablespace.) After
    log_mutex_enter(), truncate the log if the fil_names_write() was not
    needed. If an undo tablespace or the system tablespace were flagged
    and fil_names_dirty() holds for them, invoke fil_names_write() for
    them.
    
    dyn_buf_t::set_size(): New method, used for truncating unneeded
    MLOG_FILE_NAME records from the tail of mtr_t::log.
    
    mtr_write_log_t::m_len: Remove. We will write the entire log.
    The log can be truncated by dyn_buf_t::set_size().
    
    srv_undo_tablespaces, srv_undo_tablespaces_open: Set the initial value
    to 0 on server startup, so that fil_space_belongs_in_lru() will behave
    in a predictable way during redo log apply.
    
    trx_rseg_t: Note that space,page_no are constant and need not be
    protected by mutex.
    
    fil_load_single_file_tablespace(): Renamed from
    fil_load_single_table_tablespace().
    
    fil_space_system_check(): New function, to check that MLOG_FILE_NAME
    records match the system tablespace data files.
    
    fil_space_undo_check(): New function, to reopen possibly existing undo
    log files after redo log apply has completed.
    
    fil_spaces_lookup(): New function, to replace previous usage of
    fil_names_write(). At mini-transaction commit, this looks up all
    modified redo logged tablespaces.
    
    fil_names_write(): Renamed from fil_names_write_low(). We will get the
    fil_space_t* looked up by fil_spaces_lookup().
    
    fil_name_parse(): Support undo tablespaces and multi-file system
    tablespace.

[33mcommit 7d5be17136361d222e3a69d3a868c7c29f8df2a3[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Apr 15 11:59:15 2014 +0200

    Stop running rpl.rpl_optimize in valgrind as it creates havoc in the weekly testing.
    
    The test is in any case experimental for the [1;31mtime[m being.
    
    Cf. Bug#18128323
    
    Approved by Anitha over IM.

[33mcommit 7d10c12ea8e797cf75772e8967bd6e3520e6b694[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue May 13 14:17:57 2014 +0200

    bug#18558561: Replace Cost_estimate with a double when estimating join cost
    
    The member variable prefix_cost of class POSITION is made a double.
    Since partial plans and complete plans are compared with a cost represented
    as a double, it makes sense to store this as a double too.
    
    The member variable prefix_record_count of class POSITION is renamed to
    prefix_rowcount for more consistent naming.
    
    The function set_prefix_costs() is renamed to set_prefix_cost().
    
    A new function set_prefix_join_cost() that calculates the cost of a prefix
    plan has been added.
    
    The above change of type has made it possible to eliminate the arguments
    record_count and read_[1;31mtime[m that were currently shipped to some functions
    of class Optimize_table_order: consider_plan(),
    best_extension_by_limited_search(), eq_ref_extension_by_limited_search()
    and advance_sj_state(). These functions use member variables from
    POSITION instead.
    
    A new const property has_sj has been added to class Optimize_table_order,
    so it does not have to be calculated more than once.
    
    In JOIN::setup_materialized_table(), the cost of a semi-join
    materialization is correctly calculated, to keep a correct cost for a
    final plan.

[33mcommit 735caceeef4d120c7ef9edf8e1ff36aded9e182c[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Apr 10 16:06:12 2014 +0300

    WL#6068 Remove InnoDB rec_per_keys hack
    
    Add a comment explaining that index->stat_n_diff_key_vals[] and
    index->table->stat_n_rows could have been calculated at a different [1;31mtime[m.
    
    Remove a redundant comment - the same info is in a comment further below
    that starts with "/* The code below is legacy and should be"

[33mcommit da25ed8ab96a0a17d8a120c58d0a629cf3bef2bd[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Apr 9 13:32:08 2014 +0300

    WL#6068 Remove InnoDB rec_per_keys hack
    
    Use index->table->stat_n_rows for "number of rows in the table" insead of
    stats.records because the former is more up to date and thus possibly more
    in-sync with the stats that are read up from the dict_index_t object.
    
    index->table->stat_n_rows is calculated at the same [1;31mtime[m as
    index->stat_n_diff_key_vals[]. index->table->stat_n_rows is also adjusted
    on each DML.

[33mcommit 917fc1b47c14e560e598bfed787e699b3096d4bf[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Wed Apr 9 11:40:26 2014 +0530

    Bug #18329348 RESETCONNECTION DOESN'T CLEAR TIMESTAMP
    Bug #18329560 RESETCONNECTION DOESN'T CLEAR RAND SEED
    Bug #18328396 RESETCONNECTION DOESN'T CLEAR WARNINGS
    Bug #18329452 RESETCONNECTION DOESN'T CLEAR PROFILING
    
    
    Problem: resetconnection does not reset some of the system variables.
             Most of the system variables are declared in THD::variables
             struct. The variables which are declared outside this struct
             are not handled in resetconnection ex: last_insert_id, insert_id,
             [1;31mtime[mstamp, rand_seed1, rand_seed2 etc.
    
    Fix: Fix is to reset these session variables to its default values in
         COM_RESET_CONNECTION. Even though @@profiling is reset, the profiling
         information is not cleaned up before, with this fix this information
         will be cleaned up.

[33mcommit 1ac20f1b9156ed3aa38e7a144fcea921076473d9[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue Apr 8 13:37:59 2014 +0530

    Problem:
    
    In the clustered index, when an update operation is done the overall
    scenario is as follows:
    
    1.  Delete mark the old record that is to be updated.
    2.  The old record disowns the blobs. As part of this step, we assert that
        the blobs are owned by the old record.
    3.  Checks the foreign key constraints.
    4.  Insert the new record into clustered index.
    
    Scenario involving DB_LOCK_WAIT:
    
    If step 3 [1;31mtime[ms out, then we will repeat steps 1 and 2.  Since the steps 1
    and 2 are already done, the assert that the blobs must be owned in step 2
    will fail.
    
    Solution:
    
    If step 3 [1;31mtime[ms out, then skip steps 1 and 2 and proceed with 3.  This is
    achieved by checking if the record is already delete marked.
    
    rb#5081 approved by Marko.

[33mcommit 32e5550ba55415f658353477756d7fa538d8da08[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Apr 4 14:00:52 2014 +0200

    Follow-up patch for Bug#13106350: MRR initialization on a derived
                                      table caused crash
    
    Updated test case by removing out-commented setting of
    optimizer_switch materialization to off.
    This was commented out due to at the [1;31mtime[m this test was
    back-ported to mysql-trunk, the materalization switch was
    not part of optimizer_switch on mysql-trunk.

[33mcommit b9f9bf139cbbe0ee442ab3f644769a4ddeda25ca[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Apr 3 19:36:12 2014 +0300

    Followup to vasil.dimov@oracle.com-20140403070651-w1nefsafrqeid6ct:
    Increase the margin of allowed deviance for n_rows - with 4k page size
    it could some[1;31mtime[ms be 745 (27% away from the actual value 1024).

[33mcommit 8b7a460cbe5f104ea130c0be4c03e0773fbe21ff[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Apr 1 19:20:03 2014 +0300

    Fix Bug#71708 70768 fix perf regression: high rate of RW lock creation
    and destruction
    
    Lazily create dict_table_t::stats_latch the first [1;31mtime[m it is used.
    It may not be used at all in the life[1;31mtime[m of some dict_table_t objects.
    
    Approved by:    Bin (rb:4739)

[33mcommit 53c6f431412b730815665fd2f226e4fdf62cd65b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Apr 1 16:37:12 2014 +0200

    ndb - cherrypicked fix
    
      revision-id: mauritz.sundell@oracle.com-20140401081334-nrve3gyqwgf2b6vx
      parent: frazer.clement@oracle.com-20140331161153-5b4yccimejdo13v9
      parent: mauritz.sundell@oracle.com-20140401081054-quhto0gfak217e1n
      committer: Mauritz Sundell <mauritz.sundell@oracle.com>
      branch nick: mysql-7.2
      [1;31mtime[mstamp: Tue 2014-04-01 10:13:34 +0200
      message:
        merge 7.1 -> 7.2
          ------------------------------------------------------------
          revno: 2555.840.20
          revision-id: mauritz.sundell@oracle.com-20140401081054-quhto0gfak217e1n
          parent: frazer.clement@oracle.com-20140331155509-nscusf07z7kjwp47
          committer: Mauritz Sundell <mauritz.sundell@oracle.com>
          branch nick: mysql-7.1
          [1;31mtime[mstamp: Tue 2014-04-01 10:10:54 +0200
          message:
            ndb - regression: corrupt message detected with tcp checksum activated
            Bug #18426180    ENHANCED DETECTION AND DUMP OF CORRUPT MESSAGES AND SIGNALS RECEIVED

[33mcommit 4248141e2a88b1929638f77550c73b26e2f27965[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Apr 1 15:44:41 2014 +0200

    Reduce PB2 [1;31mtime[m, require --big-test to run some tests in Valgrind:
    
    innodb.innodb-import-partition
    innodb.innodb_update_[1;31mtime[m_wl6658
    main.merge_recover
    rpl.rpl_alter_repository
    rpl.rpl_extra_col_slave_innodb
    rpl.rpl_gtid_mode
    rpl.rpl_row_img_sanity
    
    Approved by Jon Olav over IM.

[33mcommit d12e2fdfab06c7badf1780b06c7abbcf607b1a6f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Apr 1 14:06:41 2014 +0200

    Bug#18486249 ASSERTION FAILED: BUF == BUFFER IN MY_DECIMAL::SANITY_CHECK
    
    This bug was uncovered by the stricter sanity_check introduced in patch for:
    Bug#18335446 DECIMAL_ROUND: ASSERTION FAILED: FROM->LEN == TO->LEN
    
    The problem is that my_qsort2 sorts blobs, rather than objects.
    When sorting an array of decimals, it will some[1;31mtime[ms leave
    objects with wrong buffer pointers.
    
    Solution: use std::sort, which is type-aware, and will copy/swap
    decimal objects properly.

[33mcommit 109e503ea6185bfbc52769cc07c5cb616c88adc6[m
Author: Pedro Gomes <pedro.gomes@oracle.com>
Date:   Fri Mar 28 17:51:27 2014 +0000

    Bug#18475390: STOP SLAVE TIMEOUT IS SET STATICALLY AT EACH SLAVE THREAD TERMINATION
    
    Whenever the terminate method is called for the slave SQL/IO threads,
    inside this, a [1;31mtime[mout variable associated to this task is always set
    based on a server variable.
    The problem with such routine is that we cannot reuse this code to be
    used with other defined [1;31mtime[mouts.
    
    As the [1;31mtime[mout can be passed as a parameter from method to method,
    instead of being statically set and used as of now, the code is
    changed to implement this pattern allowing the method reuse in
    different contexts.

[33mcommit 6396f023f2fb856a12eca6cf057a27b86ff6e154[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Wed Mar 26 15:22:41 2014 +0530

    WL#7582 - LEX_USER String transitional refactoring.
    The patch does transitional refactoring which is required to
    migrate to the new strings framework. A number of places in
    the server uses char* instead of const char* which includes
    the LEX_STRING which has a char* member. A gradual
    transformation of the server code is required to use const
    char* and use const references to pass objects around functions.
    This will enable smooth transition of ther server code to use
    the new string framework classes.
    This commit transforms the LEX_USER members user,host,
    password plugin and auth to use LEX_CSTRING. Thus the
    following classes/data members are modified to acommodate
    this change:
    1. LEX_USER (sql/structs.h)
             LEX_CSTRING user
             LEX_CSTRING host
             LEX_CSTRING password
             LEX_CSTRING plugin
             LEX_CSTRING auth
    2. ACL_USER (sql/auth/sql_auth_cache.h)
           LEX_CSTRING plugin
    3. MPVIO_EXT (sql/auth/sql_authentication.h)
           LEX_CSTRING acl_user_plugin;
    4. change of following variables in
         sql/auth/sql_authentication.h to be of LEX_CSTRING
         (1) native_password_plugin_name
         (2) old_password_plugin_name
         (3) sha256_password_plugin_name
         (4) validate_password_plugin_name
         (5) default_auth_plugin_name
    5. Event_[1;31mtime[m (sql/event_data_objects.h)
          LEX_CSTRING m_definer_user
          LEX_CSTRING m_definer_host
    6. Event_job_data (sql/event_data_objects.h)
          LEX_CSTRING m_definer_user
          LEX_CSTRING m_definer_host
    7. THD (sql/sql_class.h)
          LEX_CSTRING m_invoker_user
          LEX_CSTRING m_invoker_host
    
    The function names whose prototypes or other changes that are modified as part
    of the refactoring include:
    1. optimize_plugin_compare_by_pointer
    2. acl_insert_user
    3. ACL_PROXY_USER::store_pk
    4. ACL_PROXY_USER::store_data_record
    5. acl_update_user
    6. do_auth_once
    7. check_change_password
    8. plugin_is_ready
    9. create_string (sp.cc)
    10. sp_head::set_definer
    11. make_lex_string_root
    12. make_lex_string
    13. change_security_context
    14. check_string_byte_length
    15. check_host_name
    16. check_string_byte_length
    17. check_string_char_length
    18. plugin_find_internal
    19. plugin_status
    20. plugin_lock_by_name
    21. plugin_find_by_type
    22. append_definer
    23. reconstruct_definer_clause
    The refactoring has been done to some cohesive set of functions that relates to
    the authentication subsystem with some changes in plugin & stored procedure
    layer. Refactoring has been stopped and char* string is passed around as changes
    need to be done at storage engine handler layer or certain
    low-level platform API(memroot,string related functionality) that will affect
    every part of the code.

[33mcommit 2ec6e06eb5b9b4c539e593423917e32b32d4deb6[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Mar 24 15:08:23 2014 -0700

    This fixes the failing "[1;31mtime[mstamp 1969" test case for NDB.
    Test still fails with MySQL.

[33mcommit e79cadbb429ba2164cb1c6f3fe33af7c501887c5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Mar 19 15:07:55 2014 +0200

    Test push of WL#7142 InnoDB: Simplify tablespace discovery during crash recovery
    
    When the setting innodb_file_per_table=ON was introduced in MySQL 4.1,
    InnoDB crash recovery was changed so that the directories will be
    searched for *.ibd files if any redo needs to be applied.
    
    The scanning and opening of all *.ibd files (including ones for which
    no redo log needs to be applied) can be very slow, especially on
    deployments that contain a large number of *.ibd files. Furthermore,
    if we allow a more liberal placement of tablespace files in the file
    system, we might have to extend the search to an even broader range of
    directories.
    
    This worklog eliminates the *.ibd file scan by guaranteeing the
    following:
    
    If there are redo log records for any non-predefined tablespace, there
    will also be an MLOG_FILE_NAME record.
    
    The InnoDB redo log format will be changed as follows:
    
    MLOG_FILE_NAME(space_id, filename): A new redo log record.
    Replaces MLOG_FILE_CREATE, MLOG_FILE_CREATE2.
    
    MLOG_FILE_RENAME2(space_id, old, new): The names will be file names
    (directory/databasename/tablename.ibd). Replaces MLOG_FILE_RENAME,
    which used table names (databasename/tablename).
    
    NOTE: We will write MLOG_FILE_NAME once since the latest redo log
    checkpoint. Immediately after a checkpoint, the log may contain some
    MLOG_FILE_NAME records that were "copied across the checkpoint" and a
    MLOG_CHECKPOINT marker to signal the end of a checkpoint.
    
    On redo log apply during crash recovery, we will scan the log up to
    three [1;31mtime[ms:
    
    Recovery scan 1: Look for the first MLOG_FILE_CHECKPOINT marker since
    the latest checkpoint.
    
    If there is no MLOG_FILE_CHECKPOINT, we will skip the entire log. The
    data files will correspond to the system state as of the checkpoint.
    
    Recovery scan 2: Read the redo log since the latest checkpoint. Copy
    scanned records to recv_sys->addr_hash, and construct a map of
    recv_spaces, based on MLOG_FILE_NAME and MLOG_FILE_DELETE records.
    
    Before applying the records from recv_sys->addr_hash, we will check if
    any tablespace files are missing. If there are missing tablespaces, we
    will refuse to start up, so that the DBA can intervene, for example to
    manually rename files. This new safeguard of WL#7142 can be disabled
    by setting innodb_force_recovery.
    
    If not all redo log records in recv_sys->addr_hash, we will need a
    third log scan:
    
    Recovery scan 3: Read the redo log since the latest checkpoint. If
    recv_sys->addr_hash fills up, apply the batch of log records and read
    a new one.
    
    mlog_id_t: Remove MLOG_FILE_CREATE, MLOG_FILE_CREATE2, MLOG_FILE_RENAME.
    Add MLOG_FILE_NAME, MLOG_FILE_RENAME2, MLOG_CHECKPOINT.
    
    MLOG_FILE_FLAG_TEMP: Remove. This was a flag for MLOG_FILE_CREATE*.
    
    enum dict_check_t: Remove DICT_CHECK_ALL_LOADED. Crash recovery no
    longer loads all tablespaces.
    
    mtr_t::m_named_space: Associates a tablespace with a
    mini-transaction. A mini-transaction may be associated with up to one
    non-predefined tablespace. It may also modify predefined tablespaces
    for change buffering and undo logging.
    
    mtr_t::set_named_space(ulint space): Sets m_named_space.
    
    mtr_t::is_named_space(ulint space): Checks if the mini-transaction is
    associated with a given tablespace.
    
    mtr_t::Command::prepare_write(): Write an MLOG_FILE_NAME record if
    needed. This is executed as part of mtr_commit().
    
    mtr_t::commit_checkpoint(): A special method to emit redo log records
    to the redo log buffer when the caller already invoked
    log_mutex_enter(). This is only used by fil_names_clear().
    
    fil_space_t::max_lsn: LSN of the most recent fil_names_write() call,
    or 0 if the tablespace has not been dirtied since fil_names_clear().
    
    fil_space_t::named_spaces, fil_system_t::named_spaces: List of
    tablespaces for which MLOG_FILE_NAME has been written since the latest
    checkpoint.
    
    recv_sys_t: mlog_checkpoint_lsn: The LSN of the first scanned
    MLOG_CHECKPOINT record, or 0 if none was read yet.
    
    fil_space_get(): Look up a tablespace. This is invoked during
    mtr_t::Command::prepare_write() while not holding the log mutex, to
    prepare for a fil_names_write() call. The idea is to minimize the
    log_mutex hold [1;31mtime[m.
    
    fil_space_create(): Add an output parameter for returning a duplicate
    tablespace (same space_id).
    
    fil_space_free(): Make this an externally callable function, to free a
    tablespace from the cache when applying MLOG_FILE_DELETE.
    
    fil_space_free_low(): Renamed from fil_space_free(). The new wrapper
    fil_space_free() will acquire fil_system->mutex.
    
    fil_op_log_parse_or_replay(): Change the order of parameters. Remove
    log_flags, and rename parse_only to replay. We no longer attempt to
    replay log records of a multi-item mini-transaction, unless the
    MLOG_MULTI_REC_END was seen.
    
    fil_rename_tablespace(): Change the function signature. Take old_path,
    new_name, new_path_in. MLOG_FILE_RENAME2 is logging file names, not
    table names like MLOG_FILE_RENAME was. Also invoke fil_name_write().
    
    enum fil_load_status: Outcomes of fil_load_single_table_tablespace().
    
    fil_load_single_table_tablespace(): Do not exit on failure. Instead,
    return a status value to the caller.
    
    fil_load_single_table_tablespaces(): Remove. We no longer try to load
    all *.ibd files.
    
    fil_create_new_single_table_tablespace(): Do not write any
    MLOG_FILE_CREATE or MLOG_FILE_CREATE2. Instead, invoke
    fil_name_write() to write MLOG_FILE_NAME.
    
    fil_mtr_rename_log(): Change the signature. Take dict_table_t instead
    of names. Take a tmp_name.
    
    fil_names_write_low(): Write MLOG_FILE_NAME record(s) for a
    tablespace.
    
    fil_names_write(): Write MLOG_FILE_NAME record(s) for a tablespace if
    not already written since the latest checkpoint.
    
    fil_names_clear(): Write MLOG_FILE_NAME records and MLOG_CHECKPOINT on
    a log checkpoint or at system startup. If do_write=true, writes
    MLOG_CHECKPOINT even if no MLOG_FILE_NAME was written.
    Reset those fil_space_t::max_lsn for which fil_names_write() has not
    been invoked after the checkpoint LSN. Return true to the caller if
    any redo log was written.
    
    fil_op_write_log(): Replace log_flags with first_page_no, and replace
    table names with file paths. The parameter first_page_no is currently
    being passed as 0, because we do not have non-predefined multi-file
    tablespaces yet.
    
    fil_name_write(): Write an MLOG_FILE_NAME record for a file.
    
    Datafile::open_read_only(): Add the parameter bool strict.
    
    fsp_names_write(): Wrapper for mtr->set_named_space(). This must be
    called when a mini-transaction is going to modify a non-predefined
    tablespace.
    
    is_predefined_tablespace(): Check if a tablespace is a predefined one
    (system tablespace, undo tablespace or shared temporary tablespace).
    
    enum recv_addr_state: Add RECV_DISCARDED, so that buffered redo log
    records can be retroactively deleted if an MLOG_FILE_DELETE was
    later recovered for a tablespace.
    
    btr_free_but_not_root(), btr_free_root(): Call fsp_names_write().
    
    btr_cur_ins_lock_and_undo(), btr_cur_optimistic_insert(),
    btr_cur_pessimistic_insert(), btr_cur_update_in_place(),
    btr_cur_optimistic_update(), btr_cur_pessimistic_update(),
    btr_cur_del_mark_set_clust_rec_log(),
    btr_cur_del_mark_set_clust_rec(), btr_cur_optimistic_delete_func(),
    btr_cur_pessimistic_delete(): Call fsp_names_write() after successful
    locking and undo logging.
    
    btr_store_big_rec_extern_fields(), btr_free_externally_stored_field(),
    row_ins_index_entry_big_rec_func(): Call fsp_names_write().
    
    dict_build_tablespace(), dict_create_index_tree_step(),
    dict_recreate_index_tree(), fil_reinit_space_header(): Call
    fsp_names_write().
    
    page_cur_insert_rec_write_log(),
    page_copy_rec_list_to_created_page_write(),
    page_cur_delete_rec_write_log(), page_cur_delete_rec(), page_create():
    Assert that fsp_names_write() has been called.
    
    dict_table_rename_in_cache(): Pass old_path to
    fil_rename_tablespace().
    
    dict_check_tablespaces_and_store_max_id(): Remove the logic for
    DICT_CHECK_ALL_LOADED. We could probably remove this entire function,
    given that the maximum is also stored in the DICT_HDR page.
    
    mlog_write_initial_log_record_for_file_op(): Replaced by
    mlog_write_initial_log_record_low().
    
    log_checkpoint(): Before invoking log_write_up_to(), invoke
    fil_names_clear() to copy any MLOG_FILE_NAME records across the
    checkpoint. Flush the log up to the MLOG_CHECKPOINT marker, instead of
    only up to the checkpoint LSN. Without this step, the log between
    oldest_lsn and log_sys->lsn would be essentially corrupted (missing
    MLOG_FILE_NAME records on redo log apply). When the redo log scanner
    sees the first MLOG_CHECKPOINT since the latest checkpoint, it knows
    that there must be no missing MLOG_FILE_NAME record for any page
    operation on a non-predefined tablespace. If the MLOG_CHECKPOINT
    marker is missing, no redo log will be applied, and the system would
    be at the state of the checkpoint.
    
    fil_name_parse(): New function, to update the recv_spaces map based on
    MLOG_FILE_NAME and MLOG_FILE_DELETE records during recovery.
    
    recv_parse_or_apply_log_rec_body(), recv_parse_log_rec(): Add the
    parameter "apply". Do not apply file-level redo log records unless the
    entire mini-transaction has been recovered. Fail if an MLOG_FILE_NAME
    record is missing for a page-level operation.
    
    recv_recover_page_func(): Assert that no LSN is after the latest
    scanned redo log LSN.
    
    recv_parse_log_rec(): Check for some more log corruption.
    
    recv_parse_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    Add a parameter "apply" to specify whether log records should be applied
    (apply=false during the first scan for MLOG_CHECKPOINT). Return true
    if an MLOG_CHECKPOINT record was seen for the first [1;31mtime[m.
    Improve DBUG_PRINT output, and detect some more log corruption.
    
    recv_scan_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    
    recv_group_scan_log_recs(): Initialize the variables and data
    structures to begin reading redo log records. Add a parameter
    "last_phase" that is set when a multi-pass recovery is needed and we
    are scanning the redo log for a third [1;31mtime[m. In last_phase, we will
    invoke recv_apply_hashed_log_recs() to empty recv_sys->addr_hash
    between passes. If last_phase=false, we would stop filling
    recv_sys->addr_hash, only processing file-level redo log records.
    
    recv_init_crash_recovery(): Split some code into
    recv_init_crash_recovery_spaces(), to be invoked after the first call
    to recv_group_scan_log_recs().
    
    recv_recovery_from_checkpoint_start(): Invoke
    recv_group_scan_log_recs() up to 3 [1;31mtime[ms if needed.
    After processing all redo log, write an MLOG_CHECKPOINT marker
    so that in case we will crash before making a checkpoint, the log
    will be replayed by subsequent crash recovery.
    
    checkpoint_now_set(): Avoid an infinite loop in case an MLOG_CHECKPOINT
    marker is the only thing that was written since the latest checkpoint.
    
    rb#4700r6

[33mcommit 4bfc3ac4656a2e33f8a0420fee20145a0de6ef62[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Mar 19 13:02:47 2014 +0200

    Fix Bug#18154145 ASSERT BUF_POOL_FROM_BPAGE(BPAGE) == BUF_POOL IN PURGE
    
    Avoid having entries (buf_page_t) in buf_pool_t::page_hash for which
    buf_page_t::buf_pool_index != buf_pool_t::instance_no.
    
    The watch pages reside in a preallocated array buf_pool_t::watch[] and
    they are some[1;31mtime[ms added to the page hash, but their buf_pool_index
    members are never set and remain 0, so buf_pool_from_bpage() always
    returns the 0th buf_pool instance, which is of course incorrect.
    
    Approved by:    Marko (rb:4967)

[33mcommit 1a791b403ee2e04567ffeae004690959ee3c5f44[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Tue Mar 18 17:21:58 2014 +0530

    Bug #18196562: NDBAPI : RACE BETWEEN LATE TRANSID_AI AND API CLOSETRANSACTION()
    
    An NDBAPI application sends a scan query to a data node, which is
    processed by the TC. The TC forwards the LQHKEYREQ request to the
    appropriate LDM, and aborts the transaction if it does not receive
    a LQHKEYCONF response within the specified [1;31mtime[m limit. After the
    transaction is successfully aborted, the TC sends a TCROLLBACKREP
    to the NDBAPI client. The NDBAPI client processes the TCROLLBACKREP
    by cleaning up the Ndb objects associated with the transaction.
    
    The NDBAPI client receives the data which it has requested in the
    form of TRANSID_AI signals sent by the LDM. These TRANSID_AI signals
    are buffered for sending at the data node, and may be delivered to the
    NDBAPI client after a delay. On receiving a TRANSID_AI signal, the
    NDBAPI checks the transaction state and ID: if these are as expected,
    it processes the signal using the Ndb objects associated with that
    transaction.
    
    The bug occurs when all the following conditions are fulfilled:
    
    - The TC aborts a transaction due to delays and sends a TCROLLBACPREP
    to the NDBAPI client. Simultaneously, a TRANSID_AI which has been
    buffered for delivery at an LDM is delivered to the same NDBAPI client.
    
    - The NDBAPI client considers the transaction complete on receiving a
    TCROLLBACKREP and immediately closes the transaction.
    
    - The NDBAPI client has a separate receiver thread running concurrently
    with the transaction closing thread.
    
    - The reception of the late TRANSID_AI interleaves with the user
    thread's transaction close so that the TRANSID_AI processing passes the
    normal checks (TransId, 'magic number') before the closeTransaction
    resets the transaction state and invalidates the receiver.
    
    The receiver thread proceeds to continue working on the TRANSID_AI
    signal using the invalidated receiver. Since the receiver is already
    invalidated, its usage results in a core.
    
    Added a fix in the NDBAPI, so that the Ndb object cleanup done for
    TCROLLBACKREP now includes invalidation of the transaction ID. So, for
    a given transaction, any signal which is received after the
    TCROLLBACKREP arrives, does not pass the transaction ID check and is
    silently dropped. Duplicated the fix for TC_COMMITREF, TCROLLBACKREF,
    TCKEY_FAILCONF and TCKEY_FAILREF.

[33mcommit b989866a6303a8cfd36c2dbf2ea936b41081120e[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Mar 11 12:52:39 2014 +0000

    Bug #18375920   NDB TESTING : SYNC_SLAVE_WITH_MASTER CAN TIMEOUT AND CONTINUE
    
    This fix changes mysqltest's sync_slave_with_master
    ndb-binlog-injector-sync [1;31mtime[mout from 30s to 150s, and
    makes [1;31mtime[mout a hard test failure.
    
    This causes the 'fishy' ndb_rpl_commit_afterflush testcase to fail, as
    it relied on a [1;31mtime[mout+pass behaviour from sync_slave_with_master.
    
    This testcase is removed and a new testcase is added to replace it:
    ndb_binlog_flush_tables_with_read_lock.test.
    
    The new testcase verifies the basics of the behaviour, with 2 MySQLDs
    attached to a cluster.

[33mcommit cb2166eb54d8a78a004ac73f34cbd44481e12e40[m
Author: Atanu Ghosh <atanu.ghosh@oracle.com>
Date:   Mon Mar 10 16:46:49 2014 +0530

    Bug #11766596 UPDATE A TIMESTAMP VARIABLE EVERY TIME REACHES MAX_USED_CONNECTIONS
    
    Problem: Max_used_connections works as a high-water mark, so, you cannot
             know if it reached that level just a minute ago or at server startup.
             A [1;31mtime[mstamp needs to be updated every [1;31mtime[m it reaches that level.
    
    Fix: A new status variable 'max_used_connections_[1;31mtime[m' was introduced.This
         variable would be updated only when the connection count strictly exceeds
         the previous highest. The [1;31mtime[m would be displayed in local [1;31mtime[m zone on
         issuing the "SHOW STATUS" command.

[33mcommit da539823da56b63d546d4a406cde4831bc27f98d[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Mar 7 09:16:57 2014 +0100

    WL#7578 Don't use ndb_schema_share from ack_schema_op
    
     - The schema event handler part of the binlog thread has received
       an event(someone inserted into mysql.ndb_schema) and it should
       perform the event and reply with an ack.
     - Even if ndb_schema_share pointer should be NULL, the function
       must reply. Especially since the reply does not use the ndb_schema_share
       pointer but rather just writes to mysql.ndb_schema.
     - Should mysql.ndb_schema not exist at this [1;31mtime[m, the write will of course fail
       but then all the other mysqld(s) wil also notice the same problem
     - Also, it was wrong to check "ndb_schema_share" without taking the
       "ndb_schema_share_mutex"

[33mcommit 565d20b44f24fcc855dc616164d87b03cfad10bc[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Mar 6 12:15:07 2014 +0100

    Bug#17606098: DEADLOCK INVOLVING SRV_MONITOR_THREAD AND LOCK_THD_DATA
    
    This deadlock involved LOCK_thd_data and InnoDB's trx_sys mutex.
    It could occur if InnoDB code (e.g. the srv_monitor_thread) called
    thd_security_context() while having trx_sys locked and at the same [1;31mtime[m
    the server tried to notify InnoDB about a deadlock (which involves calling
    mysql_lock_abort_for_thread() while holding LOCK_thd_data).
    This could lead to deadlock if mysql_lock_abort_for_thread() lead to
    trx_allocate_for_mysql() being called inside InnoDB.
    
    This very rarely happens - the problem was found with RQG and is very
    difficult to reproduce.
    
    This patch solves the problem by splitting LOCK_thd_data so that a
    separate new mutex LOCK_thd_query protects the query string
    accessed by thd_security_context().
    
    The patch also strengthens the protection of the query string by
    enforcing that it can only be set by the owner thread and that this
    requires locking of LOCK_thd_query. Reading the query string can
    be done by the owner thread without holding LOCK_thd_query but
    other threads reading the query string have to have LOCK_thd_query
    locked.
    
    This also solves a separate problem where other threads could
    read the query string while it was being deleted by the owner thread.
    
    Finally, the patch updates the performance schema mutex
    heuristics - including correcting an issue introduced by WL#6369.

[33mcommit 22da28c0b9da0dd7b96ca43c2b03039e19e34967[m
Author: Prabeen Pradhan <prabeen.pradhan@oracle.com>
Date:   Wed Mar 5 13:34:23 2014 +0530

    WL#7131   : Migrate authentication tests in main suite
    Issue     : Fixed sporadic issue occuring on pb2 windows
    Solution  : Increased password_life[1;31mtime[m from 2 to 3. Was failing when there is delay in execution of next statement and password was being expired.

[33mcommit a53f8bdf1d4d11a822b8cfc671c3c3d76ef807ce[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Wed Mar 5 10:32:01 2014 +0530

    WL#6936 - Implementation of server-side statement [1;31mtime[mout.
    
    This is a follow up patch for WL#6936 to fix max_statement_[1;31mtime[m
    test failure on some(may be slower) machines.
    
    - Test has SELECTs which checks result of event and triggers.
      These selects were [1;31mtime[md out on some machines because of the delay.
      * Changed [1;31mtime[mout value for such SELECTs.
    - Disabling [1;31mtime[mr for non-read only select is done after checking
      SELECT is read only or not at run[1;31mtime[m. Since [1;31mtime[m out value was
      small for non-read only select before disabling [1;31mtime[mr query is
      [1;31mtime[md out.
      Increased [1;31mtime[mout value for non-read only select in test file.

[33mcommit 2a1975757944f5d17e5ffcc719d52d051e38720d[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Mar 3 13:53:59 2014 +0100

    WL#7082 -  Move permanent transformations from JOIN::optimize () to JOIN::prepare ().
    As one single patch.
    
    semijoin, outer-join-to-inner, parenthesis-removal,
    join-condition-to-where-clause are moved from JOIN::optimize() to
    JOIN::prepare(), which is renamed to select_lex::prepare().
    
    Old approach for WHERE/HAVING conditions:
    at end of preparation, save copies of conditions in
    sl->prep_where/having, then allow oneself to trash sl->where/having in
    optimization; in optimization make sure to keep in sync
    sl->where/having with join->conds/having. At start of next execution,
    recreate sl->where/having from the "prep_" copies.
    New approach:
    At end of preparation, sl->where/having is considered frozen. In
    optimization, make trashable copies of it, and use only those
    copies. Ditch them at end of execution.
    
    Some functions like mysql_select() are made to use select_lex->where;
    in the pre-patch situation select_lex->where was passed as argument
    AND the function assumed that this argument was
    ==select_lex->where... this change makes code simpler.
    
    Made some functions which use JOIN to rather use, and belong to, SELECT_LEX:
    record_join_nest_info
    simplify_joins
    convert_subquery_to_semijoin
    resolve_subquery
    flatten_subqueries.
    They try to use JOIN as little as reasonably possible.
    
    Moved JOIN::prepare() to select_lex, and simplified its signature
    (arguments can be found in select_lex).
    Made setup_conds() member of select_lex, with less arguments.
    Removed arguments from setup_ref_array().
    
    Simplified setup_wild(), more JOIN members are made private,
    JOIN::join_list removed, reset_nj_counters goes to select_lex.
    
    JOIN::table_list is now used only in optimization/execution.
    
    JOIN_TAB::on_expr_ref, JOIN/select_lex::where/conds/having are
    renamed, some getters/setters are added.
    
    Simplified select_lex::first_cond_optimization: rename member to be
    more specific, and removed argument in make_join_statistics().
    
    I remove some hacks which came in previous PS fixes (see the WL for
    bug numbers), because they become superfluous.
    
    Changes in opt trace tests: trace blocks for
    semijoin/outer-join-to-inner move from the join_optimization block to
    the join_preparation block. In ps-specific tests, where the shown
    trace is of EXECUTE, it implies that now the trace starts with the
    transformation already done (by PREPARE).
    
    See the WL text for overview of goal and code changes.
    
    Additional details are in these commit comments.
    
    @  mysql-test/suite/opt_trace/include/bugs.inc
    
    bug was fixed long ago
    
    @  mysql-test/suite/opt_trace/r/bugs_no_prot_all.result
    
    Also fixed a bad number for the query block where the view is merged
    ("in_select#").
    
    @  mysql-test/suite/opt_trace/r/general2_no_prot.result
    
    fixed trace of view merging/materialization
    
    @  mysql-test/suite/opt_trace/r/general_ps_prot_all.result
    
    Around line 7866, we see:
    "original_condition": "(1 and (`t2`.`s` = 'c') and (`t6`.`d` = `f1`()))",
    "steps": [
      {
        "transformation": "equality_propagation",
    -    "resulting_condition": "(1 and multiple equal('c', `t2`.`s`) and multiple equal(`f1`(), `t6`.`d`))"
    +    "resulting_condition": "(1 and (`t6`.`d` = `f1`()) and multiple equal('c', `t2`.`s`))"
    A multiple equality is not created anymore for t6.d=f1(). I think this
    is ok. Creation of such item requires f1() to be a constant (see
    check_simple_equality() which looks for field=constant), and f1()
    should not be considered constant, it is a stored function which may
    return a different value for each row of t2.
    
    @  mysql-test/t/sp.test
    
    Tests added along the way, when fixing bugs in the prototype
    
    @  mysql-test/r/subquery_all.result
    Result changes in EXPLAIN: they come from the can_skip_order change in sql_union.cc:
    we used to pass a NULL pointer to prepare(), now we instead empty the list in select_lex;
    this has the advantage of showing the optimization in the query printed by EXPLAIN;
    and this alternative technique looks ok because remove_redundant_subquery_clauses
    does it too.
    
    @  sql/item.h
    
    removed one hack ("real_items" argument), added chop_ref
    
    @  sql/item_cmpfunc.cc
    
    removed one hack ("real_items" argument)
    
    @  sql/item_cmpfunc.h
    
    removed one hack ("real_items" argument)
    
    @  sql/item_subselect.cc
    
    About the removal of "// did we changed top item of WHERE condition":
    - in-to-exists is, as before, a permanent transformation
    - pre-patch, Item_in_subselect::fix_fields() would be passed &JOIN::conds as "ref"
    argument, so when it changes the condition (injects outer=inner
    equality in subquery's WHERE), it changes *ref, which changes
    JOIN::conds, but because it is a permanent transformation, it also
    needs to manually "keep in sync" select_lex->where.
    - post-patch, fix_fields() operates on &select_lex->where_cond, and
    JOIN::where_cond is not "alive" yet (it starts its life in
    JOIN::optimize()), so no manual syncing is needed.
    
    Likewise, no manual syncing of having_for_explain is needed.
    
    @  sql/sql_base.cc
    
    No manual syncing needed (see comment of item_subselect.cc).
    
    @  sql/sql_delete.cc
    
    Don't pass "conds", just use select_lex->where_cond as input. When we
    want to optimize the condition, we make a copy of it.
    
    @  sql/sql_lex.cc
    
    Part of the end-of-prepare job of
    st_select_lex::fix_prepare_information() has moved to
    the start-of-optimize get_optimizable_conditions().
    One real_item() is removed, in this move.
    
    @  sql/sql_optimizer.cc
    
    Get trashable copies at start of JOIN::optimize() and use only them.
    Removed dead code in #ifdef.
    Transformations move to JOIN::prepare(), and the horror of "let's
    update prep_where because we did permanent transformations in
    JOIN::optimize()", is gone - this saves some copying and memory allocations.
    In simplify_joins(), the part:
              /* If join condition has a pending rollback in THD::change_list */
              join->thd->change_item_tree_place(table->join_cond_ref(), &conds);
    was useless: "conds" is a local variable, &conds could never be found
    when change_item_tree_place() searches. So I replace "conds" by an
    Item** passed in argument. In a test file I added some queries which
    used to break (lack a rollback) due to this useless code.
    In record_join_nest_info(), prep_join_cond; the corresponding job is
    now in get_optimizable_join_conditions().
    In replace_subcondition(), removed useless manual syncing.
    At the end of flatten_subqueries(), same.
    
    @  sql/sql_prepare.cc
    
    use setup_fields_with_no_wrap, makes less code lines.
    In reinit_stmt_before_use(), don't recreate select_lex->where_cond, it is
    already good (== made of permanent items); only need to clean up those
    items, to make them ready for reusal.
    
    @  sql/sql_resolver.cc
    
    JOIN::prepare now operates only on select_lex->where, not JOIN::conds
    which is now reserved for JOIN::optimize.
    JOIN::prepare does transformations at its end.
    Some functions now done at "prepare" [1;31mtime[m are moved to this file.
    
    @  sql/sql_union.cc
    
    We work around an oddity of IN->EXISTS (whose effects were only a
    strange WHERE in the trace, when I started trusting
    select_lex->where_cond instead of always passing conds=NULL
    to JOIN::prepare() of the fake select lex).
    
    @  sql/sql_update.cc
    
    When we start optimizing UPDATE, we get trashable conditions.
    
    @  sql/sql_view.cc
    
    Fixed a bad number for the query block where the view is merged
    ("in_select#"), this is visible in bugs_no_prot_all.result file.
    
    @  sql/table.h
    
    In TABLE_LIST, m_join_cond becomes the permanent condition,
    m_optim_join_cond a trashable copy created at start of optimization.
    Moved all optimization-only members to one place.

[33mcommit 3ab69696469fe5dbcfa558b53014a68bf4692cfb[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Fri Feb 28 19:27:06 2014 +0530

    BUG#18311187 - SQL_MODE IS CHANGED TO WRONG VALUE AFTER DUMP/DATA UPGRADE FOR TRIGGERS
    
    BACKGROUND:
    With the push of WL#7467, it was observed that SQL_MODE is
    changed to wrong value in triggers during live/dump upgrade
    which is not desired behaviour.
    
    ANALYSIS:
    SQL_MODE for triggers is stored in .TRG files at trigger
    creation [1;31mtime[m.
    If the user upgraded to the version in which WL#7467 was pushed,
    the value of SQL_MODES in .TRG files started mapping to the
    new SQL_MODES currently available which was not desired and
    might lead to wrong behaviour of triggers after upgrade.
    
    FIX:
    As a fix for this issue, the removed SQL_MODES are again
    introduced for backward compatibility during upgrade.
    However, Setting NO_ZERO_DATE, NO_ZERO_IN_DATE and
    ERROR_FOR_DIVISION_BY_ZERO will have no effect and
    user will get a warning if he tries to explicitly set
    these modes.
    
    NOTE TO DOCUMENTATION:
    =====================
    For stored routines that were created in 5.6 with
    NO_ZERO_* modes, after upgrade to 5.7, these modes will
    *not* show up in the output of
    SHOW CREATE PROC/FUNC/EVENTS/TRIGGERS, and SELECT * FROM
    information_schema.triggers.
    Only Select * from mysql.proc and mysql.events
    table will show the NO_ZERO_* modes after upgrade for the
    procs/events that were created in 5.6.
    In 5.7, since we cannot use NO_ZERO_* modes,
    mysql.proc/mysql.events or even SHOW CREATE will not show
    any of NO_ZERO_* Modes.

[33mcommit 6587ee2e727b93e82eff156b9e86a5de98d831d8[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Feb 28 16:33:38 2014 +0530

    WL#6936 - Implementation of server-side statement [1;31mtime[mout.
    
    This is a follow up patch for WL#6936 to fix compilation
    issue on solaris x86 for unused parameter error.

[33mcommit 564672fb706701ba839c9185d3a2619a2d2467a5[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Fri Feb 28 12:58:06 2014 +0530

    WL#6936 - Implementation of server-side statement [1;31mtime[mout
    
    Server-side [1;31mtime[m limit for the execution of SELECT
    statements. After completing specified amount of [1;31mtime[m, the
    statement should be aborted without affecting the session
    (connection). This feature will help in interrupting and
    aborting the SELECT statements when their execution [1;31mtime[m crosses
    specified amount of [1;31mtime[m limit instead of just waiting for those
    queries to complete.
    
    This work log addresses the bug
    16271666 - IMPLEMENTATION OF SERVER-SIDE STATEMENT TIMEOUT
               SUPPORT
    
    
    Original patch for this WorkLog is Contributed by "Davi Arnaut"
    and it has been developed further.
    
    Davi Arnaut Contributed original patch while working in Twitter.
    Now he works for LinkedIn.

[33mcommit 500a7e3cc61c31d77409c93a454e93316955713e[m
Author: Vinay Fisrekar <vinay.fisrekar@oracle.com>
Date:   Fri Feb 28 12:38:56 2014 +0530

    disable test on windows due to [1;31mtime[mout.
    Test is big test , need to be checked if it can be reduced or not

[33mcommit 0c60d406c00c598a7c63eb7b962af0b89e432a06[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Feb 27 23:17:23 2014 +0400

    WL#7306 "Improve MDL performance and scalability by implementing lock-free
    lock acquisition for DML" and fix for bug #18077895 "WL7305 PUSH (7249)
    CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP UP TO -8%".
    
    The idea of this task is to change the acquisition of unobtrusive locks on
    the fast path, which currently involves acquisition of MDL_lock::m_rwlock,
    some checks, increment of the packed counter and release of m_rwlock, to
    a single atomic compare-and-swap operation.
    Similarly the release of the lock on the fast path becomes single atomic
    compare-and-swap (in absence of obtrusive locks and assuming we are not
    releasing the last lock for this MDL_lock object) instead of acquisition
    of MDL_lock::m_rwlock, decrement of the packed counter, some checks and
    m_rwlock release.
    As result these operations become at least twice cheaper than their
    old versions which has a nice effect on performance/scalability.
    
    Acquisition/release of locks on the slow path (i.e. unobtrusive locks in
    presence of obtrusive locks and obtrusive locks) still has to use the old
    approach involving locking/unlocking MDL_lock::m_rwlocks and checks of
    MDL_lock::m_granted/m_waiting bitmaps/lists.
    
    This patch implements the above idea by performing the following
    three transformations:
    
    I)   MDL_lock::m_fast_path_granted_count is replaced with an atomic
         MDL_lock::m_fast_path_state member, which in the ideal case of
         "fast path" acquisition/release is checked and changed using CAS
         without holding any mutexes.
    II)  Since we would like to check in the same atomic CAS operation that
         MDL_lock object was not destroyed, its m_is_destroyed member is
         replaced by a IS_DESTROYED bit flag in the m_fast_path_state
         packed counter.
    III) Similarly, since we also would like to check in the same atomic CAS
         that there are no granted or pending obtrusive locks, we have to
         add a HAS_OBTRUSIVE bit flag in the m_fast_path_state, while
         keeping MDL_lock::m_obtrusive_locks_granted_waiting_count.
         This flag should be set when we are about to try acquiring an obtrusive
         lock and cleared once the last granted or pending obtrusive lock goes
         away.
    
    
    Most of the remaining changes in this patch are necessary in order to fix
    bug #18077895 "WL7305 PUSH (7249) CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP
    UP TO -8%".
    
    This bug manifested itself as a slowdown for workloads involving 1 connection
    in cases when there were many concurrent connections to the same server in the
    past or there were many dormant connections at the same [1;31mtime[m as 1 active
    connection.
    
    In such scenarios the release of a metadata lock meant that MDL_lock became
    unused, was removed from lock-free hash with all lock objects and we
    tried to return it back to allocator. The latter operation involved
    scanning pins for all current and past connections, which became fairly
    expensive in this scenario.
    
    This patch solves this problem by avoiding releasing MDL_lock objects
    and removing them from the hash once they are no longer used. Instead we
    keep unused objects in MDL_map and start their eviction only if their
    number passes certain threshold and the ratio of unused/total lock objects
    is big enough. We evict random unused objects so on average objects
    which are used more often will stay in the hash and rarely used objects
    will go away.
    
    The above idea is implemented by:
    
    a) Introducing a new HAS_SLOW_PATH flag in the MDL_lock::m_fast_path_state
       member, which indicates if there any tickets in MDL_lock::m_granted
       and m_waiting lists or we are about try to add one. Thanks to this
       flag, it is possible to distinguish between used and unused MDL_lock
       objects in atomic compare-and-swap operations used to implement fast
       path acquisition and release of locks.
    b) Changing code which releases locks to avoid removing unused MDL_lock
       objects from the hash and deleting them afterwards. Instead we
       atomically increment the newly introduced MDL_map::m_unused_lock_objects
       counter. Similarly, on the first acquisition of lock for MDL_lock which
       was previously unused we atomically decrement this counter.
    c) In cases when the increment of the MDL_map::m_unused_lock_objects counter
       exceeds the threshold value and the unused/total objects ratio is high
       enough, we try to reduce the number of unused objects. We look-up a random
       unused object in MDL_map, mark it as destroyed, remove it from the hash and
       return it back to allocator. As a consequence MDL_map::remove() method
       has became MDL_map::remove_random_unused().
    d) To support the change described in c), a new lf_hash_random_match()
       function was introduced which allows us to efficiently find a random
       object which matches certain condition in LF_HASH.
    e) Also to support the change described in c), a new PRNG was added to
       MDL_context class. This PRNG is used as a source for randomness for
       look-ups of random unused objects.
    
    Unit tests were added covering handling of unused MDL_lock objects and
    for the new lf_hash_random_matches() function.
    
    
    Finally, this patch fixes a violation of the pinning protocol, which was
    introduced by WL7305 and which occured when the MDL subsystem failed
    to look up MDL_lock object in lock free hash. The LF_HASH documentation
    was updated to reflect the need to call lf_hash_search_unpin in this case.

[33mcommit da6e9fa46b35824f3f10981a4c1bac9a378c9af8[m
Merge: 0e62a17f39a a93f480142b
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Thu Feb 27 18:27:51 2014 +0530

    WL#6120- Change master without stopping slave
    
    Currently, the way we move from a topology M1->S to M2->S is:
    
        a) STOP SLAVE
        b) SHOW SLAVE STATUS to get (Read_Master_Log_Pos, Master_Log_File)
        c) START SLAVE UNTIL <position>
        d) SELECT MASTER_POS_WAIT(<position>)
        e) CHANGE MASTER <new master>
        f) START SLAVE
    
       The proposal is to reduce these steps to just
    
          CHANGE MASTER <new master>
    
       wherever applicable. See points (a-d) below regarding these rules.
    
       a) If IO thread is running and SQL thread is stopped:
             - CHANGE MASTER TO RELAY_LOG_FILE/RELAY_LOG_POS/MASTER_DELAY will be
               allowed.
             - All other CHANGE MASTER options will be disallowed
    
       b) If SQL thread is running and IO thread is stopped:
             - CHANGE MASTER TO RELAY_LOG_FILE/RELAY_LOG_POS/MASTER_DELAY will be
               disallowed
             - All other CHANGE MASTER options will be allowed.
    
       c) CHANGE MASTER TO MASTER_AUTO_POSITION=1 will be allowed only if both IO
          and SQL threads are stopped.
    
       d) If the receiver/applier is running and the slave has open temporary tables,
          we print a warning on CHANGE MASTER.
    
    2) In the above mentioned change, there could be an instant of [1;31mtime[m when
       the IO thread is reading from M2 and the SQL thread is executing events that
       had been received from M1, *both at the same [1;31mtime[m*. Also there is no overhead
       of killing and spawning new threads.
    
    3) Currently, CHANGE MASTER purges relay log files unless the command uses
       RELAY_LOG_FILE/RELAY_LOG_POS option. This behavior will be kept intact when
       the both thread are stopped. The reason for this is that we can't remove the
       relay log(s) with a running SQL thread.
       When any one thread is running while we do a CHANGE MASTER, we dont delete
       relaylogs. The relaylog deletion can be handled by using the relay-log-purge
       option.
    
    4) With Statement based replication (SBR), we don't recommend using temporary
       tables. One reason is that there is a possibility that the temporary tables
       are left open forever on a failover. To warn users that there could be such
       a situation we introduce warnings in the error log when one does a change
       master or stop slave. More precisely, we follow the following rules:
    
         4.1 change master should never drop temp tables
         4.2 We introduce a new command to drop temp tables. This will be done
             in WL#7441.
         4.3 The options under change master can be grouped under three groups:
             a) To change a connection configuration but remain connected to
                the same master.
             b) To change positions in binary or relay log(eg: master_log_pos).
             c) To change the master you are replicating from.
             Change master should generate a warning if there are open temp tables
             in cases a and b above.
         4.4 Stop slave should generate a warning if there are open temp tables.

[33mcommit 2bcc8051765842dbe039f275d0b44604a744756e[m
Author: mayank prasad <mayank.prasad@oracle.com>
Date:   Thu Feb 27 14:27:48 2014 +0530

    WL#5768  PERFORMANCE SCHEMA, prepared statements instrumentation.
    
    Details :
     - During deallocating a prepared stmt, when its entry was removed from P_S buffer, it stat was not reset. Therefore next [1;31mtime[m when this slot in P_S buffer was used from
       some other statement, stale stats values were being used.
    
    Fix:
     - Modified code to reset all stats entry before using a slot in P_S buffer.

[33mcommit 48f3e6654d5039a18fbaaa51c844622f124cd3fe[m
Author: Atanu Ghosh <atanu.ghosh@oracle.com>
Date:   Wed Feb 26 16:26:05 2014 +0530

    WL#7131:Add [1;31mtime[mstamp in mysql.user on the last [1;31mtime[m the
            password was changed and implement password rotation
    
    We need to track when the password was last changed
    and implement password rotation.
    
    Put a TIMESTAMP column inside mysql.user table and
    update it when the password is updated.
    
    Put another column in mysql.user, holding the number
    of days after which the password must expire.
    
    Introduce extension of query ALTER USER as:
    
    ALTER USER foo PASSWORD EXPIRE EVERY <day> DAY;
    ALTER USER foo PASSWORD EXPIRE NEVER;
    ALTER USER foo PASSWORD EXPIRE DEFAULT;

[33mcommit e5d9961b637f871b34d7741b9f3db336c59ddec4[m
Author: kevin.lewis@oracle.com <>
Date:   Tue Feb 25 23:42:26 2014 -0600

    WL#7628 - Remove innodb_use_sys_malloc & innodb_additional_mem_pool_size
    system variables in 5.7
    
    This effort involves deleting and changing the following code, code files and
    test files.
    
    * Global setting innodb_use_sys_malloc
    * Test suite/sys_vars/innodb_use_sys_malloc_basic
    * Global setting innodb_additional_mem_pool_size
    * Test suite/sys_vars/innodb_additional_mem_pool_size_basic
    * The common memory pool and all the code that implements memory pools
    in mem0pool.cc, mem0pool.h, mem0pool.ic
    * Rename test innodb_use_sys_malloc to innodb-large-prefix and delete the
    portion of this test that relates to the innodb_use_sys_malloc setting.
    * Change ut_malloc() so that it always attempts the malloc() 60 [1;31mtime[ms
    in 60 seconds. Previously, it would try only one malloc() with
    innodb_use_sys_malloc=ON. If OFF, it used to do the 60 tries in
    60 seconds using the common memory pool.
    * Change ut_realloc so that it always attempts the realloc() 60 [1;31mtime[ms
    in 60 seconds. Previously, it would try only one realloc() with
    innodb_use_sys_malloc=ON. If OFF, it used call ut_malloc() for a new
    buffer and ut_free() for the old buffer.
    * Deleted metadata_pool_size from information_schema.innodb_metrics
    * Deleted all UNIV_MEM_DEBUG code.
    * Greatly simplified mem_heap_validate and mem_heap_check().  All other
    parts of mem0dbg are removed
    * Removed all code under MEM_PERIODIC_CHECK and all calls to
    mem_analyze_corruption().
    * Changed some conditional calls to ib_logf(FATAL,...) to just an assert
    so that the system will stop all threads sooner after a memory corruption.
    
    Approved in rb#4512 by Marko

[33mcommit 6fcb567ecca7a6d9ad9c4b60d284175840a398cf[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Feb 25 14:52:17 2014 +0100

    WL#7646 Improve ndbcluster Ndb_component usage(binlog thread part)
      - Rely solely on Ndb_component start/stop for Ndb_binlog_thread
       startup and shutdown, this involves:
      -- use is_stop_requested() to check if [1;31mtime[m to stop thread
      -- implement stop() (just for having a place the put the comment about
          not waking up)
      -- remove "running" since that state is implemented in baseclass
      -- remove "ndbcluster_binlog_terminating"

[33mcommit dbe145ae63cb9db8baa105222b1c47a03dbf3af9[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Feb 25 14:50:37 2014 +0100

    WL#7646 Improve ndbcluster Ndb_component usage (util thread part)
     - Rely solely on Ndb_component start/stop for Ndb_util_thread
       startup and shutdown, this involves:
      -- use is_stop_requested() to check if [1;31mtime[m to stop thread
      -- implement stop() which first wakes the thread from potential sleep
         before continuing in baseclass stop function
      -- remove "running" since that state is implemented in baseclass
      -- remove "COND_ready" since the case where thread started up but "failed" is
         unlikely and not handled properly anyway. (The two returns in early phase
         of Ndb_util_thread::do_run() will be removed later when util thread get rid
         of THD and Thd_ndb dependency.)
      -- furthermore there are still a few case in all ndbcluster daemon threads where
         they silently exit...

[33mcommit b8f74d6dc25d22b60096e14821a91b47305be764[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Feb 25 14:48:52 2014 +0100

    WL#7646 Improve ndbcluster Ndb_component usage (index stat part)
     - Rely solely on Ndb_component start/stop for Ndb_index_stat_thread
       startup and shutdown, this involves:
       -- use is_stop_requested() to check if [1;31mtime[m to stop thread
       -- implement stop() which first wakes the thread from potential sleep
          before continuing in baseclas stop function
       -- implement wakeup() which tells the thread that there are work to do.
       -- remove "running" since that state is implemented in baseclass
       -- remove "COND_ready" since the case where thread starteed up but "failed"
          does not happen in the index_stat_thread.
       -- move "ndb_index_stat_waiter" global variable to Ndb_index_stat_thread
     - Remove extern declaration of ndb_index_stat_thread instance from ha_ndbcluster.h
       but add that same declaration to ndb_index_stat.cc since implementation still
       uses it's own instance.

[33mcommit 2ba51263fef0b741e59e3b8c9f895b8d109c62f2[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Feb 25 12:56:50 2014 +0100

    Bug#12590101 *KORR/*STORE MACROS IN MY_GLOBAL.H ASSUME THE ARGUMENT TO BE A CHAR POINT
    
    Make the assumption even stronger, by substituting inline functions for the macros.
    Added overloaded functions taking char* rather than uchar* for C++
    
    This should catch non-char/uchar buffer usage at compile-[1;31mtime[m,
    both for C and C++ code.

[33mcommit 9b3b0f7d5f76e11b5709e53cc6aa1c0217e875d8[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Fri Feb 21 00:20:39 2014 +0000

    Bug #16693068     7.1.18 TO 7.1.22-27 UPGRADE NOT POSSIBLE IF LARGE NUMBER OF TABLES PRESENT
    
      The Dbdih::getInstanceKey() function is used to determine which LDM instance
      is responsible for storing a table fragment, based on the table fragment's
      log_part_id, which is set at table creation [1;31mtime[m, and must be stable between
      initial restarts of a node.
    
      The DbDih::getInstanceKey() function should return a value between 1 and
      NDBMT_MAX_BLOCK_INSTANCES-1 inclusive, identifying which LDM worker array entry
      should be used for a given fragment.  Value 0 refers to the LDM Proxy instance
      in ndbmtd, and so is not used.
      However getInstanceKey() has a bug where it can return numbers between 1 and
      NDBMT_MAX_BLOCK_INSTANCES inclusive.  This exceeds its range.
    
      Where it returns NDBMT_MAX_BLOCK_INSTANCES as a result, this causes an
      array out-of-bounds exception when mapping an instance key to a receiving
      thread, and this can have various bad effects.
    
      This bug is not generally observed as getInstanceKey() uses the log_part_id
      to determine the instance number, and recent code changes keep log_part_ids
      far below values which can return NDBMT_MAX_BLOCK_INSTANCES.
    
      Older code stored unlimited log part ids in the fragment definition,
      and used modulo division by the number of LDM instances at 'run[1;31mtime[m'
      to get the correct log part.
    
      More recent code stores log part ids modulo the current number of running
      LDM instances at table creation [1;31mtime[m, and continues to determine fragment
      instance keys (LDM instance numbers) based on modulo division.  It's not
      that clear exactly what problem was solved here, but it happens to hide the
      getInstanceKey() bug.
    
      The number of running LDM instances is always < NDBMT_MAX_BLOCK_INSTANCES -1,
      so normal code does not hit the bug in getInstanceKey()
    
      During an upgrade we can load table definitions on disk from an older version
      with non-modulo log part numbers.  This can expose the getInstanceKey() bug
      which has various symptoms (hangs, crashes).
    
      To solve this, this patch performs modulo division of log_part_ids by the running
      node's number of LDM instances when loading a potentially 'old' (unbounded) fragment
      definition from disk.  This extends the guarantee that fragment log_part_ids will
      be < NDBMT_MAX_BLOCK_INSTANCES - 1 to cover the upgrade scenario so that the
      getInstanceKey() bug is not exposed.  This fix is aligned with the previous modification
      which stores these 'modulo log part ids'.
    
      To further clarify the new situation, getInstanceKey() is
      modified to no longer perform modulo division on the
      log_part_id as it is no longer required.
      Additionally, getInstanceKey() is modified to require that
      the resulting instance key is in range.
      Further, existing code which sets a fragment's log_part_id
      is modified to require that the log part id is within range
      (0 to NDBMT_MAX_BLOCK_INSTANCES -2 inclusive).
      A new define NDBMT_MAX_WORKER_INSTANCES is defined as
      NDBMT_MAX_BLOCK_INSTANCES - 1, in which terms the range of the
      log_part_id is 0 to NDBMT_MAX_WORKER_INSTANCES -1 inclusive.
    
      Performing %= num_ldm_instances on the log_part_id should be
      safe for non-initial restarts, as those are only allowed when
      the number of log parts is not changed.
    
      One downside of storing log parts in their 'modulo' form is that an
      increase in the number of log parts cannot remap existing fragments
      to the new log parts over an initial restart.
      That problem was introduced by the original change to store modulo
      log parts and is considered out of scope here.
    
      testUpgrade is extended to cover the scenario required here :
       - Many tables
       - Many fragments/table
       - System restart upgrade rather than node restart upgrade
    
      An execution of this upgrade test is added to the upgrade-tests
      suite for running in Autotest.

[33mcommit 70d69f9618f3f86a419b420a475d62f55145b39d[m
Merge: 354646dda6a e2a988c8675
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Thu Feb 20 15:06:55 2014 +0530

    WL#7467: Deprecate (5.6) and remove (5.7) ERROR_FOR_DIVISION_BY_ZERO,
             NO_ZERO_DATE, NO_ZERO_IN_DATE SQL MODES and make their
             functionality part of STRICT MODE
    
    This is the 5.7 version of the patch.
    
    This patch removes the functionality of ERROR_FOR_DIVISION_BY_ZERO,
    NO_ZERO_DATE, NO_ZERO_IN_DATE SQL MODES and makes their
    functionality part of STRICT MODE.
    
    All the tests are updated according to the new behaviour.
    
    NOTE:Here STRICT MODE refers to STRICT_ALL_TABLES and
    STRICT_TRANS_TABLES.
    
    NOTE:Also a small refactoring is done for TIME_NO_ZERO_* flags
    which makes the code conceptually clear and easy to understand.
    Previously in old code there was an implicit dependency between
    MODE_NO_ZERO* and TIME_NO_ZERO* flags and they could be freely
    mixed and the code was confusing and conceptually
    wrong.
    With the elimination of MODE_NO_ZERO_* flags handled in this WL,
    TIME_NO_ZERO_* flags which control the date[1;31mtime[m behaviour
    are also updated.
    This refactoring is also done to keep the existing behaviour if
    SQL_MODE is traditional/STRICT.

[33mcommit 859fa2482883f00851eb4724b1e4e4b8b2c53086[m
Author: Shubhangi Garg <shubhangi.garg@oracle.com>
Date:   Thu Feb 20 01:41:29 2014 +0530

      WL#7440: Updating Query_event and Query_log_event
    
      Problem:
      =======
      In the previous implementation of Query event, we had two copies of the
      following (one in the parent class Query_event,  and the other one in the
      subclass Query_log_event)
     - user
     - host
     - query
     - catalog
     - db
     - [1;31mtime[m_zone_str
    
     This resulted in inconsistencies because these values were updated in the
     subclass, but did not reflect in the parent class. In works correctly if
     libbinlogevent is used only for the purposes of decoding  (since we
     initialize both the copies in the constructors, and then never use
     the data present in the parent class). However, extending the library
     to contain the encoder will create problems.
    
    
      Fix:
      ====
      Removed the duplicate copy from the sub class and update the parent class.

[33mcommit fdb72f031191a67b98b8c1c50970b508e370fd8f[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Feb 18 20:55:16 2014 +0100

    ------------------------------------------------------------
    revno: 4224
    revision-id: mauritz.sundell@oracle.com-20140218152134-5dy6o9jhozcm95zj
    parent: mauritz.sundell@oracle.com-20140217100854-0mgr9qzn8wdsf5hh
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-7.4
    [1;31mtime[mstamp: Tue 2014-02-18 16:21:34 +0100
    message:
      ndb - add support for cmake -DWITH_NDBMTD=0 again
    
      Bug #18267919  CMAKE -DWITH_NDBMTD=0 ... HAVE NO EFFECT
    
      Do not build ndbmtd, ndbsched_mt, mt-send-t if WITH_NDBMTD=0.

[33mcommit 0ced5d39b7ba907adaa124be81b1140396227bcc[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Feb 17 11:03:41 2014 +0100

    revision-id: nisha.gopalakrishnan@oracle.com-20131114050427-535pjj7ek97qhss4
    parent: shaohua.wang@oracle.com-20131114021621-wb2n9etjhxx3tvop
    committer: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
    branch nick: mysql-5.6-17246318
    [1;31mtime[mstamp: Thu 2013-11-14 10:34:27 +0530
    message:
    ndb - test: re-recording results
    
    Re-recording results for ndb_native_default_support and
    ndb_dd_restore_compat due to below change in server.
    
      BUG#17246318: ALTER TABLE SHOULD NOT ALLOW CREATION OF TABLES
                    WITH BOTH 5.5 AND 5.6 TEMPORALS
    
      Analysis
      --------
    
      'ALTER TABLE' allows creation of tables containing temporal
      columns of both mysql-5.5 and mysql-5.6 format.
    
      Recreating tables having both 5.5 and 5.6 temporals becomes
      tedious process when the metadata file(.frm) for the table
      is unavailable. This would involve recreating the table having
      the old temporal columns using 5.5 server instance and then
      adding the 5.6 temporal columns using 5.6 server instance.
      Also recreating tables having 5.5 temporals and 5.6 INNODB
      FULLTEXT indexes has to follow the same process.
    
      Currently operations are supported on the old temporal columns.
      The old temporal columns are upgraded to the new format only
      when they are altered(like renaming the old temporal column)
      else they are retained in the old format.
    
      In order to overcome the above mentioned tedious process
      and use the space efficient new temporal format, the old
      temporal types are upgraded for certain ALTER TABLE
      operations listed in the fix info.
    
      Fix info:
      ---------
    
      The columns of old temporal types of mysql-5.5 are upgraded to
      mysql-5.6 format when ALTER TABLE requests ADD/CHANGE/MODIFY
      COLUMN, ADD INDEX or FORCE operation.
    
      Since such conversion cannot be done using INPLACE algorithm,
      the attempt to use ALGORITHM=INPLACE clause in such ALTER TABLE
      for a table with mysql-5.5 temporals leads to an error.
    
      Also a 'NOTE' is reported to indicate the upgrade of the
      old temporal columns to the new format under the above
      condition.

[33mcommit 4dcdcbe3e8aeef7a5f96d2cb01b30778e19ed514[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Feb 11 15:50:40 2014 +0100

    Fix for Bug#18229003
    
      'MT-SCHEDULER: BLOCK THREADS MAY GET STUCK IN A MUTUAL WAIT-LOCK'
    
    The block-threads managed by the mt-scheduler, communicates by putting
    its signals in a queue (also known as the job-buffer) which
    is set up between all block-threads.
    
    This queue has a fixed max size, such that when the out-queue
    is filled up, the worker thread has to wait for the consumer
    to drain the queue.
    
    We have observed that in a highly loaded system, multiple
    threads may end up in a circular wait-lock due to full out-buffers.
    They are then mutually blocking each other from doing any useful
    work, and the datanode will eventually be killed by a wathdog
    [1;31mtime[mr and declared dead.
    
    This fix detects the situations where we are about to run into
    a situation with circular wait lock. It will then give a
    higher quota of the signal processing to the queues which
    are highly loaded.

[33mcommit c3ca36817ea04c3b91ae078e834f3e516f6a97fc[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Sun Feb 9 10:11:51 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Add transaction's gtid into executed_gtids at binlog flush [1;31mtime[m when
    binlog is enabled and at right after commit [1;31mtime[m when binlog is
    disabled.

[33mcommit 19965922770b2f43107296d9ae9f8460e42de79d[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Feb 4 10:09:12 2014 +0100

    Fix for Bug#18013520 DEBUG CRASH IN OPTIMIZE_TABLE_ORDER::SEMIJOIN_FIRSTMATCH_LOOSESCAN_ACCESS_PATHS.
    Bug introduced in 5.7 in Dec 2013 by the fix for Bug 11762236
    "optimizer use ref joins where it should use eq_ref"
    
    As pre-condition, semijoin loosescan cost-based logic
    semijoin_loosescan_fill_driving_table_position() needs
    Key_use::bound_keyparts/read_cost/fanout to be filled for each key
    (exactly: for the first Key_use of each key).
    This was, so far, guaranteed by a previous call to best_access_path()
    which calls find_best_ref() which updates members of Key_use-s.
    
    But the guilty bugfix has added, in find_best_ref()
      if some heuristic applies
        then choose this Key_use (of clustered pk) _and_leave_the_function_.
    
    Thus, when we leave find_best_ref(), all Key_use-s located after the
    clustered pk in the JOIN_TAB::key_use array, are not updated. Their
    bound_keyparts, for example, is left to what it was in a previous call
    to find_best_ref(), which was possibly in a completely different
    partial plan.
    
    Thus, semijoin_loosescan_fill_driving_table_position() gets wrong
    input data, and in our bug's case, bound_kepyarts has the stale value
    1, so the function selects semijoin LS, whereas, if
    it had good input data (bound_keyparts==2), it would realize that this
    strategy is impossible.
    
    Later, when more plans are explored, the heuristic does not always
    apply, so bound_keyparts some[1;31mtime[ms gets properly reset to 0.
    Finally, the chosen plan is the one with impossible LS.
    fix_semijoin_strategies () is called, wants to setup LS as commanded,
    and crashes because it's impossible (now it sees that it's impossible
    because bound_keyparts is now 0).
    
    It is important to note that the clustered pk chosen by the heuristic
    is not always what LS wants. Look at the crashing query:
    SELECT    table1 . `pk` AS field1
    FROM
    ( C AS table1 INNER JOIN ( ( CC AS table2 STRAIGHT_JOIN D AS
    table3 ON (table3 . `col_varchar_key` = table2 . `col_varchar_nokey`
    ) ) ) ON (table3 . `pk` = table2 . `pk`  ) )
    WHERE (  ( table1 . `col_int_key` , table2 . `col_int_nokey` )  IN
    ( SELECT DISTINCT  SUBQUERY1_t1 . `col_int_key` AS SUBQUERY1_field1 ,
                       SUBQUERY1_t1 . `pk` AS SUBQUERY1_field2
      FROM
      ( CC AS SUBQUERY1_t1 LEFT JOIN CC AS SUBQUERY1_t2 ON
      (SUBQUERY1_t2 . `col_int_key` = SUBQUERY1_t1 . `col_int_nokey`  ) )
      ) ) AND ( table2 . `col_varchar_nokey` = 'd' OR table1
      . `col_int_nokey` IS  NULL )
    GROUP BY field1  ORDER BY table1 . `col_[1;31mtime[m_key` ASC
    
    In our case, the partial plan was:
     "`CC` `table2`", "`CC` `SUBQUERY1_t1`", "`CC` `SUBQUERY1_t2`",
    "`D` `table3`".
    
    The clustered pk SUBQUERY1_t1.pk, though suitable for ordinary
    (non-sj) ref access, is not suitable for LS because it does not
    handle the equality of col_int_key in IN(). LS needs to examine other
    indexes. For example, if there had been an index on (pk, col_int_key)
    it would have been suitable. To examine all indexes, Key_use data must
    be correct for all of them.
    
    Fix: let LS logic force find_best_ref() to not skip any key.
    
    No testcase; it is not a repeatable crash (varying InnoDB
    statistics?); with RQG it is repeatable after few minutes;
    after applying the fix, I ran RQG for 4 hours without any problem.

[33mcommit 0f726e17c06774a3091464a1b45d6ed23db8b6c9[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jan 30 17:32:21 2014 +0100

    Bug#17922198 REMOVE OBSOLETE IFDEF HAVE_PURIFY CODE.
    
    Patch #15
    Remove all #ifndef HAVE_purify around DBUG_PRINT and DBUG_DUMP.
    In order for these to actually generate a warning,
    one would have to run with valgrind and debug at the same [1;31mtime[m,
    *and* enable the specific debug keywords.

[33mcommit 2658967e3bab5f2bd4a366d41562457130468093[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri Jan 24 13:09:51 2014 +0530

    BUG#17863690 - ASSERTION `THR_THD_INITIALIZED' FAILED.
    
    FIX AND DESCRIPTION:-
    i_rpl.rpl_innodb_disabled sporadically fails with the
    assertion THR_THD_INITIALIZED. In the course of execution
    this MTR test, mysqld server is restarted multiple [1;31mtime[ms.
    This involves the shutdown of server and there are some
    connections (including connections established at start of
    MTR) that are waiting on network read event.  If some
    connections are not terminated gracefully, connections are
    closed and waited until the thd list goes empty. Once thd
    list becomes empty, there could be some number of threads
    could be executing the thd destructor while the main
    shutdown thread could go ahead with the cleanup including
    the destruction of THR_THD and THR_MALLOC pthread keys
    which causes the assertion.
      All connection related threads (including the replication
    threads call) call the function release_resource after which
    thd is removed from the global list and hence move restore_
    globals into the release_resources to fix the test case which
    indicates ASSERTION `THR_THD_INITIALIZED' FAILED. This patch
    also moves Relay_log_info::end_info and mysql_audit_free_thd
    into release_resources.

[33mcommit 8e5300f7a5fb2cd140be5ee358fc0fb5326e4dba[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 23 14:28:57 2014 +0100

    ndb - cherrypick: disable optimization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
    revision-id: mauritz.sundell@oracle.com-20140123120237-l5p6o1m3kg8apvu3
    parent: mauritz.sundell@oracle.com-20140115081453-6cglx3zmar7ovdin
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-6.3
    [1;31mtime[mstamp: Thu 2014-01-23 13:02:37 +0100
    message:
      ndb - disable optimization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
      Bug #18122881  REVERT FIX FOR BUG #18055285 FOR LOWER VERSIONS (<4.8) OF GCC
    
      GCC_VERSION was not always defined before include of Ndbfs.cpp as in VoidFs.cpp
      so also change to test __GNUC__ and __GNUC_MINOR__ that are predefined for gnu
      compiler.
    
      note that for non gnu compiler __GNUC__ and __GNUC_MINOR__ will be treated as 0 in arithmetics
      and so optimization not disabled.

[33mcommit 0f460bd66c81c74c167595bedf9531585dc38684[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 23 14:10:57 2014 +0100

    ndb - cherrypick: disable optimization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
    revision-id: mauritz.sundell@oracle.com-20140123120237-l5p6o1m3kg8apvu3
    parent: mauritz.sundell@oracle.com-20140115081453-6cglx3zmar7ovdin
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-6.3
    [1;31mtime[mstamp: Thu 2014-01-23 13:02:37 +0100
    message:
      ndb - disable optimization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
      Bug #18122881  REVERT FIX FOR BUG #18055285 FOR LOWER VERSIONS (<4.8) OF GCC
    
      GCC_VERSION was not always defined before include of Ndbfs.cpp as in VoidFs.cpp
      so also change to test __GNUC__ and __GNUC_MINOR__ that are predefined for gnu
      compiler.
    
      note that for non gnu compiler __GNUC__ and __GNUC_MINOR__ will be treated as 0 in arithmetics
      and so optimization not disabled.

[33mcommit 05a75687ce286b5edf97ec78cc320c36fea9a830[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Jan 23 11:15:08 2014 +0100

    Bug#18047020: BROKEN BUILD WITH CLANG 3.4
    
    Fix a compilation error in extra/yassl/taocrypt/include/run[1;31mtime[m.hpp
    and a number of new warnings about unused functions/variables.
    
    Also removed all MTR suppressions for __cxa_pure_virtual.
    They are no longer needed as these tests use DBUG_SUICIDE to
    terminate the server rather than exit(1). See Bug#11759828

[33mcommit bc02f6c733c2c63a2a7073bc3d46d23a2c47365d[m
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Mon Jan 20 20:11:45 2014 +0530

    Bug#16767011 - ROW INCORRECTLY NOT UPDATED BY MULTI-TABLE UPDATE
    
    Problem:-
    Multiple table updates do not update under certain conditions.
    
    Analysis:
    In our multi update query, with join order a*b*c. We will select
    first join table (which is 'a' here, which here I am calling as
    main-table) as the table to be updated at the [1;31mtime[m of fetching,
    on the basis of some condition(which is checked in
    safe_update_on_fly()) and for all other table we make temporary tables.
    
    
    In this bug, when one of the field of main-table is
    going to be updated and that field is in where condition,
    with some other field will cause incorrect result.
    
    For example:
    CREATE TABLE t1 (c1 INTEGER , c2 INTEGER );
    CREATE TABLE t2 (c1 INTEGER , c2 INTEGER);
    INSERT INTO t1 VALUES(1,1),(1,4),(1,5);
    INSERT INTO t2 VALUES(11,1),(13,1),(15,1);
    UPDATE t1 JOIN t2
    SET
    t1.c1=30,
    t2.c2=40
    WHERE t1.c1=t2.c2 ;
    
    here first, we fetch the value of t1(1,1) and then the value of
    t2(11,1) and compare it. If it matches we change the value of t1
    from (1,1) to (1,30) and for t2 we will change the value in
    temp_table(11,40). Later when we fetch the value of t2
    again(that is 13,1) and compare to t1(that is 1,30).it doesn't
    match, so we can't update it.
    
    So,
    SELECT * FROM t2;
    c1      c2
    11      40
    13      1       <--value doesn't change
    
    Solution:-
    In safe_update_on_fly(), check if any field of main-tables is going
    to be updated and that field is in where condition with any other
    field in where clause, don't use that table as a main table and make
    a temporary table for it.

[33mcommit fa67c60284129d1d9bf11116580f8537c15cf849[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 9 22:26:55 2014 +0100

    ndb - reverting patch
    
    revision-id: ole.john.aske@oracle.com-20140109145657-6ey2qmpqap0jmykc
    parent: ole.john.aske@oracle.com-20140108150924-mabnupd65qe6gcvs
    committer: Ole John Aske <ole.john.aske@oracle.com>
    branch nick: mysql-5.1-telco-7.0
    [1;31mtime[mstamp: Thu 2014-01-09 15:56:57 +0100
    message:
      Fix for bug#18053050  POTENTIALLY SERIOUS COMPILER WARNING: "_FILE_OFFSET_BITS" REDEFINED

[33mcommit 989a691c71b8301c102ca6883766ed20a3b945d0[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Jan 7 14:58:22 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Terminate compression thread by invoking pthread_join(...).
    MySQL main thread will wait until [1;31mtime[mout on condition variable
    'COND_terminate_compress_thread' if it missed signal from
    compression thread, which will cause [1;31mtime[mout of some test cases
    after hundreds repeat.

[33mcommit 4267a9a491b139d76fc65d4f42bf4a51409d1110[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Jan 6 14:59:10 2014 +0530

    Bug#17510411: AN INDEPENDENT INSTANCE WITH SEMISYNC ENABLED
    STILL NEED ACK FROM SLAVE
    
    Problem:
    ========
    When Semi synchronous replication is configured on an
    independent server without any slave and
    "rpl_semi_sync_master_wait_no_slave" is set to "off" the
    master still waits for an ack from the slave.
    
    Analysis:
    ========
    As per the documentation if the value of the variable
    "rpl_semi_sync_master_wait_no_slave" is set to OFF, the
    master reverts to normal replication if the slave
    count drops to zero during the [1;31mtime[m out period. But in the
    existing code the above changes are not implemented.
    
    Fix:
    ===
    When "rpl_semi_sync_master_wait_no_slave" is set to off
    check if semi sync is switched on or not if on then check if
    any slaves are connected or not. If not connected switch_off
    the semi sync and turn it on when user sets the variable to
    on.

[33mcommit 54dfa8c0a1547dcfd634f9e2867eb7c24786dd1d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jan 6 09:53:38 2014 +0100

    Fix for Bug#17857442: MULTITHREADED JOB SCHEDULER SHOULD HANDLE LEAPS IN CLOCK TIME
    
    We have seen customer bug reports, like bug 17475425, where it
    looks like delayed signal processing from the '[1;31mtime[m_queue' comes
    completely out of control: There seems to be two scenarios:
    
     - Processing of delayes signals stops completely for a long periode,
       or forever. This will cause issues like the disk write speed throttling
       to completely halt any disk activity (Local checkpoints stop)
    
     - We see a burst of signals which should have been delayed relatively
       to each other, executed withing the same clock second.
    
    Inspecting the mt-scheduler code, we find that this may happen if the
    NdbTick clock moves forward or backwards in large leaps. When the [1;31mtime[m
    moved backwards, it will stop the [1;31mtime[m_queue handling until the
    same amount of clock [1;31mtime[m has passed again. A forward leap will force
    the scheduler to simulate that the same amount of [1;31mtime[m passing by expiring
    all [1;31mtime[m_queue event in the elapsed periode.
    
    Such [1;31mtime[mr leaps might happen if the platform does not support monotonic
    [1;31mtime[mrs, or we didn't implement the usage of these on the specific platform.
    Windows used to be such a platform prior to fixing bug 17647637. However,
    there are still such platforms, OSX seems to be one of these, and possible
    some Linux variants. Furthermore there seems to be several OS/HW/VM bugs
    related to monotonic [1;31mtime[mrs not always being monotonic after all. And
    there might be [1;31mtime[m leaps due to CPU starvation such that we are stuck
    for a long [1;31mtime[m - So this has to be expected and handled somehow.
    
    Looking at the single threaded scheduler we find that it actually handle
    such leaps - see ThreadConfig::scanTimeQueue(). This logic doesn't seem
    to have been ported to the mt-version !! The single threaded handling
    is to accept a backtick as the new current [1;31mtime[m. A forward leap of more
    than 1500ms will be consumed by resetting the current [1;31mtime[m to 'now-1000'ms
    and continue from there.
    
    This fix introduce the same handling in the MT-scheduler.
    
    Furthermore, we also found that the mt-scheduler allowed itself to
    yield CPU, or wait for receiving signals, even if it has possibly
    expired event to handle from the [1;31mtime[m_queue.
    (see: 'lagging_[1;31mtime[mrs')
    
    We are not sure about whether this has any ill effects - However,
    it doesn't seem like a good idea to let possible expired signals
    linger any longer than necessary - also fixed.
    
    Also removed a ndbout_c() which was a leftover from previous debugging.

[33mcommit c722778c7c56bf41500a90e2b3a90896f963ea4a[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun Jan 5 21:14:21 2014 +0800

    WL#6630 Semisync separate acks collector
    
    FEATURE
    =======
    Before this feature, acknowledgments were received in dump threads. After
    sending an acknowledgment request, dump threads needed to receive the
    acknowledgment immediately. So the following events were delayed and
    delaying [1;31mtime[m was depended on the network situation.
    
    All slave connections are based on TCP which is duplex. It means we
    can send binary events and receive acknowledgments simultaneously.
    In this way, binary events can be sent without delay.
    
    This worklog implement above feature.
    
    DESIGN
    ======
    * Start/Stop ack receive thread
      The thread is controlled automatically by semisync master.
    
      - It is started automatically when enabling semisync master through
        SET rpl_semi_sync_master_enabled = ON
    
      - It is stopped automatically when disabling semisync master through
        SET rpl_semi_sync_master_enabled = OFF
    
    * Show ack receive thread status to performance_schema.threads
    
      - Ack receive thread status can be showed through querying
        performance_schema.threads table, when semisync master is on(ack thread is
        up).
    
      - PROCESSLIST_STATE contents
        1. Waiting for semi-sync slave connection.
        2. Waiting for semi-sync ACK from slave.
        3. Reading semi-sync ACK from slave.

[33mcommit 2aaa1064cc1f461755f2a660c64a1c832cede636[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Dec 19 13:59:01 2013 +0100

    Fix for bug#17973819  NEED TO ADAPT TO MONOTONIC TIMERS NOT ALWAYS BEING MONOTONIC
    
    Also a followup fix for bug#17647637, NDBTICK_CURRENTMICROSECOND() MUST BE MONOTONIC
    
    This fix handle that Monotonic [1;31mtime[mrs seems to have bugs on several platforms
    which might result in the monotonic clock doing small jumps back in [1;31mtime[m.
    This is normally due to imperfect syncing of the clock between multiple
    CPU cores.
    
    Such small backticks are not really harmfull for our scheduler and watchdog
    algorithms, so we make the backtick protection less strict in this patch.
    However, we will still assert that the backtick is less than 10ms in the
    NdbTick_Elapsed() calculation.
    
    We removed redundant checks for backticks in several '[1;31mtime[mdiff'
    methods as such a check already exist in NdbTick_Elapsed() which is
    used in the same place.
    
    We also removed some assert('not backtick') where they are used
    in the same place where NdbTick_Elapsed() was called for the
    same reason as above.

[33mcommit a3407f6eeb9515c1d686f430aff85c8e1f115c11[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Dec 18 11:38:51 2013 +0100

    ndb - re-record test result for main.ctype_cp932_binlog_row
    
    Test is not run without adding --mysqld=--binlog-format=row to mtr command line.
    
    log event Write_rows changed to Write_rows_v1 by
    
    revision-id: frazer.clement@oracle.com-20110921101158-0js4hali1q25aa1e
    [1;31mtime[mstamp: Wed 2011-09-21 11:11:58 +0100
    message:
      WL5353 : Implement v2 of Binlog row events

[33mcommit fba37ef976d4c24d9f94c286b9dfad2fb46cfe5b[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Wed Dec 18 14:36:19 2013 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    
    1. Create a mysql system table named gtid_executed, a row in the table can
    contain an interval of several GTIDs, all having the same SID and consecutive
    GNOs as following:
    
      CREATE TABLE gtid_executed(
        sid CHAR(36) NOT NULL,       -- Source ID
        gno_start BIGINT NOT NULL,   -- First GNO of interval
        gno_end BIGINT NOT NULL,     -- Last GNO of interval
        PRIMARY KEY(sid, gno_start)  -- PK on interval start
      ) ENGINE = InnoDB;
    
    
    2. Store gtids into the gtid_executed table always if gtid_mode is enabled as
    following:
      INSERT INTO gtid_executed VALUES (SID, GNO, GNO)
      - If binlog is disabled and gtid_mode is enabled, store gtids from master's
    transactions, relayed BINLOG transactions and 'SET @@SESSION.GTID_NEXT'
    statement into gtid_executed table right before transaction prepare.
      - If binlog is enabled and gtid_mode is enabled, store GTIDs into both binlog
    and gtid_executed table.
    
    
    3. The gtid_executed table would be filled with singleton interval as we cannot
    use UPDATE instead of INSERT, because any concurrently executing transactions
    would conflict on the update. So we propose to create a FOREGROUND thread (we
    can not create a BACKGROUND thread as we have to create THD object to open
    gtid_executed table) and introduce a user variable
    'executed_gtids_compression_period' to control how often the thread should be
    woken up to compress the gtid_executed table regularly.
      - To the user variable 'executed_gtids_compression_period', if value is 0,
    the thread never wakes up, if value is 1, wakes up every transaction. If value
    is 1000, wakes up every 1000 transactions.
      - We also added the thread's state info (suspending, compressing gtid_executed
    table) into performance_schema.threads table, so that user can monitor it and
    tune the user variable 'executed_gtids_compression_period' (The thread is in
    suspended mode most of the [1;31mtime[m, wakes up only when it needs).
      - THe thread runs always, user cannot kill it as we didn't add it into
    PROCESSLIST.
      - The thread opens a transactional context to execute the following within a
    single transaction on an non-autocommit mode:
        - Read each row by the PK in increasing order, delete consecutive rows from
    the gtid_executed table and fetch these deleted gtids at the same [1;31mtime[m.
        - Store compressed intervals from these deleted gtids into the gtid_executed
    table.
    
    
    4. Report @@GLOBAL.GTID_EXECUTED and @@GLOBAL.GTID_PURGED from both binlog and
    gtid_executed table as following:
      - GLOBAL.GTID_EXECUTED = all gtids of gtid_executed table;
      - GLOBAL.GTID_PURGED = GLOBAL.GTID_EXECUTED - (GTID_EXECUTED_BINLOG -
    GTID_PURGED_BINLOG);
      (GTID_EXECUTED_BINLOG: gtid_set of gtids is ever logged in binary logs.
       GTID_PURGED_BINLOG  : gtid_set of gtids is purged from binary logs.)
    
      - GLOBAL.GTID_EXECUTED is initialized with all gtids of gtid_executed table
    during server restarting. Every transaction's gtid is added into the
    GLOBAL.GTID_EXECUTED and gtid_executed table if gtid_mode is enabled after
    server restarts.
      - GLOBAL.GTID_PURGED is initialized with gtids from gtid_executed table and
    binlogs and precomputed as above during server restarting. After server
    restarts, added purged gtids into GLOBAL.GTID_PURGED when purging logs
    (regardless binlog is enabled or not), and added every transaction's gtid into
    GLOBAL.GTID_PURGED if binlog is disabled and gtid_mode is enabled.
      - A previous log event with GTID_EXECUTED_BINLOG (GTID_EXECUTED -
    GTID_ONLY_IN_TABLE) set is created when rotating binlog, so GTID_ONLY_IN_TABLE
    variable is maintained when binlog is enabled. GTID_ONLY_IN_TABLE is initialized
    with difference gtids from gtid_executed table and the latest binlog during
    server restarting. GTID_ONLY_IN_TABLE will never change since starting server.
    (we don't need maintain GTID_ONLY_IN_TABLE when binlog is disabled)
      - If gtid_executed_table is empty, add all gtids in GTID_EXECUTED_BINLOG into
    gtid_executed table during server restarting (Handle the upgrade case, and the
    case that a slave is provisioned from a backup of the master and the slave is
    cleaned by RESET MASTER and RESET SLAVE before this.).
      - The gtid_executed table is also initialized with gtid_purged when user is
    initializing it through SET GLOBAL gtid_purged. (It is possible to update the
    value of gtid_purged, but only by adding GTIDs to those already listed, and only
    when gtid_executed is unset—that is, on a new server.)
    
    
    5. The gtid_executed table is reset when resetting master.
    
    
    6. Gtid_table_persistor class
       It manages all operations on the gtid table.
    
       - m_count
         Count the append rows of the table.
    
       - save(Gtid *gtid)
         Insert the gtid into table.
    
       - save(Gtid_set *gtid_set)
         Store gtid set into the table.
    
       - compress()
         Compress intervals into consecutive GNOs in the table.
    
       - reset()
         Delete all rows from the table.
    
       - fetch_gtids_from_table(Gtid_set *gtid_set)
         Fetch gtids from the table and store them into gtid set.

[33mcommit 20e12a891fae7d95b63e57f53618fd77aef31ca8[m
Author: Tarique Saleem <tarique.saleem@oracle.com>
Date:   Mon Dec 16 17:54:43 2013 +0530

    skipped the test perfschema.sizing_growth for query_cache enabled as it failing on daily and weekly platforms. Logged a Bug #17958039 for tracking this issue until that [1;31mtime[m.

[33mcommit 41e21a65989973258d2553b8e35fc21ff063964c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Sat Dec 14 11:18:25 2013 +0100

    ndb - adopt server bugfix to maintained version of ha_ndbcluster.cc
    
    revision-id: jon.hauglid@oracle.com-20131030232243-vyz4pd5yihqlmnjx
    parent: sivert.sorumgaard@oracle.com-20131030120943-g0i7td24pbfv1wjj
    committer: Jon Olav Hauglid <jon.hauglid@oracle.com>
    branch nick: mysql-5.6-bug17696705
    [1;31mtime[mstamp: Thu 2013-10-31 00:22:43 +0100
    message:
      Bug#17647863: MYSQL DOES NOT COMPILE ON OSX 10.9 GM
    
      Rename test() macro to MY_TEST() to avoid conflict with libc++.

[33mcommit ff3113d86537ef4d7057a17b0612dc862b34c747[m
Merge: 9bcbeda2e06 779fa3a1f5a
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Dec 12 09:20:29 2013 -0800

    merge 7.2 => 7.3 disable flaky clusterjpa [1;31mtime[mstamp test

[33mcommit 779fa3a1f5a8ccd514876254f533cad9c091b9ac[m
Merge: 962331ac80c 4b00f265509
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Dec 12 09:12:33 2013 -0800

    merge 7.1 => 7.2 fix flaky clusterjpa [1;31mtime[mstamp test

[33mcommit 4b00f265509cde125e125d6d8f39acc651e3cc81[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Dec 12 09:05:12 2013 -0800

    Disable flaky [1;31mtime[mstamp test for clusterjpa

[33mcommit fea7607a10ac4a2dd7724c65ba13c26b3400346b[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Wed Dec 11 23:47:39 2013 +0530

    Bug#17935793 UT_GET_NAMEL() IS NOT THREAD SAFE
    Problem: Static variable is used in ut_get_namel(), which is not thread safe.
             Also replace the call of ut_print_[1;31mtime[mstamp() by ib_logf(), wherever in
             InnoDB if possible.
    
    Fixed: Remove static identifier from ut_get_namel().
           Also fixed ut_print_[1;31mtime[mstamp().
    
    Approved by Kevin (rb#4151).

[33mcommit d252432836a1ff2ebc4199598a88537a3afd21fd[m
Merge: f6d0b4c6745 0619a7f42e8
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Wed Dec 11 13:40:47 2013 +0530

    Bug#16936961 INCORRECT TRANSACTION ACTIVE TIME FOR RECOVING TRANSACTION
                 AFTER CRASH
    Problem: The start_[1;31mtime[m member of trx_t structure is uninitialized in
             trx_create(), that is, at the [1;31mtime[m of transaction object creation.
             The start_[1;31mtime[m was initialized only at the start of transaction.
             So after crash, start_[1;31mtime[m was taking the garbage value and while
             rollback is running in background for uncommited trx.
    
    Fixed: Initialized the start_[1;31mtime[m member in trx_resurrect_*(), when trx
           is in either ACTIVE or PREPARED STATE.
    
    Approved by Jimmy (rb#4046).

[33mcommit eac27848c2364286524ab68e45676d3d31a6a98b[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 10 16:01:51 2013 +0100

    Followup fix for bug#17647637, NDBTICK_CURRENTMICROSECOND() MUST BE MONOTONIC
    
    Previous fix introduced a warning being printed from NdbTick_Init()
    if we were unable to find a monotonic [1;31mtime[mr to be used for the 'ticks'.
    OSX is one if the platforms where we have to fallback to the non-monotonic
    get[1;31mtime[mofday() and thus prints this warning.
    
    We find from recent PB2 tests, that not only the datanode binaries
    ndbd/dbbmtd, but also clients as ndb_mgm need [1;31mtime[mrs, and thus get
    this warning printed as part of the startup.
    
    As this is handled as part of the output from these clients, several
    tests like 'ndb_mgm -e backup' fails with verification failures.
    
    This fix removes these warning, and instead introduce NdbTick_IsMonotonic().
    This function is now used inside Watchdog.cpp to check the [1;31mtime[mr capabilities
    and print a warning from here. As the Watchdog is only part of the ndb's,
    this warning will go into the ndb.log file, and not to the terminal.
    
    Patch also adds some extra asserts to NdbTick to
    ensure that NdbTick_Init() has been called prior to the other NdbTick
    functions, and that the windows part of NdbTick_Init() returned sensible
    values. This was added in order to smoke out some Pb2 failures being
    observed on Win32/64.

[33mcommit cdff3c67f58eb401e7a326ac42b954a34dac1a98[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 10 15:38:02 2013 +0100

    Fix for Bug#17842035 LCP WATCHDOG SHOULD USE REAL CLOCKTIME
    
    Recommit of fix updated according to Frazers review comments
    
    The LCP watchdog implemented by Dblqh::checkLcpFragWatchdog is intended to be
    called by the scheduler event mechanism every 10th s. Every [1;31mtime[m it is
    called it increment its own 'pollCounter', assuming that 10s has passed.
    
    When this counts reaches a limit representing that either a 'WarnPeriod'
    (default 20s), or 'Timeout Limit' (60s) has passed, either a LCP warning
    is printed, or the datanode aborted due to 'SCAN_WATCHDOG_FAIL
    
    However, there are no guarantee that this [1;31mtime[mr fires at the
    specified 10s interval.:
    
    - There might be CPU contention which prevents the job scheduler
      from running. which results in a longer delay.
    
    - If the scheduler was 'late', as above, it will try to make up
      for that by running the internal scheduler [1;31mtime[mr faster.
      In some cases it might even cause a delayed event to fire
      immediately.
    
    To overcome these limitation the watchdog has been refactored to
    keep track of its own start[1;31mtime[m, and calculate elapsed [1;31mtime[m by
    reading the clock every [1;31mtime[m it is called.
    
    Furthermore, it will now absorbe a 'backtick' (not likely to occure)
    by taking the backwards [1;31mtime[m as new 'current [1;31mtime[m' and calcule 'elapsed'
    [1;31mtime[m for this round as '0'.
    The ill effect of a forward leap, which possibly could expire the watchdog
    [1;31mtime[mr immediately, is reduced by never calculating an elapsed [1;31mtime[m
    longer than the requested 'delay' [1;31mtime[m of the watchdog [1;31mtime[mr.

[33mcommit 2f959856864cd836c243fc7ca04ec1bc319d4318[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 10 15:37:08 2013 +0100

    Fix for bug#17647469
      'GCP LAG' WATCHDOG (DBDIH::CHECKGCPSTOPLAB) SHOULD USE REAL CLOCK TIME
    
    Recommit of fix updated according to comments from Frazer
    
    The GCP watchdog implemented by Dbdih::checkGcpStopLab is intended to be
    called by the scheduler event mechanism every 100th ms. Every [1;31mtime[m it is
    called it increment its own 'invocation counter'. When these counts reaches
    (n*10) == <[1;31mtime[mout limit>, the GCP LAG warning is printed, or the 'crashSystem'
    action taken.
    
    However, there are no guarantee that such [1;31mtime[md events actually are called
    after the specified delay:
    
     - There might be CPU contention which prevents the job scheduler
       from running. which results in a longer delay.
     - If the scheduler was 'late', as above, it will try to make
       up for that by running the internal scheduler [1;31mtime[mr faster.
       In some cases that might even cause a delayed event to fire
       immediately.
     - The resolution of the OS [1;31mtime[mrs may be to coarse to
       reliable being able to hit near the 100ms mark.
    
    To overcome these limitation the watchdog has been refactored to
    keep track of its own start[1;31mtime[m, and calculate elapsed [1;31mtime[m by
    reading the clock every [1;31mtime[m it is called.
    
    Furthermore, it will now absorbe a 'backtick' (not likely to occure)
    by taking the backwards [1;31mtime[m as new 'current [1;31mtime[m' and calcule 'elapsed'
    [1;31mtime[m for this round as '0'.
    The ill effect of a forward leap, which possibly could expire the watchdog
    [1;31mtime[mr immediately, is reduced by never calculating an elapsed [1;31mtime[m
    longer than the requested 'delay' [1;31mtime[m of the watchdog [1;31mtime[mr.

[33mcommit 8f00933dfae1a390c1547c1a6c3d2142222f8e51[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Fri Dec 6 18:11:35 2013 +0530

    WL#6755 : Use of error logging API in InnoDB
    
    a) Replace all possible occurrence of fprintf(stderr, "<MSG>") with
        ib_logf().
    b) Replace all possible occurrence of fputs() that redirect error/warning
       to stderr stream with ib_logf().
    c) Get rid of ut_print_[1;31mtime[mstamp(), where ever possible in InnoDB.
       As ib_logf() take care of emitting [1;31mtime[mstamp with each message output.
    
    Approved by: Krunal, Allen and Kevin (rb#3682).

[33mcommit b58b689fff616b84ab785fe84fa9740f2f259473[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Dec 5 10:23:39 2013 +0100

    Bug#17606098: DEADLOCK INVOLVING SRV_MONITOR_THREAD AND LOCK_THD_DATA
    
    Post-push fix: Fix valgrind warnings.
    
    The patch for Bug#17606098 changes the size of the buffer allocated for THD's query string
    to match the size of the string. Before it was larger to accomodate data needed for the
    query cache. Reducing the size of the buffer triggered valgrind warnings in the lexer
    because it some[1;31mtime[ms read past the end of the query string. Before this meant reading
    into the query cache data, now it means reading past the allocated buffer.
    
    This post-push fix adds a missing eof check to the lexer.

[33mcommit 4d9cfe106ce68452f6f092106dc25439505463e3[m
Author: mayank prasad <mayank.prasad@oracle.com>
Date:   Thu Dec 5 14:19:24 2013 +0530

    WL#5768 : PERFORMANCE SCHEMA, prepared statements instrumentation.
    
    Details:
     - Added code to get pointer to prepared statement stats in prepared_stmt_array
       once and then used that every[1;31mtime[m execute statement is called on that
       preapred statement. This will avoid calling find_or_create()
       function every[1;31mtime[m Execute statement is called.

[33mcommit 8ebd899acc571704897f4113f123b72ac189c240[m
Author: Shubhangi Garg <shubhangi.garg@oracle.com>
Date:   Thu Dec 5 12:24:35 2013 +0530

     BAPI 82: Linking binlog event processing library to the MySQL server
    
    
    The patch fixes the following in the Cmake files for libbinlogapi
    
    - Macro specifying rules to run at install [1;31mtime[m, for the static version of
      the library is removed, for as of now, it is not used outside the MySQL
      server
    
    - Destination for the shared library install is changed from libbinlogapi/lib
      to /lib
    
    - Fixed indentation
    
    - Fixing style, where, following the MySQL server convention, the cmake macros are
      in uppercase.

[33mcommit ea738404f00578b5f241c1c62c5fbf0a209a2418[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Nov 22 13:35:35 2013 +0100

    Part2 of fix (the real fix) for bug#17647637:
    
    'NDBTICK_CURRENTMICROSECOND() MUST BE MONOTONIC'
    
    Use 'QueryPerformanceCounters' on Windows instead of GetSystemTimeAsFileTime()
    to implement the NdbTick [1;31mtime[mrs. This will ensure that the Windows [1;31mtime[mrs are
    monotonic.
    
    Also removes the 'need_monotonic' argument for NdbTick_Init() and
    NdbCondition_initialize(). Now  we always tries to use a monotonic
    [1;31mtime[mr, and print a warning if this was not possible.
    
    Furthermore, CLOCK_HIGHRES is introduced as an alternative for
    CLOCK_MONOTONIC if that is not available on the platform (Solaris 9).
    
    NdbTick.cpp was refactored such that there are not two platform dependent
    versions of NdbTick_Init() and NdbTick_getCurrentTicks() any more.

[33mcommit 959dcb9a72bf1bc1d8f0583e49b859967049abf0[m
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Fri Nov 22 17:11:34 2013 +0530

    Bug#17602807 - (REMAINING_TABLES_AFTER != 0) || ((CUR_EMBEDDING_MAP == 0)
                   && (JOIN->POSITIONS[I
    
    Problem:
    Aggregate function in subquery join condition, some[1;31mtime[m cause an assert.
    
    Analysis:
    The assert happens in
    Optimize_table_order::best_extension_by_limited_search(), as it can not
    determine best plan due to wrong inner table dependences.
    The inner tables of the outer join (d2 and z) should be dependent on the
    outer table zz but in this case, this does not happen as inner tables
    also have no dependencies as well.
    
    The fix is to exclude all three pseudo table bits
    (OUTER_REF_TABLE_BIT|PARAM_TABLE_BIT|RAND_TABLE_BIT) mask from condition
    which checks if table dependences should be updated.
    
    
    Note:
    The same problem occurred in the case of (bug48483).

[33mcommit fdfabb16672d35bc76ba5cf24975d4d08893d44e[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Nov 21 17:09:21 2013 +0100

    Fix for Bug#16988465 ASSERT `LENGTH > 0 && KEYPARTS != 0' FAILED IN CREATE_REF_FOR_KEY() ON 2ND EXEC
    
    Here is the crashing sequence:
    
    PREPARE prep_stmt FROM '
      SELECT alias1.pk AS field1
      FROM (t2 AS alias1, t1 AS alias2, t1 AS alias3)
      WHERE  (alias3.col_int_key, alias1.col_int_nokey) IN (
        SELECT sq1_alias1.pk AS sq1_field1,
          MAX(sq1_alias1.col_int_key) AS sq1_field2
        FROM (t2 AS sq1_alias1
          RIGHT OUTER JOIN t1 AS sq1_alias2
          ON (sq1_alias2.col_int_key = sq1_alias1.col_int_nokey) )
        WHERE sq1_alias1.col_int_key IN (
          SELECT sq2_alias2.col_int_nokey AS sq2_field1
          FROM (view_t3 AS sq2_alias1
            STRAIGHT_JOIN t1 AS sq2_alias2
            # please focus on the line below:
            ON (sq2_alias2.col_int_key = sq2_alias1.col_int_key) )
          WHERE sq2_alias1.col_varchar_nokey = sq1_alias2.col_varchar_nokey
        )
      )
    ';
    
    EXECUTE prep_stmt;
    EXECUTE prep_stmt;
    
    During preparation, nothing notable happens.
    During the first execution:
    1) In name resolution, sq2_alias1.col_int_key, originally an
    Item_field created at parse [1;31mtime[m, is replaced with
    Item_direct_view_ref itself pointing to an Item_field which is
    t3.col_int_key (created by view merging).
    2) The most inner subquery (select_lex number 3) is transformed into a
    semi-join. This does fix_after_pullout () on the Item_direct_view_ref,
    which itself calls fix_after_pullout () on the pointed Item_field. As
    a side effect, both items receive a new "context" member, whose
    select_lex member is the middle subquery (select_lex number 2, owner
    of the semi-join), which is correct.
    3) Execution terminates correctly.
    4) rollback_item_tree_changes () runs, rolls back the replacement done
    above; this puts back into the equality item, the Item_field built at
    parse [1;31mtime[m, which still has its context pointing to select_lex number
    3:
    
    Breakpoint 16, THD::rollback_item_tree_changes
    change->old_value = $35 = (Item_field *) 0x7fff98017590
    change->place = $36 = (Item_direct_view_ref *) 0x7fff98039570
    change->place = $37 = (Item **) 0x7fff98017740
    (gdb) p change->old_value->context->select_lex->select_number
    $38 = 3
    (gdb) p (*(change->place))->context->select_lex->select_number
    $39 = 2
    (gdb)
    
    It is not normal, because a semi-join is a permanent transformation,
    so select_lex number 3 does not exist anymore. The context of this
    Item_field, still pointing to the now-gone select_lex, leads to such
    strangeness when the second execution starts:
    
    Breakpoint 2, Item_field::fix_fields (this=0x7fff98017590, thd=0x7fff98000c50, reference=0x7fff98017740)
    (gdb) p context->select_lex->select_number
    $40 = 3
    (gdb) p select_lex->select_number
    $41 = 2
    (gdb) f
    #5  0x00000000009218fb in JOIN::prepare
    You can see that, while we are doing resolution of the select_lex
    number of 2, we are meeting this item which points into the now-gone
    inner subquery.  This is the root cause of the crash in
    create_ref_for_key () during the second execution.
    
    If the query is changed, by turning the join condition:
    "ON (sq2_alias2.col_int_key = sq2_alias1.col_int_key)"
    of the most inner subquery, to a part of the WHERE clause of this same subquery, then there is no error.
    
    The reason is that items in the WHERE clause use the "generic context"
    associated with the select_lex, and such generic context is properly
    repointed in convert_subquery_to_semi join ():
      /*
        Update the resolver context - needed for Item_field objects that have been
        replaced in the item tree for this execution, but are still needed for
        subsequent executions.
      */
      for (st_select_lex *select= parent_select->removed_select;
           select != NULL;
           select= select->removed_select)
        select->context.select_lex= parent_select;
    so if the Item_field is in WHERE, its context is properly repointed above.
    But items in a join condition use some specific context (so that name
    resolution in the join condition is restricted to joined tables). And
    such context is not repointed, which is the real problem.  The fix is
    to repoint it. It is already in the tree for worklog 7082, so is copied
    here.

[33mcommit 8e0d81232d4874de3a2494bccb36306a23528b58[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Wed Nov 27 18:37:32 2013 +0530

    Bug#17713784 SEGMENTATION FAULT IN SYNC_ARRAY_CELL_PRINT
    Problem: There was race condition for DebugPolicy::enter().
             Also, the mutex might be released at the [1;31mtime[m of printing the
             mutex information.
    Fixed:  Update the DebugPolicy object only after acquiring the mutex,
            and add a check to print "NULL" string for DebugPolicy::m_file_name
            if the mutex is released when printing the mutex information.
    
    --Approved over IM by Marko.

[33mcommit e17e8482cfaa3f0798ed3f6503e6ec0cb88e56d5[m
Merge: 1ab7cd0d506 62d7da5a773
Author: mayank prasad <mayank.prasad@oracle.com>
Date:   Tue Nov 26 22:30:56 2013 +0530

    Bug# 17794846 : SERVER CRASHES WHEN SOME MTR TESTS ARE RUN WITH THE OPTION --COMPRESS
    
    Issue
     - When compressed protocol is on b/w client and server,
       packets are broken into multiple packets and send to
       and fro. P_S code was written to start a statement
       instrumentation after every packet header is read and an
       idle instrument before every packet header is read.
       In case of Compressed Protocol, packets were broken
       into multiple sub packets and for every sub packets header
       read, statement instrument was activated causing code to ASSERT.
    
    Fix
     - Made sure that statement instrument is enabled only first [1;31mtime[m
       packet header is seen and not enabled after that for further sub
       packets.

[33mcommit 0d2a88c18228c5e96a98f45058fd8c45b5ddd83c[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Nov 21 10:21:30 2013 +0530

    BUG#17242996: ATOMIC/X86-GCC.H:MAKE_ATOMIC_CAS_BODY64 POTENTIAL
                  MISCOMPILATION BUG
    
    Analysis:
    --------
    The make_atomic_cas_body64 implementation on IA32 with GCC but
    without GCC builtins might get miscompiled due to a wrong
    constraint.
    
    make_atomic_cas_body64 implementation uses CMPXCHG8B with a
    single memory operand as input/output operand. However the
    constraint '+' was not used to indicate the same due to
    issue reported by Bug#11746008. Hence it could result in
    using two different memory locations, thus accessing an
    uninitialized memory with probable run[1;31mtime[m crash.
    
    Fix:
    ---
    The assembly atomic implementation (x86-gcc.h) is not used on
    Windows and Solaris. The oldest version of GCC supported by
    Linux variants on any platform is GCC 4.1.2 which supports
    builtin atomics. On MAC, GCC versions higher than 4.1.2 or
    Clang is used which supports builtin atomics as well.
    
    Hence this patch removes assembly atomic implementation
    (x86-gcc.h) and relies on the GCC builtins. Upon using
    an older unsupported versions of GCC(A warning is flagged),
    we may see some performance regressions since we fallback
    to rw locks.

[33mcommit 60caa9d8bd8675d072ab5c8dc5dedc86c9e53046[m
Author: Shubhangi Garg <shubhangi.garg@oracle.com>
Date:   Tue Nov 19 21:46:51 2013 +0530

      BAPI 85: Moving Decoding logic of QUERY_EVENT and EXECUTE_LOAD_QUERY_EVENT
               into binlogapi library
    
      Description:
      QUERY_EVENT
      @log_event.h: class Query_log_event  is a subclass of
      @libbinlogapi/include/binlog_event.h : class Query_event
    
      EXECUTE_LOAD_QUERY_EVENT
      @log_event.h: class Execute_load_query_log_event  is a subclass of
      @libbinlogapi/include/binlog_event.h : class Execute_load_query_event
    
    Inheritance structure :
                                Binary_log_event
                                   /  \
                                  /    \
                 (1) <<virtual>> /      \ <<virtual>> (2)
                                /        \
                          Query_event Log_event
                              /  \        /
                  <<virtual>>/    \<<v>> /
                            /      \    /
                           /        \  /
           Execute_load_query_event Query_log_event
                           \         /
                            \       /
                             \     /
                              \   /
                    Execute_load_query_log_event
    
    
    Changes from previous implementation of Query_log_event:
    w.r.t data types:
      Changes to the data types are made, depending on the required.
      Earlier 16 bit variables were defined to be of type int, changed them
      to be of type uint16_t.
    w.r.t user and host:
     -user and host are defined as LEX_STRING in the Query_log-event,
      changed them to std::sring
     -user_len and host_len removed, are replaced by string::length() function
    
    w.r.t data_buf in the Query_log_event
    - Since data_buf buffer is not needed for/by the decoder, data_buf is
      to be kept a part of the server i.e. Query_log_event instead of
      bapi_mysql::Query_event.
    
    - The const char pointers to this buf has been replaced by std::string
      in the bapi::Query_event, and are declared as private members.
    
    - Since the data_buf cannot be done away with, it is allocated in
      Query_log_event instead of bapi_mysql::Query_event.
    
    - Another set of const char* pointers are members of Query_log_event on
      the server side, which would point to this data_buffer.
    
    - A function in the bapi class Query_event is added which would take a
      pointer to this data_buf, and fill the possible fields in the buffer
      (in the diff:int  Query_event::fill_data_buf(Binary_log_event::Byte* buf)).
      This method is called from within the Query_log_event constructor in
      the server.
    
    - Query_event::fill_data_buf will use the strings it has decoded in order
      to fill the buffer in the format required by the handler.
    
      Once the data_buf is filled, we set the pointers in the Query_log_event
      at the particular offsets to this data_buf.
      We then use these pointers inside the server code for referring to the
      catalog, [1;31mtime[m_zone_str, query, db, user or host
    
    - For other users of binlog api, getters and setters are defined for the
     string data members.
    
    
     TODO: Virtual link (1) will become non-virtual once decoding of all the
           events are moved into libbinlogapi, and link (2) is broken.

[33mcommit 2ec981d5f4d32a20a5993f44ba0ce8d56441cba0[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Nov 19 12:09:52 2013 +0100

    Bug#17813333 PSI KEYS WERE INCORRECTLY PASSED TO FUNCTION SET_PSI_KEYS
    
    Before this fix, the mutex
      'wait/synch/mutex/sql/MYSQL_RELAY_LOG::LOCK_sync'
    was not properly instrumented for the performance schema.
    
    In particular, no wait [1;31mtime[m was recorded for this mutex,
    and no mutex instance was instrumented,
    because the incorrect key was used when instrumenting the code.
    
    The root cause is a simple mismatch in the parameters order,
    in the call to relay_log.set_psi_keys().
    
    This fix uses the proper keys when calling set_psi_keys()
    for the relay log.

[33mcommit 80e0e8a7b45e112eb1bcfeab33f633d9f491de70[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Nov 18 11:06:19 2013 +0100

    Part1 of fix for bug#17647637: Refactor of NdbTick as prep for real fix
    
    My rational for this refactoring is that I want a safer interface
    for calculating elapsed [1;31mtime[m and doing [1;31mtime[mout calculation.
    
    The general pattern in this patch is that the common code sequence:
    
    Ex1.
      NDB_TICKS start = NdbTick_CurrentMillisecond();
      ... Do some work ...
      NDB_TICKS end = NdbTicks_CurrentMillisecond();
      Uint64 elapsed = (end - start);
    
    
    Has been refactored into:
    Ex2:
      NDB_TICKS start = NdbTick_getCurrentTicks();
      ... Do some work ...
      NDB_TICKS end = NdbTicks_getCurrentTicks();
      Uint64 elapsed = NdbTick_ElapsedMilliseconds(start,end);
    
    
    As part of this, NDB_TICKS has been redefined into an opague type
    with the max 'tick resolution' the underlying hardware can provide.
    No assumption about the resolution of a single tick is exposed
    through the interface.
    
    Thus, it is not any longer an int, and all interval calculations has
    to go through the new NdbTick_Elapsed() function.
    (There are also Micro, Nano and Second variants which all can be
    calculated from the same NDB_TICKS).
    Asserts has also been included into these functions such that
    uninitialized 'ticks' are being asserted, and negative intervals
    resulting from mixing start-end arguments(Bug#17739131)
    or non-monotonic [1;31mtime[mrs. (Bug#17647637)
    
    The problems I try to rectify in this patch are:
    
    1) NdbTick.h defined 'typedef Uint64 NDB_TICKS' which was the
       type returned from NdbTick_CurrentMillisecond(),
       NdbTick_CurrentMicrosecond, NdbTick_CurrentSecond and
       NdbTick_CurrentNanosecond. This provided no type
       safety, and was quite often mixed with using plain Uint64
       such that it didn't really offer anything of value.
    
       Furthermore it did not protect against misuse as:
    
        start = NdbTick_CurrentMillisecond()
        ... do some
        end = NdbTick_CurrentMicrosecond; << Mix ms / us
        elapsed = (end - start);
    
        or:
    
        elapsed = (start - end)  << bug#17739131
    
    2) Catch 'elapsed [1;31mtime[m' calculation based on uninitialized
       [1;31mtime[mrs - Several bugs had been filed and fixes as a
       result of this
    
    3) Avoid precision loss due to [1;31mtime[mr rollover:
       Depending on the underlying platform, the [1;31mtime[mrs
       might have resolution in the microsecond range.
       By converting into milliseconds as we usually did, we get
       less accuracy as this extra precision is truncated, considder:
    
        t1 = 1000 us -> truncates to 1ms
        t2 = 1999 us -> truncates to 1ms
        t3 = 2001 us -> truncates to 2ms
        t4 = 2999 us -> truncates to 2ms
    
       So calculating intervals between these [1;31mtime[m ticks using the old
       NdbTick_CurrentMillisecond(), we will get:
    
       (t2-t1) = 0ms ... Sort of correct as it is < 1ms
       (t3-t2) = 1ms ... Not what you would expect !!!
    
    4) Get rid of 'struct MicroSecondTimer', NdbTick_getMicroTimer(),
       NdbTick_getMicrosPassed() and NdbTick_getMillisecond().
       This was just another [1;31mtime[mr mechanism which partly overlapped
       the functionality provided by NDB_TICKS.
       In places where both a Milli- and Microseconds [1;31mtime[mr was required,
       it even caused that the system clock had to be read twice (ThreadConfig.cpp)
       or we had to convert between [1;31mtime[mrs represented either as a 'MicroSecondTimer'
       or a NDB_TICKS.

[33mcommit 582d0665b5c8f9d70e6d89406dd1d16af9eaf02c[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Nov 18 08:53:41 2013 +0100

    Fix for bug#17739131 JOB SCHEDULER IN SINGLE THREADED NDBD DOES NOT 'YIELD' CPU AS INTENDED
    
    When the internal single threaded job scheduler is configured to use
    'RealTimeScheduler=1', it is intended to temporary lower its scheduling
    priority to 'normal' every 10ms. This will allow other, non real[1;31mtime[m threads,
    to get a chance to run, and thus avoid CPU starvation of these.
    
    However, the arguments to the function which calculated elapsed [1;31mtime[m
    singe last yield where swapped. Thus, '0' was always returned as elapsed
    [1;31mtime[m from this function, and the CPU never yielded as intended.

[33mcommit b2a5da1d989ecd3fb70555731cad3a6694682d50[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sun Nov 17 21:17:32 2013 -0800

    Some work on handling [1;31mtime[mouts during index scans.

[33mcommit 1917107321942a3de442bd64764b34ca9229ef0a[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sat Nov 16 21:42:57 2013 -0800

    Implement promises for mysql-js
    
    mynode.js
      return promise from openSession and connect
    
    UserContext.js
      implement Promise contract per http://promises-aplus.github.io/promises-spec/
      create a Promise for all UserContext uses
      return promise from openSession and connect
      return promise from find, persist, save, update, load, remove, and getMapping
      return promise from commit, rollback, closeSession
      update applyCallback to fulfill or reject promise before calling user callbacks
        promise callbacks are called on next [1;31mtime[mr tick after user callbacks
      fix dbConnectionPoolCreated_callback to always call back multiple
        requestors for the same connection properties
      need to evaluate executeBatch
    
    PromisesTest.js
      correct order of parameters for test.ErrorIfNotEqual
      test for promise being an object with a then method
    
    These test case failures need to be evaluated
    [FAIL] t_basic IllegalArgumentTest.js testPersistNoArgumentNoCallback
      t11 persist with no arguments must fail.
    [FAIL] t_basic IllegalArgumentTest.js testRemoveNoArgumentNoCallback
      t12 remove with no arguments must fail.
    [FAIL] t_basic IllegalArgumentTest.js testSaveNoArgumentNoCallback
      t13 save with no argument must fail.
    [FAIL] t_basic IllegalArgumentTest.js testUpdateNoArgumentNoCallback
      t14 update with no arguments must fail.
    [FAIL] t_basic IllegalArgumentTest.js testFindNoArgumentNoCallback
      t15 find with no arguments must fail.

[33mcommit 34ccf0b63a362d66b4ee96370fb37e70ae99ce1d[m
Author: Pavan Naik <pavan.naik@oracle.com>
Date:   Fri Nov 15 11:29:31 2013 +0530

    Bug#11756699 : MYSQL-TEST MAIN.LOG_TABLES-BIG FAILS ON 5.1.40
    
    Fix :
    
    Added "set @@global.log_output = 'TABLE';" line so that the details of SQL statements that
    take more than long_query_[1;31mtime[m seconds to execute are stored in mysql.slow_log table.

[33mcommit 025d2278b136a3c5d8b97f75a4da291228f50add[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Nov 14 15:08:28 2013 +0100

    Add small portabilty function 'ndb_local[1;31mtime[m_r' which emulates local[1;31mtime[m_r on Windows

[33mcommit bbcc63369b7428f926ad83a65b1d5114e683e61a[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 12 14:30:28 2013 +0100

    Make format_[1;31mtime[mstamp() safe by always filling the passed string with
    some value even if local[1;31mtime[m_r() fails

[33mcommit d42df011d2e91bf2f471da47af8473f391bad2a1[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 12 13:45:20 2013 +0100

    Rename badly named variable "[1;31mtime[m" to "[1;31mtime[mstamp"

[33mcommit db5d3d4492b012f393dfe6087bfa7f71acd0c25e[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 12 11:22:16 2013 +0100

    Remove usage of pointer value returned from local[1;31mtime[m_r since the return value may potentially be NULL

[33mcommit bfbaa022fb8a5c3665d71be4d9c8e8ceefc0949f[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 12 11:10:07 2013 +0100

    Call local[1;31mtime[m_r outside of strf[1;31mtime[m() since local[1;31mtime[m_r may potentially return NULL.

[33mcommit 5691e24a49afc553353ce36d1bdb54d1f0899242[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 15:59:00 2013 +0100

    Remove unthread safe usage of static stack variable  in NDBT_Test and instead use
    the common [1;31mtime[mstamp function from Logger in combination with
    stack buffer

[33mcommit 889df5159d8eba23326d7150d5593178bbf269de[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 15:29:01 2013 +0100

    Move implementation of format_[1;31mtime[mstamp into Logger.cpp to
     avoid exposing too much details

[33mcommit 5b545135c2e6d77404ce0ba92b5496f32bc2c5b4[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 15:18:01 2013 +0100

    Change ndb_mgm to use new [1;31mtime[mstamp string function

[33mcommit 8c0f8e3f580756cc7d53905de2e0bb0e3f4a0a34[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 14:57:30 2013 +0100

    Remove the function getTimeAsString and instead call Logger:_format_[1;31mtime[mstamp direct

[33mcommit e1cdcc5e8403a7968653d140427027fbb2206d77[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 14:47:37 2013 +0100

    Refactor "format [1;31mtime[mstamp" code into new static function in Logger to
    be used from different places

[33mcommit 202f0f08574d90444983df06ecfeefab0a3c8f3d[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 13:03:29 2013 +0100

    Bug#17750252 USAGE OF LOCALTIME IS UNSAFE IN THREADED CODE
     - usage of local[1;31mtime[m is not safe since some platforms are using a static
       buffer shared between all threads for storing the result of the conversion.
     - replace local[1;31mtime[m() with local[1;31mtime[m_r()+stackbuf
     - add call to tzset() in ndb_init() to initialize the [1;31mtime[m conversion information
       once.  This loads information about current [1;31mtime[mzone as well as  during which periods
       daylight saving calculations should be performed.

[33mcommit c93b0d9a972cb6f98fd445f2b69d924350f9128a[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Nov 11 15:09:32 2013 +0530

    Bug #11758237 INSERT ON DUPLICATE KEY UPDATE SOMETIMES WRITES BINLOG
    POSITION INCORRECT
    
    Problem:
    
    When concurrent INSERT ... ON DUPLICATE UPDATE statement is executed on a
    table, the final outcome is some[1;31mtime[ms not serializable.
    
    Solution:
    
    When INSERT ... ON DUPLICATE UPDATE statement is execute, first the record
    would be inserted into the clustered index followed by the secondary indexes.
    If a duplicate error is encountered, we do not stop processing.  We continue to
    process all the unique secondary indexes and place the necessary gap locks.
    Once a duplicate error has been encountered, the records will not be actually
    inserted but only gap locks will be placed.
    
    rb#3196 approved by Marko

[33mcommit 5aef73a55d0638ff877ff80642f2287e11e57846[m
Merge: c26287a6f89 137f3e51dc3
Author: bin.x.su@oracle.com <>
Date:   Mon Nov 11 11:47:30 2013 +0800

    BUG#16559254 FTS OBJECT ID DOES NOT USE HEXADECIMAL ON WINDOWS
    
    == Analysis ==
    Windows format string is wrong, we should use "%016llx" rather than "%016llu"
    
    == Solution ==
    This patch will force all the aux tables to be renamed from "%016llu" format
    to "%016llx" only in Windows, keeping aux table names in Linux unchanged.
    
    Suppose all users who want to upgrade server will start the new server in
    read/write mode at the first [1;31mtime[m.
    
    Macro definitions are fixed first. Only one format "%016llx" will be used in
    fts_write(read)_object_id. And a table flags2 bit named
    DICT_TF2_FTS_AUX_HEX_NAME(HEX_NAME for short) is introduced which is used in
    the following way:
    1. If an aux table is renamed to "%016llx" format, set this aux table's
       flags2 with this bit. And if a parent table's aux tables are all renamed
       to "%016llx" format, set this parent table's flags2 with this bit. (Win)
    2. Set a parent table and all its aux tables' flags2 with this bit. (Linux)
    3. For all existing tables, this bit will be set for a parent table and all
       its aux tables, or none of them will be set. (Win/Linux)
    4. Set all new created and/or altered tables(aux tables)' flags2 with this
       bit. (Win/Linux)
    
    During the upgrade which only happens in Windows, we do these:
    1. sort all aux tables by their parent table IDs in case rename will fail.
    2. prepare a vector(V) to store aux tables which need to be renamed.
    3. try to open an aux table, whose table id is always correct.
    4. if fail to open the table or the table opened is a different table, skip
       this table.
    5. if the opened aux table doesn't have flags2 set with HEX_NAME, convert
       all ids on it to the correct ones.
    6. then we can open the parent table with a correct id.
    7. do things like original codes do.
    8. if this aux table won't be dropped, put this aux table to V.
    9. if all aux tables belonging to the same parent table are collected,
       try to rename them(stored in V) in a batch mode. If all rename operations
       succeed, set all flags2 with HEX_NAME, otherwise, rollback all the rename
       operations for this parent table.
    
    For Linux, It needs to set the flags2 for every parent and aux tables.
    
    If the server crashes during the upgrade, all those aux common tables, which
    have already been renamed, should be renamed to their original names while
    renamed index tables will roll back automatically. Otherwise, these tables'
    names in innodb_sys_tables will not consist with their data file's names.
    After that, the server can upgrade again when it restart.
    
    In case the upgrade fails, the server can access both tables with AUX_HEX_NAME
    set and without AUX_HEX_NAME set. The tables fail to rename will upgrade
    themselves when server restart. One restriction is that we can't do in-place
    add fulltext to those tables fail to rename(we will get an new error from
    server), to make sure that either the AUX_HEX_NAME flags are set in parent
    table and its aux tables or not. Other DDL operations are allowed.
    
    rb#2396, approved by Jimmy.

[33mcommit 4d46b7560a4d91c85d10ef68ee349e4b1b4a7e17[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Nov 8 20:58:48 2013 +0100

    Bug#17766582 PERFORMANCE SCHEMA OVERHEAD IN PFS_LOCK
    
    This fix is a general cleanup for code involving atomic operations in the
    performance schema, to reduce overhead and improve code clarity.
    
    Changes implemented:
    
    1)
    
    Removed 'volatile' in the code.
    The C 'volatile' keyword has nothing to do with atomic operations,
    and is confusing.
    
    This is a code cleanup.
    
    2)
    
    Added missing PFS_cacheline_uint32 to atomic counters,
    to enforce no false sharing happens.
    
    This is a performance improvement.
    
    3)
    
    Modified optimistic locks, for clarity.
    
    Pattern before:
      pfs_lock lock;
      m_lock.begin_optimistic_lock(&lock);
      m_lock.end_optimistic_lock(&lock);
    
    Pattern after:
      pfs_optimistic_state lock;
      m_lock.begin_optimistic_lock(&lock);
      m_lock.end_optimistic_lock(&lock);
    
    The new type, pfs_optimistic_state, better reflects that a state information
    is used in a begin / end section.
    This provides better typing, for type safety.
    
    Adjusted all the code accordingly.
    
    4)
    
    Modified pfs_lock allocation, for performances.
    
    Pattern before:
      m_lock.is_free();
      m_lock.free_to_dirty();
      m_lock.dirty_to_allocated();
      total: 0+1+2 = 3 atomic operations
    
    Note that free_to_dirty() could fail even for free locks,
    because the CAS can use an old version number,
    making the code attempt again another record.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.free_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      total: 2+1 = 3 atomic operations.
    
    Now the code is garanteed to detect free records,
    because free_to_dirty() does an atomic load then a CAS.
    
    Adjusted all the code accordingly.
    
    5)
    
    Modified pfs_lock deallocation, for performances.
    
    Pattern before:
      m_lock.allocated_to_free();
      Total 2 atomic operations.
    
    Pattern after:
      m_lock.allocated_to_free();
      Total 1 atomic operation.
    
    6)
    
    Modified record updates, for performances.
    
    Pattern before:
      m_lock.allocated_to_dirty();
      m_lock.dirty_to_allocated();
      Total 4 atomic operations.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.allocated_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      Total 2 atomic operations.
    
    Adjusted all the code accordingly.
    
    7)
    
    Modified record peek, for reliability.
    
    Pattern before:
      m_lock.is_populated()
      used a dirty read, causing spurious missing records
    
    Pattern after:
      m_lock.is_populated()
      uses an atomic load, for correctness.
    
    This change is expected to resolve spurious test failures,
    where some records in performance schema tables are some[1;31mtime[m missing,
    os some statistics (when computed on the fly) are some[1;31mtime[m under evaluated.

[33mcommit c5589763a4a3593f9116414b24dcf24a893a1d0a[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Fri Nov 8 09:17:09 2013 +0530

    Bug#13924750 -  PARTITION_LOCKING_4.TEST LOOSES THE CONNECTION
    
    Description:
    
    The test fails with the error message 'MySQL server has gone
    away'.
    
    Analysis:
    
    The test sets value of innodb_lock_wait_[1;31mtime[mout and
    lock_wait_[1;31mtime[mout variable as 1. The test also sets wait_[1;31mtime[mout
    variable value as 1. So, client is disconnected before it can
    get the error message ER_LOCK_WAIT_TIMEOUT from the server.
    Test executes successfully if the the value of wait_[1;31mtime[mout is
    increased.
    
    FIX :
    
    Test will not set the value of wait_[1;31mtime[mout variable.
    Updated result file according to the test.

[33mcommit b2c29af24d6b4b032b455d696c7dfe333306f40d[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Nov 8 11:40:13 2013 +0900

    Bug#12734249 : SIBLING LEAF PAGES CAN BE PREFETCHED BEFORE PESSIMISTIC OPERATION OBTAINS INDEX
    
    The problem was read IO for sibling pages might be caused while BTR_MODIFY_TREE having index->lock (X or SX).
    At the moment of the previous BTR_MODIFY_LEAF failed, the siblings can be started to read asynchronously.
    This should reduce the index->lock (X or SX) holding/waiting [1;31mtime[m.
    
    Approved by Marko in rb#3486

[33mcommit b5b35b390d4c88ef99c7cea681684e5f3997826d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 6 09:34:28 2013 +0100

    Fix for bug#17739932 'TESTSUITETIMERS' ARE NOT ALWAYS STARTED/STOPPED, WHICH LEAVES THEM UNDEFINED.
    
    This fix moves the start/stop of the testSuiteTimers out of the if-else-branch,such that
    the [1;31mtime[mrs are also used in case the else-branch is taken

[33mcommit f57b99703eeda1f087621c0fe23692022c9b962b[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 6 09:31:40 2013 +0100

    Fix for bug#17738720:
    
      ARBITRATION TIMEOUT NOT INITIALIZED -> FALSE WARN 'COULD NOT FIND AN ARBITRATOR'
    
    During arbitrator selection, QMGR runs through the states:
    (NULL, INIT, FIND, PREP1, PREP2, START, ... and some more)
    
    Currently the arbitration [1;31mtime[mout [1;31mtime[mr is not set until we reach the
    PREP-states. However, there is a check for [1;31mtime[mout already in the
    FIND-state which may then read uninitialized [1;31mtime[mstamp.
    This may cause false warnings:
    
    'Could not find an arbitrator, cluster is not partition-safe'
    
    This fix sets the [1;31mtime[mout [1;31mtime[mr in the 'INIT' state.

[33mcommit 8249ce79e9f4152efb5ecce7f98b2b192810443e[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sun Nov 3 14:47:04 2013 -0800

    jscrund SQL adapter: use real batching.
    This uses the multipleStatements feature of node-mysql to send an entire batch at one [1;31mtime[m.

[33mcommit 19a5a6cb4d2c93814667070dbf1caf1dd9ec82b6[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sun Nov 3 14:32:06 2013 -0800

    JSCrund null backend: measure [1;31mtime[m to create buffers

[33mcommit 6c350d58355591e9bb3a05702325fe255ea8efa6[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Oct 30 22:23:28 2013 -0700

    New "null" adapter for jscrund measures [1;31mtime[m for NOOP operations with callbacks.

[33mcommit a3a9f6673e0ce0814947cc294625ce37fa9a110d[m
Merge: 6ae66f22240 f643b99ad7a
Author: Neha Kumari <neha.n.kumari@oracle.com>
Date:   Wed Oct 30 20:47:00 2013 +0530

     BAPI 85: rb2997: Moving event header from class Log_event into a separate class
    
          The patch contains the following changes:
    
          - Modifying the class Log_event_header and corresponding removal of data members
            from Log_Event class, representing an event header
    
          - Decoding of the header in the constructor for Log_event_header
            instead of Log_event constructor
    
          - Modification to the event constructors   signature to call Log_event(header)
            instead of Log_event(buf, description_event)
    
          - Class Log_event inherits the abstract class Binary_log_event.
            This will, however, be changed in the final design. After decoding for all
            the events are moved into libbinlogapi, this dependency will be removed.
    
    BAPI 85: rb 3002: Moving methods and attributes used for decoding to libbinlogapi
    
          - Modified Binary_log_event and added following methods
            -get_type_code()
            -is_valid()
    
          - And correspondingly removed these methods from Log_event class
    
          - Added the definition of above two methods to all the events defined in bapi, as they are virtual methods.
    
          - Added constructor Format_description_event(uint8_t binlog_ver, const char* server_ver=0)
            and few attributes which are needed by the above ctor in the class Format_description_event
    
          - Similar constructor for FDE is still maintained in the server also,
            depending upon whether the decoding part of an event is implemented in bapi or server,
            we will pass the correct FDE object at the [1;31mtime[m of event object creation
    
    BAPI 85: rb 3087: This patch contains the integration patch for 2997(Moving event header from class Log_event into class
                      Log_event_header in binlogapi) and 3002(Moving methods and attributes used for decoding to libbinlogapi).
    
    
          - Access of Log_event_header member through the object of Log_event_header class, in server
            ex-
            {
              Log_event *ev;
              ...
              ev->common_header->when; // earlier it was ev->when, common_header is an object pointer declared inside Log_event class
            }
          - Modified the log_event_old.h/.cc bascially added one more parameter Log_event_header *header to their ctors
    ******
     BAPI 85: rb2997: Moving event header from class Log_event into a separate class
    
          The patch contains the following changes:
    
          - Modifying the class Log_event_header and corresponding removal of data members
            from Log_Event class, representing an event header
    
          - Decoding of the header in the constructor for Log_event_header
            instead of Log_event constructor
    
          - Modification to the event constructors   signature to call Log_event(header)
            instead of Log_event(buf, description_event)
    
          - Class Log_event inherits the abstract class Binary_log_event.
            This will, however, be changed in the final design. After decoding for all
            the events are moved into libbinlogapi, this dependency will be removed.
    
    BAPI 85: rb 3002: Moving methods and attributes used for decoding to libbinlogapi
    
          - Modified Binary_log_event and added following methods
            -get_type_code()
            -is_valid()
    
          - And correspondingly removed these methods from Log_event class
    
          - Added the definition of above two methods to all the events defined in bapi, as they are virtual methods.
    
          - Added constructor Format_description_event(uint8_t binlog_ver, const char* server_ver=0)
            and few attributes which are needed by the above ctor in the class Format_description_event
    
          - Similar constructor for FDE is still maintained in the server also,
            depending upon whether the decoding part of an event is implemented in bapi or server,
            we will pass the correct FDE object at the [1;31mtime[m of event object creation
    
    BAPI 85: rb 3087: This patch contains the integration patch for 2997(Moving event header from class Log_event into class
                      Log_event_header in binlogapi) and 3002(Moving methods and attributes used for decoding to libbinlogapi).
    
    
          - Access of Log_event_header member through the object of Log_event_header class, in server
            ex-
            {
              Log_event *ev;
              ...
              ev->common_header->when; // earlier it was ev->when, common_header is an object pointer declared inside Log_event class
            }
          - Modified the log_event_old.h/.cc bascially added one more parameter Log_event_header *header to their ctors

[33mcommit 77e78b4487eaa9464495a5832bf70f3557da66d9[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Wed Oct 30 10:09:54 2013 +0100

    Bug#17560636: FIND_BEST_REF(): COST FOR CLUSTERED PK IS
                  CALCULATED AS IF NOT COVERING
    
    Optimize_table_order::find_best_ref() iterates through all
    possible ways to access all usable keys for a table. The index
    access strategy with lowest cost is chosen. However, if a
    clustered primary key is applicable, it is considered to be
    covering only if all columns required for the query are in
    the primary key definition. The result is that the cost for
    clustered primary keys is calculated as though a random read
    needs to take place for each estimated row. This is incorrect
    when clustered primary keys are read since the data can be
    read from the index without going to "the table".
    
    The fix is to let handler::read_[1;31mtime[m() calculate the cost
    instead of assuming that there will be a cost of 1 x #row.
    handler::read_[1;31mtime[m() is capable of handling the distinction
    between reading the clustered index and another index.

[33mcommit 35e5a038ea9be02f4a370473249c9efbb5c642da[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Fri Oct 25 12:13:26 2013 +0530

    Bug #11758237 INSERT ON DUPLICATE KEY UPDATE SOMETIMES WRITES BINLOG
    POSITION INCORRECT
    
    Problem:
    
    When concurrent INSERT ... ON DUPLICATE UPDATE statement is executed on a
    table, the final outcome is some[1;31mtime[ms not serializable.
    
    Solution:
    
    When INSERT ... ON DUPLICATE UPDATE statement is execute, first the record
    would be inserted into the clustered index followed by the secondary indexes.
    If a duplicate error is encountered, we do not stop processing.  We continue to
    process all the unique secondary indexes and place the necessary gap locks.  We
    skip non-unique secondary indexes.  Once a duplicate error has been
    encountered, the records will not be actually inserted but only gap locks will
    be taken.
    
    rb#3196 approved by Marko

[33mcommit 6fa03ee98ca231ac863de9468f8510e2cc9fcb32[m
Author: Libing Song <libing.song@oracle.com>
Date:   Thu Oct 24 14:06:49 2013 +0800

    WL#7169 Semisync: make master wait for more than one slave to ack back
    
    Postfix:
    - Set rpl_semi_sync_master_[1;31mtime[mout longer to guarantee it doesn't [1;31mtime[mout
      on slow machines.
    - Adjusted coverage test order, so it can cover more code.

[33mcommit c1ca611ca47d1a88fc008997620a8103e260e3f1[m
Author: Shivji Jha <shivji.jha@oracle.com>
Date:   Wed Oct 23 13:01:26 2013 +0530

    BUG#17533190- SLAVE_OPEN_TEMP_TABLES SHOWING 0 WITH >0 OPEN
                  TEMP TABLES AT SLAVE
    
    Problem:
    The test rpl_mts_submode_switch.test uses debug statements
    to force the slave applier to apply events in parallel.
    However in case of temporary-tables this does not work
    since the tables should be deleted by any one thread at a
    [1;31mtime[m. This is because once a worker thread takes ownership
    of any temporary tables created by a connection, those
    tables become inaccesible to other worker threads.
    Therefore when the slave worker thread tries to execute
    DROP IF EXISTS, some tables while are owned by some other
    worker thread is left behind causing the next test to
    report a warning and hence fail.
    
    Solution:
    Removed the debug set from the master before dropping the
    temporary tables there-by allowing the tables to be dropped
    'naturally' on the slave either sequentially or in parallel
    depending on how they were dropped on the master.

[33mcommit 78821a1cd8c7c7be288dff3cd13e46d5383c2d25[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Oct 21 13:38:56 2013 +0200

    ndb - reuse lingering commit ack markers
    
    this patch makes Dbtc (more) resillient to ndbapi failing
    to TC_COMMIT_ACK transactions during node-failure,
    and accepts same marker being reported several [1;31mtime[ms
    (in different node failures) from DBLQH.

[33mcommit 5b9ec55dba3e1b299a752f1637c44ad68038d543[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Thu Oct 17 13:41:14 2013 +0530

    Bug#16851753 : CREATE TEST TO MEASURE STARTUP AND SHUTDOWN TIME FOR THE SERVER
    
    Renaming the test from startup_shutdown.test to server_startup_shutdown_[1;31mtime[m.test
    and removing it from experimental as the test is now stable on pb2.

[33mcommit f6199055ce7c5efdd6a49e022d6e903aac87b389[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Wed Oct 16 12:18:51 2013 +0530

    Bug#16851753 :
    
    Modifying the test to skip it on valgrind. The test takes a very long [1;31mtime[m to run on valgrind
    builds. Hence, skipping it as it doesn't really make sense to run the test for
    valgrind.

[33mcommit 05a0bb1d926935300edb4636434d5c2489f4a894[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Oct 11 07:42:00 2013 +0200

    Fix for bug#16961971, NDB : REALTIMESCHEDULER REGRESSION ON NDBMTD, introducing more options to ThreadConfig configuration variable for real[1;31mtime[m, spin[1;31mtime[m and cpusets

[33mcommit ea10a901a11680dedd761636ac7816ff97292621[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Oct 11 07:29:50 2013 +0200

    Fix for bug#16961971, NDB : REALTIMESCHEDULER REGRESSION ON NDBMTD, introducing more options to ThreadConfig configuration variable for real[1;31mtime[m, spin[1;31mtime[m and cpusets

[33mcommit 64b369445e26a15af17711a4bfae22774ebd3618[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Oct 11 07:28:03 2013 +0200

    Fix for bug#16961971, NDB : REALTIMESCHEDULER REGRESSION ON NDBMTD, introducing more options to ThreadConfig configuration variable for real[1;31mtime[m, spin[1;31mtime[m and cpusets

[33mcommit 21ecb6d451adb2fd0748373ba58aad43793dc785[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Oct 11 07:26:26 2013 +0200

    Fix for bug#16961971, NDB : REALTIMESCHEDULER REGRESSION ON NDBMTD, introducing more options to ThreadConfig configuration variable for real[1;31mtime[m, spin[1;31mtime[m and cpusets

[33mcommit 32d2d37176487d6e86da395f26528a564f834835[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Mon Oct 7 17:29:00 2013 +0530

    Bug #16630410 NDB:LCP FRAGMENT SCAN WATCHDOG SHOULD BE CONFIGURABLE
    
    The LCP fragment scan watchdog periodically checks for lack of
    progress in a fragment scan performed as part of a local checkpoint.
    The watchdog was hard-coded to shut down the node if there was no
    progress for 60 seconds.
    
    Added a configuration parameter LcpScanProgressTimeout to set the
    maximum [1;31mtime[m for which the local checkpoint can be stalled before
    the LCP fragment scan watchdog shuts down the node. A parameter
    value of 0 disables the LCP fragment scan watchdog.
    
    Added a unit test to set the parameter value to 0 and verify that
    the LCP fragment scan watchdog is disabled.

[33mcommit 95121b789eb03ae211eb06d8a73540e576f825bb[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Mon Oct 7 12:08:50 2013 +0530

    Bug#16851753 : CREATE TEST TO MEASURE STARTUP AND SHUTDOWN TIME FOR THE SERVER
    
    Creating a new test to measure the server startup and shutdown [1;31mtime[m.

[33mcommit 2f355baaaadcfab9597045654699cff8223035ba[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Sun Oct 6 19:40:10 2013 +0530

    BUG#11748566 - MYSQL SHOWS WRONG DEFAULT VALUE FOR PK IF
                   NOT NULL NOT SPECIFIED
    
    DESCRIPTION:
    
    Two different, but equivalent ways of declaring a column
    as primary key does not end-up with the same result. When the
    column is declared primary key inline as:
        CREATE TABLE t (a INT PRIMARY KEY)
    it has no default value. In this case the flag
    NO_DEFAULT_VALUE_FLAG for the column is set while parsing.
    On the other hand, in the second case when column is declared
    primary key as index:
        CREATE TABLE t (a INT, PRIMARY KEY(a))
    the flag is not set. This results in a default value for
    the primary key column.
    
    FIX:
    
    When a column is declared as primary key as index, the flag
    NO_DEFAULT_VALUE_FLAG is set for that column if user has not
    provided an explicit default value for the column. For the
    TIMESTAMP/DATETIME fields, if explicit_defaults_for_[1;31mtime[mstamp
    variable is not set, default value assigned due to first
    [1;31mtime[mstamp column promotion is retained. Default constant value
    assigned due to implicit promotion of second [1;31mtime[mstamp column
    is removed.

[33mcommit 548ed174c75c409b7fce4e7c94f2beb9e1dd078c[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Oct 4 21:30:18 2013 +0200

    Bug#17310878 PFS OVERHEAD ON FREQUENT CONNECT/DISCONNECT
    
    This fix is for MySQL 5.7
    
    This fix is a performance improvement.
    
    The issue was that the call to PSI_THREAD_CALL(delete_current_thread)(),
    which is executed each [1;31mtime[m a thread disconnects,
    was placed inside a critical section involving LOCK_thread_count.
    
    The fix is to perform the same call sooner,
    before entering the LOCK_thread_count critical section.
    
    Serializing all calls to pfs_delete_current_thread_v1() was the major
    cause of performance overhead, for frequent connect/disconnect.

[33mcommit 42e85b37d426265b7594979a4f49e4b8c1bf0415[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Thu Oct 3 18:30:15 2013 +0530

    BUG#13995622 - MYSQL SILENTLY REFUSES TO MAKE PRIMARY KEY COLUMN NULL
    BUG#15967545 - MISSING ERROR MESSAGE / WARNING WHEN TRYING TO CREATE A PRIMARY KEY WITH NULL VALUE
    BUG#16545198 - NOT REJECTING ALTER TABLE ADD PRIMARY KEY(X), ADD COLUMN X ... DEFAULT NULL
    
    BACKGROUND:
    The crux of the problem reported in the bug report is in the
    current behaviour when we specify NULL for any column that is
    part of a PRIMARY KEY, then the column is implicitly assigned
    a NOT NULL qualifier.
    According to bug requirement, When the user explicitly asks for
    a PRIMARY KEY column to be NULL, that statement should be rejected.
    
    ANALYSIS:
    This implicit assignment of NULL to NOT NULL is done by a parser
    rule in sql_yacc.yy NULL_SYM { Lex->type&= ~ NOT_NULL_FLAG; }.
    With this rule, we cannot distinguish whether NULL was
    specified explicitly or not. Further down the line we do not
    remember or know if we visited NULL keyword at any point of [1;31mtime[m.
    
    FIX:
    As a fix for this bug, a new flag EXPLICIT_NULL_FLAG is introduced
    in LEX::type/Create_field::flags bitmaps (similar to NOT_NULL_FLAG).
    The parser rule is modified accordingly. Then later in
    mysql_prepare_create_table() function, while iterating over all
    keys, this flag is used to identify fields participating in
    primary key which were explicitly requested to be nullable (by using
    NULL clause) and appropriate error is emitted.
    
    NOTE:This Patch also fixes problems mentioned in Bug#15967545.
    and Bug#16545198. Additional Test Coverage is added for the reported
    bugs.

[33mcommit 9e487adad0b801a23d9ab2743160d5b365c62697[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Oct 1 13:59:19 2013 +0200

    Fix for Bug#15907122
    
    INCORRECT HANDLING OF JOB-BUFFERS ALMOST FULL - 'SLEEPLOOP 10'
    
    The internal mt-job scheduler (mt.cpp) has the function
    update_sched_config() which is supposed to calculate how
    many signals we are allowed to execute in the next
    scheduling round. Allowed number of signals are restricted by
    available job-buffers (aka 'signal buffers') for the
    block-threads we may communicate with.
    
    If the available job-buffers for *any* such thread are below
    a critical threshold, update_sched_config() will wait for more
    job buffers to become available. The wait is implemented with the
    yield() function which takes a 'thr_wait*' argument (Think of this
    as a condition). A 'wakeup(thr_wait*)' (from another thread)
    will later restart the suspended thread.
    
    However, the 'thr_wait*' argument used in these yield() / wakeup()
    was the one used for signal that *incomming* signals has become
    available. This is the opposite of what we want to wait for. The
    effect of this was that these yields waited the maximum [1;31mtime[mout (1ms)
    before execution was resumed.
    
    This fix introduce an additional 'thr_wait*' which is signaled when
    job buffers are released and thus becomes available.
    
    This fix is a collection of the following sub patches:
    
    
     - Add dump of job buffer utilization before we
       crash due to 'job buffer full'
    
     - Change the signature of the yield() function such that it can
       take a more general '*check_callback()' function as arguments.
    
       Change the existing functions currently used as 'callbacks'
       to the new (relaxed) signature.
    
    -  Refactor: Splitt out compute_free_buffers_in_queue()
       from compute_max_signals_to_execute().
       New function contains common code intended for reuse in later
       patches.
    
    - (Fix) update_sched_config() will 'yield' the CPU and wait for
       more job buffers to become available when it is about
       to run out of job buffers.
    
       The yield() call will wait on a 'thr_wait' object, which
       may be sent a 'wakeup()' when the waiting condition has
       been resolved (by another thread)
    
       However, update_sched_config() yield'ed on the incorrect
       'thr_wait' object intended to be used to wait for more
       *incomming signal* - What we actually have to wait for
       are signals to be *consumed* by the destination thread.
       Luckily there is also defined a 'max wait' of 1ms which
       currently will wakeup the thread. Thus, we never waited
       forever, but added latency could be expected.
    
       This patch introduce a new 'thr_wait' object which is
       signaled by the consumer, and update_sched_config()
       will now wait on this object.
    
       Furthermore the patch also avoid a situation where a
       thread could end up waiting for itself.

[33mcommit 7222e14fabac8c65e35a7a1407f45d20f0f8beeb[m
Author: Tiago Jorge <tiago.jorge@oracle.com>
Date:   Mon Sep 30 18:08:27 2013 +0100

    BUG#11763963 - BINARY LOG ACTIVATION SETS SERVER_ID IMPLICITLY,
                   BUT REPLICATION FAILS
    
    Problem:
    When one enables binary logging without setting a server id, letting it
    default to 0, the server changes it to 1, but replication does not work.
    According to the documentation, a server id > 0, should be a legal configuration
    to allow slave to connect and replication to work.
    
    Analysis:
    Apparently, this happens since the dawn of [1;31mtime[m as a design decision, meaning
    that binlog => server_id.
    
    In mysqld.cc::main()
    
    [...]
    
      if (opt_bin_log && server_id == 0)
      {
        server_id= 1;
    #ifdef EXTRA_DEBUG
        sql_print_warning("You have enabled the binary log, but you haven't set "
                          "server-id to a non-zero value: we force server id to
    1; "
                          "updates will be logged to the binary log, but "
                          "connections from slaves will not be accepted.");
    #endif
      }
    [...]
    
    What happened is that, when a slave tried to connect, it would get an error
    stating that the master was misconfigured, even with server-id = 1.
    
    After some testing we concluded that not even the documentation was accurate
    enough. The test conducted were:
    
    1) Start master with no options. Make sure that slave cannot connect (control
    experience)
    2) Start master with only binlog active. Make sure that slave cannot connect
    (the bug situation)
    3) Start master with binlog active and server-id == 0 in the command line.
    Make sure that slave cannot connect. (as documented)
    4) Start master with binlog active and server-id == 1 in the command line.
    Change server_id to 0 using SQL before connecting slave. Make sure that slave
    cannot connect.(as documented)
    
    Situations 1) and 2) are OK and according with expected and analyzed
    behavior. With 3) and 4) things are not as documented.
    
    In Situation 3) the server starts and implicitly sets server id to 1. When
    one starts a slave after configuring the master, there is no error in "SHOW
    SLAVE STATUS". It connects and works. Bin and Relay log have a server id of
    1.
    
    In Situation 4) in which one explicitly change the server_id in run[1;31mtime[m, the
    behavior is the same as in Situation 3). Contents of the relay log and bin
    log have a server id == 0
    
    Fix:
    For trunk only the decision was:
    - Server_id must be explicitly set if binlog is active. If not, the server
    will not start.
    - if you set server_id=0, no slaves can connect, but your statements will be
    binlogged with the provided server_id.
    - if no server_id is set, it will have the default configured value and have
    the same behaviour as the above, if the default is 0.
    
    This patch will bring the code closer to which is documented. The dynamic change
    of server-id was not addressed here.
    
    For the previous version, one thinks that the documentation should be augmented
    according to the information detailed in the bug page.

[33mcommit 25cf56aec3885292efeff9b95f789c4090877787[m
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Wed Sep 25 18:46:07 2013 +0530

    Bug#15877941-MAKE REPLICATION FILTER SETTINGS DYNAMIC
    WL#7057: MAKE --REPLICATION-* FILTER SETTINGS DYNAMIC
    
    Problem:
    The slave options --replicate-* are not dynamic.
    Hence these options cannot be changed while
    the server is running.
    
    Fix: Introduced "CHANGE REPLICATION FILTER" command
    which enables users to modify replication filtering
    rules without having to stop and restart the server.
    This is accomplished by just stopping the sql thread alone
    when these options are set dynamically. Since filtering rules
    are only used by the slave SQL thread, setting them while
    the thread is not running avoids the need for locking.
    Eg:
    CHANGE REPLICATION FILTER REPLICATE_DO_DB=(db1,db2,`db3`);
    CHANGE REPLICATION FILTER REPLICATE_IGNORE_DB=(db1,db2,`db3`);
    CHANGE REPLICATION FILTER REPLICATE_DO_TABLE=(db1.t1);
    CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE=(db1.t1);
    CHANGE REPLICATION FILTER REPLICATE_WILD_DO_TABLE=('db.t%');
    CHANGE REPLICATION FILTER REPLICATE_WILD_IGNORE_TABLE=('db.t%');
    CHANGE REPLICATION FILTER REPLICATE_REWRITE_DB=((db1,db2), (from_db, to_db));
    
    Following are the few behavioral points about the command:
    
    a) Users should be able to set multiple filter rules in one
    command like
    CHANGE REPLICATION FILTER REPLICATE_DO_DB=(db1),
    REPLICATE_IGNORE_TABLE=(db1.t1), ..;
    
    b) To reset the filter value, they have to use void brackets
    "()" syntax, i.e, empty list will clear the existing values
    and set it to an empty value
    For example:
    CHANGE REPLICATION FILTER REPLICATE_WILD_IGNORE_TABLE = ();
    
    c) The non-empty list filter rules will clear the existing values
    and set the value to new list.
    CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE = (db1.t1);
    will reset all existing values and set it the rule to db1.t1 value.
    
    d) Unspecified filter rules will be unchanged. For example:
    CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE = (db1.t1);
    will change REPLICATION_IGNORE_TABLE rule only and rest of the
    filter rules will not be changed and existing values will be continued.
    
    e) If some rule is specified multiple [1;31mtime[ms, the latter list will be
    considered and the earlier list will be ignored.
    For example: CHANGE REPLICATION FILTER REPLICATION_DO_DB=(db1,db2),
    REPLICATE_DO_DB=(db3,db4); db1 and db2 list will be ignored and
    REPLICATE_DO_DB list will be updated with db3 and db4 values.
    
    f) In case of OUT OF MEMORY error, the command might get
    executed partially i.e., few of the filter rules might
    get executed and few of them might not get executed. User needs
    to verify them manually to see which ones are executed.

[33mcommit 2842462367583c32e6da619041df8966d1ee39f9[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Sep 25 12:27:11 2013 +0300

    Bug#17492672 REPLACE MEM_ALLOC() WITH UT_MALLOC()
    
    In InnoDB, the functions mem_alloc() and mem_free() are wrappers for
    mem_heap_alloc() and mem_heap_free(). They create an anonymous
    InnoDB memory heap for the single user object.
    
    Because the functions are defined inline, their invocation will generate
    quite a bit of code.
    
    It is questionable if there is any benefit of using mem_alloc() instead of
    ut_malloc(). There clearly is some CPU and memory overhead. The only possible
    benefit could be the extra debugging info, which is the subject of
    Bug#16924719 69422: small performance impact with heap block debugging info
    in release builds
    
    ut_malloc_low(): Remove the always-true parameter and rename to
    ut_malloc().  Remove ut_mem_null_ptr. Use a simple assertion also with
    the InnoDB memory heaps.
    
    ut_zalloc(): New function, to replace mem_zalloc().
    
    mem_alloc(): Replace with ut_malloc().
    mem_free(): Replace with ut_free().
    
    mem_alloc2(): Replace with ut_malloc().
    We will no longer determine the true allocated size of the block,
    but instead use the requested size only.
    
    os_mem_alloc_large(): Remove an #elif branch that contained non-compiling code.
    (The third argument of ut_malloc_low() had been removed a long [1;31mtime[m ago.)
    
    Do not check if the pointer is NULL before calling ut_free(), because ut_free()
    will check that. The caller-side check was needed for mem_free().
    
    rb#3397 approved by Kevin Lewis

[33mcommit 962f313c9038fcb2f59600d9885eaf7c0245f28a[m
Author: Pavan Naik <pavan.naik@oracle.com>
Date:   Wed Sep 25 12:19:24 2013 +0530

    Bug#17468295   ADD MISSING OUT-OF-BOUND TESTS FOR VARIABLES IN SYS_VAR SUITE
    
    Fix:
    ----
    This bug is a repreat of Bug#13875534. But there were missing tests in Bug#13875534.
    Reviewed the entire sys_vars suite and added missing testcases in following tests.
    
      mysql-test/suite/sys_vars/t/auto_increment_increment_basic.test
      mysql-test/suite/sys_vars/t/auto_increment_offset_basic.t     Bug#17468295   ADD MISSING OUT-OF-BOUND TESTS FOR VARIABLES IN SYS_VAR SUITE
    
          Fix:
          ----
          This bug is a repreat of Bug#13875534. But there were missing tests in Bug#13875534.
          Reviewed the entire sys_vars suite and added missing testcases in following tests.
    
            mysql-test/suite/sys_vars/t/auto_increment_increment_basic.test
            mysql-test/suite/sys_vars/t/auto_increment_offset_basic.test
            mysql-test/suite/sys_vars/t/binlog_max_flush_queue_[1;31mtime[m_basic.test
            mysql-test/suite/sys_vars/t/connect_[1;31mtime[mout_basic.test
            mysql-test/suite/sys_vars/t/host_cache_size_basic.test
            mysql-test/suite/sys_vars/t/innodb_adaptive_flushing_lwm_basic.test
            mysql-test/suite/sys_vars/t/innodb_adaptive_max_sleep_delay_basic.test
            mysql-test/suite/sys_vars/t/innodb_api_bk_commit_interval_basic.test
            mysql-test/suite/sys_vars/t/innodb_max_dirty_pages_pct_basic.test
            mysql-test/suite/sys_vars/t/interactive_[1;31mtime[mout_basic.test
            mysql-test/suite/sys_vars/t/key_cache_block_size_basic.test
            mysql-test/suite/sys_vars/t/lock_wait_[1;31mtime[mout_basic.test
            mysql-test/suite/sys_vars/t/read_rnd_buffer_size_basic.test
            mysql-test/suite/sys_vars/t/table_open_cache_basic.testest
      mysql-test/suite/sys_vars/t/binlog_max_flush_queue_[1;31mtime[m_basic.test
      mysql-test/suite/sys_vars/t/connect_[1;31mtime[mout_basic.test
      mysql-test/suite/sys_vars/t/host_cache_size_basic.test
      mysql-test/suite/sys_vars/t/innodb_adaptive_flushing_lwm_basic.test
      mysql-test/suite/sys_vars/t/innodb_adaptive_max_sleep_delay_basic.test
      mysql-test/suite/sys_vars/t/innodb_api_bk_commit_interval_basic.test
      mysql-test/suite/sys_vars/t/innodb_max_dirty_pages_pct_basic.test
      mysql-test/suite/sys_vars/t/interactive_[1;31mtime[mout_basic.test
      mysql-test/suite/sys_vars/t/key_cache_block_size_basic.test
      mysql-test/suite/sys_vars/t/lock_wait_[1;31mtime[mout_basic.test
      mysql-test/suite/sys_vars/t/read_rnd_buffer_size_basic.test
      mysql-test/suite/sys_vars/t/table_open_cache_basic.test

[33mcommit 44154397b91e8b50406fdae5c91544e856074a45[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Sep 25 00:03:01 2013 +0200

    Bug#17504345 - PERFORMANCE OVERHEAD, THREAD AGGREGATE ON DISCONNECT
    
    Before this fix,
    
    1)
    
    On thread connect, the performance schema needs to evaluate if the thread is
    instrumented or not.
    
    The logic used did look up the content of performance_schema.setup_actors
    for the connecting account, every [1;31mtime[m.
    
    2)
    
    On thread disconnect, the performance schema needs to aggregate statistics
    for parent users / account / hosts or globally, based on statistics
    collected by the disconnecting thread.
    
    The logic did perform this aggregation all the [1;31mtime[m, even when nothing was
    collected during the thread life [1;31mtime[m, for example when the performance
    schema is disabled.
    
    With this fix,
    
    1)
    
    A new attribute PFS_account::m_enabled is created,
    that saves the result of the setup_actors look up.
    
    Whenever the content of the setup_actors table is changed,
    the code re evaluates PFS_acount::m_enabled for every account,
    to reflect new rules in setup_actors.
    
    When a thread connects, the corresponding PFS_account::m_enabled flag is
    used to decide if the thread is to be instrumented, if found.
    
    This avoids looking up setup_actors on every connection,
    the look up only happen the first [1;31mtime[m a given account connects.
    
    This flag is internal, there are no visible end user changes.
    
    2)
    
    A new attribute PFS_thread::m_aggregate_on_disconnect is created,
    that indicates if aggregation during disconnect is needed.
    
    When the performance schema is disabled, this flag is always false.
    
    When the performance schema is enabled, and in particular:
    - when the 'global_instrumentation' consumer is enabled
    - when the 'thread_instrumentation' consumer is enabled
    - when the 'instrumented' column in table thread is enabled
    this flag PFS_thread::m_aggregate_on_disconnect is re evaluated,
    for every running thread if necessary.
    
    The m_aggregate_on_disconnect flag is only set when a thread can collect per
    thread statistics.
    
    During disconnect, aggregation is done only when the flag is set.
    
    The net effect of this change is that aggregation on disconnect only happens
    when necessary, saving CPU when no per thread stats are ever collected.
    
    This flag is internal, there are no visible end user changes.

[33mcommit 0078c465bdbfc3bb03f1e3624ecdd8acf1d669ff[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 23 13:12:00 2013 +0200

    Bug#17493868 IMPROVE PERFORMANCE_SCHEMA MEMORY INSTRUMENTATION
    
    Before this fix,
    
    1)
    
    The memory instrumentation code could spent [1;31mtime[m performing
    operations in struct PFS_memory_stat
    
    2)
    
    Memory allocated internally by the performance schema
    was not reported by the performance schema memory instrumentation.
    
    With this fix,
    
    1)
    
    A new member, PFS_memory_stat::m_used, is added.
    This flags makes the code path much shorter.
    
    For cases when no memort instrumentation is enabled (the default),
    there are no stats to maintain.
    
    Even when the memory instrumentation is used, it is very unlikely that a
    given session has statistics for every kind of memory instrument, so most
    statistics will be unused.
    
    This is a performance improvement.
    
    2)
    
    A new instrument is added, named
    "memory/performance_schema/internal_buffers".
    The performance schema now reports its own memory usage as well.

[33mcommit 96a9c8c6f99e9fdeb125ab9cbefe815f2ef89cab[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Mon Sep 23 11:47:02 2013 +0530

    Bug#11752792  NDB_ERROR_REPORTER DOES NOT INCLUDE CLUSTER LOG WHEN LOGDESTINATION IS US
    
    Backporting latest fixes for ndb_error_reporter and its mtr test to 6.3.
    
    Bug#11764570 Parse log filename from LogDestination, defaulting to
    ndb_<nodeid>*
    Bug#16765651 Add * to scp command to include all log files
    Bug#16602002 Add --connection-[1;31mtime[mout, --skip-nodegroup and --dry-scp
    options
    In 6.3, the nodegroups cannot be queried using ndb_config since the
    'Nodegroup' config parameter is not present. Therefore an 'ndb_mgm -e
    show' command is used to get the nodegroups for the --skip-nodegroup
    option.
    Bug#11756666 Add --help

[33mcommit 43f489368e081d00ea2c0a245de105ab51a1b349[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Sep 18 10:29:48 2013 +0200

    ndb - removes valgrind warnings in ndbcluster_find_files
    
    Backport of:
    ------------------------------------------------------------
    revision-id: magnus.blaudd@oracle.com-20130909114637-8e30ptt4o8xxc239
    parent: magnus.blaudd@oracle.com-20130909105843-hd2snu2chv81c7jl
    committer: magnus.blaudd@oracle.com
    branch nick: 7.3
    [1;31mtime[mstamp: Mon 2013-09-09 13:46:37 +0200
    message:
      Remove valgrind warnings bying rewriting fix for skipping
      "setup of binlog" in ndbcluster_find_files() to avoid relying
      on uninitialized variables and magic flags.
    
        - removes valgrind warnings

[33mcommit 7487251b10f96abad4320ee899f2dcc30cc9b2f3[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Sep 17 13:09:30 2013 -0700

    Remove tests that pass string parameter as [1;31mtime[mstamp

[33mcommit 774804046385659109319fb9372de69771f0daaa[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sat Sep 14 12:08:35 2013 -0700

    Implement query between operator
    
    Add generic query test to test queries on any domain type
    
    Implement query test for [1;31mtime[mstamp type
    
    Add indexes to temporaltypes columns
    
    Insert ten rows into temporaltypes table for query tests
    
    Adapt temporaltypes TypeTest to avoid first 10 ids

[33mcommit d842638dfe5aec308f6e531570ad53a2297c198c[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Sep 12 23:14:07 2013 -0700

    Fix mysql adapter scans with typecast types (date[1;31mtime[m, [1;31mtime[mstamp)

[33mcommit 008ae9e5a76bb8e0efd5bc0b24cdbee67733f2d0[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 12 09:23:38 2013 +0200

    Disable rpl_ndb_ddl since there is a small diff in .result file for output from SHOW TRIGGER which now shows a creation [1;31mtime[m which varies and is not masked.
     - Also thr rpl_ddl.test whic hthis test sources nevere runs and thus this went
    undetected upstream

[33mcommit 6e5e63b9d803c785b581afc84964885d4254b5d7[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Sep 10 07:31:30 2013 -0700

    compile-[1;31mtime[m fixes

[33mcommit 23b4ad91adda8788b73b778a108625c7bb2a3b5b[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Mon Sep 9 19:26:19 2013 +0200

    Bug#17341041: REMOVE UNNEEDED CMAKE CHECKS IN 5.7.3
    
    Additional patch #1:
    
    This patch removes:
    1) CHECK_FUNCTION_EXISTS (pthread_rwlock_rdlock HAVE_PTHREAD_RWLOCK_RDLOCK)
       This check passes on all non-Windows supported platforms.
       For Windows we have a replacement implementation.
       The patch also removes the non-Windows replacement implementation
       as this implementation is not used.
    2) CHECK_FUNCTION_EXISTS (rwlock_init HAVE_RWLOCK_INIT)
       Result not needed due to simplifications made possibly by 1)
    3) CHECK_FUNCTION_EXISTS (getcwd HAVE_GETCWD)
       Supported by all supported platforms (including Windows).
    4) CHECK_FUNCTION_EXISTS (getwd HAVE_GETWD)
       Result not needed due to simplifications made possibly by 3)
    5) CHECK_FUNCTION_EXISTS (mkstemp HAVE_MKSTEMP)
       mkstemp() is part of the POSIX standard and therefore exists
       on all non-Windows supported platforms. For Windows we have a
       replacement implementation.
    6) CHECK_FUNCTION_EXISTS (tempnam HAVE_TEMPNAM)
       Result not needed due to simplifications made possibly by 5)
    7) CHECK_SYMBOL_EXISTS(get[1;31mtime[mofday "sys/[1;31mtime[m.h" HAVE_GETTIMEOFDAY)
       Duplicate check

[33mcommit 072f2e154e4cb2f668637399b360af40b829fdc8[m
Author: Evgeny Potemkin <evgeny.potemkin@oracle.com>
Date:   Mon Sep 9 11:10:38 2013 +0400

    Bug#17428655: INCORRECT EXPLAIN JSON PRINTOUT OF MATERIALIZED VIEWS
    After refactoring done for EXPLAIN FOR CONNECTION in order to correctly
    print subqueries they should be appropriately marked during parse [1;31mtime[m.
    However, this wasn't done for views, as each view is parsed in a separate
    lex. This made EXPLAIN code to think that view's subquery were optimized
    away.
    Now mysql_make_view correctly marks top SELECT_LEX_UNIT of newly created
    lex.

[33mcommit a646975da647896858778eb72c79b665388a9c02[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Fri Sep 6 10:19:55 2013 +0200

    ndb - sync innodb_plugin with 5.1.71
    
    this should make the innodb_plugin.innodb_bug59641 pass in club.
    
    in handler0alter.cc there are only white space changes
    
    log0recv.c is aligned with the result after revision below
    ------------------------------------------------------------
    revno: 3860
    revision-id: annamalai.gurusami@oracle.com-20121109133401-7m2rvsovgxd0d5as
    parent: anirudh.mangipudi@oracle.com-20121109094516-bn06mca84n7gwy9s
    committer: Annamalai Gurusami <annamalai.gurusami@oracle.com>
    branch nick: mysql-5.1
    [1;31mtime[mstamp: Fri 2012-11-09 19:04:01 +0530
    message:
      Bug #14669848 CRASH DURING ALTER MAKES ORIGINAL TABLE INACCESSIBLE
    
      When a new primary key is added to an InnoDB table, then the following
      steps are taken by InnoDB plugin:
    
      .  let t1 be the original table.
      .  a temporary table t1@00231 will be created by cloning t1.
      .  all data will be copied from t1 to t1@00231.
      .  rename t1 to t1@00232.
      .  rename t1@00231 to t1.
      .  drop t1@00232.
    
      The rename and drop operations involve file operations.  But file operations
      cannot be rolled back.  So in row_merge_rename_tables(), just after doing data
      dictionary update and before doing any file operations, generate redo logs
      for file operations and commit the transaction.  This will ensure that any
      crash after this commit, the table is still recoverable by moving .ibd and
      .frm files.  Manual recovery is required.
    
      During recovery, the rename file operation redo logs are processed.
      Previously this was being ignored.
    
      rb://1460 approved by Marko Makela.

[33mcommit f16a9baf630e4d95e5435818c161068d4b893617[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Sep 4 13:16:17 2013 +0200

    remerge of lost innodb patch into 7.2
    
    ------------------------------------------------------------
    revno: 3959 [merge]
    revision-id: marko.makela@oracle.com-20120830190123-ae6knohexzwmy2me
    parent: jorgen.loland@oracle.com-20120830055652-i8xnnr2cx8ld3w5r
    parent: marko.makela@oracle.com-20120830185341-bmfzmrmccmawb7o9
    committer: Marko M?kel? <marko.makela@oracle.com>
    branch nick: mysql-5.5
    [1;31mtime[mstamp: Thu 2012-08-30 22:01:23 +0300
    message:
      Merge mysql-5.1 to mysql-5.5.
        ------------------------------------------------------------
        revno: 2661.810.79
        revision-id: marko.makela@oracle.com-20120830185341-bmfzmrmccmawb7o9
        parent: marko.makela@oracle.com-20120830184924-xqhoy33bvlov44jh
        committer: Marko M?kel? <marko.makela@oracle.com>
        branch nick: mysql-5.1
        [1;31mtime[mstamp: Thu 2012-08-30 21:53:41 +0300
        message:
          Bug#14554000 CRASH IN PAGE_REC_GET_NTH_CONST(NTH=0) DURING COMPRESSED
          PAGE SPLIT
    
          page_rec_get_nth_const(): Map nth==0 to the page infimum.
    
          btr_compress(adjust=TRUE): Add a debug assertion for nth>0. The cursor
          should never be positioned on the page infimum.
    
          btr_index_page_validate(): Add test instrumentation for checking the
          return values of page_rec_get_nth_const() during CHECK TABLE, and for
          checking that the page directory slot 0 always contains only one
          record, the predefined page infimum record.
    
          page_cur_delete_rec(), page_delete_rec_list_end(): Add debug
          assertions guarding against accessing the page slot 0.
    
          page_copy_rec_list_start(): Clarify a comment about ret_pos==0.
    
          rb:1248 approved by Jimmy Yang
        ------------------------------------------------------------
        revno: 2661.810.78
        revision-id: marko.makela@oracle.com-20120830184924-xqhoy33bvlov44jh
        parent: jorgen.loland@oracle.com-20120828125101-asra6q4a1jxw6zyy
        committer: Marko M?kel? <marko.makela@oracle.com>
        branch nick: mysql-5.1
        [1;31mtime[mstamp: Thu 2012-08-30 21:49:24 +0300
        message:
          Bug#14547952: DEBUG BUILD FAILS ASSERTION IN RECORDS_IN_RANGE()
    
          ha_innodb::records_in_range(): Remove a debug assertion
          that prohibits an open range (full table).
    
          The patch by Jorgen Loland only removed the assertion from the
          built-in InnoDB, not from the InnoDB Plugin.
    ------------------------------------------------------------
    revno: 2661.810.80
    revision-id: marko.makela@oracle.com-20120831065127-t8nke152vnu8x05h
    parent: marko.makela@oracle.com-20120830185341-bmfzmrmccmawb7o9
    committer: Marko M?kel? <marko.makela@oracle.com>
    branch nick: mysql-5.1
    [1;31mtime[mstamp: Fri 2012-08-31 09:51:27 +0300
    message:
      Add forgotten have_debug.inc to a test.

[33mcommit 94b1ec4b38e9c38ffd24e97ba3af5099bd790201[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Tue Sep 3 16:54:30 2013 +0530

    Bug#16602002 PROVIDE OPTION TO SKIP NODEGROUP=65536 NODES FROM NDB_ERROR_REPORTER
    
    Modified ndb_error_reporter to add a --connection-[1;31mtime[mout parameter for
    a [1;31mtime[mout while connecting to nodes, a --skip-nodegroup parameter to
    skip all data nodes belonging to a specific nodegroup, and a --dry-scp
    option to print the scp command instead of executing it.
    
    Added a unit test for ndb_error_reporter in mtr.

[33mcommit 2e514b075d0453828519e9116cc2e8de507d8ecb[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Aug 28 15:00:21 2013 +0200

    Bug#17341041: REMOVE UNNEEDED CMAKE CHECKS IN 5.7.3
    
    This patch removes:
    1) CHECK_FUNCTION_EXISTS (strtok_r HAVE_STRTOK_R)
       This function is part of the POSIX.1-2001 standard
       and is thus available on all non-Windows supported
       platforms. On Windows we replace strtok_r with strtok_s.
       Also removed replacement implementation.
    2) CHECK_FUNCTION_EXISTS (strdup HAVE_STRDUP)
       This function is part of the POSIX.1-2001 standard.
       It is also available on Windows.
    3) CHECK_SYMBOL_EXISTS(tzname "[1;31mtime[m.h" HAVE_TZNAME)
       This variable is part of the POSIX.1-2001 standard.
       It is also available on Windows.

[33mcommit 028c291ff35560eb3a08a8a6ea2ef1f3bb4797c7[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Aug 26 13:27:29 2013 +0200

    This is a fix for Bug #17177279 'NDB : IMPROVE JAM() COVERAGE AND
    EFFECTIVENESS'. This commit changes the jam trace mechanism such that each
    entry in the jam trace buffer now will refer to a unique file id (JAM_FILE_ID)
    rather than a block number. This bringe the following benefits:
    - It is much easier to map an entry to the right source code location.
      (Trace files will point to the source file rather than the block.)
    - jam should be slightly faster, since the jam value can be calculated at
      compile [1;31mtime[m.
    - Forgetting to call jamEntry() when going from one block to another is no
      longer a problem. Each jam() call records the complete context. From
      now on, there is no need to add jamEntry() calls, jam() is sufficient.
    - There is no need to maintain per-file jam offsets to distinguish between
      different files within a block.
    
    See comments for jamFileNames in Emulator.cpp for a decription of how
    to add new JAM_FILE_IDs.
    
    This commit gathers include directives at the top of the source files
    that have JAM_FILE_Ids. That way, it is easier to add a JAM_FILE_ID
    macro that will not be  redefined by some include file. Also, gathering
    include directives in one place improves code readability.
    
    Notes:
    * There were two cases of the following pattern: A.hpp defines class A. Then
      B.hpp in included, which defines class B and then defines inline methods for
      B which refers members of A. Finally A.hpp defines inline methods for A
      which refers members of B. This mutual dependency means that A.hpp cannot
      include B.hpp until *after* class A has been defined. It turned out that
      both cases of this pattern were due to methods that were not in use.
      This was LogLevel::operator=(const EventSubscribeReq&) and
      SignalCounter::operator=(const NodeReceiverGroup&). These methods have been
      removed such that the corresponding #include directives could be moved to
      the top.
    
    * SimulatedBlock.hpp had an unneeded #include of Mutex.hpp. This would not
      compile unless Mutex.hpp was included after initial definitions in
      SimulatedBlock.hpp. Therefore, the #include was removed from
      SimulatedBlock.hpp. Instead, Mutex.hpp is included directly from the files
      that need it.

[33mcommit 4d275c89954685e2ed1b368812b3b5a29ddf9389[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Thu Aug 22 11:38:26 2013 +0530

    Bug#11844915 - Hang in THDVAR mutex acquisition
    
    Analysis:
    --------------
    Storage for session system variables (THDVAR) from plugins were
    allocated lazily. On the first access to any of such variables
    memory for all of them was allocated and values were copied from
    global variables. This was done under protection of
    LOCK_global_system_variables mutex.
    
    The problem might occur when such first access happens from within
    InnoDB storage engine if at this point some internal InnoDB locks
    are also held. We can end-up in deadlock situation when the thread
    owning these locks is waiting for LOCK_global_system_variables mutex
    and thread owning LOCK_global_system_variables waits (maybe
    indirectly through several other threads) for one of these InnoDB
    locks.
    
    According to InnoDB team this doesn't happen in the current code,
    as InnoDB tries to avoid accessing session variables while holding
    internal locks, but this has happened in the past.
    
    Since this also might happen in future when code changes, we would
    like to make our code more future proof by avoiding this problem.
    
    Fix:
    --------------
    Instead of allocating storage for session system variables (THDVAR)
    lazily, modified code to populate session system variables on
    creation of thread for the connection. So now, to accesses these
    variables from plugins, mutex "LOCK_global_system_variables" need
    not be a acquired.
    
    Since, innodb is built-in storage engine, all the THDVARs of innodb
    are populated at this [1;31mtime[m with other THDVARs. So now, accessing
    these THDVARs from innodb layer never acquires mutex
    "LOCK_global_system_variables".
    
    Note that since connection object preparation happens in the connection
    thread nowadays the performance impact of such a change should be
    acceptable.
    
    For dynamically loaded plugins situations stays the same as before.
    If plugin was loaded after connection was created then memory for
    THDVARs of this plugin will be allocated on the first access to them
    and LOCK_global_system_variables mutex will be acquired. So it is a
    good idea for plugins to avoid holding internal locks while accessing
    to such variables.
    
    OTOH, since in practice most of connections will be created after the
    plugin load and thus will have memory for plugin's THDVARs allocated
    at connection creation [1;31mtime[m the probability of deadlock occurring for
    dynamic plugins is greatly reduced by this patch as well.
    
    Also, while populating THDVARs, we copy all TDHVARs and then
    iterate through ALL vars using hash "bookmark_hash". In this loop,
    now we are only checking whether var is of type string with flag
    MEMALLOC or not. If yes then strdup the value. If we do not have
    any vars of this type then we just iterate through all the vars. To
    avoid this, one more hash is introduced to hold the reference of
    variable of type STRING with flag MEMALLOC only. If this hash has
    any elements then only we iterate through them otherwise no.

[33mcommit 41e8749f3884915449f72094d15ca331644bcba8[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Aug 21 12:08:59 2013 +0300

    Bug#17338452 REMOVE UNIV_BLOB_DEBUG
    
    The compile-[1;31mtime[m debug option UNIV_BLOB_DEBUG was introduced to help catch
    bugs like
    Bug#11762662 55284: DOUBLE FREE OF OFF-PAGE COLUMNS DUE TO LOCK WAIT WHILE
    UPDATING PRIMARY KEY
    
    Its worst limitation is that it does not work across crash recovery. This
    flag has not been used much in tracking down BLOB bugs. As far as I remember,
    most BLOB bugs since then have been found by other means.
    
    rb#3147 approved by Jimmy Yang

[33mcommit 340016a8327fbb833daccf5c906aff2937211ac6[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Aug 21 12:08:03 2013 +0300

    Bug#17338432 REMOVE UNIV_SEARCH_DEBUG
    
    There is some debug code that is enabled by defining UNIV_SEARCH_DEBUG at
    compilation [1;31mtime[m.
    
    As far as I remember, these debug checks have never failed, except for a
    short [1;31mtime[m during 5.7.2 development when a bug was accidentally introduced in
    the UNIV_SEARCH_DEBUG code:
    
    Bug#16915379 ASSERT DBG_CMP >= 0 IN CURSOR HANDLING
    
    rb#3146 approved by Jimmy Yang

[33mcommit 280c2844a88885484ab8d12bc3179e6fd6f2f1bb[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Sat Aug 17 00:39:53 2013 +0100

    Tests for new variables got lost in collapse.
    log_[1;31mtime[mstamps, log_error_verbosity
    WL6661

[33mcommit e6f1e794338833cd546026d4e2cbbfa3d889a4de[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Fri Aug 16 20:17:40 2013 +0100

    WL#6661: Error logging: Allow control of verbosity
    
    In mysqld, all local messages ("things found on
    console / in error log") now go through a central
    function rather than through a lot of fprint()s.
    This means we can properly [1;31mtime[mstamp those lines
    with ISO 8601 compliant [1;31mtime[mstamps (likewise for
    slow and general log files); it also means that
    we can gag those messages with the new system
    variable / startup option --log_error_verbosity,
    which depraces --log_warnings / -W.
    As [1;31mtime[mstamps go, the DBA can choose between system
    [1;31mtime[m or zulu [1;31mtime[m with the new option --log_[1;31mtime[mstamps.
    
    Client programs now print their warnings prefixed by
    their name (no path, no ".exe") and the severity level
    in brackets, so "Warning: foo" from bin/mysql.exe
    becomes "mysql: [Warning] foo"
    This is useful especially in mysql_upgrade (which in
    turn calls other binaries) so we can see whether a
    warning was thrown by mysql_upgrade or one of its
    children (and in the latter case, which).
    
    On the tech side, we now have a generic hook for
    local messages. This prints to stderr by default
    (and in the case of client apps likely always will),
    but can be overridden to log elsewhere, mangle the
    string, etc. (as is done by the server once the
    facilities are initiliazed; then this goes to
    error_log_print, for better [1;31mtime[mstamps, filtering by
    verbosity level, and for printing to NT syslog on Win).
    
    We deprecate my_printf_warning() which does something
    similar, with a less clear name.
    
    New test cases added, comments clarified, old test cases
    updated.

[33mcommit 7a77a91ae0a854661db7190de0d073c49ff92df9[m
Merge: 4ab816a8f53 ed51bc83c39
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Aug 15 22:37:17 2013 +0100

    Bug#16732621: GQL DOESN'T MASK PASSWORDS IN PREPARED STATEMENTS
    
    Rewriting ("password obfuscation", WL#5706) now also works
    for prepared statements, i.e. we try not to log any passwords
    while handling prepared statements:
    
    In the general log,
    -- the "Query" line (for SQL PS) will no
       no longer contain the statement to prepare;
       if we prepare from a variable, its name is still
       given, but if we prepare from a string literal,
       that literal is not logged as we don't yet know
       whether it contains a password.
    
       - as always, --log_raw forces "old behaviour",
         i.e., the query is logged prior to parsing,
         and any passwords contained therein are printed
         to the log in plaintext
    
       - as before, PS protocol prepared statements
         only result in a Prepare line and an Execute
         line in the log (at prepare and at execute [1;31mtime[m,
         respectively); a Query line was not logged before,
         and is not logged now when PS protocol is used.
    
    -- the "Prepare" line will contain the statement,
       with passwords obfuscated
    
    -- the "Execute" line will contain the statement,
       with passwords obfuscated, and values filled in
    
    Query:
    prepare s4 from "set password=password('meow')";
    execute s4;
    
    General Log:
    Query     PREPARE s4 FROM ...
    Prepare   SET PASSWORD FOR `root`@`localhost`=<secret>
    Query     execute s4
    Execute   SET PASSWORD FOR `root`@`localhost`=<secret>
    
    
    In binlog, passwords are replaced with their hashes (rather than
    with a string literal) so replication still works. The
    binlog line for the above SET PASSWORD statement is,
    
    Binlog:
    Query     use `test`; SET PASSWORD FOR 'root'@'localhost'='*82DC221D557298F6CE

[33mcommit ed51bc83c39b974a86dcb41d7f56d94ccc8b76da[m
Merge: 007d33d644a f33f001cc4c
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Aug 15 21:14:41 2013 +0100

    Bug#16732621: GQL DOESN'T MASK PASSWORDS IN PREPARED STATEMENTS
    
    Rewriting ("password obfuscation", WL#5706) now also works
    for prepared statements, i.e. we try not to log any passwords
    while handling prepared statements:
    
    In the general log,
    -- the "Query" line (for SQL PS) will no
       no longer contain the statement to prepare;
       if we prepare from a variable, its name is still
       given, but if we prepare from a string literal,
       that literal is not logged as we don't yet know
       whether it contains a password.
    
       - as always, --log_raw forces "old behaviour",
         i.e., the query is logged prior to parsing,
         and any passwords contained therein are printed
         to the log in plaintext
    
       - as before, PS protocol prepared statements
         only result in a Prepare line and an Execute
         line in the log (at prepare and at execute [1;31mtime[m,
         respectively); a Query line was not logged before,
         and is not logged now when PS protocol is used.
    
    -- the "Prepare" line will contain the statement,
       with passwords obfuscated
    
    -- the "Execute" line will contain the statement,
       with passwords obfuscated, and values filled in
    
    Query:
    prepare s4 from "set password=password('meow')";
    execute s4;
    
    General Log:
    Query     PREPARE s4 FROM ...
    Prepare   SET PASSWORD FOR `root`@`localhost`=<secret>
    Query     execute s4
    Execute   SET PASSWORD FOR `root`@`localhost`=<secret>
    
    
    In binlog, passwords are replaced with their hashes (rather than
    with a string literal) so replication still works. The
    binlog line for the above SET PASSWORD statement is,
    
    Binlog:
    Query     use `test`; SET PASSWORD FOR 'root'@'localhost'='*82DC221D557298F6CE

[33mcommit 007d33d644abc853eea3f14a65b002e52412aa0c[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Aug 15 16:18:02 2013 +0200

    Bug#16661195: REMOVE UNNEEDED CMAKE CHECKS
    
    Additional patch #4.
    Remove HAVE_GMTIME_R, HAVE_LOCALTIME_R, HAVE_PTHREAD_KEY_DELETE,
    HAVE_PTHREAD_ATTR_SETSTACKSIZE, HAVE_PTHREAD_ATTR_GETSTACKSIZE.
    All are part of POSIX and available on all non-Windows
    supported platforms. For Windows we have replacement implementations.
    
    Also remove HAVE_THR_SETCONCURRENCY and the already deprecated
    thread_concurrency server system variable.
    
    Finally, remove unused non-Windows replacement implementation
    for local[1;31mtime[m_r as well as various thread_safe_add/_sub macros.

[33mcommit 51180cc85c670fc4ce0b64cc1efe467c00957d65[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Wed Aug 14 19:21:46 2013 +0530

    BUG#16498740 - CREATE/ALTER: INCORRECT ERROR MSG. SEPARATE
                   UNSUPPORTED OPS FROM OTHER FAILURES
    
    BACKGROUND:
    CREATE or ALTER table can fail due to unsupported options
    OR if the user forgot to specify something in the command.
    In this particular case in the bug report, ALTER table failed
    because the user forgot to specify tablespace for ndb during
    create table [1;31mtime[m.
    While performing alter table, the current error message said
    "doesn't have this option" which can be misleading to the user.
    The given option is supported but the user forgot to specify
    tablespace during create [1;31mtime[m.
    Instead of misleading error message we can print a useful error
    message to the user.
    
    ANALYSIS:
    On failure of Alter table, HA_WRONG_CREATE_OPTION was invoked
    which mapped to ER_ILLEGAL_HA in handler::print_error.
    This change was done in Bug#13840553.
    ER_ILLEGAL_HA can be misunderstood by user that storage engine
    (in this case ndbcluster) does not support altering column
    storage from main memory to disk whereas the problem is
    user forgot to specify tablespace for ndb during create table.
    
    FIX:
    To fix this bug, a new handler error HA_MISSING_CREATE_OPTION
    is added and it is returned when ALTER fails which is then
    mapped to newly introduced error code ER_MISSING_HA_CREATE_OPTION
    in handler::print_error() function.

[33mcommit ad3fc87d4809a89376b686bad0d7d6ce117c2eae[m
Merge: 99f1e5b6bb8 dbfdc9b7e26
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Tue Aug 13 20:48:29 2013 +0530

    Wl#6314: Prepared transactions slave parallel applier
    
    Description: Implemented intra-schema multi-threaded slave
    MySQL replication slaves can now apply events in parallel within
    the database. This is made possible by using the parallelization
    information from the master. The information is stored as logical
    [1;31mtime[mstamp of the last transaction that committed before the current
    one, when this transaction entered the prepare phase. All the
    transactions that have the same commit parent can be executed in
    parallel.

[33mcommit 8866c2aa0b0dff6ea4de857477146d0662d6d654[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Aug 13 11:46:34 2013 +0200

    Bug#12593774 NDB_MGMD TAKES TO LONG TO CONNECT TO THE DATA NODES
    
     - In setups where the ndbd was configured to use dynamic listening ports for accepting
      connections from api nodes, the dynamic port number was reported serially from ndbd
      to ndb_mgmd. This caused one roundtrip per configured api node and thus the [1;31mtime[m
       required for ndbd to connect to ndb_mgmd grew linearly with the number of configured
      api nodes. The ndbd will now report all dynamic ports at once using a new command
       in ndb_mgmd(if ndb_mgmd supports the new command).

[33mcommit a13786fe47b188baf8e1f20b36611f8ed6ea7654[m
Merge: 930ba8db357 c2f111aa119
Author: Anirudh Mangipudi <anirudh.mangipudi@oracle.com>
Date:   Mon Aug 12 23:09:49 2013 +0530

    Bug #16776528 RACE CONDITION CAN CAUSE MYSQLD TO REMOVE SOCKET FILE ERRANTLY
    Problem Description:
    A mysqld_safe instance is started. An InnoDB crash recovery begins which takes
    few seconds to complete. During this crash recovery process happening, another
    mysqld_safe instance is started with the same server startup parameters. Since
    the mysqld's pid file is absent during the crash recovery process the second
    instance assumes there is no other process and tries to acquire a lock on the
    ibdata files in the datadir.  But this step fails and the 2nd instance keeps
    retrying 100 [1;31mtime[ms each with a delay of 1 second. Now after the 100 attempts,
    the server goes down, but while going down it hits the mysqld_safe script's
    cleanup section and without any check it blindly deletes the socket and pid
    files. Since no lock is placed on the socket file, it gets deleted.
    
    Solution:
    We create a mysqld_safe.pid file in the datadir, which protects the presence
    server instance resources by storing the mysqld_safe's process id in it. We
    place a check if the mysqld_safe.pid file is existing in the datadir. If yes
    then we check if the pid it contains is an active pid or not. If yes again,
    then the scripts logs an error saying "A mysqld_safe instance is already
    running". Otherwise it will log the present mysqld_safe's pid into the
    mysqld_safe.pid file.

[33mcommit 7ab6c9bf8d53db5a228eb703df66e51fc86c2e6c[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Sat Aug 10 12:46:25 2013 +0100

    Bug#16732621: GQL DOESN'T MASK PASSWORDS IN PREPARED STATEMENTS
    
    Rewriting ("password obfuscation", WL#5706) now also works
    for prepared statements, i.e. we try not to log any passwords
    while handling prepared statements:
    
    In the general log,
    -- the "Query" line (for SQL PS) will no
       no longer contain the statement to prepare;
       if we prepare from a variable, its name is still
       given, but if we prepare from a string literal,
       that literal is not logged as we don't yet know
       whether it contains a password.
    
       - as always, --log_raw forces "old behaviour",
         i.e., the query is logged prior to parsing,
         and any passwords contained therein are printed
         to the log in plaintext
    
       - as before, PS protocol prepared statements
         only result in a Prepare line and an Execute
         line in the log (at prepare and at execute [1;31mtime[m,
         respectively); a Query line was not logged before,
         and is not logged now when PS protocol is used.
    
    -- the "Prepare" line will contain the statement,
       with passwords obfuscated
    
    -- the "Execute" line will contain the statement,
       with passwords obfuscated, and values filled in
    
    Query:
    prepare s4 from "set password=password('meow')";
    execute s4;
    
    General Log:
    Query     PREPARE s4 FROM ...
    Prepare   SET PASSWORD FOR `root`@`localhost`=<secret>
    Query     execute s4
    Execute   SET PASSWORD FOR `root`@`localhost`=<secret>
    
    
    In binlog, passwords are replaced with their hashes (rather than
    with a string literal) so replication still works. The
    binlog line for the above SET PASSWORD statement is,
    
    Binlog:
    Query     use `test`; SET PASSWORD FOR 'root'@'localhost'='*82DC221D557298F6CE9961037DB1C90604792F5C'

[33mcommit 6ef8c343445a26aaf9ebd76d72cf57db44b481f5[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Aug 9 08:37:45 2013 +1000

    WL#7047 - Optimize buffer pool list scans and related batch processing code
    
    Reduce excessive scanning of pages when doing flush list batches. The
    fix is to introduce the concept of "Hazard Pointer", this reduces the
    [1;31mtime[m complexity of the scan from O(n*n) to O(n).
    
    The concept of hazard pointer is reversed in this work.  Academically a
    hazard pointer is a pointer that the thread working on it will declare as
    such and as long as that thread is not done no other thread is allowed to
    do anything with it.
    
    In this WL we declare the pointer as a hazard pointer and then if any other
    thread attempts to work on it, it is allowed to do so but it has to adjust
    the hazard pointer to the next valid value. We use hazard pointer solely for
    reverse traversal of lists within a buffer pool instance.
    
    Add an event to control the background flush thread. The background flush
    thread wait has been converted to an os event [1;31mtime[md wait so that it can be
    signalled by threads that want to kick start a background flush when the
    buffer pool is running low on free/dirty pages.
    
    This patch was originally written by Inaam Rana.
    
    rb#2367 Approved by Jimmy Yang and Kevin Lewis.

[33mcommit b4bf2d5bb511d08a17fec540463ce020fdb322ba[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Wed Aug 7 10:03:18 2013 +0530

    WL#6606 - Offload THD and network initialization to worker
              thread.
    Initialization of THD and vio/net initialization happens
    in the acceptor thread that accepts the connection. THD and
    network initialization involves acquiring locks, memory
    allocation of various structures and system calls which are
    compute-bound as well as tasks that may block. The acceptor
    thread is an event loop that waits for new connection events
    from clients. To maximize the number of connections that can
    be handled per unit of [1;31mtime[m, the acceptor thread should
    spend as much of its [1;31mtime[m listening for new connections.
    This means thd and vio/net initialization should be
    offloaded from the accept event loop and delegated to
    worker threads that handle the client connections.
    This worklog provides a generic framework which offloads
    THD initialization and net/vio initialization to worker
    threads for all types of communication channels (shared
    memory, named pipes and sockets) that clients connect with
    server.
    In addition, this worklog refactored the existing
    interfaces of the struct scheduler_functions into an object
    oriented API, refactored and moved code related to
    connection handling and its management into a separate
    directory and files that contain implementations of specific
    related functionality. This resulted in removal of unnecessary
    #defines, modularity, better code clarity and readability in
    addition to performance improvements made in the worklog.
    As result of changes in this worklog, the follow bugs have
    been fixed:
    Bug#12951536 - THD INITIALIZATION TOO EXPENSIVE FOR
                   ACCEPT() THREAD.
    Bug#12951595 - TOO MUCH NETWORK INITIALIZATION DONE
                   IN ACCEPT() THREAD.
    Bug#12951605 - ACCEPT() SOCKET GETS TOO MUCH OF FCNTL()S.
    
    User Visible Changes:
    The system variables "bind_address", "thread_handling",
    ""thread_cache_size" and status variables "threads_cached",
    "Slow_launch_threads" are no longer visible in embedded
    server mode (where they have no effect).

[33mcommit b9c431edcbb41f749b9e8529d7a747a3c2da4a64[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Aug 7 09:00:02 2013 +0530

    - bug#17227149: INNODB_WL6501 KEEPS ON FAILING INTERMITTENTLY
      - In one of the injected error even though file was open we injected
        error to assume it is closed and this lead to issue on Windows
        as Windows will have lock on the file and will not remove it from
        system till lock is expired. Leading to [1;31mtime[mout of test-case.
      - Ensure you close the file-handle in such cases while injecting error.
    
     Approved by Sunny (over IM)

[33mcommit f67cbdd3f4f76af5832ed26933fbd099033a6856[m
Author: Sivert Sorumgard <sivert.sorumgaard@oracle.com>
Date:   Tue Aug 6 12:11:18 2013 +0200

    Bug#16587369: HANDLER FOR SQLEXCEPTION NOT CATCHING ERROR
    1452 (FK CONSTRAINT) FOR UPDATE
    
    Overview
    --------
    This is a bug regarding the handling of fatal errors. A fatal
    error is an error that should not be caught by a condition
    handler in a stored procedure. The issues described in the bug
    report are that:
    
    1. The same error is handled differently for different
       statements.
    2. Seemingly trivial errors are treated as fatal.
    
    It should be noted that in the context of this bug report (and
    bug fix), only handler errors are relevant to consider. There are
    two important issues regarding handler errors in the source code:
    
    1. Deciding whether a handler error is to be considered fatal.
    2. Ensuring that the handler error is indeed treated as fatal.
    
    For the first issue, "is_fatal_error()" is provided by the
    handler api, and returns true for handler errors that are
    fatal. The second issue can be handled in various ways; the most
    common is to call "print_error()" with a flag indicating that the
    handler error is to be considered fatal. Inspecting how the
    source code handles these two issues, in light of the problems
    reported, reveals three shortcomings in the implementation:
    
    1. Handler errors are in fact fatal by default, as defined by the
       "is_fatal_error()" function.
    2. When an error is returned from the handler api, the caller
       does not always check if the error is fatal.
    3. The presence of a fatal error is not always flagged when
       calling "print_error()".
    
    
    Suggested fix
    -------------
    The scope and ambition for the fix is to treat handler errors
    consistently for DML statements (update, insert, delete) by
    making sure "is_fatal_error()" is called before calling
    "print_error()", and to provide the "ME_FATALERROR" flag as
    appopriate. This alone does not fix the bug that is reported,
    since handler errors are fatal by default, hence, we also need to
    extend the function "is_fatal_error()" to recognize more handler
    errors as being non-fatal.
    
    An attempt was made to consider handler errors non-fatal by
    default, but this made a large number of tests fail in not very
    obvious ways, so this solution was not pursued further. Instead,
    a switch for catching non-fatal errors was added to
    "is_fatal_error()", currently catching:
    
    - HA_ERR_NO_REFERENCED_ROW: The error originally addressed by the
      bug report considered here.
    - HA_ERR_ROW_IS_REFERENCED: Another FK constraint violation
      error, an obvious extension of the bug we are addressing.
    - HA_ERR_LOCK_WAIT_TIMEOUT: See below.
    
    Changing the implementation of "is_fatal_error()" has a potential
    impact on the non-DML source code, which also calls this
    function; however, no damage has been observed in the testing
    that has been done. On the other hand, tests failed for other
    reasons:
    
    1. Some tests expected a fatal outcome in situations where the
       error is now non-fatal (e.g. for foreign key constraint
       violations). The fix for this was to modify the test.
    2. Some tests expected a non-fatal outcome where the error is now
       fatal due to the changed implementation of DML statements
       where we ensure that errors are treated the way they ought
       to. The fix for this was to extend "is_fatal_error()" to
       consider the error in question as non fatal (the error here
       was lock wait [1;31mtime[mout).
    
    Re-considering which handler errors should be considered fatal is
    still a relevant task, but is considered beyond the scope of this
    bug fix. Additionally, such a task might be the responsibility of
    the storage engine teams.

[33mcommit 1fed09d6a5002e79c0141cd19c84baf84ecfc569[m
Merge: bcd614f3547 ffede7b71cf
Author: bin.x.su@oracle.com <>
Date:   Tue Aug 6 10:00:38 2013 +0800

    The assertion 'ut_ad(oldest_lsn <= cur_lsn)' can't hold, because we get the
    current max LSN before we get the oldest LSN from the buffer pool.
    
    We can simply change the assertion to 'ut_ad(oldest_lsn <= log_get_lsn())'
    to fix it. At the mean[1;31mtime[m, the cur_lsn is used in a subtraction, which is
    'cur_lsn - oldest_lsn' to get the 'age'. If the original assertion can't hold,
    the subtraction would result in a very big number. Although the unexpected
    subtraction is harmless, I'd like to check the cur_lsn to get a reasonable
    'age'.
    
    rb#3013, approved by Marko

[33mcommit 42aedc2a5c06d6282ecea2f88ac8f9fb69de07b5[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Mon Aug 5 15:23:44 2013 -0700

    Rationalize type converters
    
    Two kinds of converters are defined:
      database type converters convert between database types and intermediate types
        database type converters are adapter-specific
      domain type converters convert between intermediate types and domain types
    User-specified converter in mapping overrides system-specified domain type converter
    
    Date and [1;31mtime[m types need both database type converter and domain type converter
      Date and Time database types are mapped to string
      The mysql adapter uses felix which maps Date types to javascript Date
        This behavior needs to be overridden to allow mapping to javascript string
      DateTime and Timestamp database types are mapped to javascript Date
        The intermediate type is a mysql-js type MySQLTime which supports fractional seconds
        The domain type is a javascript Date using a standard domain type converter
    
    DBTableHandler:
      during construction, use user's converter or database-default domain type converter
      for get, apply domain type converter and then database type converter
      for set, apply database type converter and then domain type converter
    
    MySQLTime:
      add conversions to and from DateTime strings as used in database
    
    MySQLConnection:
      change read by key and scan to use typeCast (felix name for driver type converter)
    
    MySQLConnectionPool:
      define driver type converters, database type converters, and domain type converters
        this allows non-loss use of DATETIME and TIMESTAMP database types
          supports millisecond precision for javascript Date
          supports microsecond precision for user-defined more-better-Date
      implement string to MySQLTime for database type converter
      implement MySQLTime to javascript Date for domain type converter
    
    MySQLDictionary:
      set up default domain type converters and database type converters

[33mcommit aa3be07eb1ab528dbf3911bf7d3e80acf67b0cfe[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Mon Aug 5 13:30:14 2013 -0700

    Improve temporaltypes test to avoid zero-hour bug in [1;31mtime[mzone
    
    TypeTest: use 1 AM instead of midnight on the epoch

[33mcommit 9cb3e4571db735933402b5c362c5eaaaaec6d77e[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Thu Aug 1 11:25:12 2013 +0700

    This is patch for bug#17170755: Wrong internal representation of trigger creation [1;31mtime[mstamp.
    
    The bug relates to wrong storing of [1;31mtime[mstamp value in the data dictionary (.TRG file).
    Test case was added to guarantee that internal storing format won't change in the future
    without change in the data dictionary format.
    Also minor refactoring was done - repeatable code to calculate [1;31mtime[mstamp as [1;31mtime[mval
    was moved to dedicated method of class Trigger.

[33mcommit 80995f297bd3c0b7da0c881ebe79198cafd95047[m
Author: mayank prasad <mayank.prasad@oracle.com>
Date:   Tue Jul 30 21:33:05 2013 +0530

    Bug#17182801:NESTING_EVENT_TYPE/ID ARE STILL NULL FOR EVENT NESTED STATEMENTS.
    
    Issue :
     - NESTED_EVENT_/TYPE/LEVEL was NULL/0 for events.
    
    Reason :
     - Evetns are not executed as part of some statement i.e. there
       is no parent statement for a event. Events are scheduled based
       on timing and are executed when [1;31mtime[m comes.
    
    Fix :
     - A new instrument statement/scheduler/event is added to make
       sure all executed events are recorded by PS. This instrument
       would be the parent for events executed.

[33mcommit a071ee43bfa232310e9a0a7019451fb324ff1a28[m
Merge: 8173e32db0d 4b0b3eab7e4
Author: prabakaran thirumalai <prabakaran.thirumalai@oracle.com>
Date:   Tue Jul 30 11:22:14 2013 +0530

    Bug#17083851    BACKPORT BUG#11765744 TO 5.1, 5.5 AND 5.6
    
    Description:
    Original fix Bug#11765744 changed mutex to read write lock
    to avoid multiple recursive lock acquire operation on
    LOCK_status mutex.
    On Windows, locking read-write lock recursively is not safe.
    Slim read-write locks, which MySQL uses if they are supported by
    Windows version, do not support recursion according to their
    documentation. For our own implementation of read-write lock,
    which is used in cases when Windows version doesn't support SRW,
    recursive locking of read-write lock can easily lead to deadlock
    if there are concurrent lock requests.
    
    Fix:
    This patch reverts the previous fix for bug#11765744 that used
    read-write locks. Instead problem of recursive locking for
    LOCK_status mutex is solved by tracking recursion level using
    counter in THD object and acquiring lock only once when we enter
    fill_status() function first [1;31mtime[m.

[33mcommit 86b01057eee3558e371effb4d906ebe2f86de392[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Sat Jul 27 09:09:13 2013 +1000

    WL#6578 - Create a new std:vector like class to handle read view trx IDs.
    Add infrastructure for reading the transaction [1;31mtime[m from the server
    session instance. Bypass RW code paths when a transaction is RO. Some minor
    code rearrangement in btr0sea.cc, after doing some perf tests. Seems like
    some instruction cache misses due to jumps (gotos). Make some hash functions
    const correct.

[33mcommit f1a162b029b0f1dabc693e57d8721e1acdfa9981[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Thu Jul 25 12:11:12 2013 +0530

    BUG#16633880 - VALGRIND FAILURE IN
                   THREAD_POOL.THREAD_POOL_KILL_COVERAGE
    PROBLEM AND FIX:
    The test thread_pool.thread_pool_kill_coverage fails under valgrind
    most of [1;31mtime[m indicating invalid THD read when it attempts to backtrace
    as per the PB2 logs. As per the analysis done, the reason for failure
    appear to be [1;31mtime[mout. Under [1;31mtime[mout, the mtr sends a kill signal to
    to the mysqld. As part of the sequence of the kill signal handling
    code, the handle_fatal_signal method is called. In the handle_fatal
    _signal, current_thd global variable is read. This gets the THR_THD
    and get the thd pointer associated with the connection/thread. The
    thd pointer may point to a freed THD and this causes an invalid read
    error to be issued by valgrind. In terms of code related to
    thread pool whenever THD is about to be freed, the thd pointer stored
    in THR_THD need to be set to NULL so that the handle_signal_fatal doesn't
    read a invalid thd and uses it. The fix sets the THR_THD key value
    associated with the thread to NULL before freeing the THD for thread
    pool related threads that handle connection and set THR_THD to NULL
    in no_threads_end in scheduler.cc as well.
      In addition, the test case [1;31mtime[mout issue is due to the fact that the
    test case thread_pool.thread_pool_kill_coverage includes kill_coverage.inc
    and in kill_coverage.inc the KILL QUERY/KILL is executed in a loop
    100 [1;31mtime[ms. The kill_coverage.inc is included 3 [1;31mtime[ms in the test cases
    and thus a total of 600 [1;31mtime[ms KILL QUERY/KILL is being called. In valgrind,
    every memory access is shadowed and under heavy load  when run under valgrind,
    the test case eventually [1;31mtime[ms out. Fix the test case to call KILL QUERY/KILL
    twice which covers all code coverage scenarios.
    NOTE TO REVIEWER: current_thd() calls pthread_getspecific which is not guarnteed to
    be asynchronous signal safe by POSIX.
    Calling current_thd in the signal handler itself is not signal safe and hence is
    incorrect and may need to be removed in which some thd related variables can't
    be shown.(Related link:
    https://www.securecoding.cert.org/confluence/display/seccode/SIG30-C.+Call+only+asynchronous-safe+functions+within+signal+handlers)
    Moreever pthread_getspecific is not guaranteed to return NULL if
    specific thread key is not associated with. So this bug won't address these
    scenarios as the prime issue is [1;31mtime[mout of the testcase.

[33mcommit 0a4b751f06079832596971d1dcbfb8a5d803bd4d[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Wed Jul 24 23:18:34 2013 +0100

    BUG#16662771: MULTIPLE RPL TESTS FAILING ON WINDOWS DUE TO CRASH AT PROTOCOL::STORE()
    
    The problem manifests itself when a user thread is issuing a SHOW
    SLAVE STATUS while at the same [1;31mtime[m the slave IO thread is being
    terminated because of some error (in the case analyzed, the error was
    because the IO thread had received a packet bigger than
    slave_max_allowed_packet).
    
    Looking at handle_slave_io, specifically at the part that handles
    errors, one can find that the THD structure is deleted before
    mi->info_thd is set to NULL. That is not an issue in itself, but the
    fact that the deletion happens while it is not guarded by
    mi->info_thd_lock makes it a problem. As a consequence, a race for the
    THD memory area, between the user thread and the IO thread that is
    shutting down and cleaning its data structures, can happen. To be
    precise, if the user thread accesses THD memory through mi->info_thd
    and before that the IO thread frees the memory but has not yet set the
    info_thd to NULL, then the server can crash.
    
    To fix this problem the deletion of the data structure while the IO
    thread is shutting down is moved to a later stage of this process,
    i.e., after setting mi->info_thd to NULL.

[33mcommit 374131fb997b0f6db88bf12e5431fafaf112de22[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Mon Jul 22 12:35:32 2013 +0800

    BUG#16662990 - I_INNODB.INNODB_BUG16097753 FAILING SPORADICALLY ON WINDOWS
    
    Analysis:
    The root cause is "end_u[1;31mtime[m_of_query" and "u[1;31mtime[m_after_lock" could be equal
    in microsecond when long_query_[1;31mtime[m is zero.
    
    Solution:
    Sleep 1 millisecond when waking up from lock wait using DBUG_EXECUTE_IF.
    
    rb://2926 approved by Jimmy Yang.

[33mcommit e1f471d841d60d62e154a988d5d609defb2d4e9d[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Wed Jul 17 14:17:01 2013 +0530

    fixed compile [1;31mtime[m warnings in sql/binlog.cc

[33mcommit 9d21672bc587936055b359cae4aa67d305943691[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Wed Jul 17 13:19:21 2013 +0530

    Fixed i_rpl.rpl_stop_slave_[1;31mtime[mout test failure caused after a merge with mysql-trunk.

[33mcommit 8bba44e5446369cce32e7c1e9056c0efdeb99b0c[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Jul 16 14:54:17 2013 -0700

    Improve null value handling
    
    The mysql adapter will be improved to avoid setting values for columns where
    there is no value provided by the user. The previous implementation treated
    persist different from save. With persist, two insert statements were created,
    one with values for auto-increment columns in the primary key, and one without.
    With save, one insert statement was created at initialization [1;31mtime[m, but if any
    columns were missing at the [1;31mtime[m the SQL command was sent, a new SQL statement
    was created, leaving out the columns that were null.
    
    During metadata processing, default values were obtained and saved for cases
    where the user did not provide values. This worked well for primitive column types
    but was problematic for complex column types such as [1;31mtime[m and date types.
    
    This change makes both persist and save work in the same way. When the object
    is presented to persist or save, the values are analyzed to see if any are null.
    If any values are null, a different insert statement is used. If the appropriate
    insert statement has already been created, it is used. If not, an insert statement
    is created and cached for future use.
    
    The implementation modifies DBTableHandler and in MySQLConnection.
    
    DBTableHandler:
      Add a parameter to getFields and getField to record whether there are any null values.
      For each value, call a function supplied by the user, either setDefined or
      setUndefined.
    
    MySQLConnection:
      Create a function to track defined and undefined column values. This function
        constructs a key e.g. 'DUUUD' where each character corresponds to the value
        of the fields bend Defined or Undefined.
      Use the fieldValueDefinedKey to create INSERT and INSERT... DUPLICATE SQL
        statements. The statements with all column values defined are created at
        initialization. Other statements are created on demand.
      Modify buildInsertOperation and buildWriteOperation to use the new undefined
        column protocol.

[33mcommit 1f32c34ceb8392233bc5f24575d57ee77f880e75[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Tue Jul 16 18:03:52 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    - worked on minor comments- whitespace etc
    - Addressed failures on trunk branch in reference to
      Last_error_[1;31mtime[mstamp field.

[33mcommit 6d74dc4fa379fab063612b113c5ae851ae49dff6[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Tue Jul 16 16:17:21 2013 +0800

    Fix Bug#17057168 LARGE PERFORMANCE REGRESSION FOR INNODB
    GEOMETRY/SPATIAL INDEX LOOKUP
    
    For the new datatype DATA_GEOMETRY, comparison of this data always
    return 0. This caused it can't find the correct key range of the index
    on this data type, so the query on this type column will always do full
    index scan. That's why we got this performance regression.
    The solution is: Since we still use still use BLOB as underlying
    datatype, so we should compare geometry data following the compare BLOB
    way.
    In this patch, I also fixed the occasionally failure of test case
    i_innodb.innodb_bug15963619. According to Krunal's suggestion, I
    increased the restart wait [1;31mtime[m from 10 to 60.
    
    rb#2890 Approved by Jimmy.

[33mcommit 31e915b111c9a7b365b368273d1b3a32b850fedc[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Mon Jul 15 22:04:10 2013 +0530

    WL#6314:
    fixed [1;31mtime[mout issues.
    Fixed rpl.rpl_mts_submode_switch by making D3.t transactional.
    removed MTR runs on Pushbuild branch to make the tests faster.

[33mcommit 1858058c0ce8691cbceba72ea5d0e9824faee048[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Mon Jul 15 18:01:40 2013 +0530

    WL6314: fixed test failures in rpl.rpl_parallel_switch_sequential.test
    and some other tests failing due to [1;31mtime[mout.

[33mcommit b33bc9ede9d90ab67313ea8b6a658c653ca619fc[m
Author: Venkata Sidagam <venkata.sidagam@oracle.com>
Date:   Thu Jul 11 12:43:59 2013 +0530

    Bug #12368577 MY_STRNNCOLLSP_UTF16: ASSERTION `(SLEN % 2) == 0' FAILED
    
    Problem Description: mysql server gets assertion failure with the below set
    of queries in debug build.
    
    Analysis:
    a) set @a='a'; - Here the charset value is _utf8 and the length is '1'.
    These values will be stored for the variable '@a' (by "variable" we mean
    user_var_entry object).
    
    b) select 0 &&  @a:=_utf16'q'; - Here the charset is getting updated in
    the prepare stage hence '@a' gets new charset value i.e _utf16. In the
    execution stage the Server evaluates the expression (0 &&  @a:=_utf16'q')
    and sees '0'. Hence, the value and the length is not updated. But the
    charset for '@a' was changed to '_utf16' at preparatory stage and remains
    as it is. We can observe that change with ("SELECT @a,CHARSET(@a),LENGTH(@a);")
    before this step and after this step.
    
    c) SELECT ''=@a; - If we give this statement then it will try to compare @a
    using my_strnncollsp_utf16() function. Because the charset of '@a' is seen
    as '_utf16' so the _utf16 comparison function is chosen. Since the execution
    of the 'b' statement is not complete because of '0' in the expression the
    length of '@a' remains '1'. But per the design _utf16 should have length
    as multiples of 2. So, the query ends in debug assert.
    
    Fix: Before fix the charset change is happening in the query preparation
    stage and also at the execution stage.
    The fix is having the charset change for user variables from
    args[0]/save_result *only* at execution phase, through
    Item_func_set_user_var::update() function. The appropriate code already
    exists in the current code base. In the uploaded patch I have initialized
    the charset value for the user variables at the [1;31mtime[m of variable
    creation [1;31mtime[m. That solves the issue.

[33mcommit 3e0c928538af3684c86c66df04791a8764276e4a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Jul 8 11:01:53 2013 +0200

    Bug#17041705 DO NOT BUILD PERFORMANCE SCHEMA FOR EMBEDDED
    
    Before this fix, the performance schema storage engine was build for the
    embedded library.
    
    There is no performance instrumentation when building for embedded,
    this is dead code.
    
    The fix is to introduce a new option to the MYSQL_ADD_PLUGIN cmake macro,
    named NOT_FOR_EMBEDDED.
    
    When this flag is set,
    - the plugin is not built for the embedded server,
    - the plugin is conditionally included (#ifndef EMBEDDED_LIBRARY)
      in the sql/sql_builtin.cc generated file.
    - the plugin is not in the list of libraries used to build libmysqld
    
    Now, because some instrumentation can be expanded at compile [1;31mtime[m,
    in particular with PSI_XXX_CALL macros when building code,
    the option RECOMPILE_FOR_EMBEDDED is also added to plugins that
    can contain references to the server itself (define MYSQL_SERVER).
    
    This affects the blackhole and the federated engine.
    
    Last, when executing tests with MTR, when using the --embedded server option,
    FRM files for performance schema tables could still be present on disk,
    which affects queries on the information_schema,
    causing spurious ER_UNKNOWN_STORAGE_ENGINE warnings.
    
    The root cause was that the installation process,
    when using mysqld --bootstrap to install a new database,
    was always creating performance schema tables (and FRM files),
    because the performance schema plugin was always loaded.
    
    mysqld --bootstrap has been relaxed a bit,
    to allow for not always loading the performance schema storage engine.
    
    The MTR script, when installing a database for an embedded test,
    now installs a database without performance_schema tables.

[33mcommit 74c32661480d840baf60a23b3c4b199eb49d64e5[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Sat Jul 6 07:24:52 2013 +0530

    Post fixed of [1;31mtime[mout failure in pb2 for innodb-wl6045.test
    --Added not_embedded.inc as include/restart_mysqld.inc does not work in embedded mode

[33mcommit b841d9972c8e84025c34814b497b99aaf339e456[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Fri Jul 5 16:14:26 2013 +0530

    Fixed [1;31mtime[mout failure in pb2 for innodb-wl6045.test
    --Divided the original innodb-wl6045.test into smaller test cases:
      innodb-wl6045.test, innodb-wl6045-1.test and innodb-wl6045-3.test

[33mcommit 663b29f89fa76d7503eb145fd45be0b62399ea00[m
Author: viswanatham gudipati <viswanatham.gudipati@oracle.com>
Date:   Fri Jul 5 12:26:23 2013 +0530

    wl6504:
    * added one more [1;31mtime[m the wait condition to pervent failing in parallel & repeat options
    * removed not parallel inc file

[33mcommit ef168e4dfdc7afcbe45e7d4a7293b0c2d2646262[m
Author: Serge Kozlov <serge.kozlov@oracle.com>
Date:   Thu Jul 4 20:46:51 2013 +0400

    BUG#16743913. Removed breaking test execution for wait_condition.inc for [1;31mtime[mout. Added wait_condition_or_abort.inc for rpl/binlog suites

[33mcommit ceb1ffaf3cbd9b178f0a23afeee5e8fe0aaf944e[m
Author: Igor Solodovnikov <igor.solodovnikov@oracle.com>
Date:   Thu Jul 4 14:00:34 2013 +0300

    Bug #17037022   MYSQL_CLIENT_TEST FAILS WHEN RUN AGAINST PREVIOUS VERSIONS OF THE SERVER
    
    test_wl5928: Tests of new features are skipped when mysql_client_test run against older versions of server.
    test_date[1;31mtime[m_ranges: conditionalized to mirror behavior changes introduced by wl5928
    test_bug36004: conditionalized to mirror behavior changes introduced by wl5928

[33mcommit daea07347f35e5dfa35a82690bca4368f95090f3[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Wed Jul 3 17:44:50 2013 +0530

    Bug#16957721: VALIDATE_PASSWORD_LENGTH ALLOWED TO HAVE
                  BELOW VALIDATE_PASSWORD PARAMETER VALUE
    
    Description: Value of validate_password_length variable
                 is set in following manner:
                 max(validate_password_length,
                   (validate_password_number_count +
                     validate_password_special_char_count +
                     (2*validate_password_mixed_case_count)))
    
                 While this is done each [1;31mtime[m value of
                 any of the above mentioned variable is
                 changed, same check should be performed at
                 the [1;31mtime[m of plugin installation which was
                 missing and hence it was possible to set
                 validate_password_length to a value which
                 does not satisfy above mentioned criteria.
    
                 This patch fixes the issue by introducing a
                 check at plugin installation [1;31mtime[m as well.

[33mcommit 24e6b53456b48e98158f68499fbf067bb21a4ded[m
Author: Libing Song <libing.song@oracle.com>
Date:   Tue Jul 2 11:09:58 2013 +0800

    BUG#16988017 *GROUP_COMMIT_DEADLOCK* TESTCASES ARE FAILING SPORADICLY
                 ON FREEBSD9-X86-64BIT
    
    Slave IO thread stopped sporadically with an error. The error was caused by an
    unexpected heartbeat. The heartbeat event was between Rotate_log_event and
    Format_description_log_event. But slave IO thread doesn't accept any event
    immediately after receivng a Rotate_log_event,
    except Format_description_log_event.
    
    Binlog rotation has two steps. First, it appends a Rotate_log_event into
    current binlog and close it. Second, it creates a new binlog and appends
    a Format_description_log_event. Immediately after the Rotate_log_event was
    appended, binlog_end_log_pos was updated and dump threads were notified to
    send the event. After the event was sent, dump threads began to wait
    for new events coming until the new binlog file was created. Dump threads
    could get [1;31mtime[mout and send heart beat events to slaves if the new
    binlog was not created so fast.
    
    To solve the problem, the code updating binlog_end_log_pos immediately after
    Rotate_log_event is appended is removed. So dump threads will not know the
    Rotate_log_event is appended until active binlog is switched. Because active
    binlog is switched, dump threads will switch to next binlog file directly
    without any heartbeat after sending Rotate_log_event.

[33mcommit 505e618da9dbed89611d96eba0893bca8bdfb024[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Mon Jul 1 17:43:08 2013 +0100

    Bug #14623333   MYSQL CLUSTER - NDBREQUIRE CRASH IN EXECGCP_TCFINISHED, DBDIH (LINE: 10381)
    
    There was a DIH-TC race in node failure handling during a
    GCP_COMMIT protocol round.
    
    During node failure, GCP_COMMIT is paused to allow TC takeover on the
    DIH/TC Master node to commit/abort the transactions coordinated by the
    failed node.
    
    Where the TC Master has already 'committed' the GCP, it is asked to do
    so again, including any taken-over transactions.
    
    There was a race between :
      1.  DIH Master being informed of the node failure, and informing
          TC Master of the need to re-commit the GCP
      2.  TC Master informing DIH-Master of the commit of the GCP
          (via 'DIH-Slave')
    
    If these two interactions were overlapping then the DIH Master could
    incorrectly assume that the GCP was committed, and potentially proceed
    to the next.  This could later result in assertion failures and crashes.
    
    The fix is to be more careful here, and :
      1.  Ensure that only one request from DIH-Master to TC-Master to
          commit a GCP is in-flight at a [1;31mtime[m, simplifying the identification
          of responses.
      2.  Include the highest handled node failure number in the TC response,
          so that DIH-Master can detect a race, and send a new commit request.
    
    A new testcase is added to the daily-devel series.

[33mcommit 1971e0fe51c1dacc8827ca87ece1c81f90baec4d[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Mon Jul 1 16:32:04 2013 +0530

    Bug #16971432 MYSQL CLIENT WASTE TIME IN SPRINTF MAKING INFO MESSAGES THAT
                  ARE NOT PRINTED
    
    Problem: When the client program is in batch mode by default the message
    like  "Query OK, 1 row affected" are not shown, but instead the buffer is
    prepared using sprintf call which takes some cpu [1;31mtime[m when the import file
    is too large.
    
    Fix: Fix is to check if the client is running in batch mode then skip the
    sprintf statement in /client/mysql.cc in com_go(). put_info ignores the
    buffer built by prepare only when in batch mode and verbose < 2. Hence
    skip the sprintf in com_go() only in this case.

[33mcommit 5094d53243b174a4994973f6e2c3396c26eb83d1[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Mon Jul 1 13:35:15 2013 +0530

    Bug#16586697 NO NEED TO CALL UT_PRINT_TIMESTAMP BEFORE IB_LOGF
    Fixed: Remove the ut_print_[1;31mtime[mstamp() call if it is followed by the
           ib_logf()
    Approved by: Jimmy (rb#2759)

[33mcommit 4233545cd15568f5c8cfdc673fc43e4c613f4aee[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Jun 19 13:54:46 2013 +0200

    Bug#16661195: REMOVE UNNEEDED CMAKE CHECKS
    
    Additional patch #2:
    Remove HAVE_TIMESPEC_TS_SEC CMake check.
    None of our supported platforms pass this check.
    According to the POSIX standard, the [1;31mtime[mspec struct
    contains tv_sec and tv_nsec, not ts_sec and ts_nsec.

[33mcommit e0abdb2a05f8fea0b11d668ffb4e466dd92a6d51[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Jun 18 12:30:47 2013 +0530

    - WL#6501: added some info level message to track truncate life[1;31mtime[m

[33mcommit 988dda81410bcb6f1fece7a2516e8f364d48fae3[m
Author: Maitrayi Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Fri Jun 14 17:04:59 2013 +0200

    Bug#16877045 - 5.6-CLUSTER-7.3 WIN32 SQL_YACC.CC BUILD PROBLEM
    
    Seems like sql_yacc.cc is regenerated by both sql_embedded(49) and
    sql(50) causing the latter to fail some [1;31mtime[ms because file is in use and
    thus it get permission denied, in windows.

[33mcommit 257f4b13fe557f6e0341dffb5613d3bc4ee65edf[m
Author:  <Dyre.Tjeldvoll@oracle.com>
Date:   Fri Jun 14 14:32:47 2013 +0200

    Add cmake variable for light install (without python and dojo) during development.
    Add a failOnError option to status-polling on the deployment page. The [1;31mtime[mr is still started when entering the page so that the status of running clusters gets displayed, but if the connection to the mgmd fails  we cancel [1;31mtime[mr and start it again when the mgmd is started. The [1;31mtime[mr is also cancelled when the cluster (mgmd) is stopped. (It is also cancelled when leaving the deployment page, as before). This will remove many bogus "connection refused" messages  which currently make the log hard to read.

[33mcommit cbbe33df5334bbdff2fcc49dbc02c17a2d65ea3d[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Jun 13 12:44:40 2013 -0500

    Bug#16932439-INNODB.INNODB-WL5980-DISCARD FAILED WITH UNEXPECTED
    WARNINGS IN MYSQL.ERR
    
    Some [1;31mtime[ms the following warning will be logged when running
    INNODB.INNODB-WL5980-DISCARD in PB2;
    
    2013-06-09 19:10:48 5126 [Warning] InnoDB: Tablespace 'test/t5980'
    exists in the cache with id 598 != 604
    2013-06-09 19:10:48 5126 [Warning] InnoDB: Freeing existing tablespace
    'test/t5980' entry from the cache with id 604
    
    This patch adds those warnings to that test and fixes a few places
    where there was extra quotes aound a pre-formatted table name.

[33mcommit 8a8767e20f2ec68baf502e9dc103f231c010cb5a[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Thu Jun 13 18:29:20 2013 +0530

    BUG#16906827-CAN'T SET QUERY_CACHE_TYPE TO 0 WHEN IT IS ALREADY 0
    
    BACKGROUND:
    The crux of the problem reported in the bug report is User
    does not want to see the error message when the server is
    started with query cache disabled and User is disabling the
    query cache by setting query_cache_type to 0 during run[1;31mtime[m
    (in client session).
    
    ANALYSIS:
    The check in check_query_cache_type() function was throwing
    error for all three values 0, 1 or 2 when query cache is disabled.
    According to the bug or user requirement, User wants to bypass
    this error check when we are setting query_cache_type to 0 (or OFF)
    during run[1;31mtime[m when it is already disabled 0 (or OFF) during startup.
    
    FIX:
    As a fix for this bug, the check in check_query_cache_type()
    function is modified to take into account to *NOT* throw the
    error when we are setting it 0 (or OFF) when query cache is
    already disabled.

[33mcommit 7e66dd6734ff30eed58b8e7a93f434fe68eed7fd[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 13 12:36:01 2013 +0200

    ndb - check return value of [1;31mtime[ms()
    
    [1;31mtime[ms() should not fail, but there have come out strange values in a customers logs, like:
    2013-04-23 20:47:08 [ndbd] INFO     -- Watchdog: User [1;31mtime[m: 1417878170  System [1;31mtime[m: 1843610996115
    
    this patch adds check of [1;31mtime[ms() return value and also print errno in the log if [1;31mtime[ms failed.

[33mcommit db870c2b526648f0e9a2c9767ae0c3aa8901265c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 13 12:35:19 2013 +0200

    ndb - remove race in watchdog, to not be able to report wrong action stuck
    
    this is probably very unlikely, but it nice to be able to fully depend on
    it then there are stuck threads.
    
    this patch also introduce memory barrier, but since the watchdog counter
    is checked at most 10 [1;31mtime[ms per second per each watched thread that should
    be ok.

[33mcommit 7bd35391eb2cc0ff63d9708b7fae311ca291fe98[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 13 12:26:59 2013 +0200

    ndb - save source revision and patches in autotest
    
    New option --patch[01]=file to autotest-boot.sh, specifies patches to be
    applied after source code are exported from repository.
    
    Patches should be in a form accepted by 'patch -p0'.
    
    Option can be specified several [1;31mtime[ms and patches will be applied in
    specified order.
    
    For upgrade tests use --patch0 or --patch1 to specify patches only to
    be applied to either source tree, or use --patch to apply patch to both
    trees.
    
    
    Also save the bzr version-info plus patches for source tree into files
    code0.txt and for upgrade tests also code1.txt which will end up in the
    result from autotest-run.sh

[33mcommit 150a2e004a762aaab549034c5331e2a34d4298bc[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Thu Jun 13 10:50:26 2013 +0200

    ndb - fix randomly (but rarely) failing testcase due to not handling [1;31mtime[mouts (266)...weird

[33mcommit 5d3d6327b771bb35127403eeaf51be4706af825d[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Wed Jun 12 23:40:44 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Implemented following PS tables for replication monitoring:
    1) replication_connection_configuration
    2) replication_connection_status
    3) replication_execute_configuration
    4) replication_execute_status
    5) replication_execute_status_by_coordinator
    6) replication_execute_status_by_worker
    
    Change in this patch:
    --------------------
    Changed data type of [1;31mtime[mstamp fields from string to
    [1;31mtime[mstamp. Updated tests accordingly.

[33mcommit 58703792a211a8e30ce35e8595e8fcf40079639f[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Jun 11 13:37:08 2013 +0200

    ndb - fix lost keyinfo for short scan tabreq
    
    bug introduced in
    revision-id: pekka.nousiainen@oracle.com-20130117092404-ah0c40re4mbpzw10
    [1;31mtime[mstamp: Thu 2013-01-17 11:24:04 +0200
    
    failing test:
    testPartitioning -nordered_index_dk --seed 35015525 --forceshortreqs

[33mcommit df9d6c6b7ca24caf8db682de0635ac24b17962c0[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jun 11 11:09:34 2013 +0300

    Fix Bug#16369955 MYSQL_INSTALL_DB EXITS WITH "UNKNOWN/UNSUPPORTED STORAGE
    ENGINE: INNODB"
    
    Do not try to create mysql.innodb_table_stats and mysql.innodb_index_stats
    during mysql_install_db [1;31mtime[m if InnoDB is not present (not compiled in).

[33mcommit 08dcf1e136e1df71129ed56ca027d2fa0dda2aab[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu May 30 14:43:21 2013 -0700

    Change expected error for 1969 [1;31mtime[mstamp from 22008 to 22007

[33mcommit 63f626fcd001afaa7f502555606e3bd1aec4a113[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu May 30 15:56:13 2013 +0100

    Disable testcase failing due to bug#16884594, and add warning for deadlock / [1;31mtime[mout case.

[33mcommit 5567c605c3d477e43d872f06b1fc74a6859260e0[m
Merge: 093585bcf95 c401cbac0cc
Author: magnus.blaudd@oracle.com <>
Date:   Thu May 30 12:17:05 2013 +0200

    ndb fk no fk
    
     - Merge in patch for CreateFk_NoVerify
     - Add more test cases
     - Impossible to "exercise" the one in copy_fk_for_online_alter() since table is empty
      at the [1;31mtime[m, keeping it anyway.

[33mcommit fbc8e2034e469dd31938ebb9dbca20d9366d7d5c[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Thu May 30 05:49:03 2013 +0200

    ndb adopt runTestApiConnectTimeout to new default higher connect [1;31mtime[mout

[33mcommit 1aeb6612c90923f20002187a6f3e0c9bddaa5955[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Wed May 29 16:28:39 2013 -0700

    mysql-js add support for type converters
    
    MySQLConnection.js:
      add adapter parameter to newResultObject and getFields to perform type conversions
    
    MySQLConnectionPool.js:
      add typeConverterMap with key column type and value converter object
      add type converters for [1;31mtime[mstamp and date column types
    
    MySQLDictionary.js:
      check type converter map for each column and store in column.typeConverter.mysql
    
    DBTableHandler.js:
      store column.typeConverter in field.typeConverter at initialization
      add adapter parameter to newResultObject and getFields to perform type conversions

[33mcommit 0cf5f65758ff03eabc4db50a9ae3040cfecbadff[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Wed May 29 15:06:16 2013 +0100

    WL#5928: Most statements should clear the diagnostic area
    
    Previously, the errors/warnings list was only cleared when
    a statement accessed tables. This was non-standard. We now
    clear the DA for every procedure statement (mostly anything
    that is executed and is not a diagnostics statement like
    GET DIAGNOSTICS or SHOW WARNINGS).
    
    This changes both code and behaviour for
    - direct invocation
    - stored procedures/functions
    - prepared statements
    - query cache
    
    GET DIAGNOSTICS and the old SHOW WARNINGS, SHOW ERRORS,
    SHOW COUNT(*) WARNINGS, SHOW COUNT(*) ERRORS are diagnostics
    statements. They do not clear the diagnostics area even
    when issued repeatedly.
    
    @@error_count and @@warning_count are "diagnostics variables"
    and will appear in non-diagnostics statements like SELECT.
    For backward compatibility, they will contain the correct
    values describing the previous statement when queried
    immediately after execution of the previous statement:
    DROP TABLE does_not_exist;
    SELECT @@error_count;      # renders 1 (result of DROP)
    SELECT @@error_count;      # renders 0 (result of SELECT)
    This is discouraged; use GET DIAGNOSTICS instead.
    
    Likewise, SHOW WARNINGS could be used as the first
    statement of a HANDLER (or preceeded only by other
    diagnostics statements), but this is discouraged --
    use GET [STACKED] DIAGNOSTICS instead.
    
    Errors during parsing correctly set the DA; any
    errors thrown there replace the previous command's
    diagnostics, even if it was a diagnostics statement
    the user failed at issuing.
    
    The SQL standard requires the diagnostics statement
    (GET DIAGNOSTICS) not to be preparable; likewise,
    SHOW WARNINGS, SHOW ERRORS, and statements containing
    @@error_count or @@warning_count must not be used in
    prepared statements.
    
    Code:
    DA resetting was sprinkled all over the code, some[1;31mtime[ms
    in the form of a "maybe" (opt_reset_condition_info()).
    All occurences of the latter were removed.
    Care was taken to reduce the number of occurences of the
    former, usually to one central point for each subsystem
    (parsing, SP, PS, QC, signaling, logging).
    Self-printing of SP instructions was slightly extended
    and refactored.
    
    Tests:
    wl5928.test tests and documents the new expected behaviour.
    Various other tests were adapted to provided similarly
    expressive results as they used to under the previous
    circumstances.

[33mcommit 97bf1d952cb98028c6549fece6c2c402bf42d6a3[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue May 28 16:19:40 2013 +0530

    - Bug #16664150: INNODB.INNODB-WL6445-2 FAILS - MYSQLD FAILED DURING TEST RUN
      Trying to start innodb instance in read only mode when previous instance
      was not shutdown properly will result in error.
      Previous instance is shutdown using shutdown_server <[1;31mtime[mout> command.
      Time taken by server to shutdown on most machines if <= 10 secs and so
      default used when test is written.
      weekly-trunk test frmk exercises lot of tcs with more than feasible
      parallelism and on machines that seems to be dead-slow.
      In other words 10 secs is not enough so increasing it to 60 so that
      TCs passes on all machine configuration.
    
      Approved by: Sunny (over IM)

[33mcommit b7bbfc9fcc08de50ff8d0b908eb9766fc24d48e9[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue May 28 14:52:32 2013 +0530

    - WL#6501: reintroducing big-test given the [1;31mtime[m it takes on pb2

[33mcommit 49330f6b6df1054b5bbd220187b5ccd05c2031c7[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Tue May 28 12:09:37 2013 +0530

    BUG#14765080 - DEBUG_SYNC CAN LOSE SIGNAL
    PROBLEM:
    i_innodb.innodb_bug14712710 fails sporadically on PB2 branches
    with warning debug sync point [1;31mtime[md out and error code of
    ER_DEBUG_SYNC_TIMEOUT. The problem could be that the wait
    thread which is  waiting at a sync point for a signal from the
    signalling thread could not process the signal because another
    signalling could have overwritten the debug_sync_global.ds_signal.
    This causes the waiting thread to wait till [1;31mtime[mout expires leading
    to ER_DEBUG_SYNC_TIMEOUT error. This happens because the wait thread
    when woken up from a signal could contend with another signal for
    grabbing the debug_sync_global.ds_mutex. If the signal thread succeeds
    in the mutex, then new signal will be overwritten in debug_sync_global.
    ds_signal before the wait thread could process it. Also there exists
    the possibly of wait thread arriving late at a wait point before some
    signal threads could have overwritten the signal that the wait thread
    possibly wants to wait for. NOTE: The mysql documentation also needs
    to be modified.
    FIX:
    In debug_sync_global maintain list of signals that have been signalled
    in a std::set and remove the signal from the list when the corressponding
    wait thread has processed that signal. Introduce a wait_for attribute
    no_clear_event that allows signal not to be cleared from the global thus
    allowing a single signal event to wake up multiple threads that wait for
    this signal. This is useful for the test cases rpl.rpl_semisync_deadlock
    and i_rpl.rpl_lost_events_on_rotate.

[33mcommit ab1c6fca7b6f7cbfdd636988985e22443142b202[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Tue May 28 07:08:47 2013 +0200

    ndb - remove setting of short (mysterious) [1;31mtime[mout, that cause test to fail frequently

[33mcommit 1ffd9d81b44468b9ce4718462d0b508e3ae4b0ca[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon May 27 15:50:21 2013 +0530

    - WL#6915: making the QA TCs big-test given the [1;31mtime[m it takes (checkin on behalf of Vinay)

[33mcommit 0105cc9af10d78fbddb71ab05b3fe4fabe0004b4[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu May 23 15:23:17 2013 +0200

    ndb - dbacc - Remove support for double linked list of used containers in a page.
    
    This double linked list was used by LCP long [1;31mtime[m ago.

[33mcommit ac8313eeac223b91e0499e28a2e9579fc8a46629[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu May 23 12:47:03 2013 +0200

    ndb - intrusive list
    
    one list implementation that will replace the old
    SLList, DLList, SLFifoList, DLFifoList and DLCFifoList
    
    the needs for this patch suite was
    1) SLCFifoList (a single linked list with both first and last and count in list head), and
    2) appending a double linked list to a single linked list with counts.
    but instead of adding these i choosed to replace the old lists with a new.
    
    the main differences from the old implementations are
    1) add/seize methods removed and it usages was replaces with addFirst/seizeFirst
    for SLList and DLList and with addLast/seizeLast for XXFifoList
    2) add(p,loc) + remove() was replaced with new prependList()
    3) release() was removed and its usage replaced with while(releaseFirst());
    
    for all methods execution [1;31mtime[m should be bounded by constant [1;31mtime[m
    (independent of list size)

[33mcommit d2756719e7cd7a36feef2cd95fb68d1150ac2d7c[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jun 5 12:40:50 2013 +0300

    WL#6658 Implement update_[1;31mtime[m for InnoDB tables
    
    Non-functional change: use trx_mod_tables_t instead of std::set<dict_table_t*>

[33mcommit 609e10ae22698f28c39bb73b04747cde12bf243a[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jun 5 12:39:50 2013 +0300

    WL#6658 Implement update_[1;31mtime[m for InnoDB tables
    
    Remove the to-be-dropped table from the list of modified tables
    by parent_trx in ha_innobase::delete_table().

[33mcommit 893c93ce5d191f3bb6dda79723ba25413a9eec8c[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jun 5 11:16:37 2013 +0300

    WL#6658 Implement update_[1;31mtime[m for InnoDB tables
    
    Wipe away a table object that is going to be destroyed from
    trx_t::mod_tables.

[33mcommit 2a7099478a560e818e4c6a2fefa1d262284c5017[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jun 4 18:37:46 2013 +0300

    WL#6658 Implement update_[1;31mtime[m for InnoDB tables
    
    Do not maintain update_[1;31mtime[m for temporary tables since they are not
    present in INFORMATION_SCHEMA.TABLES.

[33mcommit 0c3b1de710d583834aa43bcced7397c918f4f48a[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Mon May 20 14:03:03 2013 +0200

    fix0 - bug#16834242
    
    This patch fixes a regression in that data-nodes crash
    due to a data race in the [1;31mtime[m window between execNODE_FAILREP is recevied
    in QMGR and execNODE_FAILREP is recevied in DBLQH/DBTC.
    
    The race is that the version found in getNodeInfo(n).m_version
      return *global* information, in should only be dependant on
      in certain scenarios. (as far as i can tell usage was correct prior to this)
    
    This version of the patch changes so that m_version can be used
    up until all blocks has replied that they handled the node failure
    
    Prior to this patch, there was no real promises about when
    the information in getNodeInfo().m_version was consistent but
    no code prior to frazer.clement@oracle.com-20130123165838-rqdlut9tt7lco7uh
    depended on it in a dangerous way.
    
    Problem was introduces in
    2013-01-23 - frazer.clement@oracle.com-20130123165838-rqdlut9tt7lco7uh
    
    ---
     storage/ndb/src/kernel/blocks/qmgr/QmgrMain.cpp |   24 +++++++++++++++++-------
     1 file changed, 17 insertions(+), 7 deletions(-)

[33mcommit 3474f664cf2810cb078a52c224b24eca1c3da3af[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon May 20 15:30:43 2013 +0530

    - increasing [1;31mtime[mout from 30 to 60 to ensure safe shutdown on all h/w machines

[33mcommit d1b420164fa1ffa0bb5de50391054cc106aa8d50[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri May 17 16:50:00 2013 +0530

    BUG#16354789 RPL_SLAVE_SYSVAR_DEADLOCK CRASHING OCCASIONALLY
                 ON OS X 10.7
    PROBLEM DESCRIPTION AND FIX:
    i_rpl.rpl_slave_sysvar_deadlock test case crashes most of
    [1;31mtime[m on OS X 10.7 on PB2. The crash is due to race condition
    involving vio_shutdown and select based implementation of
    vio_io_wait. If vio_shutdown sets the file descriptor of the
    vio.fd to INVALID_SOCKET which is negative one in one thread
    after which the select-based implementation of vio_io_wait use
    this fd value with FD_* macros in another thread of control, it
    segfaults. The reason for segfault is because FD_* macros use
    this fd value as an index into a static buffer or array.
      The fix checks the copied fd from mysql_socket_getfd is not
    INVALID_SOCKET so that FD_SET doesn't incorrect index which avoids the
    segfault.

[33mcommit be6e9921e6a23ade921d453b3e5b9b0df4582d6c[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Fri May 17 10:19:03 2013 +0200

    ndb - bug#16817928
    
    In dropTableGlobal(DropTableCascadeConstraints == 0) make sure to perform all
    checks before starting to perform actual drops.
    
    Otherwise the it might have dropped some indexes and/or FK and then return an error.
    
    That behaviour confused the black-art-ninja-code in ha_ndb_ddl_fk.cc that makes ndb
    become bug compatible wrt foreign_key_checks. The result of that confusion was that
    a FK was getting dropped twice, and the second [1;31mtime[m one got 4238 (Trigger Not Found)
    
    After this patch, if dropTableGlobal return 21080, no drops of anything will have been performed.

[33mcommit 1384db9b5b36a77d579851b2f36726eeb9cfd70f[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu May 16 16:35:06 2013 +0300

    Bug#16723431 REMOVE SRV_LATIN1_ORDERING
    
    Remove the special handling of the latin1_swedish_ci collation.
    
    srv_latin1_ordering, DATA_MYSQL_LATIN1_SWEDISH_CHARSET_COLL: Remove.
    
    fts_get_charset(): Inlined, to replace innobase_get_fts_charset().
    
    innobase_mysql_cmp_prefix(): Remove.
    
    cmp_dfield_dfield_like_prefix(): Call the collation function directly,
    instead of calling innobase_mysql_cmp_prefix().
    
    innobase_mysql_cmp(): Make inline in rem0cmp.cc, remove parameters.
    
    cmp_whole_field(): Call the collation functions directly.
    
    MYSQL_PLUGIN_IMPORT: Remove; unused definition.
    
    eval_cmp_like(): Abort on IB_LIKE_SUFFIX or IB_LIKE_SUBSTR.
    Earlier, the abort was triggered later, in the removed functions
    cmp_data_data_slow_like_substr() and cmp_data_data_slow_like_suffix().
    
    fts_utf8_string_cmp(): Replace with innobase_fts_text_cmp(). The
    function name was misleading; this function actually used
    latin1_swedish_ci aka srv_latin1_ordering[] instead of UTF-8.
    
    fts_utf8_string_cmp_prefix(): Remove; unused function.
    
    cmp_data_data_slow_varchar(), cmp_data_data_slow_like_prefix(),
    cmp_data_data_slow_like_suffix(), cmp_data_data_slow_like_substr(): Remove.
    
    cmp_dfield_dfield_like_prefix(): Make inlined. Always invoke
    innobase_mysql_cmp_prefix() instead of some[1;31mtime[ms invoking
    cmp_data_data_like_prefix().
    
    cmp_dfield_dfield_like_substr(), cmp_dfield_dfield_like_suffix(): Remove.
    
    cmp_collate(): Remove.
    
    cmp_whole_field(): Handle DATA_VARCHAR and DATA_CHAR as
    my_charset_latin1.  These types should only be used by the internal
    SQL parser and for the internal data dictionary.
    
    cmp_data_data(), cmp_dtuple_rec_with_match_low(),
    cmp_rec_rec_simple_field(), cmp_rec_rec_with_match():
    Handle most types with cmp_whole_field(). Do not call cmp_collate().
    
    rb#2350 approved by Jimmy Yang

[33mcommit 6eaa50a6b2f7231068c9b3a7cc0cf6eadda1834b[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed May 15 18:53:38 2013 +0300

    Add a comment to clarify that a minor misbehavior of
    buf_page_peek_if_too_old() is known. The comment comes
    as a followup to "Bug#68981 bpage->access_[1;31mtime[m is unsigned int".
    
    This commit does not fix the bug.
    
    Suggested by:   Inaam (via IM)

[33mcommit 98b93142cbab56fc61461b947e95ca13b3109e74[m
Author: horst.hunger@oracle.com <>
Date:   Tue May 14 15:27:33 2013 +0200

    Introduction of the variable "ADDITION_OPTIONS" as some[1;31mtime[ms a test depends on other variables to be set to special values. There also simple tests possible with valid values.

[33mcommit 57009bc5b81548fac331c436ad070ecdeba86ba5[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Tue May 14 16:53:31 2013 +0530

    BUG#16449659 - ER_OUTOFMEMORY: WRONG PARAMETER COUNT
    
    BACKGROUND:
    The ER_OUTOFMEMORY error message is declared as "Out of memory;
    restart server and try again (needed %d bytes)", so it requires
    one numeric parameter.
    But in some places in server code, it is used without the size
    paramater which some[1;31mtime[ms results in crash/garbage in the output.
    
    ANALYSIS:
    Found by code inspection.
    
    FIX:
    Fixed those places with an additional size paramater where
    ER_OUTOFMEMORY is reported without it.

[33mcommit 377774689bf6a16af74182753fe950d514c2c6dd[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue May 14 11:53:18 2013 +0200

    Bug #16244691 SERVER GONE AWAY ERROR OCCURS DEPENDING ON THE NUMBER OF
    TABLE/KEY RELATIONS
    
    Problem:
    
    When there are many tables, linked together through the foreign key
    constraints, then loading one table will recursively open other tables.  This
    can some[1;31mtime[ms lead to thread stack overflow.  In such situations the server
    will exit.
    
    I see the stack overflow problem when the thread_stack is 196608 (the default
    value for 32-bit systems).  I don't see the problem when the thread_stack is
    set to 262144 (the default value for 64-bit systems).
    
    Solution:
    
    The recursive approach to load tables related via foreign key is replaced
    with an iterative approach.
    
    Previously the cascade operations were also done in recursive manner.
    Now it is changed to be done in an iterative manner using an explicit
    stack.
    
    rb#2089 approved by Marko

[33mcommit 305bf60b1560db45adc3b1ad0287aa15960cdfe3[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Mon May 13 16:15:53 2013 -0700

    Fix bug in clusterj handling of fractional [1;31mtime[mstamp values

[33mcommit 00eaa738a8a12bfac47e60b21231fe844083a2ba[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri May 10 18:01:19 2013 -0700

    Revert change to ndbjtie for 5.1/7/1 that included [1;31mtime[m2, date[1;31mtime[m2, and [1;31mtime[mstamp2
    These types will be introduced in 7.3 and the symbols will be reintroduced at that [1;31mtime[m

[33mcommit 38bd2784c72caaaa23b77d3224d4da69c8e8aa3e[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu May 9 20:33:16 2013 -0700

    Ignore tests that fail [1;31mtime[mstamp tests

[33mcommit 2561aa5bde96e278a80b19cc6d8f98b73bb5ff98[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu May 9 18:05:45 2013 -0700

    Clusterj [1;31mtime[m2 types
    Support new mysql 5.6 [1;31mtime[m with fractional seconds
    Support is for milliseconds as implemented by java.util.Date
    Future plans to support microseconds for java.sql.Timestamp
    These changes are being pushed to mysql 5.1 cluster 7.1
    because they are not specific to any version
    
    clusterj-api ColumnType.java
    added types corresponding to new types Date[1;31mtime[m2, Time2, Timestamp2
    
    clusterj-core DomainFieldHandlerImpl.java
    support new [1;31mtime[m types the same as existing [1;31mtime[m types
    
    clusterj-test AbstractClusterJModelTest.java
    avoid setting Calendar DATE for [1;31mtime[m types
    
    clusterj-test TimestampAsSqlTimestampTypesTest.java
    enable this test
    
    clusterj-test TimestampAsUtilDateTypesTest.java
    enable this test
    
    clusterj-tie ColumnImpl.java
    support new [1;31mtime[m types
    improve error message to log error in addition to throwing exception
    
    clusterj-tie NdbRecordImpl.java
    support new [1;31mtime[m types aligned on 8 byte boundary
    
    clusterj-tie Utility.java
    support new [1;31mtime[m types as long (milliseconds per java.util.Date representation)
    
    ndbjtie NdbDictionary.java
    support new [1;31mtime[m types

[33mcommit 862453b437376238ba22234a484c66df0d35ccbb[m
Author: Libing Song <libing.song@oracle.com>
Date:   Thu May 9 10:37:07 2013 +0800

    WL#6355 Semisync: externalize transactions only after ACK received
    
    Postfix
    rpl_semi_sync_after_sync got [1;31mtime[mout sporadically. That happened because
    'end' signal was cleared too soon to receive by the waiting thread(server_1).
    So the waiting thread hung until [1;31mtime[mout.
    
    'SET DEBUG_SYNC="RESET"' for master connection should be executed after the
    waiting thread(server_1) gets "end" signal. Because the SET statement clears
    not only current thread's debug action but the global signal as well.

[33mcommit 07e1ca1f5f72c9e025f1399e4defcd956786326b[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed May 8 16:20:21 2013 +0530

    - WL#6501:
      - replaced a heavy set with simple boolean given that we don't need
        to track all the space-ids being truncated at same [1;31mtime[m now.

[33mcommit 3469095ab793ed812bd4769bca69bbe894be4ecf[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed May 8 08:33:16 2013 +0300

    WL#6947 InnoDB: Use DBUG_PRINT for tracing
    
    InnoDB contains a number of Boolean flags that are only settable at
    compilation [1;31mtime[m or from a debugger, to enable certain diagnostic
    printouts. Most of this has not been used in the past decade.
    
    Replace the InnoDB-internal flags with DBUG_PRINT labels as follows:
    
    ib_buf  buf_debug_prints
    ib_cur  btr_cur_print_record_ops
    ib_lock lock_print_waits
    ib_log  log_debug_writes
    ib_que  que_trace_on
    ib_purge        srv_print_thread_releases
    
    The following flags are unused and will be removed:
    
    srv_print_buf_io
    srv_print_lock_waits
    srv_print_log_io
    srv_print_latch_waits
    
    The following flags are close to useless and will be removed:
    
    os_aio_print_debug      (error handling enables a never-seen printout)
    pars_print_lexed        (only used in UNIV_SQL_DEBUG)
    
    rb#2190 approved by Jimmy Yang, Inaam Rana

[33mcommit ddd573420c4010d9376c1ef94a46f5948179a762[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Fri May 3 13:18:15 2013 +0200

    Bug #14095855 FOREIGN_KEY_CHECKS SHOULD APPLY TO NDB TABLES
    This is a patch for supporting toggling on/off
    of foreign key checking in ndb.
    Innodb supports
    
    set foreign_key_checks = 0;
    
    to immediately disable all foreign key checking, even
    if set inside an ongoing transaction. All operations
    prior to the setting will have foreign key constraints
    checked and all operations after will not. For Innodb
    these checks are done immediately, but for Ndb the setting
    
    set ndb_deferred_constraints = 1;
    
    will defer the foreign key checks until commit [1;31mtime[m.
    The setting of foreign_key_checks = 0; inside a transaction
    will only affect operations executed after the change,
    operations done before will cause foreign keys to be checked.
    
    The setting of foreign_key_checks = 0 will of course also
    disable any defined cascade of deletes or updates.
    
    The implementation disables Ndb's internal triggers for
    foreign key checking inside TUP so it will also have a
    performance improvement during restoring backups with
    foreign key checks disabled.

[33mcommit 50917379039b3248f377d19c2c15ce781eeeb0d1[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri May 3 12:03:52 2013 +0300

    Bug#16759605 PAGE_ZIP_VALIDATE() SHOULD NOT AFFECT COMPRESSION STATISTICS
    
    The debug function page_zip_validate(), which is enabled when
    UNIV_ZIP_DEBUG is defined at compilation [1;31mtime[m, is invoking
    page_zip_decompress(), which in turn would update some statistics on
    compression. This would make some mysql-test-run tests fail when
    UNIV_ZIP_DEBUG is defined.
    
    page_zip_decompress_low(): Renamed from page_zip_decompress(). Remove
    some header and trailer for updating compression statistics.
    
    page_zip_decompress(): wrapper for page_zip_decompress_low(), with the
    header and trailer for updating the compression statistics.
    
    page_zip_validate_low(): Invoke page_zip_decompress_low() instead of
    page_zip_decompress(). This fixes the bug.
    
    Approved on IM by Jimmy Yang

[33mcommit 336b712a7c1fcc484a734b733cf22ebf8f54ee10[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu May 2 09:57:42 2013 +0200

    ndb - add debug compile protection against failing clock_get[1;31mtime[m

[33mcommit 34839589cc5e2a2f19b13334a27f49333efd2a4c[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu May 2 09:30:54 2013 +0200

    ndb - fix error printout referring to wrong function clock_getreal[1;31mtime[m

[33mcommit 28124c6c418c85d1d9015e9ac2cafccf69b812db[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed May 1 10:42:00 2013 -0700

    Fix so scan tests don't hang on lock [1;31mtime[mout failures

[33mcommit 0c02f44a5ef17567e9afd89e70132554b1524c4a[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Apr 30 10:57:46 2013 +0200

    ndb - Add safe_clock_get[1;31mtime[m()  wrapper
    
     - printouts in debug mode if clock_get[1;31mtime[m() fails.
     - retry with CLOCK:REALTIME in case CLOCK_MONOTONIC fails
     - fix spelling errors in previous printouts

[33mcommit ffd1bd3568921cbc78805707da2ab63c0c1c1435[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Thu Apr 25 07:03:43 2013 +0530

    WL#6045 Improve Innochecksum
    --Separated the some test case of innodb-wl6045.test into
    innodb-wl6045-2.test in order to overcome the [1;31mtime[mout failure in pb2 for window.

[33mcommit 1b59fd09e6ed9e8e7a4aa3c492fbcf21bd32a289[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Apr 23 20:27:03 2013 -0700

    Fixes for new [1;31mtime[m types

[33mcommit 1f4d4c860d4c3c436c8eb7c18eb4b9bb4a76d813[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Apr 22 15:40:28 2013 +0300

    WL#6871 cursor code cleanup.
    Remove the unused mode PAGE_CUR_LE_OR_EXTENDS.
    It was both introduced and disabled in MySQL 4.0:
    
      revno: 1416.1.150
      revision-id: sp1r-heikki@hundin.mysql.fi-20030207154409-50631
      parent: sp1r-heikki@hundin.mysql.fi-20030207153315-50621
      committer: heikki@hundin.mysql.fi
      [1;31mtime[mstamp: Fri 2003-02-07 17:44:09 +0200
      message:
        page0cur.c:
          Disable PAGE_CUR_LE_OR_EXTENDS because it does not work for
          non-latin1 char sets now
    
      revno: 1352.2.13
      revision-id: sp1r-heikki@hundin.mysql.fi-20021029211646-41337
      parent: sp1r-Sinisa@sinisa.nasamreza.org-20021029122123-37723
      committer: heikki@hundin.mysql.fi
      [1;31mtime[mstamp: Tue 2002-10-29 23:16:46 +0200
      message:
        Many files:
          Merge InnoDB-4.0.5: new isolation levels READ COMMITTED and
          READ UNCOMMITTED now supported, selective deadlock resolution
        mysqld.cc:
          Change MySQL default isolation level to REPEATABLE READ; note
          that InnoDB has always had that default, and BDB and MyISAM
          always run at SERIALIZABLE level anyway

[33mcommit be7b0558a52b93f820c838928ce665df08f83885[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Apr 22 16:39:03 2013 +0530

    Bug #16244691 SERVER GONE AWAY ERROR OCCURS DEPENDING ON THE NUMBER OF
    TABLE/KEY RELATIONS
    
    Problem:
    
    When there are many tables, linked together through the foreign key
    constraints, then loading one table will recursively open other tables.  This
    can some[1;31mtime[ms lead to thread stack overflow.  In such situations the server
    will exit.
    
    I see the stack overflow problem when the thread_stack is 196608 (the default
    value for 32-bit systems).  I don't see the problem when the thread_stack is
    set to 262144 (the default value for 64-bit systems).
    
    Solution:
    
    The recursive approach to load tables related via foreign key is replaced
    with an iterative approach.
    
    Previously the cascade operations were also done in recursive manner.
    Now it is changed to be done in an iterative manner using an explicit
    stack.
    
    rb#2089 approved by Marko

[33mcommit 5802e8958759a2a28ca98b078ce5926f949dd405[m
Merge: 875cbd4f154 4f6a90c57d8
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Sat Apr 20 13:02:29 2013 +0530

    Bug#16073689 : CRASH IN ITEM_FUNC_MATCH::INIT_SEARCH
    
    Problem:
    In query like
    select 1 from .. order by match .. against ...;
    causes a debug assert failue.
    
    Analysis:
    In union type query like
    
    (select * from order by a) order by b;
    or
    (select * from order by a) union (select * from order by b);
    
    We skip resolving of order by a for 1st query and order by of a and b in
    2nd query.
    
    
    This means that, in case when our order by have Item_func_match class,
    we skip resolving it.
    But we maintain a ft_func_list and at the [1;31mtime[m of optimization, when we
    Perform FULLTEXT search before all regular searches on the bases of the
    list we call Item_func_match::init_search() which will cause debug assert
    as the item is not resolved.
    
    
    Solution:
    We will skip execution if the item is not fixed and we will not
    fix index(Item_func_match::fix_index()) for which
    Item_func_match::fix_field() is not called so that on later changes
    we can check the dependency on fix field.

[33mcommit b890823b287cee10592b8c6e1cdd4e12e08c51f3[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Apr 18 08:59:43 2013 +0200

    Bug#16629996: Remove more unused #ifdefs
    
    Additional patch.
    
    This patch removes the following #ifdefs which were never set:
    - DONT_HAVE_TO_BE_INITALIZED
    - HAVE_AIOWAIT
    - HAVE_CLOSE_SERVER_SOCK
    - HAVE_CRYPTED_FRM
    - HAVE_DARWIN_THREADS
    - HAVE_EMBEDDED_PRIVILEGE_CONTROL
    - HAVE_FILE_VERSIONS
    - HAVE_GETTIMEOFDAY_H
    - HAVE_LINUXTHREADS
    - HAVE_SETUPTERM
    - HAVE_SYS_VADVICE_H
    - HAVE_TIME
    - HAVE_VIDATTR
    
    This patch also removes #ifdefs HAVE_FTIME and HAVE_SYS_TIMEB_H.
    According to the man page about f[1;31mtime[m "This function is obsolete.
    Don't use it". Including the header file gave compilation warnings
    on FreeBSD. The one remaining use of f[1;31mtime[m() has been replaced
    with get[1;31mtime[mofday().

[33mcommit 52b138408101be4cdfd93a05034a2a31d8e0af04[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Apr 16 16:57:46 2013 +0530

    - bug#16670211: innodb_wl6560_debug failing
      Reason: Shutdown of server is not completed in given [1;31mtime[m-frame (10 secs)
      and so server is killed. Now if you try to ls for ibtmp1 (temp-tablespace)
      you will find it as server will remove it only in case of clean shutdown.
      Fixed: Increased the shutdown [1;31mtime[m-frame.

[33mcommit acc04a1a5d6a70b5c9a81407092ecd62ddf78fd7[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Apr 16 16:00:59 2013 +0530

    - bug#16669091: INNODB.INNODB-WL6445-1 FAILS
      bug reports multiple problem.
      Problem-1: server is getitng killed due to low [1;31mtime[mout value
      and so follow-up attempt to start server in innodb read only mode
      is failing as it is triggering recovery.
      Increased the [1;31mtime[mout value from 10 -> 30
      30 is just a safe bet as we might encounter a machine where in
      it might [1;31mtime[mout even with 30 sec shutdown [1;31mtime[m.
      I guess best way to solve this issue is upgrading machine
      and reducing load. We are currently running them with 24 threads
      and I doubt machines are upgraded to support latest developments.

[33mcommit bd5b27f9bbf296cb3e88925ad01e5e6177d30dad[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Apr 16 14:09:35 2013 +0530

    - bug#16669476: INNODB_WL6470_DEBUG TIMES OUT AFTER 1200 SECONDS
      This is suspected to be [1;31mtime[mout issue.
      This test does lot of operation on big blobs which probably
      was never included in any of the mtr level tc.
      When we wrote this test-case we faced same problem on QA machine
      but this TC passed 240 [1;31mtime[ms under heavy disk load (chtest frmk).
      TC is failing on weekly-trunk with [1;31mtime[mout. There are other TCs
      that are failing on weekly trunk with same [1;31mtime[m out issue while
      they are passing successfully on pb2 regularly.
      Let's scale down this TC a bit and check if behavior persist.

[33mcommit 10f06dbdd37df16d7678546653c49622ed320e3f[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Apr 16 13:00:43 2013 +0530

    - Splitting tc for better manangability. This will also help us track
      if [1;31mtime[mout of tc only on weekly trunk that too with 4k page size
      on solaris is because of real-issue in code or it is timing problem.

[33mcommit a83ebe27ca6fc76ef33f75346585b6f2262c97a7[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Mon Apr 15 15:24:51 2013 +0200

    Bug#16661195: REMOVE UNNEEDED CMAKE CHECKS
    
    This patch removes 48 CMake checks for which the results were never used.
    This reduces the [1;31mtime[m needed to run CMake and simplifies our CMake
    configuration files.

[33mcommit 19dd96189f4bd4a8d1452c44a7b5c981133dd0c8[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sun Apr 14 19:21:01 2013 -0700

    Fix list of running tests when suite [1;31mtime[ms out

[33mcommit aa4089495679c25ecdb1c9b68f3585e0e0a9265a[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Apr 12 08:45:14 2013 +0300

    Bug#16621694 CLEAN UP TRANSACTION ROLLBACK, PURGE, AND MVCC
    
    row_purge_remove_clust_if_poss_low(): Assert that the record must be
    delete-marked.
    
    row_purge_record_func(): row_undo_mod(): Assert that the undo record
    is not for a fresh insert.
    
    row_undo_ins(): Assert that the undo record is for a fresh insert.
    
    row_undo_ins_remove_clust_rec(): Assert that the record is not
    delete-marked.
    
    row_undo_node_create(): Assert that the transaction is in ACTIVE or
    PREPARED state.
    
    row_undo_search_clust_to_pcur(): Avoid recomputing offsets if the
    record was not found.  Assert that the DB_TRX_ID matches the
    being-rolled-back transaction.
    
    row_upd_clust_rec(): Eliminate some code duplication.
    
    read_view_print(): Unused function, remove.
    
    trx->undo_no_arr: Remove. We will simply roll back the latest row
    (trx->undo_no) and decrement it at each step until reaching the start
    of savepoint or the start of the transaction.
    
    Revert a 5.0 patch that added dead 'framework' code for VIEW_HIGH_GRANULARITY:
    >revno 1883.2.1
    >revision-id sp1r-jan@hundin.mysql.fi-20050722111003-04798
    >committer jan@hundin.mysql.fi
    >[1;31mtime[mstamp Fri 2005-07-22 14:10:03 +0300
    >Implement MySQL framework to support consistent read views in
    >cursors for InnoDB. The idea of the patch is that if MySQL requests
    >a consistent read view, we open one when open a cursor, set is as the
    >active view to a transaction when fetch from the cursor, and close
    >together with cursor close. This patch is associated to bugs #11813,
    >#11832, and #11833. Contains after review fixes.
    
    rb#2295 approved by Jimmy Yang

[33mcommit 2eafc317c7307782b69d40cefbad241cbbad074a[m
Author: bin.x.su@oracle.com <>
Date:   Wed Apr 10 15:07:21 2013 +0800

    Bug #16005310
    
    Fix: Set diff_[1;31mtime[m to 0, if finish_[1;31mtime[m < start_[1;31mtime[m.

[33mcommit 9ab9c53c90bff7c0857cdb24337951ec6e90838a[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Apr 9 13:36:39 2013 +0300

    WL#6658 Implement update_[1;31mtime[m for InnoDB tables
    
    Also update information_schema.tables.update_[1;31mtime[m by TRUNCATE TABLE.
    
    Spotted by:     avinash.potnuru@

[33mcommit a285d513c4cb8cd35b8b51410fdf441069c0b4e6[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Apr 4 10:23:09 2013 -0700

    Free Value Object on GC
    Make sure the NdbRecordObject, Buffer, and Column Proxies are deleted
    when JS no longer needs the Value Object.
    The patch also does this for Record and NdbClusterConnection,
    which however are expected to have much longer life[1;31mtime[ms.

[33mcommit cf2c5cb18b70d8bd5d690260856cb1881cf876b4[m
Author: Vinay Fisrekar <vinay.fisrekar@oracle.com>
Date:   Thu Apr 4 11:15:43 2013 +0530

    Clean up of unwanted statements to reduce test execution [1;31mtime[m.

[33mcommit bb69959ea0497ae2f64e419f01e39f9d0badaa44[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Apr 3 13:37:31 2013 -0700

    Temporaltypes improvements
    MySQLTime distinguishes between expected local [1;31mtime[m & expected UTC in conversions
    All NDB temporaltypes tests pass but one

[33mcommit 3b07a376c8241a93f8e4a3e295672341ac748b5d[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Apr 3 16:29:45 2013 +0530

    - given that innodb_wl6470_2 takes good amout of [1;31mtime[m to complete
      under valgrind enviornment as discussed over forum we have decided
      to enable it only with --big-test enable.
      Accordingly adding the prerequsite.

[33mcommit 66b32390724b605f7f3a44a22a89a89afce0b2f7[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Mar 30 18:10:52 2013 -0700

    Add ReadNDB/WriteMySQL tests for Date[1;31mtime[m and Timestamp

[33mcommit ea567c77b781ae6f9ea72620e340ef48c7c32cc4[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Mar 29 13:16:33 2013 -0700

    TypeConverters
    
    Fully implement TypeConverters in NdbConnectionPool.
    The registered typeConverter for a column is stored in its TableMetadata.
    We introduce two "standard" TypeConverters for NDB, for Time and Date[1;31mtime[m.
    These are registered by default.
    They are implemented under Converters/ at the top of the project.
    STILL TO DO:
      API support for registerTypeConverter
      Test cases for Time & Date[1;31mtime[m with the default converters
      Test cases for Time & Date[1;31mtime[m with custom converters

[33mcommit 7edccb1d2cd0a0628eae22dc0fe1f6454297d828[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Mar 27 20:13:56 2013 +0200

    Adjust main.partition_innodb test - update_[1;31mtime[m is no longer NULL,
    but contains the current [1;31mtime[mstamp, so it needs to be replaced with
    a dummy value like #.

[33mcommit e0b91ac887ffc55cbe8612b34e291a8eedc3cfc6[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Mar 26 14:19:27 2013 +1100

    WL#6906 - Add new sync levels for the pool manager and pool mutexes. Avoid
    creating extra pools if N threads run out of a slot at the same [1;31mtime[m.

[33mcommit 093689f370411e91cb29b501ea2519c2674b001f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Mar 22 14:07:07 2013 +0100

    Bug#11755370 - CRASH ON STARTUP WHEN XA SUPPORT FUNCTIONS
    
    Post push fix: disable msync/fsync in unit test.
    (reduces execution [1;31mtime[m from seconds to milliseconds)

[33mcommit 2b57d68446f93df5f464f740c34f78fe803e77bc[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Mar 22 13:52:45 2013 +0200

    Disable the newly added innodb_update_[1;31mtime[m mtr test in embedded
    mode since it tries to restart the server.

[33mcommit 31826708b1b929b5192d5f599c748fd41759f9bf[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Wed Mar 20 13:43:54 2013 +0530

    Bug #16078466:EXPLAIN CRASH IN FIELD_ITERATOR_TABLE_REF::
                  GET_OR_CREATE_COLUMN_REF
    
    Analysis:
    ---------
    Server crashes on a debug build when:
    a) Update and delete operation is performed using an
    invalid column as the join clause within a stored
    procedure or function and
    b) When the stored procedure or function is invoked a
    second [1;31mtime[m.
    
    In case of SP/SF, the lex object associated with the
    instructions are cached when the SP/SF is first invoked.
    The flag 'is_join_columns_complete' which is part of the
    lex object indicates that the resolution of the join
    columns is complete. This flag is set when the SP/SF is
    first invoked though the resolution is not complete
    since the column in the joining clause does not exist.
    
    Since the lex object is cached, when the SP/SF is invoked
    the second [1;31mtime[m, it is assumed that the resolution is
    complete and the server crashes while trying to access
    the join column during debug assert.
    
    Note: On a release build, it gives an appropriate error.
    
    Fix:
    ---
    Mark the lex object as broken when there is an error which
    forces the lex object to be re-created during the subsequent
    invocation of the SP/SF.

[33mcommit 0e78d1d8487ff10dd9415bd8c3114436d7bc1480[m
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Wed Mar 20 11:20:47 2013 +0530

    BUG#15850951-DUPLICATE ERROR IN REPLICATION WITH SLAVE
    TRIGGERS AND AUTO INCREMENT
    
    Problem:On filtered slaves, IRU(IntVar, RandVar and UserVar)
    events are getting executed twice in the span of one query
    execution [1;31mtime[m if the query involves in firing a 'AFTER' trigger.
    
    Analysis: IRU events are getting cleaned up at the end of
    the query execution. If a query, with IRU events, involves
    in firing a 'AFTER' trigger and if the 'AFTER' trigger contains
    DML queries, the same IRU events are getting executed once again.
    This result in unexpected results while inserting tuples in
    tables which has auto_increment columns.
    
    Fix: Delete these IRU events immediately after it gets
    exectued for the first [1;31mtime[m.

[33mcommit 945e26015322be76405bd4c4b67cb061d262debd[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Mar 18 15:55:23 2013 +0100

    Followup fix for Bug#16491986 AVOID EXTRA READTUPLE() WHEN UPDATE IS EXECUTED AS DELETE+REINSERT
    
    That fix introduced a bug where all PK columns was not always read
    by the readTuple() prior to the delete. Thus random values was
    used to produce the key of the tuple to be deleted - Some[1;31mtime[ms
    causing the tuple expecting to conflict with our update
    to be deleted instead of the correct tuple.
    
    This fix ensures that 'm_pk_bitmap_p' is always a part
    of the read_set whenever delete+reinsert us used
    to perform the update.

[33mcommit ab9f200af68f609dc01ef99667f17ac11bc17e76[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Mar 16 18:11:14 2013 -0700

    Initial support for the new MySQL 5.6 / Cluster 7.3 precision [1;31mtime[m types

[33mcommit df381240f6fe37a87d955f64a6e001913dea78ba[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Fri Mar 15 08:06:25 2013 +0100

    Adding special test suite for valgrind run. Reduced set of tests so that valgrind can finish in a reasonable [1;31mtime[mframe

[33mcommit e6a832e09f4cb7228591b5042f363583d4ace5f0[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Mar 14 14:55:30 2013 +0200

    Speedup a little innodb_update_[1;31mtime[m.test
    
    Suggested by:   Marko (rb:2091)

[33mcommit 989c991c936bde45babd1a6f7e1396a164d880e9[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Mar 13 11:01:07 2013 +0200

    Add a comment to the code which updates dict_table_t::update_[1;31mtime[m that
    it is fine to do so concurrently.
    
    Echo some of the comments from innodb_update_[1;31mtime[m.test into
    innodb_update_[1;31mtime[m.result, so the latter is easier to read.
    
    Suggested by:   Kevin (rb:2091)

[33mcommit 618ab4c226f829e8da45fc3676844526f43373d1[m
Author: kevin.lewis@oracle.com <>
Date:   Mon Mar 11 10:36:18 2013 -0500

    This patch pushed to mysql-trunk last week;
    
    revision-id: kevin.lewis@oracle.com-20130307172527-h2vtnnmd3w4004xr
    branch nick: mysql-trunk
    [1;31mtime[mstamp: Thu 2013-03-07 11:25:27 -0600
    message:
      Bug #11763660 - PLEASE ASSERT INSTEAD OF CALLING EXIT(1)
    
    caused a regression in innodb_bug13450566 due to an error message format change.
    It now needs to be suppressed.  This mtr.add_suppression() is added to
    innodb_bug13450566 along with some other test case cleanup where this message
    is suppressed.
    
    Approved by Vasil on IM

[33mcommit 61a465368982e3efd24478d5b45504dac6c2a1aa[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Mar 11 12:53:14 2013 +0100

    Fix for Bug#16020945 HA_NDBCLUSTER ADAPTION TO 5.6 RBWR
    
    RBWR (Read Before Write Removal) previously relied on
    the optimizer calculating the 'read_set' for which rows
    requiring to be read as part of an update or delete
    operation. The handler::table_flags() returned
    HA_PRIMARY_KEY_REQUIRED_FOR_DELETE which caused the
    optimizer to add primary key columns to the read_set
    for update and delete operations.
    
    The problem with this approach is that the optimizer
    has insufficient information about how the handler
    will perform the read & update/delete operation at
    this stage. A conservative strategy was thus used which
    in several cases added the primary key column even when
    not required. This later prevented RBWR to 'remove' the
    read where this would have been possible.
    
    This fix removes returning HA_PRIMARY_KEY_REQUIRED_FOR_DELETE
    from ha_ndbcluster::table_flags(). Thus the RBWR optimizer
    will only see the read_set required by the run[1;31mtime[m to evaluate
    any conditions and the new updated values. Any additional
    columns required by ha_ndbcluster in order to update or delete
    the rows and BLOB columns are now added to read_set inside
    the handler after RBWR has been decided.

[33mcommit df9375aaa1cef4d10eabba5c66f1e88637196ddc[m
Author: Ahmad Abdullateef <ahmad.abdullateef@oracle.com>
Date:   Mon Mar 11 17:20:15 2013 +0530

    BUG#16367039 - LOCK TIMEOUT IN UPDATE SUBQUERY CAUSES ASSERT
    
    ANALYSIS :
    When an UPDATE statement containing a nested SELECT statement is being executed
    and another connection happens to hold locks to the rows being accessed by this thread
    then optimize_cond() in mysql_update() fails due to a lock wait [1;31mtime[mout and the
    Diagnostic Area status is set to DA_ERROR.
    
    However this DA status is not checked and the execution continues, subsequently call to
    info.read_record() returns -1 which is a Success condition stating that all records have been
    processed. This results in a call to my_ok() which causes an ASSERT because the DA has already
    been set to DA_ERROR.
    
    In release build however Diagnostics_area::set_ok_status() has explicit code to which set DA_OK
    only if the DA state has not already been set, therefore the behavior remains consistent with debug
    builds.
    
    FIX :
    The DA status is checked for an error status and if it has been set, the execution of the statement
    aborted.

[33mcommit 88104b83225760d892ea7597e2e57367dfae1290[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Mar 6 09:48:25 2013 +0200

    Extend the update_[1;31mtime[m test with a XA transaction and a server restart
    
    Suggested by:   Marko (rb:2091)

[33mcommit d2f9d2afb5391802ee198e66569804ffd08e6ed5[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Mar 5 13:38:41 2013 +0200

    Swap the calls to trx_commit() and trx_update_mod_tables_[1;31mtime[mstamp()
    per Marko's suggestion from rb:2091

[33mcommit c2b867e128b590699fda4bf99562c936d5f43090[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Mar 4 20:39:51 2013 +0200

    Extend the mtr testcase for update_[1;31mtime[m per Marko's suggestion

[33mcommit f9917c4eb592cbd94ad14157cbf048b18a08a5d1[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Mar 4 16:39:27 2013 +0100

    Fix compiler warning in optimized build caused by previous commit:
    
    5094 Jan Wedvik 2013-03-04
          This is a fix for bug#16176006 (TUPLE WITH CHECKSUM ERROR SILENTLY DISCARDED).
          The fix corrects an error in an if-predicate that checks which error codes
          that should be propagated to the application (via TC). As it was, any error
          code less than 6000 would cause the current row to be skipped, even those
          codes that should have caused the query to be aborted.
    
          This commit also fixes a related issue: If the scan aborts due to an error from
          TUP and *no* rows had been sent to the API, the correct behavior is to return
          SCAN_FRAGREF to TC right away. If one or more rows has been sent, and locking
          is used, LQH should send a SCAN_FRAGCONF, wait for the next SCAN_NEXTREQ and
          then release the locks and then send SCAN_FRAGREF. As it was, LQH would send
          SCAN_FRAGCONF even when zero rows had been sent. This led to a situation where
          TC would eventually get a [1;31mtime[mout while waiting for the api to close the scan.

[33mcommit 881069f46479424eff600c836bb32aff4bd80a64[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Mar 4 17:39:18 2013 +0200

    Per Marko's belief - remove check_[1;31mtime[m from this WL.

[33mcommit e38fac911946fd279dd7e6b9a16a0a8a9e1c64bf[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Mar 4 13:03:46 2013 +0100

    This is a fix for bug#16176006 (TUPLE WITH CHECKSUM ERROR SILENTLY DISCARDED).
    The fix corrects an error in an if-predicate that checks which error codes
    that should be propagated to the application (via TC). As it was, any error
    code less than 6000 would cause the current row to be skipped, even those
    codes that should have caused the query to be aborted.
    
    This commit also fixes a related issue: If the scan aborts due to an error from
    TUP and *no* rows had been sent to the API, the correct behavior is to return
    SCAN_FRAGREF to TC right away. If one or more rows has been sent, and locking
    is used, LQH should send a SCAN_FRAGCONF, wait for the next SCAN_NEXTREQ and
    then release the locks and then send SCAN_FRAGREF. As it was, LQH would send
    SCAN_FRAGCONF even when zero rows had been sent. This led to a situation where
    TC would eventually get a [1;31mtime[mout while waiting for the api to close the scan.

[33mcommit b971cabaaa0d2d7d4c7050f1f363de27ff9ef4d0[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Feb 27 15:14:18 2013 +0200

    part of WL#6658 Implement update_[1;31mtime[m and check_[1;31mtime[m for InnoDB tables
    
    Remove diagnostic printouts.

[33mcommit 5477f00935d10a2a12adc6f2c59d5b0baa369d11[m
Merge: 1e6af3993f3 d2bb5f41115
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Feb 26 14:29:19 2013 +0530

    - WL#6469: Optimizing CREATE/DROP performance for temporary tables
      goal: optimize temp-table ddl performance.
      how: temp-table life[1;31mtime[m is limited to connection/server life[1;31mtime[m
      and so lot of actions like redo logging (needed for recovery),
      writing metadata to SYSTEM tables (needed for re-loading table
      on re-start) can be avoided.
      Post Optimization create/drop performance of innodb-temp-table is
      comparable with memory-temp-table.
    
      Review: rb#1544
      Approved by: Sunny + Jimmy + Michael

[33mcommit 1ff551ee095c6abbb89e8185063c033d6f49c349[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Mon Feb 25 14:07:35 2013 +0530

    BUG#11790057 KILL QUERY WILL AFFECT THE NEXT COMMAND INCORRECTLY
    Bug#11754124:KILL QUERY NOT BEHAVING CONSISTENTLY AND WILL HANG IN SOME CASES
    
    BACKGROUND OF BUG#11790057
    SCENARIO 1:
    When a very long SELECT is running in one connection and it
    is just flushing its output on the console and, In another
    connection a KILL QUERY command is issued, the next query
    executed in the first connection would throw
    ER_QUERY_INTERRUPTED which is wrong.
    
    SCENARIO 2:
    When an INSERT query is running in one connection and it is
    in final stage to commit itself by acquiring MDL locks and
    waiting for the lock (i.e it is blocked because Flush Tables
    With Read Lock (FTWRL) is issued in another connection) and,
    In another connection, a KILL QUERY command is issued to kill
    that blocked INSERT query, it would not result a Query
    Interrupted Error.
    Rather the next query issued after INSERT in the connection
    would throw error ER_QUERY_INTERRUPTED.
    This behaviour is wrong.
    
    ANALYSIS OF BUG#11790057:
    SCENARIO 1:
    In this scenario, SELECT has cleared all the stages and it is
    just flushing its output on the console through send() system
    call which is non blocking.
    Here if a user issues kill query in another connection,
    in the kill reception code there was no check to determine if
    the command is finished executing and it should not set the kill
    flag for that thread.
    
    SCENARIO 2:
    The state of blocked INSERT query is set to MDL_wait::KILLED due
    to KILL QUERY being issued in another session. No error was thrown.
    The state of INSERT executing thread would be set to THD::KILL_QUERY
    by kill reception code and it would affect the next command incorrectly.
    
    FIX FOR BUG#11790057:
    SCENARIO 1:
    A check has been added in the kill reception code that if any
    kill request comes in after the command is processed but
    before the next command is received, it should not set the kill
    flag for that thread because there is no active command being
    processed.
    SCENARIO 2:
    When the query waiting on MDL Lock is killed, its state is set to
    MDL_WAIT::KILLED. As a fix, an error ER_QUERY_INTERRUPTED is thrown
    from there.
    After the error is thrown from inside trans_commit_stmt() function,
    the thread state is reset to NOT_KILLED, by rearranging the code
    inside if (!thd->sub_stmt) so that next command does not get affected.
    
    BACKGROUND OF BUG#11754124:
    This bug is pretty similar or we can say duplicate of Bug#11790057,
    Only difference is more number of scenarios of unexpected behaviour
    of KILL QUERY are presented in this bug than Bug#11790057, and they
    are pretty much clear from the bug description.
    
    ANALYSIS OF BUG#11754124:
    All the scenarios mentioned in this bug are already been taken care
    by the patch submitted for Bug#11790057. Only Additional test cases
    are written to test the scenarios mentioned in this bug.
    DETAILED ANALYSIS OF SCENARIOS:
    1. Scenario 1, 2, 4 and 8 mentioned in the bug report are reproducible
       and are fixed with the current patch.
    2. Scenario 3 is Reproducible in 5.1. It is not reproducible in 5.5,
       5.6 and trunk.
       The above scenarios 1, 2, 4 and 8 were fixed by the check
       added in sql_kill() function submitted as a patch for
       Scenario 1 of Bug#11790057.
    3. Scenario 5, 6, 7 are not reproducible as they are
       described in the bug report in any of the versions.
       When we CALL a successfully created stored procedure, and a
       KILL QUERY is issued before in another session, the CALL to
       stored procedure terminates with ER_QUERY_INTERRUPTED error.
       After that if we again CALL it, it works fine.
       The above is applicable to Scenarios 5, 6 and 7 mentioned
       in the bug report.
       Scenario 9 is special because here KILL is issued concurrently
       when we are calling the stored procedure.
       For Scenario 9 there are two cases:
       CASE 1:
       A SELECT SLEEP(10) query is present inside a stored procedure
       and a handler is declared for it. In this case, When we do KILL
       when the sleep is executing, it goes to  pthread_cond_[1;31mtime[mdwait()
       system call. This system call does not return an error code of EINTR,
       when it is killed. The system call passes and only the state of
       thread is set to KILL_QUERY.
       Since there is no error generated by the SLEEP(10), handler does not
       get activated in any of the versions. This behaviour is present in all
       the versions starting from 5.1 and is correct.
       CASE 2:
       A very long running SELECT query is present inside a Stored Procedure
       and a handler is declared for it. In 5.1, CONTINUE handler for this is
       executed when a FATAL error happens which should not be the behaviour.
       See Bug #15192 for more details.
       In 5.5, 5.6 and trunk, handler for this is not executed since it is a
       FATAL error. Therefore, this behaviour is correct.
    
    
    FIX FOR BUG#11754124:
    The fix for this bug is same as the patch submitted for Bug#11790057.
    Additional test coverage is submitted as patch for this bug.

[33mcommit 930f28ab64d29ac34b07e7756c28bf1edee40fa6[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Feb 23 10:24:10 2013 -0800

    DBDictionaryImpl is seeing an NDB API bug?
    Temporary workaround: some[1;31mtime[ms a getTable() call will fail with error "NDB BUG. TRY AGAIN."

[33mcommit 691c3a172b7699ecfad2347623c2f1013e290906[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Sat Feb 23 12:53:22 2013 +0530

    WL#6314: Intra-schema multi-threaded slave
    
    fixed the prepare commit [1;31mtime[mstamping
    fixed checkpointing with MTS_BGC

[33mcommit 81a73b8cc109ee180584a7751a9354285fe4e168[m
Author: prabakaran thirumalai <prabakaran.thirumalai@oracle.com>
Date:   Thu Feb 21 16:04:23 2013 +0530

    Bug#16293702 REMOVE DEAD CODE DEFINED WITH SIGNALS_DONT_BREAK_READ
    Description: Removed code defined under SIGNALS_DONT_BREAK_READ
    compile [1;31mtime[m macro

[33mcommit 3d2adc82e9f579b3e9bb72a01f776ec30402bd1a[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Feb 18 18:37:53 2013 -0800

    Starting now, we strongly prefer Node v 0.8 or higher.
    However, this commit adds compile-[1;31mtime[m compatibility with libuv 0.6 on unix.
    On Windows, 0.8 or greater is a requirement.

[33mcommit da1d3a591feaa969454f39b3f06f9d89dbf1cec0[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Feb 13 14:54:19 2013 +0100

    Make util thread wait binlog to setup schema_ndb
    
     - Fix problem caused by  the ndbcluster threads now starting
       at the same [1;31mtime[m which casues util thread trying to access schema_ndb
       pointer before it has been created by the binlog thread and thus return an
       ugly error during creation of the ndb_schema table.
     - In short, ndb util thread should wait until ndb binlog thread has
       created schema_ndb

[33mcommit 18f451dcdfde78492402c7ae8acad0fae1edb1c9[m
Author: Tanjot Uppal <tanjot.uppal@oracle.com>
Date:   Fri Feb 8 14:34:35 2013 +0530

    Added the --debug-server and --testcase-[1;31mtime[mout=60 options to the weekly run with QC enabled

[33mcommit 8a183852885dce52cb4eb9a599b29992178ac1f0[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Feb 8 14:01:26 2013 +0530

    - WL6470:
      - given life[1;31mtime[m and visibility of temp-table, locking overhead of temp-table
      can be reduced. in light of this we are trying to optimize locks acquired by
      temp-tables.

[33mcommit 1d7265aff942341ac73d35cfd8ced4959b0989e1[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Feb 6 16:49:26 2013 +0100

    ndb - revert Create large hashmaps in 7.0 and 7.1
    
    Remove support for new build [1;31mtime[m flag NDB_LARGE_DEFAULT_HASHMAPS.
    The new parameter DefaultHashmapSize should be used instead.
    
    Reverted patch: revision-id: mauritz.sundell@oracle.com-20130206090144-48yabax38u32g6k3

[33mcommit 4e8cbbd0a5ebfdac9a91a7f2ac55e7960b88e477[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Feb 6 10:01:44 2013 +0100

    ndb - Create large hashmaps in 7.0 and 7.1
    
    This patch adds a build [1;31mtime[m flag to make 7.0 and 7.1 to
    create large hashmaps (3840 buckets) as 7.2 do.
    
    That make it possible to have more even distribution of data
    than possible with only 240 buckets in hashmaps in some
    configuration.
    
    Benefits,
    1) bigger hashmaps implies more evenly distributed data if
    the number of partitions does not divide 240 evenly.
    2) make it possible to downgrade from 7.2.7 and above even
    if tables or indexes are created or altered so they use
    large hashmaps.
    
    Drawbacks,
    1) it is a new build [1;31mtime[m flag, so it needs a custom rebuild
    to be enabled.
    2) will not be possible to upgrade or downgrade to release builds of 7.0 and 7.1 if the new flag is used
    
    How to use
    
    When building use one of the following to enable large hashmaps:
    a) ./configure --with-ndb-ccflags=-DNDB_LARGE_DEFAULT_HASHMAPS
    b) storage/ndb/compile-cluster --with-ndb-ccflags=-DNDB_LARGE_DEFAULT_HASHMAPS
    c) make NDB_EXTRA_FLAGS=-DNDB_LARGE_DEFAULT_HASHMAPS
    
    to verify that the build creates large hashmaps (3840 entries instead of 240),
    start an initial cluster and do ndb_show_tables,
    then a hashmap named DEFAULT-HASHMAP-3840-NN
    should show up, where NN is the number of nodegroups.

[33mcommit 39a07f1e2228afeaf46895adc0756044c1b555bd[m
Merge: 00591f75ee5 8aab52d91fa
Author: Tarique Saleem <tarique.saleem@oracle.com>
Date:   Mon Feb 4 17:39:57 2013 +0530

    Commented the --source include/not_valgrind.inc line. The Bug#15922655 was raised for this issue. Since the failure is not getting replicated it looks like it was a machine issue or envirnment issue at th
    at [1;31mtime[m. To reconfirm this I am commenting this option again to see the behaviour. If it occurs again will take the necessary logs for more analysis.

[33mcommit 24b2702fd8a54754134c57764407de86943a8390[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Jan 31 17:15:49 2013 +0200

    Fix linkage error with InnoDB unit tests - some libraries need to be
    specified a few [1;31mtime[ms on some platforms.

[33mcommit 199d9511a954aaa957ddf56b6326ac07dc30eae4[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Jan 29 14:23:44 2013 +0800

    Bug#16097753 : SLOW QUERY LOG LOCK_TIME NOT LOGGED CORRECTLY
    
    Solution     :
    row_ins_check_foreign_constraint(),row_update_cascade_for_mysql():
    Set thr->lock_state to QUE_THR_LOCK_ROW before entering
    lock_wait_suspend_thread() in order to make lock [1;31mtime[m calculated.
    
    rb#1886 approved by Marko
    innodb_bug16097753.test

[33mcommit 8d689777b0809b5b1f6b14e0c47adebd1bf8a663[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Thu Jan 24 03:25:00 2013 +0530

    WL#6314:Inter-schema multi-threaded slave
    
    -fixed prepare & commit sequencing.
    -At binlog-prepare
            -prepare clock steps by one and leaves the [1;31mtime[mstamp on
             the entering thread.
            -commit clock does not step but leaves a [1;31mtime[mstamp on the
             entering thread. This will be used as a commit parent on the slave side.
    -At commit
            -before entering the ha_commit_low i.e. actual commit the
             commit clock steps by one.
    TODO: call binlog_prepare for implicit commits.
    TODO: use these sequences to assign group ids to transactions on slave.

[33mcommit 6300f44500923c931cf42d498ab5d6f6b355ea33[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Mon Jan 21 00:55:43 2013 +0530

    WL#6314: Inter-schema multi-threaded slaves
    
    -Fixed the status-var field for Query_log_event to
    store the Prepare and Commit [1;31mtime[mstamps
      -TODO: fix the same for GTID events.
    - Fixed prepare commit sequencing on the master.
      -TODO: use the two [1;31mtime[mstamps to mak taxonomical infrerences
    on the slave side and find the group id to events.

[33mcommit 1bea006e2c660847608e76b82818b271c049a168[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Sun Jan 20 19:18:25 2013 +0800

    wl#6501 Increase the [1;31mtime[m of waiting for redo record is written into redo log

[33mcommit 8e0efaf5122082d7607fdf315b272611f08f6e0e[m
Author: Astha Pareek <astha.pareek@oracle.com>
Date:   Fri Jan 18 12:05:32 2013 +0530

    Description
    The test, binlog.binlog_spurious_ddl_errors was failing on pb2 at the statement
    "UNINSTALL PLUGIN example;" with this warning:
    "Warning        1620    Plugin is busy and will be uninstalled on shutdown "
    
    Fix
    Spurious warnings occur in the test since we do not empty the Query cache,
    used by the example plugin at the [1;31mtime[m of creating tables using the plugin.
    Hence, the query chache is flushed before uninstalling the plugin.
    Also, as part of running the test across platforms, the plugin installation
    script is changed.

[33mcommit b1264996a5f37448ec4d85480e8b043b166a60ba[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Fri Jan 18 01:12:56 2013 +0530

    WL#6314: Intra-schema multi-threaded slave
    
    - Implemented state clock for prepare & commit sequencing
    - Added new fields in status var section of Query log events
      to store the prepare and commit logical [1;31mtime[mstamp.
    - update to these [1;31mtime[mstamps will be done in do_write_cache().

[33mcommit ccae471ecbab0a54314e678eb64fa29fb02ca882[m
Author: Manish Kumar <manish.4.kumar@oracle.com>
Date:   Wed Jan 16 19:40:38 2013 +0530

    BUG#13886150 - RPL.RPL_INCIDENT FAILS ON PB2 SPORADICALLY DUE TO TIMEOUT
    
    Problem: rpl.rpl_incident fails on pb2 sporadically due to [1;31mtime[mout in
             include/wait_for_slave_param.inc. Apart from generic
             linux(linux2.6-x86-64bit, where we have knows [1;31mtime[mout issues),
             the failure was observed on gcov-linux-x86_64 and
             rhel5-x86-32bit. The test is passing in retry attempts in just
             257 milliseconds, it means it might be hanging some[1;31mtime[m in
             include/wait_for_slave_param.inc. This looks like a problem with
             the test itself.
    
    Fix: Modifying the test a bit by removing the .opt file for this test and
         changing some lines to use the debug sync point in the code.

[33mcommit ac82d939b714357bbbd748d8ee2908476659ce2c[m
Author: Pedro Gomes <pedro.gomes@oracle.com>
Date:   Tue Jan 15 14:43:45 2013 +0000

    Bug#14678248: SLAVE_OPEN_TEMP_TABLES_LOCK INITIALIZED MULTIPLE TIMES
    
    The slave_open_temp_tables_lock can currently be initialized
    multiple [1;31mtime[ms because this action is made on the Relay_log_info
    constructor. In fact, this constructor is not only used by the
    SQL thread itself but also by clients to execute Binlog
    statements where dummy relay log info objects are needed. Not
    belonging to the class, the double initialization of this lock
    can be problematic.
    
    Guaranteeing that the lock is only initialized on the relay log
    info object that belongs to the SQL thread solves this issue.

[33mcommit 12a6af8e97aeca3fa0fce283eaee48095906a68a[m
Merge: fc672513079 cc677f4ca8f
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Tue Jan 15 17:22:09 2013 +0530

    BUG#14040155 - FORMAT FUNCTION WITH LOCALE CRASHES WITH BIG NUMBERS
    
    
    Problem:
    When we are using modulo operation with big number inside FORMAT function the
    mysql daemon crashes due to this assertion statement.
    
    DBUG_ASSERT(dec_length <= str_length);
    
    dec_length-Decimal places we want.
    str_length-string length of the resultant string.
    
    
    Analysis:
    We save our decimal data in mydecimal class which has member decimal_digit_t
    buffer[DECIMAL_BUFF_LENGTH] to store decimal values. The decimal_digit_t is a
    signed integer and DECIMAL_BUFF_LENGTH is macro of 9.
    
    Single decimal_digit_t can carry at most 9 'decimal' digit value(that is it
    can take value less then  999,999,999) and we have 9 decimal_digit_t which
    mean we have space for 81 decimal digit max(that is  max value it can hold
    is 81 [1;31mtime[ms repeated '9') and when we have to define a decimal digits after
    point they have to allocate space of multiple of 9.
    
    So that means if we have value like 12345123456789.1234
    
    it will save it like this
    
    12345        123456789        123400000
    buf[0]        buf[1]            buf[2]
    
    -->Integer part is save in buf[0] and buf[1] with buf have max 9 digits
    and fraction part must carry 9 digits(like in fraction we have 1234 and we change it to
    123400000 in buf[2]).
    
    suppose we have a query like this-->
    select format((72 [1;31mtime[ms repeated '9')%(72 [1;31mtime[ms repeated '9'),27,'pl_PL');
    
    In this case we have expression((72 [1;31mtime[ms repeated '9')%(72 [1;31mtime[ms repeated '9'))
    whose value should be rounded to 27 decimals.
    
    We save our expression in decimal_digit_t. When we do module of this
    expression, the result will have 72 [1;31mtime[ms repeated '0',then the space available
    for fraction part is of 9 decimal digit, so we truncate the decimal digit(that
    is given to us as a second argument in format function in the query ) from 27
    to 9, as that much is available to us.
    After that we calculate the length of the integer part (0), so the
    str_length will be 0+9(truncated value of fraction part) and
    dec_length(27), result in debug assertion fail as string length(9) is
    less then decimal length(27).
    
    example of module operation->
    
    (71 [1;31mtime[ms repeated '9')%(70 [1;31mtime[ms repeated '9'+'8')
    
    numerator buffer will be store like this.
    
    99999999        999999999       999999999       999999999       999999999       999999999       999999999       999999999       some garbage
    buf[0]          buf[1]          buf[2]          buf[3]          buf[4]          buf[5]          buf[6]          buf[7]          buf[8]
    
    denominator buffer will be store like this.
    
    99999999        999999999       999999999       999999999       999999999       999999999       999999999       999999998       some garbage
    buf[0]          buf[1]          buf[2]          buf[3]          buf[4]          buf[5]          buf[6]          buf[7]          buf[8]
    
    
    the result of this module will be in numerator then store to resultant buffer.
    the resultant numerator may carry sumthing like this
    
    somethg nt usable       0               0               0               0               0               0               1               some garbage
    buf[0]                  buf[1]          buf[2]          buf[3]          buf[4]          buf[5]          buf[6]          buf[7]          buf[8]
    
    from this numerator we store the result in resultant buffer.
    So result in resultant buffer will be
    0               0               0               0               0               0               1               some garbage    some garbage
    buf[0]          buf[1]          buf[2]          buf[3]          buf[4]          buf[5]          buf[6]          buf[7]          buf[8]
    
    Now as we see, we have space for 2 buf(that is buf[8] ) so we truncate our
    decimal value from 27 to 18.
    
    The length that will be 18(truncated value)+
    1(for decimal point)+1(moduler result that is 1).
    
    Which cause our debug assert to fail.
    
    DBUG_ASSERT(dec_length(27) <= str_length(18));
    where dec_length is the dec_length which we want while running the
    query and str_length is string length of resultant query.
    
    
    Solution:
    
    We remove the leading zeroes from the result of the division operator,
    so that we can get actual length of the result.Which will help us to
    decide whether we need to truncate the decimal part of the result.

[33mcommit e936ca60268a96ff13092e76356b32eee4bd49c5[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 10 14:19:59 2013 +0100

    Cherrypick from mysql-5.1
    
    revno: 3900
    committer: Nirbhay Choubey <nirbhay.choubey@oracle.com>
    branch nick: B16046140-5.1
    [1;31mtime[mstamp: Thu 2012-12-27 17:33:34 +0530
    message:
      Bug#16046140 BIN/MYSQLD_SAFE: TEST: ARGUMENT EXPECTED
    
      Some shell interpreters do not support '-e' test
      primary to construct conditions.
    
      man test 1 (on S10)
      ...skip...
      -e file True if file exists. (Not available in sh.)
      ...skip...
    
      Hence, check for the existence of a file using
      '-e' might result in a syntax error on such
      shell programs.
    
      Fixed by replacing it by '-f'.

[33mcommit 87a502c4354640912389ca2979ad5757df47d285[m
Author: kevin.lewis@oracle.com <>
Date:   Wed Jan 9 11:32:13 2013 -0600

    Bug14762796 was first fixed in rb:1416, reviewed and approved by Jimmy.
    
    But that fix showed several test cases failing intermittently on PB2.
    CreateThread() does not immediately start the new thread so that in
    some small amount of calls to os_thread_create(), the CloseHandle()
    added by the previous patch actually closed the thread object before
    the thread itself could open a handle to it.  Then when the thread
    actually started, some of that thread info was lost.  This solution
    of closing the thread handle as soon as the thread was created was
    found in the MSDN documentation as a comment to the official docs.
    It never mentioned being timing dependent.
    
    I also noted that contrary to the MSDN documentation, not calling
    ThreadExit() from the thread itself before exiting in C++ code
    caused certain testcases to fail because of unreleased resources,
    at least some of the [1;31mtime[m on Windows 32-bit Vista.  I could never
    reproduce either problem on Windows 7-64.
    
    So I pushed a small patch to mysql-trunk to fix the failing
    testcases by adding a Sleep(0) before the CloseHandle() in
    os_thread_create() and to go back to ALWAYS doing ThreadExit() in
    os_thread_exit().  This seems to have cleaned up the failing testcases.
    
    But that 'Sleep(0)' solution is too indeterminate.  This patch
    introduces a global stl::map object to remember the Windows Handles
    returned from CreateThread() and then close them in os_thread_exit().
    This way they are not closed immediately while the thread is still
    getting started, which relys on thread switching and timing to work
    correctly.
    
    In addition, this patch makes sure that the Linux code always returns
    the new_thread_id in the thread_id parameter to os_thread_create().
    
    Approved by Sunny in rb:1788

[33mcommit ed1e21573b6bf2ed94f0f9609d219f286310aa2f[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Sun Jan 6 22:13:43 2013 +0530

    Bug #14801721: WRONG ERROR REPORTED ATTEMPTING TO CREATE AN
                   ALREADY EXISTENT TRIGGER
    
    Analysis:
    -----------
    While creating trigger, we first check whether trigger with same
    action [1;31mtime[m and event already exists. (Since we do not support
    creation of multiple triggers with same action [1;31mtime[m and event,
    we report an error ER_NOT_SUPPORTED_YET.) Then later we have logic to
    check for duplicate trigger name.
    
    Fix:
    -----------
    Moved logic to check duplicate trigger name above the logic to check
    whether trigger with same action [1;31mtime[m and event already exists
    or not.

[33mcommit 97a090f715e1dfee7d7005a3c97b788d653525a8[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Sat Dec 15 16:54:22 2012 +0200

    part of WL#6658 Implement update_[1;31mtime[m and check_[1;31mtime[m for InnoDB tables
    
    Add the basic functionality of maintaining check_[1;31mtime[m and update_[1;31mtime[m
    in InnoDB in-memory together with two mtr tests.

[33mcommit 30e1d9dfdf58970ac2a6ae379bf118e6ba99a809[m
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Dec 13 14:40:36 2012 +0000

    Bug#14475946: Cannot retrieve MySQL 5.0 stacktrace on Linux
    
    In 5.5+, we use the backtrace() interface on linux
    which was added to glibc in 2.1, circa 1999.
    (At the [1;31mtime[m of this patch, we officially support glibc
    2.3 and up.)
    
    This package removes our homebrew solution, which is
    now essentially dead code, and prints a message to the
    DBA if they somehow manage to reach it anyway.
    
    (Mind that this would presumably necessitate linking
    against a stone age glibc, or a libc that is not GNU.
    Either way, we fail gracefully in such cases.)

[33mcommit 094ebb80a96a6b4b0841044b10a47a229bf8b871[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Thu Dec 13 14:33:23 2012 +0100

     3853 Annamalai Gurusami        2012-11-01
          Bug #14669848 CRASH DURING ONLINE ALTER MAKES ORIGINAL TABLE INACCESSIBLE
    
          When a new primary key is added to an InnoDB table, then the following
          steps are taken by InnoDB plugin:
    
          .  let t1 be the original table.
          .  a temporary table t1@00231 will be created by cloning t1.
          .  all data will be copied from t1 to t1@00231.
          .  rename t1 to t1@00232.
          .  rename t1@00231 to t1.
          .  drop t1@00232.
    
          The rename and drop operations involve file operations.  But file operations
          cannot be rolled back.  So in row_merge_rename_tables(), just after doing data
          dictionary update and before doing any file operations, generate redo logs
          for file operations and commit the transaction.  This will ensure that any
          crash after this commit, the table is still recoverable by moving .frm files.
          Manual recovery is required.
    
          During recovery, the file operation redo logs are processed.  Previously
          this was being ignored.
    
          rb://1460 in review.
    
     3854 akhil.mohan@oracle.com    2012-11-29
          applying patch for BUG15912213
    
     3855 Joerg Bruehe      2012-12-07
          Last-minute fix to 5.1.67,
          taking a change done to main 5.1 by Dmitri Lenev.
    
          This is the original comment:
    
          > committer: Dmitry Lenev <Dmitry.Lenev@oracle.com>
          > branch nick: mysql-5.1-15954896
          > [1;31mtime[mstamp: Wed 2012-12-05 19:26:56 +0400
          > message:
          >   Bug #15954896 "SP, MULTI-TABLE DELETE AND LONG ALIAS".
    
            Using too long table aliases in stored routines might
            have caused server crashes.
    
            Code in sp_head::merge_table_list() which is responsible
            for collecting information about tables used in stored
            routine was not aware of the fact that table alias might
            have arbitrary length. I.e. it assumed that table alias
            can't be longer than NAME_LEN bytes and allocated buffer
            for a key identifying table accordingly.
    
            This patch fixes the issue by ensuring that we use
            dynamically allocated buffer for table key when table
            alias is too long. By default stack based buffer is used
            in which NAME_LEN bytes are reserved for table alias.
    
    added:

[33mcommit e9ea1fb30db8102bbe23e24cb7641fba28b17cdd[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Wed Dec 5 23:56:42 2012 +0600

    This patch fixes bug#14729757 - MY_HASH_SEARCH(&XID_CACHE,
                                    XID_STATE->XID.KEY(),
                                    XID_STATE->XID.KEY_LENGTH())==0
    
    This bug is a regression of bug#11759534 - 51855: RACE CONDITION
                                               IN XA START.
    
    The reason for regression is that the changes that fixes the original
    bug wasn't merged from mysql-5.1 into mysql-5.5 and mysql-trunk.
    Only null-merge was done for the patch changeset.
    
    To incorporate lost changes the manual merge have been done.
    
    Additionally the call of trans_rolback() was added into trans_xa_start()
    in case if xid_cache_insert is failed() after transaction has been started.
    If we don't call trans_rollback() we would never reset the flag
    SERVER_STATUS_IN_TRANS in THD::server_status and therefore all subsequent
    attempts to execute XA START in the connection where the error was occurred
    will be failed since thd->in_active_multi_stmt_transaction() will return
    the true every [1;31mtime[m when trans_xa_start is called.
    
    The latest changes were absent in patch for mysql-5.1

[33mcommit 499242ab0214e8fce069119edd4eaaf23ce4b55a[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Mon Dec 3 13:06:36 2012 +0530

    BUG#14059662 - DDL ON EVENT SCHEDULER MEMORY LEAK WITH
                   SKIP-GRANT-TABLES ENABLED
    
    BACKGROUND:
    When MySQL server is started with --skip-grant-tables option
    Event Scheduler is disabled,i.e the event scheduler is rendered
    nonoperational.
    At low level, Event_queue and Event_scheduler object are not
    instantiated.
    When an event DDL like Create Event or Alter Event is performed
    during this [1;31mtime[m, it is added in mysql.event table and appropriate
    operations either create or update are done respectively.
    Then memory is allocated for the event queue element to be
    created, But it is not put in the event queue since event queue
    object has not been instantiated in the beginning itself.
    
    While doing server shutdown, when it cleans up scheduler's resources
    The memory for the Event Queue Element which is created is not
    deallocated, destructor for it is not called and it hogs the memory
    without deleting the element. Valgrind reported memory leak
    because of this issue.
    
    FIX:
    As a fix for this bug, A Check has been added in Create and
    Update event code. If the Event Scheduler is in DISABLED state,
    Memory for the event queue element is not allocated.
    We are allowed to create the event even when the scheduler is
    DISABLED.
    Later When the server is restarted again without skip-grant-tables,
    it picks up the event from mysql.event table and starts executing the
    event normally.

[33mcommit d2ec670be66d3f3cbbef37822a87360a0aff1f7a[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Nov 29 11:15:41 2012 -0800

    Implement persist, save, update, and remove
      with table name and values; and constructor and values
      for Session and Batch
    
    Session.js, Batch.js:
      For persist, save, and remove:
      Use first parameter as table indicator; it is a mapped domain object,
      or a table name, or a constructor
      If first parameter is a mapped domain object, there is only one parameter plus callback
      If first parameter is a table name or constructor, there is an extra parameter
      For update, there are two extra parameters: keys and values
    
    UserContext.js:
      For persist, save, update, and remove:
      Analyze first parameter and allow domain object, table name, or constructor
      Use cached table handler in case of multiple threads obtaining table metadata
    
    MySQLDictionary.js:
      Add a bit of debugging
    
    InsertTest.js:
      Add variants of persist: table name and constructor
    
    SaveTest.js:
      Remove illegal argument tests, which are now legal
      Add variants of save: table name and constructor
    
    UpdateTest.js:
      Remove illegal argument tests, which are now legal
      Add variants of update: table name and constructor
    
    UnmappedDomainObjectTest.js:
      Test unmapped domain object with persist, save, update, remove, and load
    
    driver.js:
      Fix lint errors
      Change --[1;31mtime[mr to --[1;31mtime[mout; default to no [1;31mtime[mout
    
    harness.js:
      Fix lint errors

[33mcommit 66bfcd8ae65cf9d191c61606d8935666e54e5339[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu Nov 29 16:35:49 2012 +0000

    Bug #15935206   NDB : IMPROVE VISIBILITY OF TRANSPORTER OVERLOAD OCCURRENCES
    
    NdbInfo.transporters extended with 5 new columns :
    
    connect_count  (Number of [1;31mtime[ms connection established on this transporter)
    overloaded     (Whether this transporter is currently overloaded.  0 or 1)
    overload_count (How many [1;31mtime[ms this transporter has entered overloaded state since connecting)
    slowdown       (Whether this transporter is currently in scan slowdown state.  0 or 1)
    slowdown_count (How many [1;31mtime[ms this transporter has entered scan slowdown state since connecting)
    
    Note that the counters are reset on *CONNECT*, so will retain their values after
    the remote node disconnects.
    Note also that the existing bytes_send and bytes_received counters are
    *CHANGED* to be reset on *CONNECT* as well, so will retain their values after the
    remote node disconnects.  This is a change to the previous behaviour, where
    they were reset on DISCONNECT.
    
    Additionally 6 new per-LQH-instance counters are added to NDBINFO.COUNTERS :
    
     LQHKEY_OVERLOAD
       Number of primary key requests rejected at LQH block instance due
       to transporter overload
     LQHKEY_OVERLOAD_TC
       Count of instances of LQHKEY_OVERLOAD where the TC node transporter
       was overloaded
     LQHKEY_OVERLOAD_READER
       Count of instances of LQHKEY_OVERLOAD where the Api reader (only reads)
       node was overloaded.
     LQHKEY_OVERLOAD_NODE_PEER
       Count of instances of LQHKEY_OVERLOAD where the next backup data node
       (only writes) was overloaded
     LQHKEY_OVERLOAD_SUBSCRIBER
       Count of instances of LQHKEY_OVERLOAD where a event subscriber
       (only writes) was overloaded.
     LQHSCAN_SLOWDOWNS
       Count of instances where a fragment scan batchsize was reduced
       due to scanning Api transporter overload.
    
    These new values and counters make transporter overload and SendBuffer
    sizing problems more visible and debuggable.

[33mcommit 28a3e44921e5ca28b2a47a9cd516b7940908e434[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 27 10:59:20 2012 +0100

    ndb
     - fix warnings from VS10 about unused tablePtr, it's used by sizeof() but only at compile [1;31mtime[m
      so we expect TablerecPtr tablePtr to be optimized away(and if not, it's not much extra code)

[33mcommit 6543e53d7e1f63a42ca0fd31caaa1094121d655c[m
Author: Ahmad Abdullateef <ahmad.abdullateef@oracle.com>
Date:   Mon Nov 26 16:55:57 2012 +0530

    BUG#15847447 - THD->TRANSACTION.STMT.IS_EMPTY()
                   || (THD->STATE_FLAGS & OPEN_TABLES_STATE::BACKU
    
    DESCRIPTION:
    When a view is based on a Heap (In memory) backed table and this
    Table is altered to drop some field which was in use by the view, then
    a SELECT on this view results in an ASSETION failure when the server
    is started with Bin-Log enabled.
    
    
    ANALYSIS :
    The triggered assert in open_normal_and_derived_tables() checks that
    the current statement transaction is empty if something fails during
    opening of tables and handling of derived tables. This makes sure that
    there is nothing that the statement has done that needs to be rolled
    back at this point.
    
    The reason that the assert is triggered in this case is that a
    statement transaction is started by THD::binlog_query() during
    open_tables() -> open_table_entry_fini() if we are opening a
    HEAP table for the first [1;31mtime[m, subsequently mysql_handle_derived()
    fails while processing the view since a column has disappeared.
    
    FIX :
    During implicit_emptied processing in the call to THD::binlog_query() direct
    is set to TRUE, as a result the event is immediately written to the log bypassing
    a transaction, hence the assert is not triggered.

[33mcommit bc85c5e347789c39b59e8201f2e612d33f018aa4[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Nov 22 10:34:24 2012 +0100

    ndb - remove false unused warning for NDB_STATIC_ASSERT
    
    In module there my_attribute.h is included but not my_global.h,
    __attribute__ will be redefined and the
    definitions of compile_[1;31mtime[m_assert will produce an unused variable warning.

[33mcommit 9f6d8102a0131e58c59d0ddee37b99b12d0a6c85[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Nov 20 23:37:20 2012 +0000

    Bug #14787522   PROD: NDBD NODE HANGING IN PHASE 3 DUE TO DICT LOCK WITH UNKNOWN HOLDER.
    
    The various distributed protocols involved in dropping a table in 6.3 were
    not well protected against node failures.
    This could result in the drop table operation waiting indefinitely for a
    reply from some node which had failed.
    The indefinite wait caused the originating DDL operation to block until
    NdbApi [1;31mtime[md out.
    Additionally, the 'DICT' (cluster internal dictionary) lock taken by
    the drop table operation is held indefinitely.
    Additionally, the logical 'Global schema lock', taken by the
    MySQLD originating the drop table operation is held until the NdbApi
    operation [1;31mtime[ms out.
    
    This is fixed by adding node failure handling to the drop table protocols
    in 6.3, for DICT Slave node failures.
    
    A new testcase is added to testDict, and the daily-basic suite.
    
    Debugging / visibility enhancements :
    
    A new dump code, 1228 (DictDumpLockQueue) is added to dump the contents
    of the DICT lock queue.
    
    A delayed DICT lock grant is now logged.
    
    The DICT_TRACE mechanism is extended with DICT LOCK debugging.

[33mcommit 7db9499861df1dc1a197219c6ece773236b26238[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 19 16:57:32 2012 +0100

    Adapt MySQl Cluster to 5.6
     - fix existing tests and utility scripts since mysql.host has been removed
     - fix existing .results since a lot more Errors and Warnings are printed when
       an error is provoked.
     - remove code to handle a few old LOGCOM_ command since those are
      never  used. Of course keep the SOT_ commands intact to avoid upgrade
      problems.
     - Add hack preventing db to be NULL when calling ndbcluster_log_schema_op()
     - Add long comment describing why the above hack is needed and that it works anyway.
     - Add new testcases which checks that binlog, data and functionality on remote mysqld
       end up looking the same way as when system tables are local(i.e in default engine).
       Same testcase and .result file is run two [1;31mtime[ms with different configs and the result
       should be same.
     - remove NDB_WITHOUT_DIST_PRIV define

[33mcommit a6aa6caf2473dab92e082b6d606d2836b39d2c75[m
Author: Sneha Modi <sneha.modi@oracle.com>
Date:   Mon Nov 19 14:46:42 2012 +0530

    Adding the big-test option to innodb/innodb-wl5522-debug.test as it takes a ling [1;31mtime[m to
    execute and removing the big-test option from innodb/innodb-wl5522.test as it does not
    take a very long [1;31mtime[m to execute.

[33mcommit 79ac2c0e1c5e8afa24f044beda6d9e5b333b2670[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Nov 15 18:35:56 2012 -0800

    Support wl#946 Timestamp2 column.
    This patch implements sufficient support for memcache expire [1;31mtime[m to work in the existing test cases.
    Work remaining to be done: support the other two new wl#946 data types, and add a new mtr test to verify the behavior of the new types.

[33mcommit 40af3d33bfb09c9a9c70ebede915cfa4aa531b3f[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 12 18:19:25 2012 +0100

    compile-cluster
     - turn off perls autoflsuh for stdout to avoid that no output
    comes out from the warning filter for a long [1;31mtime[m on certain platforms

[33mcommit 26f35d77f97298dfcd92b97b69e813ec5de82b68[m
Merge: c589a12cb2e 17aa11cbf31
Author: Aditya A <aditya.a@oracle.com>
Date:   Thu Nov 8 15:35:03 2012 +0530

    Bug#14234028 - CRASH DURING SHUTDOWN WITH BACKGROUND PURGE THREAD
    
     Analysis
     ---------
    
     my_stat() calls stat() and if the stat() call fails we try to set
     the variable  my_errno which is actually a thread specific data .
     We try to get the  address of this thread specific data using
     my_pthread_getspecifc(),but for the purge thread we have not defined
     any thread specific data so it returns null and when dereferencing
     null we get a segmentation fault.
            init_available_charsets() seen in the core stack is invoked
     through  pthread_once() .pthread_once is used for one [1;31mtime[m
     initialization.Since free_charsets() is called before innodb plugin
     shutdown ,purge thread calls init_avaliable_charsets() which leads
     to the crash.
    
     Fix
     ---
     Call free_charsets() after the innodb plugin shutdown,since purge
     threads are still using the charsets.

[33mcommit 7e0c4a22eaac3fb665d187ad747774fe948936f0[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Nov 7 17:29:01 2012 +0000

    Bug #14680057   NDB : LCP FRAG WATCHDOG GIVES ERROR WITH V HIGH TABLE, FRAGMENT AND ROW COUNTS
    
    The recently added LCP fragment scan watchdog occasionally reported
    problems with LCP fragment scans with very high table id, fragment id,
    and row count values.
    
    This was due to the watchdog not accounting for the [1;31mtime[m spent draining
    the 'Backup' buffer used to buffer rows before writing to the fragment
    checkpoint file.
    
    This is fixed so that in the final stage of an LCP fragment scan, the
    watchdog switches from monitoring rows scanned, to monitoring the
    buffer size in bytes.  The buffer size should decrease as data is
    written to the file, after which the file should be promptly closed.

[33mcommit aa60d4a9eec4baa406857df2c3fa5b14b2077036[m
Merge: f0174d3c7b5 4e2487bda9a
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Wed Nov 7 15:19:45 2012 +0530

    Bug#14021323 CRASH IN FIELD::SET_NULL WHEN INSERTING ROWS TO NEW TABLE
    
    Problem:-
    In myisam engine. If we run a query which is
    creating a table with select statement on a function statement, which
    return null as one of its column result in select query.Mysql deamon crashes.
    
    If we are not able to generate the problem we have to run this query few [1;31mtime[ms.
    
    Analysis:-
    
    If we have a null-constant in our select query, we are treating it as a binary
    type field, in both innodb and myisam engine.
    
    mysql> create table `t1` select null as a;
    
    mysql> show create table t1;
    +-------+-----------------------
    | Table | Create Table
    +-------+-----------------------
    | t1    | CREATE TABLE `t1` (
      `a` binary(0) DEFAULT NULL
    ) ENGINE=MYISAM DEFAULT CHARSET=latin1 |
    +-------+---------------------------------
    
    If we have a null returning function in our select query, we are treating it as
    null type field in myisam and giving error in innodb.
    For example:-
    
    mysql> Create table t1 engine=myisam
    select coalesce(null);
    
    
    mysql> show create table t1;
    +-------+----------------------------------
    | Table | Create Table
    +-------+----------------------------------
    | t1    | CREATE TABLE `t1` (
      `a` null DEFAULT NULL
    ) ENGINE=MyISAM DEFAULT CHARSET=latin1 |
    +-------+----------------------------------
    
    which is creating a problem.
    
    Solution:-
    
    Instead of creating a field type of null, now we are creating a binary of
    length zero.

[33mcommit ce74bf7c41964a44bf02ac5974fda8b50c0467b9[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Nov 2 22:03:12 2012 +0100

    ndb
     - disable pfs_upgrade, it fails every[1;31mtime[m we change mysql-system_tables.sql
     and also have been removed in the future(causing conflicts).

[33mcommit 6b39379e12cf243525014c14d5cbedff90625a69[m
Author: horst.hunger@oracle.com <>
Date:   Thu Nov 1 16:40:09 2012 +0100

    Slow launch [1;31mtime[m is too slow for long running test jobs like in valgrind. increaed it from 1000 to 10000.

[33mcommit 9d210e8b2d8205b2138cc0e5c14802d1b9232a7f[m
Author: Vamsikrishna Bhagi <vamsikrishna.bhagi@oracle.com>
Date:   Tue Oct 30 10:31:11 2012 +0530

    Bug#14547920 I_MAIN.BUG13115401 CAUSING VALGRIND REPORT
                 FAILURE ON PB2
    
    Issue: The test i_main.bug13115401 which was added to fix
           the bug #13115401 has introduced a sporadic valgrind
           failure report on pushbuild specific to yaSSL build.
           It being a sporadic issue, it can be reproduced once
           or twice when the command below is run for around
           50 [1;31mtime[ms.
    
           perl mysql-test-run.pl --[1;31mtime[mr --debug-server
           --force --comment=n_mix-debug --vardir=var-n_mix
           --mysqld=--binlog-format=mixed
           --experimental=collections/default.experimental
           --skip-ndb
           --skip-test-list=collections/disabled-per-push.list
           --unit-tests --valgrind
           --valgrind-option=--gen-suppressions=all
           --valgrind-option=--show-reachable=yes
    
    
    Solution: The problem arises on a multi-threaded scenario
              when a thread tries to create a session object
              from GetSessions() method as another thread is
              already doing a creation. As the memory allocated
              in first thread is overridden by memory allocated
              by the second thread, the memory allocated by the
              first thread will be a leak in this case. This is
              because the yassl session instance is not thread
              safe. To make this instance creation process
              compatible with multi-threading, a new instance
              is created by with the help of yassl_pthread_once,
              which will essentially check if the function used
              for creating an instance is already called
              previously or not.

[33mcommit fd3dfc487952cd7ce1ffca4707a951030032ab3f[m
Author: kevin.lewis@oracle.com <>
Date:   Tue Oct 23 15:51:46 2012 -0500

    Bug #14520559 - INNODB; LARGE BLOB INSERTS ARE SLOW AND TROUBLESOME
    
    Large BLOBS are slow with debug builds due to the large number of
    calls to mtr_memo_contains() and the extra large size of the main
    MTR.  Non-debug builds do not seem to suffer noticeably. That
    problem is not addressed by this patch.
    
    This patch fixes the 'troublesome' assert that happens with 4k page
    size and with some Compressed tablespaces.  The assert is described
    fully in the bug report.  It has to do with how a system tablespace
    is initially expanded.  At 32Mb, it is normally expanded by more than
    one extent at a [1;31mtime[m.  But with 4k pages, that must happen sooner.
    
    http://bur03.no.oracle.com/rb/r/1301/ approved by Marko.

[33mcommit 7c5c7d15faeb2d9525911978de8fafbf22e1c448[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Sat Oct 20 08:57:40 2012 -0700

    Patch clusterj to print the class name each [1;31mtime[m setUp is called, to track down failing CLUB test.

[33mcommit b0ebbb963c266b8e06f9efa6a97e5c8e3e7cd59b[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Oct 16 13:56:54 2012 +1100

    WL#6047 - Interim fix for query cache invalidation logic.
    
    i_innodb.innodb_bug14658648 was failing because the design requires a
    trx id for the logic to work. In WL#6047 RO transactions don't have a
    transaction id and that breaks this checks.
    
    The interim fix is to use transaction start [1;31mtime[m. However, this is too
    coarse because it is in seconds. Also, because when transactions are
    promoted to RW they retain the original start [1;31mtime[m. Needs to be verified
    if this is correct.

[33mcommit d7c6ec8cbdcec0c9cb191a05be72f52cb567b453[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Thu Oct 11 16:16:25 2012 +0200

    Worklog #6073: Remove INSERT DELAYED: First patch.
    
    This patch causes the DELAYED keyword to be stripped off in the
    parser, thereby making the feature unavailable. All tests of the
    INSERT DELAYED operation have been deleted from MTR, many [1;31mtime[ms entire
    files are deleted. Also, all references to such files are removed,
    such as in disabled-gtid-on.list and disabled-per-push.list.
    
    Nothing is done to the deprecated system variables pertaining to
    INSERT DELAYED, i.e. manipulating them will still give a deprecation
    warning.
    
    The mutexes are not removed either and hence most performance schema
    tests are intact.

[33mcommit e0441cfe8c0296d1f3108b21cc962d49a3d83758[m
Author: Vinay Fisrekar <vinay.fisrekar@oracle.com>
Date:   Wed Oct 10 10:26:06 2012 +0530

    Skip run for valgrind as test contain restart with send_shutdown which kill server (after wait [1;31mtime[m) and memeroy leak will occur

[33mcommit 82cba3c4fece7786e6df1296bec2449128359f63[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Oct 4 09:23:52 2012 +0200

    Move hack for mtr.pl which enables many tests to fail
    later in the file making it possible to skip the hack on valgrind hosts
    to save some cpu [1;31mtime[m...

[33mcommit 27c6c64f1132fc3a5ca36d6ca90217788b7f6401[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Fri Sep 28 15:15:29 2012 +0200

    ndb - (refactor) change logic for expand/shrink control in dbacc
    
    change expandFlag to expandShrinkQueued
    
    The only values currently used for expandFlag was 0 and 2,
    reenable_expand_after_redo_log_exection_complete() was never
    called.
    
    So i replaced the logic with on boolean flag expandShrinkQueued
    that is set true on send of expand or shrink signal, and set
    false when signal is processed. The flag is checked before send
    of such signal and if set true, the send is suppressed, meaning
    that at most one expand or shrink signal can be queued at the
    [1;31mtime[m.

[33mcommit 02afec4557775ac11b4283507b3af27c4eb131be[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Sep 27 11:10:30 2012 -0700

    Support Timestamp in NdbTypeEncoders.
    Use [1;31mtime[mstamp rather than date[1;31mtime[m in tweets demo.

[33mcommit 6e9bdaf97e522d880a5f4d6a9318f0fbd9b00d56[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 27 15:36:48 2012 +0200

    WL#6224 Adapt MySQL Cluster to 5.6
     - update .result file for ps_7ndb since test sources shared .inc
     files which have been changed to work better with new and old behaviour
     of [1;31mtime[mstamp

[33mcommit aa09a572a11649ed0685ad1e9d39a7453546a7fb[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Sep 26 17:22:49 2012 -0700

    Fake stub support for date[1;31mtime[m in NDB.

[33mcommit e7fad42ab11dd76016b670a46e2985b6bbdde760[m
Author: Akhila Maddukuri <akhila.x.maddukuri@oracle.com>
Date:   Tue Sep 25 16:49:40 2012 +0530

    Description:
    ------------
    sys_vars.all_vars fails on Windows 32 and 64 bit on daily trunk commercial
    branch with multiple combinations.
    
    Failure snippet-
    sys_vars.all_vars                        w1 [ fail ]
            Test ended at 2012-08-22 09:20:13
    
    CURRENT_TEST: sys_vars.all_vars
    'diff' is not recognized as an internal or external command,
    operable program or batch file.
    ---
    G:/pb2/test/sb_1-6672625-1345599364.71/mysql-advanced-5.7.0-m10-winx64/mysql-t
    est/suite/sys_vars/r/all_vars.result    2012-08-22 01:05:07.000000000 +0300
    +++
    G:\pb2\test\sb_1-6672625-1345599364.71\mysql-advanced-5.7.0-m10-winx64\mysql-t
    est\suite\sys_vars\r\all_vars.reject    2012-08-22 10:20:13.251978100 +0300
    @@ -12,5 +12,7 @@
     select variable_name as `There should be *no* variables listed below:` from
    t2
     left join t1 on variable_name=test_name where test_name is null ORDER BY
    variable_name;
     There should be *no* variables listed below:
    +LOG_THROTTLE_QUERIES_NOT_USING_INDEXES
    +LOG_THROTTLE_QUERIES_NOT_USING_INDEXES
     drop table t1;
     drop table t2;
    
    mysqltest: Result length mismatch
    
    How to repeat:
    --------------
    Seen on daily trunk commercial.
    
    mysql-test-run.pl  --[1;31mtime[mr --force --parallel=8 --comment=n_mix_4k_size
    --vardir=var-n_mix --mysqld=--binlog-format=mixed
    --experimental=collections/default.experimental --skip-ndb
    --skip-test-list=collections/disabled-per-push.list
    --mysqld=--innodb-page-size=4k --skip-test=innodb_ignore_builtin
    
    Suggested fix:
    --------------
    The purpose of all_vars.test is to make sure that there are tests for all
    system variables. In daily trunk commercial it is failing because it could find
    test for a system variable called log_throttle_queries_not_using_indexes.
    
    In windows platform the test-name is truncated to
    log_throttle_queries_not_using_indexes_basic.tes
    Hence all_vars.test couldn't find a .test file for the above variable.
    
    Inorder to get rid of the truncation, suggested fix is to change the name of the
    test to a smaller name which is log_throttle_qni_basic.test
    
    Also changing the file-name must happen in conjunction with another alias-rewrite:
    
    update t2 set variable_name= replace(variable_name, "_THROTTLE_QUERIES_NOT_USING_INDEXES",
                                         "_THROTTLE_QNI");

[33mcommit fa0b92b0c0b3bd371d74c2a86dc9cfb48b104e3d[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Sep 24 16:57:07 2012 +0200

    WL#6224 Adapt MySQL Cluster to 5.6
     - Move the call to THD::init_for_queries in util and binlog thread
       to after they has waited for "mysqld_server_started" since the function
       uses parts of MySQl Server which are not guaranteed to bee created until
       that [1;31mtime[m.
     - Not a nice solution but it's hardly impossible to reaarrange the spagetthi
       so that for example THD object is created after mysqld_server_started

[33mcommit f63e374cd6f8219c2b1fcefbceb58e12a884e75d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Sep 24 16:11:36 2012 +0200

    Fixed possible deadlock / [1;31mtime[mout in ndb_spj_dict.test:
    
    This test has two parallel server connection:
    
    1) Started a (pushed) join possibly creating a lot of result rows
    2) Do some alter table DDLs which is expected to terminate query in 1)
    
    However, as 1) created a lots of result, which is sent through the client-server
    interface with a TCP 'write()', the TCP send/recv buffers may fill up. If these
    buffers became  completely filled before the query was killed by the the DDL, we
    seems to enter a deadlock state where neither the DDL is able to execute, and no
    further communication can take place. Thus, the MTR test [1;31mtime[md out with
    'Lost connection to the MySQL server'.
    
    This fix change to using a 'select count(*)' instead of a plain 'select *' in
    order to avoid saturated TCP buffers.

[33mcommit aa0203ba8595b3eb5a09e6a5081a859c94bfa1bb[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri Sep 21 13:41:35 2012 +0530

    BUG# 12793893 - binlog_innodb [1;31mtime[md out in pushbuild.
    
    === Description ===
    
    The test contained a transaction that ran a loop 2000
    [1;31mtime[ms. In every iteration, a row was being being
    inserted into a table. 2000 iterations to build up the
    long transaction some[1;31mtime[ms proved to be too costly on
    slow platforms.The purpose behind 2000 iterations was to
    make sure that the transaction used the temporary binary
    log cache and also exceeded the value of
    "binlog_cache_size".
    
    === Fix ===
    
    I toned down the loop at the same [1;31mtime[m I made sure it
    did not break the purpose.
    1) Made the loop run 600 [1;31mtime[ms.
    2) Decreased the value of "binlog_cache_size" to 4096,
       the minimum allowed value defined in the manual.

[33mcommit 0daaf8aecd8f84ff1fb400029139222ea1f0d812[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Sep 20 12:34:31 2012 +0530

    BUG#11763447: 'YOU CANNOT 'ALTER' A LOG TABLE IF LOGGING IS ENABLED'
                   EVEN IF I LOG TO FILE.
    
    Analysis:
    ----------
    
    MYSQL_UPGRADE of the master breaks the replication when
    the query logging is enabled with FILE/NONE 'log-output'
    option on the slave.
    
    mysql_upgrade modifies the 'general_log' and 'slow_log'
    tables after the logging is disabled as below:
    
    SET @old_log_state = @@global.general_log;
    SET GLOBAL general_log = 'OFF';
    ALTER TABLE general_log
    MODIFY event_[1;31mtime[m TIMESTAMP NOT NULL,
    ( .... );
    SET GLOBAL general_log = @old_log_state;
    
    and
    
    SET @old_log_state = @@global.slow_query_log;
    SET GLOBAL slow_query_log = 'OFF';
    ALTER TABLE slow_log
    MODIFY start_[1;31mtime[m TIMESTAMP NOT NULL,
    ( .... );
    SET GLOBAL slow_query_log = @old_log_state;
    
    In the binary log, only the ALTER statements are logged
    but not the SET statements which turns ON/OFF the logging.
    So when the slave replays the binary log,the ALTER of LOG
    tables throws an error since the logging is enabled. Also
    the 'log-output' option is not checked to determine
    whether to allow/disallow the ALTER operation.
    
    Fix:
    ----
    The 'log-output' option is included in the check while
    determining whether the query logging happens using the
    log tables.

[33mcommit 5e34b233bd3149486f94b2afe3b2e1554a5604f4[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Thu Sep 13 10:12:30 2012 +0200

    Bug#14095506: DUBIOUS CHOICE OF INDEXES TO MERGE IN
                  INDEX_MERGE_MYISAM.TEST
    
    handler::multi_range_read_info_const() uses
    index_only_read_[1;31mtime[m() to estimate io cost when only the index
    is accessed. However, if the row estimate is two or less
    handler::read_[1;31mtime[m() is used instead:
    
    if ((*flags & HA_MRR_INDEX_ONLY) && total_rows > 2)
    
    This makes little sense, and also contrasts the io cost
    estimate made by handler::multi_range_read_info() even though
    the two functions claim to be equal wrt io cost estimate.
    
    Before this patch, it was considered cheaper to read a range
    with 3-5 rows than a range with 1-2 rows.

[33mcommit aab349dd7a0de8cdbcfe6f39cad0a20a3224eaf7[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Sep 7 10:12:38 2012 +1000

    WL#6044 - Mutexes created in BSS will be destroyed by C++ run[1;31mtime[m on shutdown.
    Mutex destructors should be no-ops. Release any resources in a destroy() method.
    
    TODO: Prohibit creation of mutexes on the C++ heap using the new operator.

[33mcommit 38009d3f4b52475a993db6b1045776397bb3d3af[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Tue Sep 4 19:22:55 2012 +0530

    BUG#14309019 - mysqlbinlog -v does not properly decode
                   DECIMAL values in an RBR log.
    
    Post push commit to handle tests failing in pb2 build.
    
    === Fix ===
    
    1) Deleted the variable 'end'.
    2) Added an if condition before printing decimal point.
    3) Deleted sprintf line after the two for loops because
       it prints garbage values at [1;31mtime[ms.

[33mcommit 3a6d4838c8c536ed24b95aff07941633a405eab2[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Mon Sep 3 14:56:46 2012 +0530

    BUG#14309019 - mysqlbinlog -v does not properly decode
                   DECIMAL values in an RBR log.
    
    === Description ===
    
    mysqlbinlog -v reads a binlog file and prints it in a
    readable format. In case of decimal values, it converts
    the number in base of pow(10,9). It used to print a
    decimal point after every block of 9 digits. As a result,
    decimal point was some[1;31mtime[ms printed more than once or
    some[1;31mtime[ms mispalced. As an example, The value: -0.938582
    was printed as -938582000.000000000 and 4294967296.001 as
    000000004.294967296.001000000.000000000
    
    === Fix ===
    
    Modified the for loop used for printing digits of the
    decimal value. Broke it into two for loops. Below is the
    sequence of steps followed:
    1) A for loop prints digits before the decimal point,
    2) A decimal point is printed between the two for loops.
    3) The second for loop prints digits after the decimal
       point.

[33mcommit 9cd4e81ca8e0a0567b451ab91cf260f399eac936[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Fri Aug 31 15:04:20 2012 +0530

    wl#6443 : Deprecate features in mysql-5.6-rc - SrvGen
    
    Description : Introduced deprecation warnings for
                  [1;31mtime[m_format, date_format, date[1;31mtime[m_format
                  and max_tmp_tables.
                  Inotroduced deprecation warnings for
                  --plugin-xyz options.
                  Removed host table creation and introduced
                  deprecation warning in mysql_upgrade for
                  a non-empty host table.

[33mcommit 49c003748228ef03c8680bcb574b6899d9a05cce[m
Author: Aditya A <aditya.a@oracle.com>
Date:   Mon Aug 27 10:57:55 2012 +0530

    BUG#11761646 - INNODB SHOULD RETRY ON FAILED READ OR WRITE,
                   NOT IMMEDIATELY PANIC
    Related:
    http://bugs.mysql.com/bug.php?id=54430
    (innodb should retry partial reads/writes where errno was 0)
    
    Summary
    -------
    In the Linux flavours the read and write system calls can
    do partial reads and writes.
    
    The man page of read says that " The return value is
    is smaller than the number of bytes requested; this
    may happen for example because fewer bytes are actually
    available right now (maybe because we were close to
    end-of-file,or  because  we  are  reading from  a pipe,
    or from a terminal), or because read() was interrupted by
    a signal."
    
    
    The Fix
    -------
    Initially InnoDB was not handling the partial read and
    writes.With this fix innodb tries NUM_RETRIES_ON_PARTIAL_IO
    (which by default is equal to 10)[1;31mtime[ms to read or write the
    partial data .The fix also takes care of partial read and
    write in case of Linux native API's where read and write
    are asynyncronus operations.
    
    rb:1158 [approved by inaam.rana]

[33mcommit bffaf6d8e5e6f794fdda2ab2d4b2917cbfab6da4[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Aug 24 14:00:05 2012 +0200

    Fix for bug#14143553:JOB BUFFER FULL - DATA NODE CRASH (Blizzard)
    and duplicate Bug#13799800 NDBMTD CRASHES DURING SONY-QUERY WITH 128 PARTITIONS ON 4 NODES WITH 4 LDM EACH
    
    The patch extends and redifines the signals GSN_DIH_SCAN_GET_NODES_REQ, _CONF and _REF.
    On order to avoid generating too many signals, which breaks the 1::4 fanout rule for
    signals consumed::produced these signals may now be 'long'.
    
    Both a short and long version of the modified signals are defined.
    The short signal is only used for a single fragment.
    This (the short) is mainly for SUMA and BACKUP which never request info for
    more than a single fragment at at a [1;31mtime[m. All modified blocks will
    handle both long and short version of the signal. If a long signal
    was received, the reply will also be 'long'.

[33mcommit 90897fc9cc1e54213b5b515001cba3896413162b[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed Aug 22 12:49:42 2012 +0200

    Bug#14190114: CLUSTER CRASH DUE TO NDBREQUIRE IN ./LOCALPROXY.HPP DBLQH (LINE: 234)
    
    This patch fixes a set of errors that causes node failure (or block new
    dictionary operations) if the master node crashes in certain states of a drop
    table operation. This covers bug 14190114 and some related errors that showed
    up when running the regression test:
    
    1)
    This patch fixes the direct cause of bug 14190114. The patch ensures that
    the master will reject SCHEMA_TRANS_BEGIN_REQ and SCHEMA_TRANS_END_REQ messages
    while there are outstanding DICT_TAKEOVER_REQs. If SCHEMA_TRANS_BEGIN_REQ was
    allowed, the system could end up in a situation where two transactions had
    outstanding DROP_TAB_REQs at the same [1;31mtime[m. This caused bug 14190114.
    Likewise, SCHEMA_TRANS_END_REQ cannot be processed before the new master knows
    the state of the transaction (i.e. after it has receibed the
    DICT_TAKEOVER_CONFs).
    
    2)
    This patch fixes an error in the construction of the CONTINUB message that
    DICT sends to itself if it receives a DICT_TAKEOVER_REQ while it still
    has active operations.
    
    This patch also substitutes sendSignal with sendSignalWithDelay. This is done
    for two reasons:
    I) To avoid waisting CPU cycles by doing busy wait.
    II) To prevent CONTINUB messages from filling the jam trace buffer (this made
    the error report fro the customer harder to analyze.)
    
    3)
    This patch disables counting of SCHEMA_TRANS_IMPL_CONF and
    SCHEMA_TRANS_IMPL_REF messages during takeover. Normally the master counts
    these to know when all participants have completed an operation. But during
    a takeover, the new master will not know the number of outstanding messages
    until it has received DICT_TAKEOVER_CONF.
    
    4)
    This patch ensures that drop table operations are set to start OS_COMPLETED
    after finishing RT_COMPLETE requests. As it was, these would remain in state
    OS_COMPLETING. This meant that DICT could never send DICT_TAKEOVER_CONF, since
    this can only be done when all operations are in 'passive' states.

[33mcommit 3ecd7517e951fc7d84cacc0726342d48a08a92ac[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Aug 22 11:09:04 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    Do not select compress_[1;31mtime[m and uncompress_[1;31mtime[m because their values are
    nondeterministic. On slow platforms (like Valgrind) it may end up 1 instead
    of 0, causing the whole test to fail.

[33mcommit dd5bfe8c95a060763d360ab3c5ee6872bd47a9b9[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Thu Aug 16 20:49:20 2012 +1000

    WL#6044 - Use consts to tighten up the mutex spin code. Fix non-debug compile
    [1;31mtime[m issues. Change some ibool variables to bool.

[33mcommit 3884fa4e5e5a3db129257e37be328c4e5fd6e917[m
Author: viswanatham gudipati <viswanatham.gudipati@oracle.com>
Date:   Mon Aug 13 13:07:51 2012 +0530

    wl6347: The following listed testcases are included.
    * 1.1 Multiple tables/indexes having same name
    * Interaction between wl6347 and wl6344
    * Interaction between wl6347 and wl6345
    * Fetch multiple [1;31mtime[ms same query check stat
    * Restart the server and fetch and check stat

[33mcommit 418d168b9dd4af95980ca4d91c6efcb77da64d10[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Mon Aug 13 12:33:41 2012 +0530

    BUG#11748012 - 34804: AFTER RE-START OF THE SCHEDULER DISABLED
                          EVENTS ARE NOT DROPPED FROM DISK
    
    BACKGROUND:
    STEP 1: If the event_scheduler is ON, and we create an event,
    event scheduler checks for events present in the event queue
    and starts executing. If the event has not been executed/invoked,
    it is still enabled.
    STEP 2: Then we switch OFF the event scheduler.
    STEP 3: When the event scheduler is switched ON after some [1;31mtime[m
    when [1;31mtime[m period of event expires, it will mark the event as
    DISABLED in Event queue and drops it from there, But it does
    not drop the event from mysql.event table. This problem is
    mentioned as part of this bug report.
    
    ANALYSIS:
    An event is marked DISABLED in the event queue when its [1;31mtime[m
    period gets expired. Then it gets dropped from the event queue,
    but there was no logic to drop the disabled event from
    mysql.event table. As part of the fix, logic to drop the event
    from mysql.event table has been added.
    
    FIX:
    This bug is fixed by Dropping all the events with status
    DISABLED from mysql.event table.

[33mcommit c7fb63d4597602478b98ccbe1a93c790b66c5e85[m
Author: Manish Kumar <manish.4.kumar@oracle.com>
Date:   Fri Aug 10 15:44:29 2012 +0530

    BUG#13886150 - RPL.RPL_INCIDENT FAILS ON PB2 SPORADICALLY DUE TO TIMEOUT
    
    Problem: rpl.rpl_incident fails on pb2 sporadically due to [1;31mtime[mout in
             include/wait_for_slave_param.inc. Apart from generic
             linux(linux2.6-x86-64bit, where we have knows [1;31mtime[mout issues),
             the failure was observed on gcov-linux-x86_64 and
             rhel5-x86-32bit. The test is passing in retry attempts in just
             257 milliseconds, it means it might be hanging some[1;31mtime[m in
             include/wait_for_slave_param.inc.
    
    Fix: Adding a force-restart option to the test will restart the server
         before starting to execute the test.

[33mcommit f432808e2697e912a6a8d61c22c92b8ddf67bf8e[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Thu Aug 9 20:39:05 2012 -0500

    Bug#14234997 - PERFSCHEMA.SOCKET_SUMMARY_BY_INSTANCE_FUNC_WIN FAILS WITH RESULT MISMATCH ON PB
    
    Fixed an incorrect variable name in the test script that prevents the wait [1;31mtime[mout from being set.

[33mcommit d248f7902cd2a6fb50b4f4f27a15c707f7bb8eba[m
Merge: c5edbcb1093 737a06113e7
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Aug 2 17:38:54 2012 +0100

    Bug#11753308: MYSQL_KILL() AND COM_PROCESS_KILL ONLY HANDLE 32-BIT IDS
    
    The connection ID / "thread-ID" is currently 32 bit on the wire, but may be 64 bit in the API,
    depending on the definition of ulong on a given platform. If we silently drop half the
    bits on the floor, mysql_thread_id() in Connector-C may give incorrect results, and mysql_kill()
    might ask for the wrong (and usually long expired) thread to be terminated, even if a 64-bitly
    correct ID were obtained with "SELECT connection_ID()".
    
    We now guard against some 32/64 issues.
    We do this in both the client and the server to address mixed scenarios of old server /
    new client and vice versa.
    
    Using SQL instead of the C API (SELECT CONNECTION_ID() / KILL CONNECTION ...) should
    produce better results. mysqladmin already does this.
    
    Docs implications:
    
    - Discourage use mysql_thread_id() / mysql_kill(). current status is that they produce incorrect
      results / try to kill the wrong thread once more than 2^32 threads are created (IOW, this only
      affects extremely busy servers or those with an extreme up[1;31mtime[m. The server also needs to use
      64-bit thread_IDs for this to apply. This is platform specific, not a mysqld configurable.)
      Patched status is that the calls will work normally as long as thread_IDs are in safe range
      (< 2^32), but will throw an error a) if mysql_kill() in Connector-C is called with a value
      >= 2^32, or b) if mysql_kill() is used at all, even with ostentably correct values, once
      thread_ID in the server has exceeded what can be expressed within 32 bits.
    
    - encourage use of the SQL equivalents, SELECT connection_ID() / KILL CONNECTION ...
    
    - mysqladmin already does this, and is therefore safe.
    
    - mysql_kill() will throw CR_INVALID_CONN_HANDLE if it is given a larger value than can be
      expressed in the 32-bit wire protocol.
    
    - mysqld will throw ER_DATA_OUT_OF_RANGE on mysql_kill() once thread_ID in mysqld exceeds
      32-bits.

[33mcommit 8fcae37740385d02230f8a3aca83e8800a3ba410[m
Merge: 0ecca78f207 2f2e0d9cc11
Author: Gopal Shankar <gopal.shankar@oracle.com>
Date:   Wed Jul 25 16:05:49 2012 +0530

    BUG#14364558 - ASSERT `TABLE_LIST->PRELOCKING_PLACEHOLDER == FALSE'
                   FAILED IN CHECK_LOCK_AND_START_STMT
    
    Problem:
    rqg_signal_resignal test hits the assert
    DBUG_ASSERT(table_list->prelocking_placeholder == false), in
    check_lock_and_start_stmt();
    
    Analysis:
    The above assert was added as part of fix for Bug#13036505,
    to capture the fact that prelocking_placeholder should always
    be false in check_lock_and_start_stmt() in theory. But in practice,
    this assertion is hit. Even without the fix Bug#13036505 this
    problem exists. So, this problem is not an effect to Bug#13036505,
    and hence can be treated as a new issue.
    
    For now we shall remove this assert which will keep the
    old behavior. And we keep this bug open, so as to
    dig into the issue more and provide proper fix.
    
    Current observation is that, the problem is not reproducible with
    --threads=[<=3] and could be due to concurrent operations.
    
    perl runall.pl --threads=10 --queries=1M --duration=300
    --grammar=conf/run[1;31mtime[m/signal_resignal.yy
    --mysqld=--max-sp-recursion-depth=10
    --reporters=Deadlock,ErrorLog,Backtrace,Shutdown
    --basedir=$BASEDIR --vardir=vardir  --mysqld=--log-output=file
    --mysqld=--loose-lock-wait-[1;31mtime[mout=1
    --mysqld=--loose-innodb-lock-wait-[1;31mtime[mout=1
    --testname=rqg_signal_resignal

[33mcommit 263dd25f6bf51bbd372dfa1f2815c76163e3eb3b[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Jul 20 11:30:19 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    * Reset INFORMATION_SCHEMA.innodb_cmp_per_index whenever we enable it
    during run[1;31mtime[m, so we start with a clean stats.
    
    * Release and reacquire dict mutex when filling
    INFORMATION_SCHEMA.innodb_cmp_per_index to allow other threads to proceed.
    
    Suggested by:   Inaam (http://bur03.no.oracle.com/rb/r/1116/)

[33mcommit 0911753c554c875565a88cf36343355d65c0a5b8[m
Author: Manish Kumar <manish.4.kumar@oracle.com>
Date:   Wed Jul 18 11:44:37 2012 +0530

    BUG#14237992 - RPL_SEMI_SYNC FAILURES ON TRUNK AS PART OF WL#5223
    
    Problem: The rpl.rpl_semi_sync test fails sporadically and more
             frequently after wl#5223. The problem is that the slave is
             unable to get in sync with the master on slow platforms in the
             default duration of the [1;31mtime[mout. So the wait_for_status_var fails.
    
    Fix: The problem is fixed by hardening the test case with new checks
         and also increase the [1;31mtime[mout duration during the call of the
         .inc file in the test, in order to let the slave catch up with the
         master.

[33mcommit 7d50a198486681ded647c278ca6cc819766e69ec[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Tue Jul 17 23:51:43 2012 +0530

    Bug#13819275 - Huge performance bottleneck because views
    definitions aren't cached
    
    Analysis:
    ----------
    Whenever view is opened, the view definition is read from
    the .frm file of view. And then this definition is used
    by the File_parser.
    Reading definition from .frm file every [1;31mtime[m is one of the
    main reason of performance bottleneck.
    
    Fix:
    ----------
    When view is opened for the first [1;31mtime[m, TABLE_SHARE is
    created for it (But no open count is mentioned for it).
    The TABLE_SHARE for view is available in the table share
    cache till there is enough space. Till its in the table
    share cache, same TABLE_SHARE of view is used.
    
    To avoid reading view definition from the file every [1;31mtime[m,
    added new member in TABLE_SHARE to cache File_parser object
    for the view's .FRM file. So that next access to the same
    TABLE_SHARE need not have to read the definition from .FRM,
    but instead can reuse definition stored in the File_parser
    object.
    
    To verify the improvement in the performance, I used the
    same stored procedure provided in this report. Executed
    p1(100000) with view "v1" (say A)  and p1(100000) with
    table "t1" (say B), with and without this fix.
    
    Delay in execution of A compared to B
                      without fix= ~3.99 secs
                      with fix= ~1.66 secs
    
    Improvement in performance = ~58.39%
    
    We also tried to cache results of a later stage of view
    processing, i.e. cache results of File_parser::parse() call,
    but straight forward implementation of this approach didn't
    bring any noticiable improvement.
    praveen@Praveen-MySQL:~/Documents/Bugs/Bug13819275$ cd inter
    bash: cd: inter: No such file or directory

[33mcommit b301b8a406e4d90f588b15fec056f6041f29b768[m
Author: prabakaran thirumalai <prabakaran.thirumalai@oracle.com>
Date:   Tue Jul 17 13:22:54 2012 +0530

    Bug#11765744    58738: DOUBLE LOCKING MUTEX WHEN QUERYING I_S
    TABLES IN SUBQUERY
    
    Analysis:
    fill_status() holds LOCK_status mutex for the entire duration
    of the function call show_status_array(). In case of sub query,
    show_status_array() during its condition evaluation again
    calls fill_status() for the subquery which leads to double
    locking on LOCK_status mutex.
    
    Fix:
    Changed LOCK_status mutex to read write lock such that in case
    of subqueries it can be acquired multiple [1;31mtime[ms for read operations.
    Also changed necessary helper functions and test scripts.

[33mcommit 78f10f9208b56129abf103300d4c66bd76ac9839[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Jul 12 15:58:36 2012 +0530

    BUG#14221840: KILLING HELP COMMAND: ASSERTION FAILED: ! IS_SET(),
                  FILE SQL_ERROR.CC, LINE 465
    
    Analysis:
    --------
    
    MySqld crashes at [1;31mtime[ms on a debug build, when the HELP command
    is killed using KILL QUERY statement.
    
    The crash is observed under a timing window where:
    
    connection1: Run the HELP statement.
    
    connection2: KILL the HELP query running on connection1. This sets
                 the connection1 THD 'killed' status as 'KILL_QUERY'.
    
    connection1: Since the THD is killed, a kill message is sent while
                 attempting to read the help records. This sets the
                 status of the diagnostic area to DA_ERROR.
    
    At the end of execution of the HELP statement, the status in the
    diagnostic area is set to DA_EOF only if no errors are reported
    (DA_EMPTY). On a debug build, MySqld asserts if the status of the
    diagnostic area is not DA_EMPTY. Since the status of diagnostic
    area is set to DA_ERROR in the case mentioned above, MySqld crashes.
    [On a release build, an appropriate error message ER_QUERY_INTERRUPTED:
    Query execution interrupted is reported].
    
    Fix:
    ---
    
    Check the THD 'killed' status prior to setting the EOF status in the
    diagnostic area. If the THD is killed, skip setting the EOF status
    and perform the query cleanup.

[33mcommit d5163c901282e3ede01c3c28b4f1fe4246620c93[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Mon Jul 9 22:38:46 2012 +0530

    BUG#11759333: SBR LOGGING WARNING MESSAGES FOR PRIMARY
    KEY UPDATES WITH A LIMIT OF 1
    
    Problem: The unsafety warning for statements such as
    update...limit1 where pk=1 are thrown when binlog-format
    = STATEMENT,despite of the fact that such statements are
    actually safe. this leads to filling up of the disk space
    with false warnings.
    
    Solution: This is not a complete fix for the problem, but
    prevents the disks from getting filled up. This should
    therefore be regarded as a workaround. In the future this
    should be overriden by server general suppress/filtering
    framework. It should also be noted that another worklog is
    supposed to defeat this case's artificial unsafety.
    
    We use a warning suppression mechanism to detect warning flood,
    enable the suppression, and disable this when the average
    warnings/second has reduced to acceptable limits.
    
      Activation: The supression for LIMIT unsafe statements are
      activated when the last 50 warnings were logged in less
      than 50 seconds.
    
      Supression: Once activated this supression will prevent the
      individual warnings to be logged in the error log, but print
      the warning for every 50 warnings with the note:
      "The last warning was repeated N [1;31mtime[ms in last S seconds"
      Noteworthy is the fact that this supression works only on the
      error logs and the warnings seen by the clients will remain as
      it is (i.e. one warning/ unsafe statement)
    
      Deactivation: The supression will be deactivated once the
      average # of warnings/sec have gone down to the acceptable limits.

[33mcommit 4e941105671f8eb32a9d3484557f023d6977adff[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed Jul 4 10:15:39 2012 +0200

    After the fix for Bug#14145442 'INSERT IGNORE .. SELECT : PROTOCOL::END_STATEMENT(): ASSERTION `0' FAILED.',
    errors in JOIN::optimize() are not ignorable anymore.
    The scenario of Bug#11747970 '34660: CRASH WHEN FEDERATED TABLE LOSES CONNECTION DURING INSERT ... SELECT'
    was that:
    - subquery's optimization fails, no error is sent (statement has IGNORE)
    - this failure makes subquery's first execution be skipped
    - this failure is forgotten
    - subquery is evaluated a second [1;31mtime[m, second execution is
    attempted, and crashes.
    That cannot happen anymore as a failed optimization is not ignored.
    So the fix is undone: it is a good thing because it makes one less
    member variable in subselect_single_select_engine.

[33mcommit ed45098a82c9f5df21b35de6e42f559be1a068cb[m
Author: Hemant Kumar <hemant.hk.kumar@oracle.com>
Date:   Wed Jul 4 10:26:11 2012 +0530

    Considering the [1;31mtime[m taken for this test on slow solaris platforms making it a big test.

[33mcommit 39b33cd4f98204e6370b60c3a00dfd4e662de4d2[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Jul 2 15:39:24 2012 +0530

    Bug #11760365: SLAVE_SQL_ERRNO IS SOMETIMES SET TO MY_ERRNO
    
    Follow up patch to fix failing "i_rpl.rpl_slave_init_errchk" test script to
    work with "rpl_crash_safe_relay-debug" mode.
    
    Problem:
    =======
    "i_rpl.rpl_slave_init_errchk" test script is failing on daily trunk
    under "rpl_crash_safe_relay-debug" combination as shown below
    
    perl mysql-test-run.pl --force --[1;31mtime[mr --debug-server --parallel=auto
    --experimental=collections/default.experimental
    --comment=rpl_crash_safe_relay-debug
    --vardir=var-rpl_crash_safe_relay-debug --suite=rpl
    --mysqld=--relay-log-info-repository=TABLE
    --skip-test-list=collections/disabled-daily.list
    
    Fix:
    ====
    The above script tries to simulate error during slave init operation. And the
    debug simulataion was added only in "sql/rpl_info_file.cc" as shown below.
    
    bool Rpl_info_file::do_update_is_transactional()
    {
         DBUG_EXECUTE_IF("simulate_update_is_transactional_error",
                         {
                           return TRUE;
                         });
         return FALSE;
    }
    
    But when the test is invoked with above mentioned command line arguments the
    script was entering the following code  "sql/rpl_info_table.cc" and the debug
    simulation was missing over there and hence the test script was not able to
    catch the reported error. Added the missing debug error simulation statements.
    
    bool Rpl_info_table::do_update_is_transactional()

[33mcommit 5afcc9148d0a572c1e469a283dfd920e1a93adf8[m
Merge: 6e3030aa2e4 da4ae7966f3
Author: Alexander Barkov <alexander.barkov@oracle.com>
Date:   Mon Jul 2 13:59:38 2012 +0400

    Bug#14167911 CRASH IN ITEM_FIELD::FIELD_TYPE DERIVED DATE TYPES
    
    Problem:
    Item_cache_date[1;31mtime[m can cache values in two formats:
    a. packed numeric format ("value_cached" is set in this case)
    b. string format     ("str_value_cached" is set in this case)
    
    "str_value_cached" was not checked in Item_cache_date[1;31mtime[m::val_decimal().
    Only "value_cached" was checked and if it was not set,
    cache_value_int() was erroneously called to get "int_value",
    instead of just using the cached string value in "str_value".
    
    cache_value_int() crashed because the Item_cache_date[1;31mtime[m::example
    pointed to an already freed and cleared memory
    (which was earlier occupied by Item_ident, during subquery execution).
    
    Fix:
    Run val_decimal_from_{[1;31mtime[m|date} if str_value_cached is set,
    to use the cached str_value.

[33mcommit d78119629c38a8a949c92e3a2aa0b045b19e4b21[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Thu Jun 28 16:56:38 2012 +0530

    Bug #11760365: SLAVE_SQL_ERRNO IS SOMETIMES SET TO MY_ERRNO
    
    Problem:
    ========
    When an error occurs in the slave's SQL thread, this is manifested in the code
    by calling rli->report(). This causes the Slave_SQL_Error and Slave_SQL_Errno
    columns of SHOW SLAVE STATUS to display the reason for the error.
    
    However, in some cases the Slave_SQL_Errno is set to something else than
    an ER_* number, and some[1;31mtime[ms the Slave_SQL_Error is set to a hard-coded error
    message instead of a translatable string from sql/share/errmsg.txt. "my_errno"
    is used instead of ER_* in several places. Please find the hunk ids listed
    below.
    
    File:sql/log_event.cc
    @@ -7965,9 +7961,9 @@
    -    rli->report(ERROR_LEVEL, my_errno,
    -                "Error in Create_file event: could not write to file '%s'",
    -                fname_buf);
    @@ -7976,14 +7972,18 @@
    -    rli->report(ERROR_LEVEL, my_errno,
    -                "Error in Create_file event: could not open file '%s'",
    -                fname_buf);
    @@ -8143,14 +8143,18 @@
    -      rli->report(ERROR_LEVEL, my_errno,
    -                  "Error in %s event: could not create file '%s'",
    -                  get_type_str(), fname);
    @@ -8159,22 +8163,25 @@
    -    rli->report(ERROR_LEVEL, my_errno,
    -                "Error in %s event: could not open file '%s'",
    -                get_type_str(), fname);
    -    rli->report(ERROR_LEVEL, my_errno,
    -                "Error in %s event: write to '%s' failed",
    -                get_type_str(), fname);
    
    Solution:
    =========
    OS specific "my_errno" has been replaced with thd->get_stmt_da()->sql_errno()
    which will report the global error number in ER_* domain. Hard coded error
    strings are replaced with `thd->get_stmt_da()->message()' call which will
    print translatable error string from sql/share/errmsg.txt file.  New error
    codes have been introduced in the place of error code '0'.
    Please find following new error codes and their hunk ids.
    @@ -4706,12 +4701,7 @@
    ER_INCONSISTENT_ERROR
    @@ -8417,8 +8424,8 @@
    ER_FILE_CORRUPT
    
    For hard coded error messages appropriate translatable strings have been
    added in sql/share/errmsg.txt file. Please find the following hunk id.
    @@ -4652,13 +4652,8 @@

[33mcommit da4ae7966f39551ebaf87b5b2367e9042754496e[m
Author: Alexander Barkov <alexander.barkov@oracle.com>
Date:   Thu Jun 28 13:32:51 2012 +0400

    Bug#14167911 CRASH IN ITEM_FIELD::FIELD_TYPE DERIVED DATE TYPES
    
    Problem:
    Item_cache_date[1;31mtime[m can cache values in two formats:
    a. packed numeric format ("value_cached" is set in this case)
    b. string format     ("str_value_cached" is set in this case)
    
    "str_value_cached" was not checked in Item_cache_date[1;31mtime[m::val_decimal().
    Only "value_cached" was checked and if it was not set,
    cache_value_int() was erroneously called to get "int_value",
    instead of just using the cached string value in "str_value".
    
    cache_value_int() crashed because the Item_cache_date[1;31mtime[m::example
    pointed to an already freed and cleared memory
    (which was earlier occupied by Item_ident, during subquery execution).
    
    Fix:
    Run val_decimal_from_{[1;31mtime[m|date} if str_value_cached is set,
    to use the cached str_value.

[33mcommit 1cfeaea2d8116cd2a45e7fc9501f3179b2bbfa66[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Jun 11 15:20:48 2012 +0200

    ndb - NdbSleep_MicroSleep
    
    Added sleep function with microseconds resolution.
    Uses waitable [1;31mtime[mr in windows and nanosleep in posix, as
    fallback it rounds up to milliseconds and uses
    NdbSleep_MilliSleep

[33mcommit d738b7a2f5adfbc04e35bb0302df2c5b48ef2b38[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Jun 8 09:53:18 2012 +0200

    WL#6224 Adapt MySQL Cluster to 5.6
     -Fixed problem with ndb_alter_table_online by adding a "DEFAULT 0" to the new
     [1;31mtime[mstamp column, thus giving it a consistent value also in future versions

[33mcommit e8e2366e92acb785be83d7920deecd91c956cb4a[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Mon May 21 23:27:28 2012 +0100

    Bug #14075825 LCP Watchdog : Fragment scan check
    
    The Local Checkpoint (LCP) mechanism allows Ndb to trim its redo and
    undo logs.  This keeps node and system recovery [1;31mtime[m within bounds,
    and maintains free redo and undo space to support logging DML
    operations.
    
    A watchdog mechanism is added to supervise fragment scans occurring
    as part of a local checkpoint.  This is intended to guard against any
    scan related OS level IO errors or bugs causing issues with LCP and
    endangering write service and recovery [1;31mtime[ms.
    
    Each node independently monitors the progress of local fragment
    scans occurring as part of an LCP.  If no progress is made for
    20 seconds, warning logs will be generated every 10 seconds.
    If no progress is made for one minute then the fragment scan is
    considered to be hung, and the node will be restarted to enable
    LCP to continue.
    
    Any occurrence of warning logs or other action taken by this
    watchdog should be reported as a bug.
    
    A new test scenario is added to testNodeRestart.

[33mcommit 36f82b0063e3444b4941c81c1811ab7c44854562[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Thu May 3 12:49:00 2012 +0300

    bug#13834481 a03_fix.diff
    TC(6.3)->LQH(7.x) hashHi caused [1;31mtime[mout

[33mcommit bf0f829e5f007f2e6c4098d22648fca0333c97c8[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Apr 20 20:08:31 2012 +0200

    WL#6224 Adapt MySQL Cluster to 5.6
     - update .result file with the new warnings from
       ALTER ONLINE|OFFLINE
     - update .result to new Date[1;31mtime[m2 and Time2 datatype output from ndb_desc

[33mcommit 6bb31d68ccdef3080d2d09c1c09e8c17c03753e1[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 18 11:32:17 2012 +0200

    Fix for bug#13901890 SQL NODE CRASHES DURING INSERT SELECT
    
    There was a bug in the life[1;31mtime[m handling of NdbQuery objects
    where the 'root' of the pushed join had 'type == const'
    
    As part of integrating the pushed join feature into the
    ha_ndbcluster handler implementation, the default implementation
    of handler::index_read_idx_map() was redefined. This method is
    used as the access function to read the (single) row when
    a table is const'ified by the optimizer.
    
    Basically handler::index_read_idx_map() does (ignoring error handling):
    
      index_init(index, 0);  // Open table for access
      index_read_map();      // Virtual method reading single row
      index_end();           // Return table to 'closed' state + release
    
    As the above 'index_end' would also destruct the resultset from
    the pushed join, including the child result, we could not do that
    for a pushed join.
    
    Therefore, ha_ndblcluster::index_read_idx_map() was implemented *without*
    the final index_end() call - Our investigation at that [1;31mtime[m indicated that
    this could be omitted as the open tables was cleaned up anyway, either by the
    next operation on the handler instance, or when the NdbTransaction was terminated.
    
    However, this bugs uncovers that there are codepaths where we both:
    
    - Terminate NdbTransaction, which destruct the NdbQuery object.
    - Then call ha_ndbcluster::reset() to clean up the handler, which will
      result in NdbQuery::close() to be called -> Crash!
    
    Working with this problem I realized that:
    
    - In order to increase pushability we have already introduced the
      handler::test_push_flag(HA_PUSH_BLOCK_CONST_TABLE) inside the optimizer
      which will block the const'ifying during optimize.
    - It will then only be confusing to still explain these query plan
      having a type == 'const' as they are now really executed as an eq_ref
    - Override ::index_read_idx_map() to suite pushed join execution is
      obsolete if we instead handle these queries as an eq_ref.
    
    These changes does *not* affect the pushability of the queries, nor
    changes number of handler call required to retrieve the data.
    
    However, several EXPLAINS will change where 'type == const' will
    now show 'type==eq_ref' instead which IMHO is more correct.
    
    Also fixes incorrect destruction of NdbQueryImpl objects if
    construction of these fails in NdbQueryImpl::buildQuery().
    In order to ensure propper cleanup of these we should use
    NdbQueryImpl::release() instead of deleting them directly.

[33mcommit 4f73d9a4bebb30f3e962dfa37e1e577997574dab[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Wed Apr 4 22:45:15 2012 -0700

    Improve clusterj NdbRecord handling for long values ([1;31mtime[m, date, [1;31mtime[mstamp) for big endian.

[33mcommit 33343956e35c2745258addb9c7e1e5219761d114[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Mar 29 10:51:49 2012 +0200

    ndb
     - record new .result file for ndb_join_pushdown*
      - this [1;31mtime[m a missing <cache> in the printout

[33mcommit 3dc227d5e2cf3d40d9406c16ef43b3aa4058d1d9[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Mar 22 15:18:19 2012 -0700

    Fix for bug#13868787
    Set Commit flag, so that we don't spend [1;31mtime[m waiting for synchronous close().
    Use committed read for basic PK read operations.

[33mcommit 9916594a018ad046ebe06c0c26e163b9773b3d2c[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Mar 21 15:42:47 2012 +0000

    bug#13714648 MYSQL CLUSTER ALTER TABLE REORGANIZE PARTITION UNDEFINED BEHAVIOR
    
    Background
    
    ALTER ONLINE TABLE REORGANIZE PARTITION (ATRP) is used to create new table partitions
    (fragments), usually after new empty nodes have been added to a cluster.
    
    ATRP determines the number of fragments to create automatically, and if
    it determines that no new fragments are required, then no new fragments
    will be created.
    
    Tables are some[1;31mtime[ms created with the MAX_ROWS option set.  This can
    be used to indicate to Ndb that extra fragments should be created to
    store the large number of rows.
    
    If MAX_ROWS was specified when a table was created, then ATRP will
    use it to decide the number of partitions required, which will be the
    same, so no new fragments will be created.  The table will not be
    rebalanced, and the new nodes remain empty.
    
    One option considered was to scale the configured MAX_ROWs with the
    relative increase in the number of nodes, but this results in different
    numbers of partitions depending on when a table is created.
    
    Solution
    
    The solution implemented here is to support
    ALTER ONLINE TABLE MAX_ROWS=<bigger_value> (ATMR).  Where the bigger
    MAX_ROWS value implies that more fragments are required, these will
    be allocated on the new data nodes, restoring table balance.
    
    For tables with MAX_ROWS explicitly defined, ATRP does not work.
    
    ndb_add_partition and ndb_addnode are modified to test the new
    mechanism.

[33mcommit 845a9da7a1877bf10bcfa787f3c26a3fe118d6ee[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Mar 21 16:31:06 2012 +0100

    ndb - bug#13825163 - decrease [1;31mtime[mouts on transporter handshake to prevent live-locking

[33mcommit df003f2c9a092db3eb44eb6e36e1ce38f03d6f12[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Mar 21 15:39:22 2012 +0100

    ndb - increase test[1;31mtime[m for Bug44065_org which increased when adding columns to T6

[33mcommit 9039c28ebb42ffa02b45160a910d0ab98b1abefc[m
Merge: bb0b0156c61 7c8722d4fab
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Mar 12 22:41:16 2012 -0700

    Compile-[1;31mtime[m fix for FreeBSD where inet_addr can some[1;31mtime[ms be expanded as a macro.

[33mcommit 7c8722d4fabb7d8bff617dbbf72f9fa4a266fdcd[m
Author: john.duncan@oracle.com <>
Date:   Mon Mar 12 21:41:46 2012 -0700

    Compile-[1;31mtime[m fix for FreeBSD where inet_addr can some[1;31mtime[ms be expanded as a macro.

[33mcommit 5d6a3cf1f2c262641851390c4f8e87bd8af9fd0f[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Feb 7 16:42:30 2012 +0100

    ndb - add unit tests for DynArr256
    
    Tests available in testDynArr256 are now also
    available in DynArr256-t.
    
    Replace micro() based on get[1;31mtime[mofday() with
    my_micro_[1;31mtime[m().

[33mcommit 8f038bf8d2a662555fde72fa83d0c3a8e2273947[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Feb 1 11:23:10 2012 +0100

    ndb - change ndb_statistics.inc to perform insert into myisam and the alter to ndb, to avoid [1;31mtime[mout issue on really slow machines in PB2

[33mcommit 5f9167ae6d17f9bea4cc4e3e48c53b6b5d6f855c[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Tue Jan 31 08:37:40 2012 +0100

    ndb - wl-5929 make threads allocate THR_SEND_BUFFER_ALLOC_SIZE pages at a [1;31mtime[m. To decrease contention on send-buffer-pool

[33mcommit ccaf87640e4c51668ad08773f89ccb2f0d2b36a8[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Jan 25 13:32:36 2012 +0100

    ndb - add possibility to have thread_local_pool seize more than 1 object at a [1;31mtime[m from thr_safe_pool (to reduce mutex contention)

[33mcommit 3806f0952265a19eb02071d609fa0e829737c5fb[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Fri Dec 23 17:51:59 2011 +0100

    ndb - more aggressive [1;31mtime[mout detection

[33mcommit 880423883f89fc512ea20fa812b72f1d23f48afc[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Thu Dec 22 10:44:50 2011 +0100

    ndb - disable also query_cache_size_basic_64 fails all the [1;31mtime[m on sol10-cmt

[33mcommit bb1359e7877f660b7f3eec35b2fa8175fd824e09[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Fri Dec 16 14:57:51 2011 +0100

    ndb - add mechanism for testcases to terminate (if they feel like it) when getting close to ATRT [1;31mtime[mout (needed not to have to multiply [1;31mtime[mouts by factors on certains operating systems)

[33mcommit d7cd8e5ec883b4ad76541fc30c28f3cae01823c2[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Fri Dec 16 10:13:46 2011 +0100

    ndb - this adds the following atrt features: 1) export TIMEOUT in environment so that cooperative programs on slow hosts (guess) can avoid [1;31mtime[mouts 2) add cmd-type: mysql, which runs cmd in mysql-client mode (normal is ndbapi) 3) add support for per-test case mysqld options

[33mcommit c6c8321de5247d1f8ef9d798c8c91f4cd202a193[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Thu Dec 15 11:01:04 2011 +0100

    ndb - set fragments-per-node to min(#lqh-workers) in alive cluster (when calling function first [1;31mtime[m)

[33mcommit f444ba0e9fe4a02d131b904f53829520fb426d6d[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Dec 10 23:31:26 2011 -0800

    Completed patch implementing expire [1;31mtime[ms (both for NDB data and for locally cached data) and flags.

[33mcommit 401ee0f9e797a5a2eb0fcb73f55fea2f96710ff0[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Dec 10 17:09:34 2011 -0800

    This patch includes most of the code to support expire [1;31mtime[ms in the database.
    Test cases are still to come.

[33mcommit 1acb2ae5e8e613e8aa36b5e1f65a398b43cd99c3[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Dec 9 20:12:56 2011 +0100

    ndb - remove patch for bug#45899, it's been disabled for a long [1;31mtime[m now

[33mcommit d386e38fdaa9ebab3f5e8bc38abf68a9c004e2b9[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Wed Dec 7 12:07:42 2011 +0200

    wl#4124 x39_fix.diff
    handler: do [1;31mtime[m calc as longlong for safety

[33mcommit d332ca045c408cdca0685725bf3c44d448dab8be[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Wed Nov 16 22:58:12 2011 +0100

    ndb - yet another redo-parts fix. Since page 0 (file 0) is rewritten every [1;31mtime[m log wraps...also set ZPOS_NO_LOG_PARTS in initLogpage...

[33mcommit b68f415ae350e77c4a0c37b2ce46fcf2ee662c0f[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Fri Nov 11 08:46:17 2011 +0100

    ndb - new template for intrusive hash table, intended to the one
    
    There was several variants of DLHashTable such as DLHashTable2, KeyTable,
    KeyTable2 and KeyTable2c.  In [1;31mtime[m this new template should replace them
    all.
    
    The objective to introduce it now is to simplify the use of two hash-tables
    on the same object, now needed for DictObject.
    
    The M in DLMHashTable can mean Meta or Methods, since the template takes
    a Meta/Methods-class as parameter to tell the hash table how to get
    next/prev link, calculate hash-value and determine equalness of objects.
    
    The default Meta/method-class assumes the same interface that is already
    used in classes using DLHashTable.
    
    DLMHashTable logic are copied from DLHashTable.  Direct references like
    obj->nextHash are replaced with M::nextHash(i*obj) and the like for
    prevHash, hashValue() and equal().

[33mcommit ac2121835f2128cf46e2625231df9ca28e7f7129[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Thu Nov 10 14:33:56 2011 +0100

    ndb - add optimizer_search_depth=10 to see if this helps daily RQG not to [1;31mtime[mout

[33mcommit 03141c46f42f85ffcd241f0d43364c473c1f98a2[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Nov 10 09:29:32 2011 +0100

    ndb schema dist
     - remove handling of SOT_RENAME_TABLE_NEW which hasnt been used in a long [1;31mtime[m.

[33mcommit 1f726ec5f230ddc575c901108d24ebc1d6368b6c[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Mon Nov 7 23:38:30 2011 +0100

    ndb schema dist
     - add m_is_post_epoch flag for checking that functions are called at the correct [1;31mtime[m

[33mcommit 224cd36bf485ed7fd52cd4d2d42a1dc7f6fa1d90[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Oct 26 13:37:29 2011 +0200

    ndb binlog
      - remove the binlog [1;31mtime[mr code, it does not compile and also clutters the
      binlog main loop

[33mcommit 71e24844e0362ea39cc1d7f674d30967f94efaca[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Oct 18 15:54:36 2011 -0700

    Reimplement query optimization to improve performance:
    Analyze where clause when it is set on the query.
    Rank possible indexes by the number of terms.
    Highest rank for unique indexes (including PRIMARY)
    At query execution [1;31mtime[m, decide if a unique index is usable (no null parameters)
    If not, choose the ordered index based on whether the first comparison is usable.

[33mcommit 69c7417ba66db85252852e033b3ff7426959a6d5[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Oct 18 12:43:35 2011 +0200

    Resurected code which implemented the new MRR interface from branch mysql-6.0-telco-7.0.
    
    Several fixes was required in order to make that code work as a looong [1;31mtime[m
    has passed since it was originally developed:
    
     - Addapted fix for Bug#57481:'multi range read' may fail to close completed NdbScanOperations'
     - Start transactions on demand when MRR operations are created.
     - Corrected bug in calculating size of MRR buffer to allocate.
     - Addapted code to correctly count pruned/sorted & plain scans in MRR
     - Addapted code to handle user specified partition.
    
    SPJ has not yet been integrated into the new MRR code - instead we will (temporary)
    use the default MRR impl when a join is pushed.
    
    Old MRR code is still present as a reference. (Plan to remove that in a later push)

[33mcommit 7521ef07d789c7327de50fe2e74d745d59ae90e3[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Mon Oct 17 20:51:52 2011 +0200

    ndb - call RUN_RQG 6 [1;31mtime[ms with [1;31mtime[mout 600 instead of once with [1;31mtime[mout 3600...
          as CluB get unhappy when a testcase is silent for 1800s

[33mcommit b7f4444bcda0224012e11255b795af5777f6ec8b[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Mon Oct 17 09:58:27 2011 +0200

    ndb - increase MTR [1;31mtime[mout of rqg_spj...to see it it helps

[33mcommit 1d988d08e543402aa3a54757c287fc8119706c04[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Oct 10 10:30:55 2011 +0200

    Turn off --check-testcase when valgrinding to save [1;31mtime[m

[33mcommit 7f1cf22ca776a000eb94762f8edbb59fc74974cf[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Oct 10 09:40:12 2011 +0200

    ndb_big
     - disable smoke since it's not ready for automated testing yet
     - increase default testcase [1;31mtime[mout for bug37983 to 30 seconds

[33mcommit bc7aa324af9313e032eeac6bb6b9ee7683328093[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Sat Oct 8 19:54:19 2011 +0300

    wl#4124 h03_update.diff
    put run[1;31mtime[m status in status variable

[33mcommit a5976a67a25f452aeb10369848b2c941fb7c95ac[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Sep 29 12:13:43 2011 -0700

    Column->getSizeInBytes() seems to work for all cases of getColumnRecordSize().
    It also does better for determining alignment; for instance NdbApi seems to treat
    a [1;31mtime[mstamp column as an array of 4 chars, so getSize() was 1.

[33mcommit 11c622c89fd45612a5591b71f9c80562728fb9df[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Sep 28 14:43:28 2011 -0700

    Part 2 of alignment change: float, double, and date[1;31mtime[m are now aligned within the Record buffer.
    Call NdbDictionary::Column::getSize() on any non-string column, and if size is 2,4,8, or 16, add padding to align it to its size.

[33mcommit 2f8d34c7512f66ed2726d9bdfc808a365452a122[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Sep 28 11:06:37 2011 +0200

    ndb - this patch changes result file for innodb-index.result. The reason for the change is assumed to be online alter, that forces innodb to reopen the table one more [1;31mtime[m. The results that we now record can also be achived by restarting the mysqld before doing the explain...so somwthing is broken here anyway

[33mcommit 5cc4aa3302cd418153dbc6f87086b295977bed3d[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Sat Sep 24 12:00:49 2011 +0200

    ndb - fix incorrect [1;31mtime[m_passed calc, can lead to ending scan-batch too early

[33mcommit 278fc07b45d343383ebc6a48e2b9f8cdc935003b[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Sat Sep 24 11:58:29 2011 +0200

    ndb - fix incorrect [1;31mtime[m_passed calc, can lead to ending scan-batch too early

[33mcommit ddf558cc42c8042fbc9521503c322df66af30496[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Sep 21 17:48:32 2011 -0700

    Trying to use __builtin_ffs() with Sun Pro compiler fails at link-[1;31mtime[m (not compile-[1;31mtime[m)

[33mcommit da1aa77c2ad60cee42e39fa6169638fc07cbf1bf[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Sep 20 11:33:33 2011 -0700

    libevent links with librt on some platforms (for clock_get[1;31mtime[m)

[33mcommit f1d6812c8860d27406fb3735de0976ffd1b2f96d[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Tue Sep 13 11:47:16 2011 +0200

    ndb - bug#12974714 - spurious [1;31mtime[mout on slave, when using unique indexes (introduced by bug#47952)

[33mcommit 8722ad7689582194751e5cc9f69881dd2afd9744[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Fri Sep 9 14:41:37 2011 +0200

    Added new abort phase for online alter to support the rollback of add index during failure in mysql-5.5 (for example at DDL lock [1;31mtime[mout) in mysql-5.5.
     Enabled innodb_mysql_sync and kill tests and added new test case in ndb_alter_table_online2.

[33mcommit b4975b249ace3813d4eb2ce6b55103ef5333e1fe[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Fri Sep 9 09:23:13 2011 +0200

    This is a fix for deviations in statistics counter values that occur when running ndb_join_pushdown.test on
    big-endian (Sparc) and with Valgrind.
    
    This fix does two things:
    
    * Some ndb$info counters are removed from the test, because their values depend on endian-dependent key hashing.
    This includes READS_NOT_FOUND, LOCAL_READS_SENT,  REMOTE_READS_SENT and SCAN_ROWS_RETURNED.
    
    * Ndb_scan_count was some[1;31mtime[ms off by one when testing with Valgrind, possibly because table statistics was refreshed
    at unpredictable intervals. Instead of testing the exact value of Ndb_scan_count, the test will fail if
    Ndb_scan_count deviates from the expected values by more than 10%.

[33mcommit ff15ecc910157b7d5502d396db5976ce84870ee7[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 8 11:04:25 2011 +0200

    BUG#12543299 ndb_mgmd
     - increase number of [1;31mtime[ms that ndbd will retry allocation of
       nodeid, making it wait more than the default alloc nodeid [1;31mtime[mout
       of 20 seconds.

[33mcommit c96f7c50befebaacd12dbeeb4d27b28e3cab1433[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 8 09:33:23 2011 +0200

    ndb_mgmd
     - move retry_[1;31mtime[mout variable into scope where it's used

[33mcommit 0dbd6fc246b7642bc636c03b74e0566d24d68282[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Sep 7 19:20:19 2011 +0200

    Configurable [1;31mtime[mr in the NDB API send thread (from 1 ms to 10 ms)

[33mcommit ef8918f3a3c4856c2762ff563922a13eb881f976[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Sep 6 11:03:41 2011 +0200

    Bug#12543299 RESETTING MACHINE PREVENTS NODES FROM GETTING ID AFTER RESTART
     - Rewrite the MgmtSrvr::alloc_node_id function and descendants which
      changes so that:
     -- API nodes always allocate nodeid in the data nodes
     -- NDB nodes prefer to allocate in data nodes, if data nodes aren't available
      fall back to reserve the nodeid locally only in one ndb_mgmd. Clear reservation
      as soon as the data node has connected with transporter.
    
     - The general idea is that since the local reservation is cleared and instead
       held by a connected transporter which has heartbeating, the nodeid
      will be available as soon as the transporter connection has failed. This will
      cover also the case reported in bug where remote node was zapped by reseting
      the machine. NOTE! There is a _very_ small [1;31mtime[m window inbetween the nodeid
      is reserved locally in the ndb_mgmd and the transporter connects where the
      problem may still exist. This is handled by keeping the 20s [1;31mtime[mout of
      the local nodeid reservation, the [1;31mtime[mout check is however done before
      trying to allocate the nodeid.
    
     -Increase the number of [1;31mtime[ms the NdbApi retries  to allocate nodeid since
      the data nodes now need to be started before API nodes can allocate a nodeid.
For keyword perf:
[33mcommit fe9d5a26c801d0d9660a42028b397aa5fd56d503[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Sep 23 16:06:28 2015 +0300

    Bug #21865901   MTS COORDINATOR AND WORKER THREAD SYNCHRONIZATION LIMITS PARALLELISM
    
    In LC (Logical Clock) MTS mode, Coordinator thread distributes transactions to
    workers respecting at most one transaction to a Worker at a time constrain.
    The rule was introduced yet by wl6314 and not been revised since that time.
    A part of the rule implementation involved my_sleep() on the Coordinator side
    with the idea the Coordinator to have a nap while every Worker is busy.
    
    This has turned out to be costly for overall slave applier [1;31mperf[mormance and
    my_sleep() is replaced with sched_yield(), except Windows where the latter
    is not defined.
    
    Notice with this method a potential excessive busy-waiting by
    Coordinator, e.g when all Workers are busy to process big
    transactions, happens only when the yielded thread's CPU is not requested by
    any other user thread.
    Otherwise there should be no negative cost for this operation except
    a Worker can become available sooner than Coordinator gets rescheduled.
    by OS.
    
    There're still few alternatives to the current solution, declined
    due to either higher risks or rather significant complexity.

[33mcommit f07512827f68b41cc2aae15e0e05ebbcd45f02bf[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Thu Sep 10 19:10:42 2015 +0200

    Bug#20675180 INSTRUMENTATION FOR BINLOG CACHE FILE
    
    Updated [1;31mperf[mschema.relaylog.test

[33mcommit 86c19fe60bb54eb048f4b17f84d35072237520d7[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Wed Sep 9 16:42:21 2015 +0200

    Bug#17846246 ASSERTION FAILED: ENGINE_TYPE() != HASH_SJ_ENGINE
    
    The problem is related to the changing value of found_rows during the
    query.  It is evaluated more than once and the value in one case gives
    a NULL value for dayofmonth whereas another value gives a non-NULL
    value, which breaks things. In this case the subquery is materialized.
    
    The reason it changes is that initially, the value of found_rows is
    the the number of rows found during the previous query. Then, while
    evaluating the query, the sub-select modifies that value to the number
    of rows returned by the subquery. If those two values are sufficiently
    different (e.g. 101 in the first case and 1 in the second), the
    difference in the resulting null-ness of dayofmonth ensues.
    
    subselect_indexsubquery_engine::copy_ref_key has the
    following information:
    
    if (s_key->null_key) {
      /*
        If we have materialized the subquery:
        - this NULL ref item cannot be local to the subquery (any such
        conditions was handled during materialization)
        - neither can it be outer, because this case is
        separately managed in subselect_hash_sj_engine::exec().
      */
    
    The latter case is of interest here: in this case the search key was
    indeed *not* NULL before the materialization, but now it is NULL in
    the repro, because the number of rows returned from the subquery
    materialization is 1, not 101 as we had initially, and the former
    gives a NULL when fed into dayofmonth, whereas 101 gives 1.
    
    So, when we do the check above we enter and hit the assert. Because
    the code presumes a null value for the search argument would have been
    handled earlier, this case cannot be a case of HASH_SJ_ENGINE
    (asserted).
    
    The patch snapshots (into THD::previous_found_rows) the value of
    THD::limit_found_rows (renamed to current_found_rows) at the end of
    each query and consults that if/when FOUND_ROWS is [1;31mperf[mormed
    throughout the next query.
    
    Also removed some inactive settings of limit_found_rows in
    opt_explain.cc
    
    The patch adds the repro test case to the regression tests.

[33mcommit 40b7b1efc6b00456a7a5363c64228a8bc390b427[m
Author: Maria Couceiro <maria.couceiro@oracle.com>
Date:   Wed Sep 9 17:41:02 2015 +0100

    Bug#21354712 SHOW VARIABLES RETURN WARNING ABOUT @@SESSION.GTID_EXECUTED BEING DEPRECATED
    
    post-push fix updating the result file for test [1;31mperf[mschema.show_sanity

[33mcommit 48e95062eb793853d217391f1f80885b49bfa6bd[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Wed Sep 9 16:42:28 2015 +0200

    BUG#21776505: GR DOES NOT HANDLE BIG GTID_EXECUTED CORRECTLY
    
    There were several issues on how big GTID_EXECUTED sets were handled
    on Group Replication pipeline, in particular when GTID_EXECUTED size
    exceeded the 65535 characters length. The issues were:
      1) snapshot_version length computation was incorrectly limited to
         65535 characters length on Transaction_context_log_event;
      2) snapshot_version contained on certification info, which is
         serialized on View_change_log_event, was incorrectly limited to
         65535 characters length;
      3) internal interface between Group Replication plugin and server
         was incorrectly limiting GTID_EXECUTED to 65535 characters
         length.
      4) Column Transactions_committed_all_members column on
         [1;31mperf[mormance_schema.replication_group_member_stats was limited
         to 65535 characters length.
    
    This patch fixes the 4 above issues. Since issue 2) requires the
    change of the serialization of View_change_log_event, this patch
    makes Group Replication version 0.6.0 incompatible with the previous
    ones. Incompatibility rules were added and also a test (with
    simulated versions) to validate them.

[33mcommit 8c36b7b648679ac7c5dac8fa685230c5cccd3132[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Wed Sep 9 16:26:10 2015 +0200

    BUG#21809172: UPDATE RPL SUITE TESTS TO WORK ON GROUP REPLICATION
    
    Some new tests need to be updated in order to work on Group
    Replication, those tests are:
     * rpl_row_rollback_to_savepoint
       Not supported due to use of both
       --transaction-write-set-extraction!=OFF and ROLLBACK TO
       SAVEPOINT.
     * rpl_group_replication_user
       Updated result file.
     * rpl_group_replication_wait_for_executed_gtid_set
       Removed, this test was a wrapper to
       extra/rpl_tests/rpl_wait_for_executed_gtid_set.inc which was
       removed.
     * rpl_gtid_delete_memory_table_after_start_server
       Not supported since it tests asynchronous replication specific
       behavior.
     * rpl_[1;31mperf[mschema_threads_processlist_status
       Updated to support parallel applier.
     * rpl_stm_ignore
       Not supported since it uses non-transactional engine.

[33mcommit 71e7d828e4e5f20fc2341b846ec96c37ce4f9446[m
Author: Maria Couceiro <maria.couceiro@oracle.com>
Date:   Mon Sep 7 18:42:15 2015 +0100

    Bug#21354712 SHOW VARIABLES RETURN WARNING ABOUT @@SESSION.GTID_EXECUTED BEING DEPRECATED
    
    Problem:
    The warning 1681 "'@@SESSION.GTID_EXECUTED' is deprecated
    and will be removed in a future release." was printed even
    when the session variable gtid_executed was not included in
    the result of the query.
    In addition, the query "SELECT @@SESSION.GTID_EXECUTED"
    result included a duplicate warning.
    Both issues occurred because the warning was printed when
    the value of the variable was accessed, which will happen
    whether the variable is included in the result of the query
    or not.
    
    Fix:
    To be consistent with the current behaviour of the deprecated
    variable @@GLOBAL.SQL_LOG_BIN, the following changes were
    implemented concerning @@SESSION.GTID_EXECUTED:
    - the variable is no longer included in the P_S table, therefore
    it is not included in the result of:
      a) SELECT * FROM [1;31mperf[mormance_schema.session_variables
      b) SHOW VARIABLES (when SHOW_COMPATIBILITY_56= OFF)
      c) SHOW SESSION VARIABLES (when SHOW_COMPATIBILITY_56= OFF)
    - The variable is still a part of the I_S table, but the warning
    is not issued in the following statements:
      a) SELECT * FROM information_schema.session_variables
    (when SHOW_COMPATIBILITY_56= ON)
      b) SHOW VARIABLES (when SHOW_COMPATIBILITY_56= ON)
      c) SHOW SESSION VARIABLES (when SHOW_COMPATIBILITY_56= ON)
    - the above is true even in statements like
    SHOW VARIABLES LIKE 'REGEX', even when 'REGEX' matches
    'gtid_executed'.
    The warning is still issued when the value of the variable is
    directly accessed.

[33mcommit c8d84db54c0de66bcaf25b72d066ceb1b408007f[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Tue Sep 8 21:55:12 2015 +0200

    Bug#20519832 TRUNCATED SQL_TEXT VALUES ARE NOT SUFFIXED WITH ...
    
    Set no-protocol in [1;31mperf[mschema.misc for more consistent results.

[33mcommit ff915d72b54187d205286d8a710ad543470534bc[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Tue Sep 8 21:44:49 2015 +0200

    Bug#20519832 TRUNCATED SQL_TEXT VALUES ARE NOT SUFFIXED WITH ...
    
    Set no-protocol in [1;31mperf[mschema.misc for more consistent results.

[33mcommit 3a7a715ef9f47ab5e1e32b8c4f270f8214b3b97d[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Tue Sep 8 21:50:29 2015 +0530

    Bug#20001173: RPL P_S: MOVE SQL THREAD STATUS TO WORKER TABLE
    
    Analysis:
    ========
    As part of [1;31mperf[mormance schema enhancements following tables
    were added to provide enhanced replication monitoring
    information.
    
    Table: replication_applier_status_by_coordinator
      --The status of coordinator thread was reported as part of
        this table in multi threaded slave mode.
      --The status of SQL thread was reported as part of this
        table in single threaded slave mode.
    Table: replication_applier_status_by_worker
      --The status of worker threads is reported in this table.
    
    In case of single threaded slave mode report the status of
    SQL thread as part of 'replication_applier_status_by_worker'
    table.
    
    Fix:
    ===
    Added code to display SQL thread's status information as
    part of 'replication_applier_status_by_worker' table in
    single threaded slave(STS) mode. Hence
    'replication_applier_status_by_coordinator' table will
    be empty now in STS mode. MTS mode will be reported as it
    was earlier.

[33mcommit 06df126a112ab13a357656413ce0a1beee0936c0[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Tue Sep 8 10:47:20 2015 +0530

    Bug#21485997: WRONG INSTRUMENTATION OF CLASS GTID_SET
    
    Fixing a build issue found on weekend runs with
    'no[1;31mperf[mschema' option.

[33mcommit c4ce65ca22ee207be543bd07dcbb8cff30853199[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 7 14:44:59 2015 +0200

    BUG#20483278 LAST_QUERY_COST SHOWS DIFFERENT VALUES FOR SHOW_COMPATIBLITY
    FOR ON VS OFF
    BUG#21788549 LAST_QUERY_PARTIAL_PLANS VALUES NOT SAME WITH
    SHOW_COMPATIBILITY_56
    
    Before this fix,
    
    With SHOW_COMPATIBILITY_56=ON,
    
    The statement
      SHOW STATUS like 'Last_query_cost';
    was executed with dedicated code in the information schema
    (sql/sql_show.cc), which is -- not -- implemented as a storage engine.
    As a result, this statement did not execute code in the optimizer,
    which did not overwrite the session status variable 'Last_query_cost'
    for the current select statement.
    
    As a result, the value of 'Last_query_cost' reported was
    for the previous SELECT statement executed in the session.
    
    With SHOW_COMPATIBILITY_56=OFF,
      SHOW STATUS like 'Last_query_cost';
    is executed as a SELECT ... FROM [1;31mperf[mormance_schema.session_status.
    This table is implemented with a storage engine,
    and the optimizer code is used.
    By the time the value is returned, the value of 'Last_query_cost'
    reported is the value just overwritten by the optimizer for the current
    statement, which is not the desired result.
    
    The fix is to separate clearly in the session attributes:
    - the value of 'Last_query_cost' for the current query,
    - the value of 'Last_query_cost' for the previous query,
    and only save the former in the later when query execution is complete.
    
    The same issue exists for Last_query_partial_plan,
    which is fixed the same way.

[33mcommit d658107db1f995a4bd22f3448a0170a25172bda5[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Mon Sep 7 13:13:38 2015 +0400

    Bug#21286261: QUERY DIGEST DOES NOT REFLECT NEW OPTIMIZER HINTS
    
    When processing statements with hint comments (/*+...*/), the
    digest calculation was skipped for hint expressions, so the same
    statement had identical digests with and without hints.
    Also, the normalized query text missed hint expressions
    in the [1;31mperf[mormance_schema.events_statements_history table.
    
    The fix updates several parts of the server:
    
    1. The gen_lex_token.cc token information file generator:
    some tricks to add hint parser internals(*) to the global token
    list (lex_token_array) have been introduced.
    
    *) Hint parser internals:
     a. Hint keywords: their integer token values may interfere with
        token values of the main SQL parser, so the TOK_HINT_ADJUST()
        adjustment macro has been introduced to resolve ambiguities.
     b. Tokens for "/*+" and "*/" hint comment delimiters to represent in
        the normalized query text.
     c. The TOK_IDENT_AT token to distinguish unqualified identifiers
        from table names in the table@query_block_name expression.
        Such a special token is necessary to generate normalized
        query texts without a space character between the
        table name and the "@" sign:
    
          "`table` @`qb_name`" is wrong,
          "`table`@`qb_name`"  is correct.
    
    2. The sql_digest.cc digest calculator and query text normalizer:
    the support for the TOK_IDENT_AT token has been added to not output
    a space between a table name and a "@query_block_name" suffix.
    
    3. The main lexer (sql_lex.{h,cc}):
    the Lex_input_stream::skip_digest flag has been introduced to
    append tokens to a digest buffer in correct order: add a
    hintable SQL keyword (SELECT, INSERT, UPDATE...) first, then add
    a hint expression (/*+ ... */).
    
    4. The hint parser (sql_lex_hints.{h,cc}):
    the Hint_scanner::add_hint_token_digest() function has been
    added to translate hint parser tokens into main parser-compatible
    ones and to add them into a digest buffer.
    
    5. Minor code cleanups:
    lex.h and sql_hints.yy: unused token declarations have been removed.

[33mcommit 9676f0df9f70ccd999f293f9a51b1d92ca32867d[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Fri Sep 4 13:04:12 2015 +0200

    Bug#21696206: ASSERTION `TRANSL->ITEM->FIXED' FAILED IN SELECT_LEX::DELETE_UNUSED_MERGED_COLUMN
    
    It turns out that the standalone PREPARE doesn't call
    apply_local_transforms from mysql_prepare_insert, in contrast to
    mysql_prepare_delete and mysql_prepare_update.
    
    This has the effect that this resolve phase operation isn't [1;31mperf[mormed when
    a standalone prepared insert is [1;31mperf[mormed. Now, at PS execution time, the code
    in Sql_cmd_insert::mysql_insert does call apply_local_transforms - for the
    first time.  The fact that it is the first call implies this method's call to
    fix_prepare_information hasn't been called before.
    
    This again means that the value of st_select_lex::first_execution == true when
    we first enter apply_local_transforms (during EXECUTE).
    
    Now, apply_local_transforms tests on this flag in this stanza:
    
      if (derived_table_count &&
          first_execution &&
          !(thd->lex->context_analysis_only & CONTEXT_ANALYSIS_ONLY_VIEW))
        delete_unused_merged_columns(&top_join_list);
    
    Since first_execution is true, the call to delete_unused_merged_columns is
    done.  This is not supposed to have happened, the unused item is no longer
    fixed, and the ASSERT happens. Lifting the ASSERT doen't help; we crash later
    in sql_authorization.cc:2274 instead.
    
    Adding the call to apply_local_transforms at the end of
    mysql_prepare_insert makes the statement work, since we now do the work of
    apply_local_transforms also at prepare time, as for DELETE and UPDATE.
    
    We only do this if mysql_prepare_insert is not called for a INSERT..SELECT
    since for those statements, apply_local_transforms *are* called during
    prepare already (from st_select_lex_unit::prepare), and calling it can cause
    things to break, e.g. one of the tests for WL-5275 in insert.test,
    cf. discussion on the bug entry.
    
    This patch also fixes Bug#21696641, so test cases have been added for
    both 21696206 and 21696641.

[33mcommit 5249f4f0790de54eb872d4d4af5874b72add41e9[m
Author: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
Date:   Fri Sep 4 17:12:02 2015 +0530

            Bug#21775221 ADD NUMACTL-DEVEL AS BUILDREQ TO RPM SPEC FILES
    
            Description:
    
            Set Numa Mempolicy for optimum mysqld [1;31mperf[mormance added
            libnuma.so as build prerequisite on Linux.
    
            To build with support for this feature numactl devel files are required.
    
            Fix:
    
             Added numactl-devel as build pre-requisite for OL/RHEL/Fedora/docker
             and libnuma-devel for SLES in the spec file.

[33mcommit d55f41304b948ce5e68491217c802ea013364658[m
Author: Menelaos Karavelas <menelaos.karavelas@oracle.com>
Date:   Fri Sep 4 13:55:30 2015 +0300

    Bug#21689998 ST_UNION() RETURNS AN INVALID GEOMETRYCOLLECTION
    
    The bug is due to the way Boost.Geometry handles linear geometries that have
    spikes when [1;31mperf[morming the difference and intersection set operations with
    areal geometries. By default Boost.Geometry discards spikes present in linear
    geometries, which can easily result to set operation results that are invalid
    (linestring with topological dimension 0 instead of 1).
    
    Fix: deactivate the removal of spikes present in linear geometries when [1;31mperf[morming
    the difference(L,A) and intersection(L,A) set operations.
    
    Note: The Boost.Geometry related patch that is part of this commit has been submitted for
    inclusion to Boost.Geometry; it has not been reviewed yet, so it may change as part of the
    Boost.Geometry reviewing process.

[33mcommit 227ba53c167ee2bfc4493ba18c7f0b9badaa56cc[m
Author: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
Date:   Thu Sep 3 17:52:13 2015 +0530

        Bug#21775221 ADD NUMACTL-DEVEL AS BUILDREQ TO RPM SPEC FILES
    
        Description:
    
        Set Numa Mempolicy for optimum mysqld [1;31mperf[mormance added
        libnuma.so as build prerequisite on Linux.
    
        To build with support for this feature numactl devel files are required.
    
        Fix:
    
         Added numactl-devel as build pre-requisite for OL/RHEL/Fedora/docker
         and libnuma-devel for SLES in the spec file.

[33mcommit 1fbfab58385e4675137a4d0b45aeda8c8f754fa7[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Thu Sep 3 10:42:59 2015 +0530

    Bug#21485997: WRONG INSTRUMENTATION OF CLASS GTID_SET
    
    Analysis:
    ========
    At runtime, some GTID_SET objects are instrumented with a
    [1;31mperf[mormance schema mutex key of 0, which is incorrect (not
    instrumented).
    
    The root cause is the declaration of constructors, like:
    
    Gtid_set(Sid_map *sid_map, Checkable_rwlock *sid_lock= NULL
    #ifdef HAVE_PSI_INTERFACE
            ,PSI_mutex_key free_intervals_mutex_key= 0
    #endif
            );
    
    The issue is giving a default value of 0 when the mutex key
    is not passed. This allows some caller to not pass any key,
    causing a bug.
    
    
    Fix:
    ===
    Moved the PSI_mutex_key from Gtid_state class to Gtid_set
    class.

[33mcommit 1bb65653be713ef73e93c5f64312cf85c5801638[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Wed Sep 2 19:47:25 2015 +0200

    Bug#20519832 TRUNCATED SQL_TEXT VALUES ARE NOT SUFFIXED WITH ...
    
    Re-recorded [1;31mperf[mschema.start_server_low_digest_sql_length to reflect truncated SQL queries.

[33mcommit 9462ce3d3c171ac0ead6de70eaa4ef24ce7337ec[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Wed Sep 2 16:08:48 2015 +0200

    BUG#21755890: DO NOT ALLOCATE MEMORY ON GR PLUGIN P_S INTERFACE IMPLEMENTATION
    
    On the previous interface between Group Replication plugin and
    Performance Tables there was memory allocation which was being
    passed to server side. This memory allocation should be avoided,
    also it can cause problems when it is released from a different
    entity that allocated it.
    
    To avoid that issues, now the interface uses callbacks that are
    invoked by plugin to send information to Performance Schema tables,
    or other callers. The callback implementations are the ones that
    are responsible to allocate new memory, if needed.
    
    We also changed the definition of
    [1;31mperf[mormance_schema.replication_group_members table member_state
    column to be a arbitrary string, being now the plugin the full
    responsible of its values. The values continue to be: ONLINE,
    OFFLINE and RECOVERING.

[33mcommit 203f87c4c726a0a667ccf369bdde83422d0d27a4[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Sep 1 15:06:17 2015 +0200

    BUG#21669500 CONFUSING ERROR FOR SHOW COMMAND WITH SHOW_COMPATIBILITY OFF
      AND PFS COMPILED OUT
    
    Before this fix, when building a server without the [1;31mperf[mormance schema,
    commands like SHOW STATUS would fail with errors like:
    
    mysql> show status;
    ERROR 1146 (42S02): Table '[1;31mperf[mormance_schema.session_status' doesn't exist
    
    The reason for the error is that SHOW STATUS is rewritten internally
    as a SELECT ... FROM [1;31mperf[mormance_schema.session_status,
    which fails when the [1;31mperf[mormance schema storage engine is missing.
    
    In MySQL 5.7, compiling without the [1;31mperf[mormance schema is no longer
    supported, because tables like:
    - table [1;31mperf[mormance_schema.host_cache
    - the replication tables [1;31mperf[mormance_schema.replication_*
    - the status tables like [1;31mperf[mormance_schema.session_status
    - the system variables tables like [1;31mperf[mormance_schema.session_variables
    are exposed, which rely on the [1;31mperf[mormance schema -- storage engine --,
    even when the [1;31mperf[mormance schema -- instrumentation -- is not used.
    
    With this fix, the [1;31mperf[mormance schema storage engine itself is now mandatory.
    
    Compiling:
    - the storage engine
    - the various instrumentation calls
    is now decoupled, as the former is needed independently of the later.
    
    To build a server without the -- instrumentation --,
    which is the main motivation to build without the [1;31mperf[mormance schema in
    general, the following cmake options can be used instead of
    option -DWITH_PERFSCHEMA_STORAGE_ENGINE=0
    - DISABLE_PSI_THREAD
    - DISABLE_PSI_MUTEX
    - DISABLE_PSI_RWLOCK
    - DISABLE_PSI_COND
    - DISABLE_PSI_FILE
    - DISABLE_PSI_TABLE
    - DISABLE_PSI_SOCKET
    - DISABLE_PSI_STAGE
    - DISABLE_PSI_STATEMENT
    - DISABLE_PSI_SP
    - DISABLE_PSI_PS
    - DISABLE_PSI_IDLE
    - DISABLE_PSI_STATEMENT_DIGEST
    - DISABLE_PSI_METADATA
    - DISABLE_PSI_MEMORY
    - DISABLE_PSI_TRANSACTION

[33mcommit 208c99166e20048a281ebd912969881d9e60342a[m
Merge: d5ab02a9f6c ba7f29a48a7
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Tue Sep 1 21:02:42 2015 +0200

    Merge branch 'mysql-5.6' into mysql-5.7
    
    Conflicts:
            include/mysql/psi/psi.h
            include/mysql/psi/psi_abi_v1.h.pp
            storage/[1;31mperf[mschema/pfs.cc
            storage/[1;31mperf[mschema/pfs_events_statements.h
            storage/[1;31mperf[mschema/table_events_statements.cc
            storage/[1;31mperf[mschema/table_events_statements.h

[33mcommit c4e3acb332b7494e2df994b2f92a3683b57be548[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Fri Aug 28 22:47:48 2015 +0530

    BUG#20367116: ALTER TABLE BREAKS ON DELETE CASCADE FOREIGN KEY
                  CONSTRAINT
    
    Analysis
    ========
    ALTER TABLE operation on a table with FOREIGN KEY CASCADE
    constraint while a concurrent DML operation is being
    [1;31mperf[mormed on the table referenced may break the referential
    integrity.
    
    In the case of ALTER TABLE operation using COPY algorithm:
    a) The ALTER TABLE operation on the table with FOREIGN KEY
       CASCADE constraint, the data is copied from the original
       file to temp file and transaction is committed releasing
       InnoDB locks (also, as an optimization, InnoDB commits
       transaction and releases locks every time ALTER TABLE
       copies 10000 rows).
    b) Concurrently if DELETE operation is executed on the
       referenced table, it will trigger delete of records
       on the table being altered. This will end up deleting
       records from the original file since the rename of
       the temp file has not yet completed and locks on the
       original file are not held.
    c) The ALTER TABLE operation continues, renaming the temp
       file to the original file leaving behind orphaned rows.
    
    In the case of ALTER TABLE operation using INPLACE algorithm:
    a) The DELETE operation on the referenced table acquires an
       LOCK_IS on the table being altered due to the CASCADE
       FOREIGN KEY constraint.
    b) The ALTER TABLE operation tries to acquire LOCK_X during
       the commit phase of INPLACE alter and is added to the
       waiting queue since LOCK_IS is acquired by DELETE.
    c) DELETE operation upon finding the record to delete, tries
       to acquire LOCK_IX on the table and is added to the wait
       queue. This results in a deadlock with ALTER operation
       being rolled back.
    Fix:
    ====
    a) INNODB has introduced a new handler API
       'get_cascade_foreign_key_table_list()' which returns a full
       closure of all tables ordered by the dependency on FOREIGN
       KEY CASCADE constraint.
    b) A function called 'lock_fk_dependent_tables()' is added
       which locks the list of tables ordered by the FOREIGN
       KEY CASCADE constraint for the table being altered.
       It is invoked in two places.
       1) Before the copy of data is invoked for the ALTER TABLE,
          COPY algorithm.
       2) Before ALTER TABLE, INPLACE commit.
    This avoids the orphaned rows and deadlock.
    
    Please note that this is a temporary workaround which is
    necessary until WL#6049 "Meta-data locking for FOREIGN KEY tables"
    is implemented
    
    NOTE: Some of the test cases have been removed, since the condition
    of concurrent DDL/DML operation on a parent table while a DDL
    operation is [1;31mperf[mormed on the the child table having a CASCADE
    foreign key constraint is restricted with this patch

[33mcommit db041d1a5546e88c9d16f0160231f2713dc91ad6[m
Author: V S Murthy Sidagam <venkata.sidagam@oracle.com>
Date:   Fri Aug 28 17:47:11 2015 +0530

    Bug#11757169: MyISAM share list scalability problems
    
    As the number of open tables is increased, table lookup
    (testing if a table is already open) and in particular
    the case when a table is not open, became increasingly more
    expensive.
    
    The problem was caused by the open table lookup mechanism,
    which was based on traversing a linked list comparing the
    file names.
    
    Fixed by storing a pointer to the MYISAM_SHARE on
    the designated handler share area, so it could skip
    traversing the linked list on open.
    And that there are additional changes for have_rtree,
    which are not related to the bug.
    
    Now with the patch we can see good [1;31mperf[mormance improvement
    in SELECT on huge number of tables for bigger values for
    TABLE_OPEN_CACHE and TABLE_DEFINITION_CACHE.

[33mcommit 854f9267446faeb8fe83b82b2c38cee38150e0ea[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Fri Aug 28 02:49:33 2015 +0400

    Bug #17865675: REGRESSION: EXPLAIN: FAILURE BELOW ITEM_FUNC_GROUP_CONCAT::PRINT
    
    The server failed on EXPLAIN of some GROUP_CONCAT() expressions.
    
    The fix consists of two parts.
    
    The 1st one (item_sum.{h,cc}) simplifies the code base by the removal of
    su[1;31mperf[mluous Item_sum::orig_args and Item_sum::tmp_orig_args fields.
    
    The 2nd one (item.{h,cc}) improves Item_field::print() to output column
    names with 3D-qualified identifiers:
    <original database>.<original table>.<column name> instead of 1D identifiers
    (<column name>).

[33mcommit f0b64bcc9b37a44ed209283f57357d2826fa65ea[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Thu Aug 20 15:16:10 2015 +0800

    BUG#21266784 - PATCH FOR 21052754 SHOWS UP TO AN 11% DROP IN PERFORMANCE
    FOR UPDATE OPS
    
    The patch would fix the L1 cache miss issue. The on/off check and counters
    per mutex should now be on the same cache line.
    
    The patch also fix an issue that we should distinguish BlockWaitMutex from
    WaitMutex.
    
    Finally, the patch does lots of code cleanups to improve the [1;31mperf[mormance,
    by doing statstics in batch mode.
    
    The regression observed is 4-5% now.
    
    The original patch is provided by Sunny.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 9608

[33mcommit 4b7f7617a9d4a95fbf5abf9b85c8be8a4f6d381b[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Aug 12 14:06:11 2015 +0200

    Bug#21562212 MEMORY/PERFORMANCE_SCHEMA INSTRUMENTS SHOULD BE ALWAYS ENABLED
    
    Before this fix,
    - "memory/%" instruments could be configured with
      TIMED='YES' or TIMED='NO' in table setup_instrument.
    - "memory/[1;31mperf[mormance_schema/%" instruments could be configured with
      ENABLED='YES' or ENABLED='NO' in table setup_instrument.
    
    This does not make sense, because:
    - memory instruments in general are never timed,
    - [1;31mperf[mormance_schema builtin memory instruments in particular can not be disabled
    
    With this fix,
    - "memory/%" instruments are never timed.
    - "memory/[1;31mperf[mormance_schema/%" instruments are never disabled.
    
    This fix avoids confusion, by displaying more truthfully in table
    setup_instrument what the server actually does with the memory
    instrumentation.

[33mcommit 14d664f0ddca7c69d44599693ffcd3455557e7e6[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Jul 29 19:15:52 2015 +0300

    Bug #21455603   REGRESSION: REPLICATION FAILS FOR INSUFFICIENT PRIVILEGES
    
    Refactoring of SHOW handler in WL6629 left some flaws in privileges that
    were exposed by this bug report. When @@global.show_compatibility_56 is OFF,
    access to show variables is made through [1;31mperf[mormance_schema so that
    a slave account with minimal privileges could not actually connected to the master
    anymore.
    That happens to the regular slave as well as to the semi-sync slave.
    
    Fixed with converting SHOW-VAR-LIKE queries into SELECT-based ones
    working on target variables directly.

[33mcommit aa233d268bcd60b37a856340cef55aeab0aceda2[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Tue Aug 4 15:41:24 2015 +0300

    Bug #21251297 and Bug #21436364: Make some P_S tables world readable
    
    The MySQL ACL system does not currently support granting stuff to public.
    So the only currently possible fix is to make [1;31mperf[mormance schema override
    the ACL code and return that the tables are accessible to public.
    
    Fixed by reverting parts of the fix for bug #14569746 pertaining to
    PERFORMANCE_SCHEMA.SESSION_ACCOUNT_CONNECT_ATTR, namely re-introducing
    the PFS_readonly_world_acl derived class and returning it into the tables
    share.
    
    Also used PFS_readonly_world_acl (and another derivate
    of PFS_truncatable_world_acl) to ensure [1;31mperf[mormance_schema.session_status,
    [1;31mperf[mormance_schema.global_status, [1;31mperf[mormance_schema.session_variables and
    [1;31mperf[mormance_schema.global_variables are using the world readable derivates
    too.
    Test cases updated.

[33mcommit 02b00b15476536619a95252c06133c36d3f7fc9a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Jul 30 11:17:50 2015 +0200

    Bug#21528683 SLOWDOWN CAUSED BY MEMSET IN SQL_DIGEST_STORAGE.RESET()
    
    Before this fix, an unnecessary memset was called in
    sql_digest_storage.memset().
    
    This affected:
    - the server under normal operations,
      when collecting digests for a query,
      with typically an extra memset(1024) for each query.
    
    - the [1;31mperf[mormance_schema.events_statements_current,
      _history and _history_long tables, during a SELECT,
      with typically an extra memset(1024*1024) for each row.
    
    This memset is un necessary, because the code making
    copies of struct sql_digest_storage, or computing md5 of it,
    is already only using the first m_byte_counts initialized bytes
    and not the full array.
    
    The fix is to simply remove the memset().

[33mcommit b5380e092c1cac3050a711c308136eee15c51826[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Jul 23 10:47:58 2015 +0530

    BUG#19886430: VIEW CREATION WITH NAMED COLUMNS, OVER UNION,
                  IS REJECTED.
    
    Analysis
    ========
    
    View creation with named columns over UNION is rejected.
    Consider the following view definition:
    
    CREATE VIEW v1 (fld1, fld2) AS SELECT 1 AS a, 2 AS b
    UNION ALL SELECT 1 AS a, 1 AS a;
    
    A 'duplicate column' error was reported due to the duplicate
    alias name in the secondary SELECT. The VIEW column names
    are either explicitly specified or determined from the
    first SELECT (which can be auto generated if not specified).
    Since a duplicate column name check was [1;31mperf[mormed even
    for the secondary SELECTs, an error was reported.
    
    Fix
    ====
    
    Check for duplicate column names only for the named
    columns if specified or only for the first SELECT.

[33mcommit aff9330ed2eb1d7dc1661b9c81d2308bde47e288[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Jul 21 13:40:45 2015 +0200

    Bug #18871046 SET NUMA MEMPOLICY FOR OPTIMUM MYSQLD PERFORMANCE
    
    Post merge fix, adjust [1;31mperf[mschema.show_sanity test in 5.7 and up.

[33mcommit 2cae38135e150ff23a87901ab7878c5a4f61d762[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Jul 2 09:42:18 2015 +0200

    Build cleanup.
    
    Fixed inconsistent prototypes for storage/[1;31mperf[mschema/pfs.cc.

[33mcommit 12ada74af397ef78d2738e4278cbda56ab6dacfd[m
Author: Dyre Tjeldvoll <Dyre.Tjeldvoll@oracle.com>
Date:   Tue Jul 14 10:11:09 2015 +0200

    BUG#21021848: ASSERTION `M_STATUS == DA_ERROR' FAILED.
    
    Problem: When I_S query which needed to [1;31mperf[morm full table open
    encountered a corrupt table it tried to repair it. To do this I_S
    query tried to acquire X metadata lock on the table. When I_S query
    was used in the middle of transaction this sometimes led to deadlocks
    if there was concurrent DDL. As result transaction was aborted and
    rolled back without appropriate error reported. For XA transaction in
    debug builds assert fired.
    
    Solution: Since it is undesirable to have an I_S query attempt repair
    (or discovery) the patch modifies the behavior of
    Open_table_context::recover_from_failed_open() to skip these
    operations if MYSQL_OPEN_FAIL_ON_MDL_CONFLICT set, and instead sets
    the error ER_WARN_I_S_SKIPPED_TABLE which will be converted to a
    warning. This will avoid deadlocks during I_S query execution.
    
    This returns the behavior of I_S queries to what they were before
    BUG#18075170 was fixed. The assert in conjunction with XA abort, which
    was present even before BUG#18075170, is now fixed.

[33mcommit 2200d95c0990f3379d41cfb7174ac22aa4c709af[m
Author: Ramil Kalimullin <ramil.kalimullin@oracle.com>
Date:   Tue Jul 14 12:01:59 2015 +0400

    Bug #20968596: MYSQL_UPGRADE IN 5.7 FAILS ON MYSQL.PROC TABLE, WORKS WHEN RUN AGAIN
    
    Problem: mysql_upgrade treats a warning with severity 'ERROR' as a real query execution error, so it stops when such a warning appears. In this particular bug, musql_upgrade got a warning after "DROP DATABASE IF EXISTS [1;31mperf[mormance_schema" query and finished, although the database is dropped, and the next mysql_upgrade run will go with no problem.
    
    Fix: relax it, don't treat warnings as query execution errors.
    
    Note: all warning messages are prepending with 'non fatal' from now on.

[33mcommit 8b8b26006a913f383b22731a97334345e0e34156[m
Author: Robert Golebiowski <robert.golebiowski@oracle.com>
Date:   Mon Jun 29 11:56:37 2015 +0200

    Bug #21216433 MYSQLD.EXE CRASHES WHEN TRYING IN-PLACE UPGRADE FROM
    5.0.96
    
    When reading the ACL tables in acl_reload() and acl_load() failing to
    [1;31mperf[morm for some reason the server calls init_check_host().
    
    Bug-fix: host cache is rebuild based on the previous acl_users. In case
    the previous acl_users is empty - empty host cache is generated.

[33mcommit f3c3da14c6bd6db42eeb565f9fece7edc432b1da[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Mon Jul 13 11:13:19 2015 +0530

    - Bug#21407023: DISABLING AHI SHOULD AVOID TAKING AHI LATCH
      Currently if AHI is disabled check for it was protected by AHI latch which
      caused latch overhead even though the feature is not adding any value.
    
      This has been fixed to avoid taking latch if AHI is disabled. If AHI is
      enabled while the API is [1;31mperf[morming the check then we may use stale value
      but that is fine. API active during that snapshot will miss using AHI which
      is far less costlier than keeping it active when feature is disabled.
    
      Reviewed by: Jimmy Yang (jimmy.yang@oracle.com)
      RB: 9570

[33mcommit e554bbda2c9d67c816a6e44aa6775111b96496a2[m
Merge: ca642d4ac1c 087c2f04c7c
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Jul 10 21:14:22 2015 +0200

    Merge branch 'mysql-5.6' into mysql-5.7
    
    Conflicts:
            mysql-test/suite/[1;31mperf[mschema/r/query_cache.result

[33mcommit 087c2f04c7c94713cea6e5596ec1a946f23f551c[m
Merge: 5639195ee29 49667f04419
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Jul 10 20:58:36 2015 +0200

    Merge branch 'mysql-5.5' into mysql-5.6
    
    Conflicts:
            storage/[1;31mperf[mschema/pfs_timer.cc

[33mcommit 0fd8555617e8e8d309acf41808610e5d5cb97ef1[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 10 10:08:14 2015 +0200

    Bug#21271736 UNITTEST UT0CRC32-T HANGS ON SOLARIS 10 SPARC
    
    Reduce execution time for unit test.
    Change the constant n_megabytes_to_checksum and build optimized when doing
    [1;31mperf[mormance analysis.

[33mcommit 45cc58eafd2f441bd2689bc8d1422efbdfe26c84[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Thu Jul 9 16:55:35 2015 +0200

    Bug21338077 HANDLE_FATAL_SIGNAL (SIG=11) IN __STPCPY_SSE2_UNALIGNED FROM MY_
    
    PROBLEM: Derived tables with no name caused issues when pre-check
     was [1;31mperf[mormed.
    FIX: For derived tables: Skip pre-check and initialize the table privilege
     with a SELECT_ACL grant

[33mcommit a65a981211d337fb1362815408f22c4a3b56b97b[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Wed Jul 8 19:27:06 2015 +0200

    Bug#19929832 EVENTS_STATEMENTS_HISTORY HAS ERRORS=0 WHEN THERE ARE ERRORS
    
    Adjust [1;31mperf[mschema.mist.test for gcov builds.

[33mcommit 368abfd3f4b0b7b0af442e518cad8bfc57549d51[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Mon Jul 6 14:47:32 2015 +0200

    Bug#17493646 NDB: CALL TO MY_ERROR() LACKS ERROR CODE AND ERROR STRING
    ARGUMENTS
    
     - rewrite ndbcluster_discover() to avoid using my_error()
     - [1;31mperf[morm the check early in function so it's ok to just return
       with an error when the problem is detected.
     - improve the error message being written to mysqld log file
     - this is safe since:
     -- the output generated from my_error() was never ignored anyway
     -- the my_errno variable was never set by call to my_access

[33mcommit 4affe8acf27b044bff874028173baf13e6d44d37[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Sat Jul 4 11:42:59 2015 +0200

    Bug#20563954 - INNODB: SUPPORT MOVING A WHOLE DATADIR WITH GENERAL TABLESPACES.
    Bug#19896685 - INNODB: RECOGNIZE DIFFERENT PATH STRINGS THAT POINT TO THE SAME LOCATION
    Bug#20555168 - INNODB: MAKE GENERAL TABLESPACES PORTABLE FROM WINDOWS TO UNIX
    Bug#21068487 - INNODB: PREVENT RELATIVE ISL PATHS UNDER THE DATADIR
    
    Post-push fix: Fix broken build without [1;31mperf[mormance schema.

[33mcommit d230f7377e980d0a03dfb5020c79b8676ceeb654[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Fri Jul 3 18:15:40 2015 +0530

    WL#7755: mysqlpump: Extend mysqldump functionalities
      Mysqlpump is a new client utility similar to mysqldump, however mainly
      focussed to improve the [1;31mperf[mormance of taking dumps. This new tool
      would achieve the following:
      1. Provides basic functionality of mysqldump.
      2. Allows to take dumps in parallel using multiple threads and queues
         which can be configured using command line options.
      3. Allows various object filtering options which can be used to specify
         what databases or database objects to be included or excluded from
         dump.
      4. Allows the dump files to be compressed using LZ4 or ZLIB library.
      5. Provides an option to watch progress of dump.

[33mcommit b123655ae46170828cf17c19d383e40fa9e7848e[m
Author: Hemant Dangi <hemant.dangi@oracle.com>
Date:   Fri Jul 3 15:20:11 2015 +0530

    Bug#21355202: PROVIDE ACCESS TO SERVER REPORT_HOST AND REPORT_PORT VARS TO GROUP REPLICATION
    
    [1;31mperf[mormance_schema.replication_group_members.MEMBER_HOST and MEMBER_PORT column
    will use startup option report-host and report-port when set, as value provided
    by glob_hostname, which used gethostname() function internally to determine
    hostname, will not always provide correct network interface,
    especially in case of multiple network interfaces. And report-port should be
    used when provided as it may not be listening on a nondefault port.

[33mcommit 7eda2af100098982188b904d1ce4ca80d04ac0cb[m
Merge: eac8249708b d7b0c4fba2d
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Thu Jul 2 18:30:35 2015 +0200

    Merge branch 'mysql-5.6' into mysql-5.7
    
    Conflicts:
            mysql-test/suite/[1;31mperf[mschema/r/misc.result
            mysql-test/suite/[1;31mperf[mschema/t/misc.test
            storage/[1;31mperf[mschema/pfs.cc

[33mcommit b5bbc1b38b0319709b3dc5d196e11d708ad6463d[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed May 27 09:03:04 2015 +0200

    BUG#21127308: REPLICATION THREAD STATUSES NOT SHOWN IN PERFORMANCE_SCHEMA
    
    Some thread statuses related to replication were not included in the
    all_server_stages[] array.  That resulted in
    [1;31mperf[mormance_schema.threads showing NULL in the PROCESSLIST_STATUS
    column (but SHOW PROCESSLIST showed the correct status).  Fixed by
    including these thread statuses in the array.
    
    Additionally:
    - Changed "Waiting for slave workers to finish." to "Waiting for slave
      workers to process their queues".
    - Changed "Waiting for GTID to be written to the binary log" to
      "Waiting for GTID to be committed"
    - Changed "Waiting for its turn to commit." to "Waiting for preceding
      transaction to commit"
    - Changed "Waiting for dependent transaction to commit." to "Waiting
      for dependent transaction to commit"
    
    Tests:
    - mysql-test/suite/rpl/t/rpl_mts_logical_clock_crash.test failed
      becuase it was waiting for
      stage_slave_waiting_for_workers_to_finish.  Fixed the test.  Also
      noticed that despite there was a timeout in wait_condition.inc, it
      just printed a message and continued running the test; it did not
      source show_rpl_debug_info.inc and did not execute 'die'.  Added
      call to show_rpl_debug_info.inc if $show_rpl_debug_info is set (also
      in some other include/wait_*.inc files), and made rpl_init.inc set
      $show_rpl_debug_info.

[33mcommit af205c05d8a75ed64bf99a7d6bd055923647d401[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Thu Jun 11 20:58:02 2015 +0300

    Bug #20866059   CAN'T ENABLE EXPLICIT_DEFAULTS_FOR_TIMESTAMPS W ROLLING UPGRADES IN CHAINED RPL
    
    The bug is essentially in that the binlog event applier (slave, or
    mysqlbinlog client) miss out a necessary part of Query event execution
    context. Specifically the part is
    
    MASTER@@session.explicit_defaults_for_timestamp
    
    value in the context of CREATE or ALTER table queries whose attribute
    list includes at least one TIMESTAMP column.
    
    ***Here goes some analysis***
    
    More specifically the TIMESTAMP attribute declaration that does not
    have explicit NULL or NOT-NULL is subject to dual interpretation at
    the query execution, which is controlable by the @@session var.
    Which of the two methods (per the two var's values) was chosen by
    the Master the binlog applier might have no idea.
    
    The applier can learn about this type of execution context when events
    are received (relay-logged) immediately from pre-WL6292 server thanks
    to WL6292 introduced server version adaptation framework VAF (and
    Bug19630322 fixing an MTS issue in it).
    
    But once such events are relayed further downstream chain that info gets lost.
    
    This patch addresses the following points of the issue.
    
     Legends:
    
     M - stands for Master server, S - for the applier (slave) server.
     The LaTeX style subscipt `_x' stands for a version `x'.
     `wl6292' is a version value
     corresponding to that of where the WL was pushed to.
     `fixed' is the current being fixed server version.
     `pre#' is for a version before that it prefixes.
     `[]' designates any in a range.
     `()' parameteries the server with a specific values of global var.
    
        Pre-WL6292 master events relayed downstream of chain replication:
    
    M_pre#wl6292 -> S_fixed -> S_fixed -> ...
    
    Data on S:s are consistent with the M.
    
        Cross "@@explicit_defaults_for_timestamp"-replication,
        from/to servers having different values of the var in the logger and in the applier.
        The chain replication is covered as well:
    
    M_fixed(the-var := true)  -> S_fixed(false) -> S_fixed(true) ...
    
    Notice that a problem of replicating from the WL6292 and later pre-fixed M
    to fixed S is unsolvable in principle:
    
    M_[wl6292..pre#fixed] --X-> S_fixed
    
    because the var's value can't be learned by S_fixed even through VAF
    (in contrast, on the pre#wl6292 servers the "var"'s value is false,
    "" - as there's no such variable).
    The patch treats this case with an assumption that M's value, awareness
    of which is detected by VAF, is the same as the S' one.
    
    In more details the first link of p.1 M_[0..pre#wl6292] -> S_fixed remains to be fixed by VAF
    (see MTS fixes in bug19630322).
    
    S_fixed -> S_fixed is fulfilled in the current bug's patch with
    enriching the execution context that is recorded along with the Query
    log event. The so called status var:s set is augmented with a specific
    token when the master's parser faces a timestamp column
    declaration. The token is essentially a boolean value
    corresponding to @@session.explicit_defaults_for_timestamp.
    
    Here is an example how the "old" intepretation settles down
    timestamp column definition:
    
    --server master
      start mysqld args: \empty
    
    --connection master
    select @@version;
    =>  5.7.8-rc-debug-log
    SELECT @@global.explicit_defaults_for_timestamp;
    => 0
    
    CREATE TABLE tc (ts TIMESTAMP);
    => OK
    
    show create table tc;
    => CREATE TABLE tc (
    ts timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=latin1
    
    To recollect the docs, by rules of pre#WL6292 server which are in force in above
    as @@global diagnoses, when the explicit NULL is not provided the
    "implicit" NOT-NULL is added up to the actual column definition.
    Moreover, the absence of the explicit NULL is the rules' necessary condition.
    
    ***Solution***
    
    A solution that bases on exhausive analysis of parsed query details (presense of the explicit NULL)
    would be [1;31mperf[mect.
    However that is rather tedious and the patch does not attempt precise investigation for that.
    This could be always/possibly refined later.
    A more coarse approach is taken to mark potentially vulnerable queries.
    
    Dwelling into the low level details, a new three-enum-value member added to THD.
    Two values correspond to of the two values of @@session.explicit_defaults_for_timestamp.
    When the parser processes TIMESTAMP column declaration it memorizes the actual value of
    the session var.
    Whether the member's value is changed from the default UNSET is checked out
    by the binary logger which further encodes it into a new Query-log-event status
    variable of the bool sematic.
    The meaning of the latter is to instruct the event applier about
    the actual value of @@session.explicit_defaults_for_timestamp for executing
    the Query-log-event.
    
    Notice that @@explicit_defaults_for_timestamp is turned into writable
    which is necessary because there's no other way to inform the "mysqlbinlog"
    applier about the correct context but through the variable.
    
    This type of approach is rather economic, extending only "suspicious"
    Query-log-events with two bytes. A sign of the original context is
    relayed along replication chain to lift out the reported issue in
    particular.
    
    The fixes enroll correction to main.mysqlbinlog test which dealt with
    binlog files indexed explicitly. That turned to be not future-changes safe,
    as the test limits max binlog size. And now when the new status var is added
    the index have shifted.

[33mcommit 31d6837b4c2bed949a06be6131c1853d8652b6de[m
Author: Sreeharsha Ramanavarapu <sreeharsha.ramanavarapu@oracle.com>
Date:   Mon Jun 29 08:23:39 2015 +0530

    Bug #20238729: ILLEGALLY CRAFTED UTF8 SELECT PROVIDES NO
                   WARNINGS
    
    Issue:
    -----
    No warning is delivered when MYSQL is unable to interpret
    a character with the given charset.
    
    SOLUTION:
    ---------
    Check is now [1;31mperf[mormed to test whether each character can
    be interpreted with the relevant charset. Failing which, a
    warning is raised.

[33mcommit d28a4a54f54f52c33e70c5ad299f6c8ca597e65f[m
Author: Terje Røsten <terje.rosten@oracle.com>
Date:   Thu Jun 25 09:43:35 2015 +0200

    BUG#20770671 LIBMYSQLD SHOULD NOT ACCEPT DEFAULT SECURE_FILE_PRIV_DIR VALUE FROM CMAKE
    
    libmysqld is s shared library, using a specific directory for
    secure-file-priv option by default will not work for multiuser
    systems.
    
    Fix issue by adding CMake option to set default value of
    secure-file-priv for embedded library.
    
    CMake option is called INSTALL_SECURE_FILE_PRIV_EMBEDDEDDIR
    
    Default value is "NULL".
    
    This means import and export operations, such as those [1;31mperf[mormed by
    the LOAD DATA and SELECT ... INTO OUTFILE statements and the
    LOAD_FILE() function are disabled by default for embedded library.

[33mcommit 33f010ace32ac55c954b7303ff6ba1861bb76931[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Tue Jun 23 10:27:51 2015 +0530

    Bug#21108296 : --SSL-CIPHER OPTION CAUSES SSL INITIALIZATION FAILURE
    
    Description : 1. If --ssl-cipher is specified at the time of starting
                     Server, SSL certificates are not generated automatically.
                     Automatic generation is limited to OpenSSL linked
                     MySQL server.
    
                  2. If --ssl-cipher is specified at the time of starting
                     server, automatic detection does notwork.
    
    Solution : Modified checks [1;31mperf[mormed before certificate generation/detection
               to make sure that --ssl-cipher is not a cause to skip either of
               these actions.

[33mcommit 0559db37b3cc31533da016ec511b92a9b6547ea7[m
Author: Todd Farmer <todd.farmer@oracle.com>
Date:   Tue Jun 16 17:17:39 2015 -0600

    Fixing [1;31mperf[mschema.sanity_check test case.
    
    (cherry picked from commit 0196d8aef01fdcb9173b4366ad1c9f7dbe67a669)

[33mcommit 33c8a52eb56f21ee8de4305e84d8be67a49e95e9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Jun 16 23:55:40 2015 +0200

    Part2 (of 2) fix for Bug#18390321
    
      MT-SCHEDULER: POSSIBLE BUSY WAIT IF PERFORMSEND() FAILED TO SEND
    
    This fix is for the above issue when the send threads are
    responsible for doing the sends. For different reason
    the send threads may have no progress when trying to send
    a chunk of data (buffers full, slow receivers, ++). It is
    then no point in busy-retrying the send. Instead the send thread
    should take a short break. Other theads will then be able
    to do their work which migh help in resolving the stuck send.
    
    Part of this patch is a refactoring / fix of a patch
    introducing a configurable 'max_send_delay' for the
    send threads. The idea behind that patch is that having a
    short delay before sending available data, might allow us
    to piggy back more data to the same node. This was
    a usefull mechanism to extend into a more general send-delay
    mechanism when the send threads should wait due to overloaded
    transporters. However, I found a refactoring was required in
    order to 'open it up' for such reuse, that included:
    
     - Setting of 'delay' was moved from insert_node() into
       own methods: set_max_delay() / set_overload_delay().
       This also fixed a problem in ::run_send_thread() where
       we re-inserted a node having more data to send, *and*
       started a new delay. In this situation it does not make
       sense to delay more, as we were likely unable to complete the
       current send due to too much data.
    
     - Decision of when to take a delay-sleep, or possible ignore it,
       was moved from get_node() into ::run_send_thread().
       get_node() will now first search for a send node not being
       delayed- If that cant be found, the node with the shortest
       delay is returned. It is then up to the send schedulling
       logic in ::run_send_thread() to either wait for this delay to
       expire, or ignore the delay and send immediately (As previous)
    
     - Furthermore, previously get_node() contained logic deciding
       how many send threads should be kept awake. This is now also
       handled by ::run_send_thread().
    
     - There were also a bug as a send delay actually resulted
       in a busy wait for the delay to expire: Even if get_node() didn't
       return any (non-delayed) node, yield() was called with
       the check_callback function check_available_send_data().
       As 'first_node != 0' when there were delayed sends, the yield()
       would not be taken (Assumed more data had arrived)
    
       That problem were fixed by introducing a 'm_more_nodes' flag
       which is set if more send nodes is inserted inbetween we release the
       send_threads_mutex, and yield has grabbed its own mutex.
       (Some memory barrieres required in addition)
    
    The 'overload' delay has been added on top of this delay
    mechanism, Mainly implemented by [1;31mperf[morm_send() now returning
    'bytes_send'. Together with the existing 'more' return value,
    ::run_send_thread() detect overload as 'more && bytes_sent==0',
    and then request a set_overload_delay(). Much more than
    that is not required with the refactored send-delay...

[33mcommit 28787ac4861543d6ef0ac6f679fa73d676c7536d[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Jun 16 09:59:15 2015 +0200

    Rework the test case [1;31mperf[mschema.status_reprepare to be more robust

[33mcommit e7808b18096ecb4779c0141ea52cd360d36cfba6[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Jun 15 14:44:57 2015 +0200

    Bug#21223458 UPDATE TO SETUP_CONSUMERS DOES NOT UPDATE CLIENT HISTORY
    LOGGING FLAG
    
    Before this fix,
      UPDATE [1;31mperf[mormance_schema.setup_consumers
    when changing the consumers for history / history_long events,
    has no effect on threads already running.
    
    Disabling a consumer did not stop event collection for running threads.
    
    Enabling a consumer did not start event collection for running threads.
    
    Only threads connected after the update would function properly.
    
    The root cause is that some derived flags per thread,
    like pfs_instr::m_flag_events_waits_history,
    were not re-evaluated.
    
    The root cause is the definition of the history consumers,
    for which the flag row_setup_consumers::m_thread_refresh
    was not set.
    
    This fix sets row_setup_consumers::m_thread_refresh to true
    for the history consumers, so that per thread derived flags are
    re evaluated when a consumer is updated.

[33mcommit 29a1fc4fabd8a05fc3a12ca339ee687a88b56808[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Sat Jun 13 15:18:14 2015 +0200

    Bug#21052754 ADD SHOW ENGINE INNODB MUTEX FUNCTIONALITY
    
    Post-push fix: Fix broken build with [1;31mperf[mormance schema disabled.

[33mcommit 349ec4b089652c7e27a247658863b5d6896c3b96[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jun 10 13:49:37 2015 +0200

    Bug#17243619 NEGATIVE VALUES FOR LOW COLUMNS
     IN MEMORY_SUMMARY_GLOBAL_BY_EVENT_NAME
    
    This bug is similar to
     bug 17473077 - EVER INCREASING MEMORY USAGE REPORTED FOR
    EVENT SCHEDULER
    
    Some data structures are shared in the server,
    which imply that the memory instrument used should be flagged with
    PSI_FLAG_GLOBAL.
    
    What is particular to this fix is that the data structures used are
    containers, such as lists, hash tables, etc.
    
    Currently, the [1;31mperf[mormance schema instrumentation,
    for containers, works as follows.
    
    Elements X, Y and Z are instrumented with their dedicated keys,
    "memory/sql/X", "memory/sql/Y" or "memory/sql/Z",
    so that statistics are separate for X, Y and Z
    
    Generic containers are instrumented with a common key,
    so that Container<X>, Container<Y> and Container<Z>
    all end up instrumented under the same key "memory/sql/Container".
    
    This leads to the following issues:
    
    - Statistics for Container<X>, Container<Y> and Container<Z> are all counted
      under the same bucket. This makes very difficult to understand where
      memory is really used.
    
    - Statistics for 'X' and 'Container<X>' are not aggregated,
      which makes it difficult to really measure the total footprint
      of a "Container of X" data structure.
    
    - Container<X> can be a global structure (shared) while Container<Y>
      can be a per session structure. Each need different accounting flags
      (PSI_FLAG_GLOBAL or not), but this is impossible with a common
      key like "memory/sql/Container", leading to the bug reported.
    
    What this fix implements is as follows:
    
    - Container keys like "memory/sql/Container" are removed.
    
    - Containers now take the memory instrumentation key from the caller,
      so that Container<X> is instrumented with "memory/sql/X"
      and Container<Y> is instrumented with "memory/sql/Y"
    
    - 'X' and 'Container<X>' are now instrumented together,
      with the key for 'X'.
      This affects tests output, which measure now a bigger (and more accurate)
      memory usage.
    
    The affected containers are:
    - HASH and my_hash_init()
    - DYNAMIC_ARRAY and my_init_dynamic_array()
    - class hash_filo and hash_filo::hash_filo()
    - class Hash_set and Hash_set::Hash_set()
    
    The change is big in size, but mechanical: always pass the instrumentation
    key explicitely from the calling code, for containers.

[33mcommit e70e66ce5a6ab482fd4a957d9ea88ed05ea789fb[m
Author: Libing Song <libing.song@oracle.com>
Date:   Wed May 20 08:12:05 2015 +0800

    BUG#20857660 REPLICATION_APPLIER_STATUS_BY_WORKER SHOWS
                 WRONG WORKER_ID WHEN SLAVE STOPS
    
    Query from replication_applier_status_by_worker table, all worker ids
    were 1 if slave was stopped. When stopping workers, the original
    Slave_work objects of workers will be released. To support [1;31mperf[mormance_schema
    table, new Slave_worker objects are created and part of the infomations are
    copied from the original objects before they are released. internal_id of
    the new Slave_worker objects was not initilaized correctly, all of them were
    intialized as 1.
    
    To fix it, internal_ids are initilaized same to the original objects.

[33mcommit 9583a3bd86fc0471a7bfc5d24ab43dcfe8edecbb[m
Author: Mark Leith <mark.leith@oracle.com>
Date:   Fri Jun 5 09:50:06 2015 +0100

    Bug #20971120 MTR TESTS FAIL WHILE CREATING SYSTEM TABLES ON NON PS BUILD
    
    If compiling without [1;31mperf[mormance schema, use a generated empty SQL file, that just contains an empty sys schema (like [1;31mperf[mormance_schema does when not compiled in), to compile in to mysql_upgrade, mysql_install_db, and mysqld --initialize. Set the version as 1.0.0 for the sys schema, so that mysql_upgrade etc. will install the empty schema, but will not overwrite an existing schema.
    
    mysql_upgrade and mysql_install_db still have the --skip-sys-schema options, and can still be used to not create the empty schema if desired.
    
    Also add a --skip-sys-schema option to MTR to allow skipping use of the sys schema. This has to be done in this way, as we can't rely on checking the compile time option. It also leaves the possibility of not using the sys schema when [1;31mperf[mormance schema is compiled in too, should people want to use it.
    
    Like mysql_install_db etc., this currently creates an empty sys schema when used and removes the sysschema suite from the set of default suites to run.
    
    Reviewed-by: Marc Alff <marc.alff@oracle.com>
    Reviewed-by: Bjorn Munch <bjorn.munch@oracle.com>
    RB: 9009

[33mcommit 8e9227281c1af7b594ac295963df2e6139054f8b[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Fri May 29 08:43:51 2015 +0530

     bug#20985298: [1;31mperf[mormance fix ahi by splitting the single search system mutex
     into n parts.
    
      Follow-up fix:
      - Max threshold for innodb_adaptive_hash_index_parts is bumped to 512
        (from 64 before)

[33mcommit 055c521e60fa64398bcd32840d6193413712ac87[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Thu May 28 14:59:55 2015 +0200

    Bug#20949117: Remove obsolete code from UNION processing
    
    After the last refactoring work in preparation and optimization,
    there are some unused code blocks in sql_union.cc.
    This bug fix eliminates those code blocks completely and [1;31mperf[morms
    some simple additional refactoring.
    
    - Added an interface st_select_lex_unit::is_simple() that wraps
      testing for !(is_union() || fake_select_lex)
    
    - Cleaned up global_parameters() a bit: Only ORDER BY/LIMIT/OFFSET
      should be accessed through it, otherwise use fake_select_lex.
    
    - Initialization of JOIN::do_send_rows was moved from optimization to
      execution, since it is used only in the latter.
    
    - st_select_lex_unit::prepare() has mostly cosmetic changes and improved
      comments. Call to set_current_select() eliminated for error case.
    
    - Deleted unused function Query_result::reset_offset_limit_cnt
    
    - st_select_lex_unit::optimize() had an unused code block started with
      if (sl == global_parameters() && is_union()).
      It was unused because global_parameters() always return the "fake" object
      for a UNION query. Besides, with the introduction of
      Query_result_union_direct, LIMIT/OFFSET handling is correct without
      adjusting offset_limit_cnt and select_limit_cnt.
    
    - An equivalent code block is removed from st_select_lex_unit::execute().
      Variable rows_at_start was found to be redundant.
      Calls to set_current_select() were removed in error case.
      offset_limit_cnt did not need to be assigned here, since it is done
      in set_limit().
      Call info(HA_STATUS_VARIABLE) was moved to a more logical place
      (used to get row count from temporary table used by UNION).
      add_rows was never assigned so it could be removed:
      sl->join->calc_found_rows is never true for a query block that is
      part of a UNION (either braces=true or m_select_limit=HA_POS_ERROR,
      see JOIN::optimize()), search for comment "Calculate found rows if".
      join->examined_rows is reset in JOIN::exec() so assignment is deleted.
    
    - Added more extensive tests for LIMIT and OFFSET to limit.test
    
    (cherry picked from commit b50c7dc66c6c894772d5da463cd08b9cd9ebd154)
    
    Conflicts:
            sql/query_result.h
            sql/sql_union.cc

[33mcommit 48825ce0110f27ab86769d482d85bd52d85771a7[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu May 28 16:16:59 2015 +0200

    Fixed link issue with building without [1;31mperf[mormance schema

[33mcommit 436582f8ef8a17f3cee004af2cbab8062ca80ac0[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu May 28 15:05:40 2015 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - use --innodb-read-only to avoid that innodb starts recovery
     threads etc. since that will only cause mystery crashes
     when the mysqld startup is aborted due to the invalid
     option tests [1;31mperf[mormed below. Would have been nice to
     skip innodb for these test but that is not an option in 5.7

[33mcommit 654cfab9e21c4656e7ba485cdc8512cdac1b265c[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu May 28 12:09:23 2015 +0200

    - bug#20985298: [1;31mperf[mormance fix ahi by splitting the single search system mutex
      into n parts.
    
      Checkin related to it broke Windows build due to data-type inconsistency.

[33mcommit ab17ab91ce18a47bb6c5c49e4dc0505ad488a448[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu May 28 10:31:07 2015 +0200

    - bug#20985298: [1;31mperf[mormance fix ahi by splitting the single search system mutex
      into n parts.
    
      Before this patch AHI search system was protected using single mutex named
      "btr_search_latch". It often use to cause contention resulting in [1;31mperf[mormance
      hit.
    
      With this patch we are introducing ahi-partitioning that will split search
      systems into n parts and each index will be bound to a specified part based
      on its attributes (index_id and space_id).
    
      This will reduce contention and will help improve [1;31mperf[mormance when AHI is
      heavily used.
    
      Reviewed by: Sunny Bains (sunny.bains@oracle.com)
      RB: 8773

[33mcommit 0fbb8a0306317a3144d6e14d5d20e3d1445dab10[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue May 26 18:03:16 2015 +0200

    Bug#17473077 EVER INCREASING MEMORY USAGE REPORTED FOR EVENT SCHEDULER
    
    Before this fix, statistics for the memory instrumentation
    were inaccurate, in the following scenario:
    
    At T1, thread A allocates some memory, of size N
    At T2, thread A gives ownership of the allocated memory to thread B
    At T3, thread B de allocates the block memory, of size N
    
    The event at T1 is instrumented with the [1;31mperf[mormance schema,
    and an allocation of size N is counted against thread A.
    
    The event at T2 is not instrumented.
    The [1;31mperf[mormance schema is unaware of the memory transfer.
    
    The event at T3 is instrumented with the [1;31mperf[mormance schema,
    and a de allocation of size N is counted against thread B.
    
    The problem with this approach is with statistics maintained by thread.
    
    When the same code is executed many times (in a loop with the event
    scheduler),
    - thread A appears to use an ever increasing amount of memory
    - thread B appears to consume an ever increasing amount of memory
    - the global statistics for the server diverge.
    
    To resolve this bug, two different fixes are implemented.
    
    Which fix to use depends on the nature of the instrumented memory.
    
    FIX NUMBER 1
    
    For memory instruments that are by nature measuring a global resource,
    there is no point in maintaining per thread (and per account, per user, per
    host) statistics.
    
    A typical example of such usage are shared, global structures,
    like the query cache.
    
    For these memory instruments, the [1;31mperf[mormance schema now supports
    defining the instrument with PSI_GLOBAL_FLAG.
    
    For instruments defined as global, the [1;31mperf[mormance schema
    only maintains the global memory statistics.
    
    In this case, nothing needs to be done for event T2,
    as instrumentation for the allocation (T1) and de allocation (T3)
    is enough to maintain everything.
    
    FIX NUMBER 2
    
    For memory instruments that are by nature measuring a local resource,
    maintaining statistics per thread is desirable.
    
    In order to do so, the [1;31mperf[mormance schema needs to be told about event T2,
    which is now instrumented explicitly in the code.
    
    A new entry point in the instrumentation, PSI_MEMORY_CALL(memory_claim),
    is used to instrument the change of ownership of a block of memory.
    
    When a thread in the server allocates a data structure,
    starts a child thread, and gives ownership of the memory structure to the
    child, the code in the child thread now needs to claim ownership of the
    data.
    
    With this added instrumentation, [1;31mperf[mormance schema statistics now
    reflect more closely how memory is actually used in the server.
    
    Various claim_memory_ownership() methods are implemented to support this.

[33mcommit d4c94e99a492fee40fb44089b4de998e40c68550[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed May 27 15:19:24 2015 +0200

    Bug#20914786 : MYSQL_UPGRADE FAILS WITH SHOW_COMPATIBILITY_56 = OFF
    
    This is a rework on the previous fix for mysl_upgrade.
    
    mysql_upgrade can -- not -- use [1;31mperf[mormance_schema tables
    to read the server version,
    or use SHOW VARIABLES that rely on the same tables,
    because these tables might not even be installed (by the upgrade script)
    yet.
    
    Use
      SELECT @@global.version
      SELECT @@global.datadir
    which is robust and also works for in place upgrades.

[33mcommit 56175c3af1e947090e0719e4ebcaace094a70a5b[m
Author: Mikael Ronström <mikael.ronstrom@oracle.com>
Date:   Mon Apr 27 09:27:45 2015 +0200

    WL#8525: BUG#20904721: Part6: Improve [1;31mperf[mormance of checksum calculations, remove unnecessary ones and simplify bit toggling ones. Also solves BUG#20980229 that ensures that also header bits are included in checksum calculation.

[33mcommit d1f2732b3da3e652227830742e845b16d2a0176b[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed May 20 11:47:20 2015 +0200

    Bug#20956599 CAN'T SET PERFORMANCE_SCHEMA=OFF IN 5.7
    
    Before this fix,
    
    For a server compiled with [1;31mperf[mormance schema support,
    when starting the server with the [1;31mperf[mormance schema
    disabled at startup time, as in:
      [1;31mperf[mormance_schema = OFF
    
    the server would fail to create [1;31mperf[mormance_schema tables,
    which in turn causes the creation of views provided by sysschema to fail.
    
    The root cause is as follows:
    - during the server startup, the code detects that
      the [1;31mperf[mormance schema is not initialized.
    - the variable load_[1;31mperf[mschema_engine stays as false as a result
    - later, when loading the [1;31mperf[mormance schema plugin,
      the plugin is loaded with the option PLUGIN_OFF (aka, not loaded)
    - during the database initialization in mysql_system_tables.sql,
      the scripts looks for entries in table information_schema.engines,
      and finds none, leading the script to assume the server is not compiled
      with [1;31mperf[mormance schema support.
    - [1;31mperf[mormance schema tables are not created during the install.
    - installation of the sysschema views fail.
    
    The fix is to simplify and streamline the entire process,
    which is too convoluted.
    
    In particular,
    
    - a server compiled with [1;31mperf[mormance schema support always
      load the [1;31mperf[mormance schema engine, now mandatory,
      regardless of the runtime configuration.
    
    - a server compiled with [1;31mperf[mormance schema support always
      create the [1;31mperf[mormance schema tables during install.
    
    The database layout of the installed product should depend only on build options
    (to compile with of without the [1;31mperf[mormance schema),
    and never depend on runtime configurations options
    (to start the server with or without instrumentation)
    
    When the [1;31mperf[mormance schema is not used during server startup,
    no data will be collected and no memory allocated,
    so there is no [1;31mperf[mormance impact: there are no changes here.
    
    The only change is to always create [1;31mperf[mormance schema tables
    when the server is compiled with [1;31mperf[mormance schema support.

[33mcommit 72b75708175097c226436830217b3b870f6104e0[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Thu May 14 13:13:42 2015 +0400

    Bug #20720615: GEN_LEX_HASH FAILS WITH MEM.LEAK ERRORS IF COMPILED WITH CLANG-3.5 + ASAN
    
    If we use clang-3.5+ instead of gcc and enable its AddessSanitizer (ASAN),
    the gen_lex_hash utility aborts on clang's LeakSanitizer memory leak
    checks.
    
    This patch adds memory deallocation to gen_lex_hash.cc to make
    LeakSanitizer happy (normally this is su[1;31mperf[mluous, since gen_lex_has is
    a compile-time utility, and OS does the job itself).

[33mcommit ef216847bae04d424db5471b1fc565fa2f41ab3d[m
Merge: 39f83c586ad d402696855d
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Sat May 9 04:52:02 2015 +0200

    Merge branch 'mysql-5.6' into mysql-5.7
    
    Conflicts:
            sql/sql_class.h
            storage/[1;31mperf[mschema/pfs_account.cc
            storage/[1;31mperf[mschema/pfs_con_slice.cc
            storage/[1;31mperf[mschema/pfs_digest.cc
            storage/[1;31mperf[mschema/pfs_events_stages.cc
            storage/[1;31mperf[mschema/pfs_events_statements.cc
            storage/[1;31mperf[mschema/pfs_events_waits.cc
            storage/[1;31mperf[mschema/pfs_global.h
            storage/[1;31mperf[mschema/pfs_host.cc
            storage/[1;31mperf[mschema/pfs_instr.cc
            storage/[1;31mperf[mschema/pfs_instr.h
            storage/[1;31mperf[mschema/pfs_instr_class.cc
            storage/[1;31mperf[mschema/pfs_setup_actor.cc
            storage/[1;31mperf[mschema/pfs_setup_object.cc
            storage/[1;31mperf[mschema/pfs_user.cc

[33mcommit 4b5bcd8386f8c5533f8e4b92aeedc1e2539e1b1a[m
Merge: 5b7889c6c5c 71504da0f99
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue May 5 11:07:47 2015 +0200

    Merge branch 'mysql-5.6' into mysql-5.7
    
    Conflicts:
            mysql-test/r/mysqld--help-notwin.result
            mysql-test/r/mysqld--help-win.result
            mysql-test/suite/[1;31mperf[mschema/r/sizing_default.result
            mysql-test/suite/[1;31mperf[mschema/r/sizing_high.result
            mysql-test/suite/[1;31mperf[mschema/r/sizing_low.result
            mysql-test/suite/[1;31mperf[mschema/r/sizing_med.result
            mysql-test/suite/[1;31mperf[mschema/r/sizing_off.result
            sql/mysqld.cc
            storage/[1;31mperf[mschema/pfs_server.h

[33mcommit 5b7889c6c5c6ac8ce9584da5b6252fecd20bae47[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Apr 29 23:58:43 2015 +0200

    Bug#20963147 DECOUPLE DIGEST SIZE FROM SERVER AND PFS
    
    Before this fix, the server variable 'max_digest_size'
    was used in two places.
    
    1)
    
    During query parsing, the server collects the statement digest text,
    and store the digest in a buffer of size 'max_digest_size'.
    
    Only 1 digest text per session is needed at the same time,
    which limits the memory consumption.
    
    2)
    
    In the [1;31mperf[mormance schema, the digests collected in 1) are
    copied in various tables, and the memory allocated to
    keep digest historical or aggregated data also depends on 'max_digest_size'.
    
    Many digests text per session are preserved at the same time,
    so that the total memory consumption is a multiple of 'max_digest_size',
    making this parameter sensitive.
    
    The problem is that for some deployments,
    namely when using either the firewall plugin or the query rewrite plugin,
    DBA typically need to have at the same time:
    - a big value for 1), to avoid truncation
    - a lower value for 2), to limit memory consumption,
      even if truncations can occur in the recorded [1;31mperf[mormance schema data.
    
    The solution is to keep variable 'max_digest_size' for part 1),
    and create a separate variable '[1;31mperf[mormance_schema_max_digest_size' for part
    2), so that sizing for the firewall/query rewrite can be independent of the
    [1;31mperf[mormance schema.

[33mcommit dc05b72d33e8ff2a4a9204a06bea1374830c5097[m
Author: Mayank Prasad <mayank.prasad@oracle.com>
Date:   Mon May 4 10:59:03 2015 +0530

    Bug #20914786 : MYSQL_UPGRADE FAILS WITH SHOW_COMPATIBILITY_56 = OFF
    
    Issue:
     In 5.7, server variables values are exposed by [1;31mperf[mormance_schema tables.
     When this SHOW_COMPATIBILITY_56 is ON, information_schema tables could
     be used to fetch variables values, but if it is OFF, information_schema
     doesn't provide variables values but [1;31mperf[mormance_schema does.
    
    Fix:
     During mysql upgrade, this utility checks for various server variables'
     values. So when SHOW_COMPATIBILITY_56 is ON/OFF, these values are to be
     fetched from correct source. Did code change to make sure it is
     followed.

[33mcommit d8970c17e67e4cc66552cb43e85bd080c252f482[m
Author: Vamsikrishna Bhagi <vamsikrishna.bhagi@oracle.com>
Date:   Tue Apr 28 15:17:37 2015 +0530

    WL#6940 Server version token and check
    
    Fix for compilation errors in builds
    without [1;31mperf[mormance schema.

[33mcommit 09f9f7dc3ba870c9d9b060a571dafbec7e9499f1[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 24 14:27:50 2015 +0200

    Fix for Bug#20954804
    
      'WATCHDOG DETECTS NO PROGRESS IN SEND THREAD AND KILLS DATANODE'
    
    Patch is a backport of 'part 3 of 3' of WL#7654:
    
      'Reduce lock contention in NDB mt-scheduler'
    
    That WL was originaly pushe dto Cluster-7.4 as a [1;31mperf[mormance
    optimization, but it has later turned out that it also
    fixed a potential deadlock between send- and worker-threads
    in the multithreaded scheduler.
    
    Pasted commit comment from original WL:
    
    ---------------------------------------
          Reduce lock contention on send_lock when using send threads.
    
          Previously multiple send threads could be invoked for
          handling sending to the same node. These would then
          compete for the same send_lock. Furthermore, while
          being blocked on the send lock, there could be available
          sending work to other nodes.
    
          This patch ensures that new send threads are not activated
          while there already is an active send thread assigned to
          a node. Neither would a node being handled by an active send
          thread be visible to other, already active, send threads.
          This is achived by not putting the node into the 'node-list'
          while it is assigned to a send thread.
    
          See comment in code how different states are now defined for
          handling this.

[33mcommit 39dc143d968242bb9f8271f6f8b82b4c9ac2fe56[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 24 14:27:50 2015 +0200

    Fix for Bug#20954804
    
      'WATCHDOG DETECTS NO PROGRESS IN SEND THREAD AND KILLS DATANODE'
    
    Patch is a backport of 'part 3 of 3' of WL#7654:
    
      'Reduce lock contention in NDB mt-scheduler'
    
    That WL was originaly pushe dto Cluster-7.4 as a [1;31mperf[mormance
    optimization, but it has later turned out that it also
    fixed a potential deadlock between send- and worker-threads
    in the multithreaded scheduler.
    
    Pasted commit comment from original WL:
    
    ---------------------------------------
          Reduce lock contention on send_lock when using send threads.
    
          Previously multiple send threads could be invoked for
          handling sending to the same node. These would then
          compete for the same send_lock. Furthermore, while
          being blocked on the send lock, there could be available
          sending work to other nodes.
    
          This patch ensures that new send threads are not activated
          while there already is an active send thread assigned to
          a node. Neither would a node being handled by an active send
          thread be visible to other, already active, send threads.
          This is achived by not putting the node into the 'node-list'
          while it is assigned to a send thread.
    
          See comment in code how different states are now defined for
          handling this.

[33mcommit ded3155def2ba3356017c958c49ff58c2cae1830[m
Author: Mark Leith <mark.leith@oracle.com>
Date:   Wed Apr 22 12:43:01 2015 +0100

    Bug#20902791 MYSQLDUMP DUMPS SYS_SCHEMA
    
    Do not dump the sys schema when using mysqldump --all-databases.
    
    Note: It is still possible to dump the sys schema individually (mysqldump --databases sys), as it may still be something that a user wants to do (given the sys.sys_config persistent table could be recovered for instance if the user wanted to). Further, the sys schema is not ignored for LOCK TABLES as with the [1;31mperf[mormance_schema and INFORMATION_SCHEMA, as the sys schema stores "regular" (InnoDB) objects, that option is still feasible.
    
    Reviewed-by: Bharathy X Satish <bharathy.x.satish@oracle.com>
    Reviewed-by: Georgi Kodinov <georgi.kodinov@oracle.com>
    RB: 8677

[33mcommit ffbf0c1a872b1bb5b7a789b264bdfd282b3d486d[m
Merge: 01b1b9be5b1 cc8c2b42474
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Mon Apr 20 13:09:09 2015 +0200

    Merge 7.4 -> 7.5
    
     - as of commit '8a7036400aa7bc478c303334885bc72ebc26882b'
    
    Conflicts:
            VERSION
            mysql-test/r/sp.result
            mysql-test/suite/[1;31mperf[mschema/r/start_server_low_digest.result
            mysql-test/suite/sys_vars/r/transaction_alloc_block_size_basic.result
            mysql-test/suite/sys_vars/r/transaction_prealloc_size_basic.result
            mysql-test/suite/sys_vars/t/transaction_alloc_block_size_basic.test
            mysql-test/suite/sys_vars/t/transaction_prealloc_size_basic.test
            mysql-test/t/sp.test
            packaging/WiX/CMakeLists.txt
            packaging/WiX/CPackWixConfig.cmake
            packaging/WiX/create_msi.cmake.in
            sql/item_func.cc
            sql/mysqld.cc
            sql/sql_class.h
            sql/sql_partition.cc
            sql/sql_select.cc
            storage/ndb/VERSION
            storage/ndb/src/ndbapi/NdbDictionaryImpl.hpp

[33mcommit a0ce1bdd2dce8094fc3b82d4bf4e6fcff1fd8c89[m
Merge: b3c8fc90223 e0e6ffc566e
Author: Bjorn Munch <bjorn.munch@oracle.com>
Date:   Thu Apr 9 10:55:47 2015 +0200

    Merge branch 'mysql-5.7.7-rc-release' into mysql-5.7
    
    Conflicts:
            mysql-test/r/show_variables.result
            mysql-test/suite/[1;31mperf[mschema/r/show_coverage.result
            mysql-test/suite/[1;31mperf[mschema/r/show_sanity.result
            sql/sql_show.cc
            storage/innobase/fil/fil0fil.cc

[33mcommit 51683776adb1e83d14635566724adfcaa29b3d4c[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Tue Apr 7 10:44:07 2015 +0200

    Bug#20755430 COST CONSTANT CACHE MUTEX NOT INSTRUMENTED BY PERFORMANCE SCHEMA
    
    The mutex used by the Cost Constant cache was not instrumented in
    [1;31mperf[mormance schema. This fix adds this mutex to the set of server
    mutexes instrumented in [1;31mperf[mormance schema.

[33mcommit 3517ebfd81ef288f5d64191bad3b2c8121ecb7e0[m
Merge: e7fb1ac32b6 d4e5370aebb
Author: Sunny Bains <Sunny.Bains@oracle.com>
Date:   Tue Apr 7 10:34:16 2015 +1000

    Merge branch 'mysql-5.7' of myrepo:mysql into mysql-5.7-wl7696
    
    Conflicts:
            mysql-test/suite/innodb/r/create_tablespace.result
            mysql-test/suite/innodb/t/create_tablespace.test
            mysql-test/suite/[1;31mperf[mschema/r/show_sanity.result

[33mcommit 300f50f659660886c3359c0f1cc455f2bd5ae3aa[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Apr 1 21:06:33 2015 +0200

    Bug#20811494 SHOW_COMPATIBILITY_56 = OFF INCOMPATIBLE WITH CONNECTORS
    
    Before this change, running a server with show_compatibility_56 = OFF
    caused failures in Connector/J (the java connector for MySQL)
    
    Root cause of failures is multiple.
    
    1) SHOW VARIABLES WHERE ...
    
    The java connector uses these queries,
    but the server no longer support them.
    
    2) SHOW VARIABLES output
    
    The output has changed from SESSION + GLOBAL variables
    to SESSION only variables.
    
    The java connector is not using SHOW GLOBAL VARIABLES,
    and is then missing some data.
    
    Both items are the result of the proposed deprecation
    related to SHOW_COMPATIBILITY_56 introduced in 5.7.6.
    
    With this fix, the restrictions are lifted,
    so that a server running with SHOW_COMPATIBILITY_56 = OFF
    behave in a way which is compatible with previous, established, usage.
    
    In particular:
    
    A)
    
    The deprecation of SHOW VARIABLES/STATUS WHERE is abandonned.
    WHERE clauses are fully supported, even with SHOW_COMPATIBILITY_56 = OFF.
    
    B)
    
    The table [1;31mperf[mormance_schema.session_variables
    which is the underlying implementation of SHOW VARIABLES
    has been changed to report:
    - SESSION variables (no change)
    - and GLOBAL variables (changed)
    
    C)
    
    Likewise, table [1;31mperf[mormance_schema.session_status
    reports both SESSION and GLOBAL status variables.
    
    As a result, the behavior of:
    - SHOW VARIABLES
    - SHOW STATUS
    is the same as the historical bahavior of MySQL,
    and in particular:
    - WHERE clauses are supported
    - both SESSION and GLOBAL data is reported.
    and this now happens both:
    - with SHOW_COMPATIBILITY_56 = ON
    - with SHOW_COMPATIBILITY_56 = OFF
    
    Note that the variable SHOW_COMPATIBILITY_56 itself still exists,
    but the scope controlled by this variable is reduced.

[33mcommit 22ba38218e1d76c24f69b5a5595ad3bf5933acb0[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Mar 30 11:57:46 2015 +0530

    Bug #20531208 COMPUTE INNODB REDO LOG BLOCK CHECKSUMS FASTER
    
    Problem:
    
    The redo log file is divided into equal sized blocks.  For each block, checksum
    is calculated and stored to verify its correctness.  The only algorithm that is
    supported is "innodb".  But the data files we support three checksum algorithms
    namely "none", "crc32" and "innodb".  We can do the same for redo log file
    blocks also.  This can improve [1;31mperf[mormance.
    
    Solution:
    
    Introduce a new system variable --innodb-log-checksum-algorithm, which takes
    same values as that of --innodb-checksum-algorithm.
    
    Note:
    
    This patch has been contributed by Laurynas Biveinis.
    
    rb#8155 approved by Sunny.

[33mcommit f06b514cce9f3b0062265fbeaeeb69d404f902c4[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Mar 25 14:56:43 2015 +0100

    Bug#19585938 Crash in get_full_func_mm_tree with null item_field->table_ref
    
    The problem may occur if we have a grouped query with a non-grouped
    subquery that contains a reference to an aggregate function, e.g. in
    the WHERE clause. The aggregate function must not reference any columns.
    
    Example query:
    
    SELECT (SELECT 1
            FROM t1
            WHERE SUM(1) < id
           )
    FROM t1
    GROUP BY col1+col2;
    
    The range optimizer is attempting to optimize the predicate SUM(1) < id.
    SUM(1) is represented by an Item_aggregate_ref object in get_mm_tree().
    Since aggregation is [1;31mperf[mormed in the outer query, this item is an outer
    reference. get_mm_tree() will then call get_full_func_mm_tree() with
    the real_item() of the Item_aggregate_ref object as predicand, which is
    an Item_field. This Item_field represents a field in the temporary table
    allocated for grouping the outer table. But since this table has no
    assigned TABLE_LIST object, trying to calculate the map() for this table
    fails.
    
    However, this seems to be a legacy issue. Before WL#7540 was pushed,
    table map was 1. But this is actually an outer reference, so it was
    wrongly seen as a local table. Actually, being an outer reference, this
    item is const during evaluation of the inner query, so this is not a
    candidate for range optimizer analysis at all.
    
    The fix is just to skip the call to get_full_func_mm_tree() if the
    argument representing the Item_aggregate_ref is an outer reference.
    
    The problem was also identified for other predicate types like BETWEEN.
    Fix has been provided for this too, together with test cases.
    
    (cherry picked from commit 6a71c3bf2c2f8f3e771173c38157fd42a6806cd3)

[33mcommit ab0715d163b44d23d986a4ac9aef2e91dac6ad81[m
Author: Mayank Prasad <mayank.prasad@oracle.com>
Date:   Thu Mar 26 23:14:43 2015 +0530

    Bug #20697446   COM_STMT_REPREPARE MISSING IN PERFORMANCE SCHEMA IMPLEMENTATION OF SHOW STATUS
    
    Issue :
     Information com_stmt_reprepare (counter) was not available using
     [1;31mperf[mormance_schema session/global status tables.
    
    Fix :
     Made this information available. Added com_stmt_reprepare information
     to be displayed in [1;31mperf[mormance_schema session/global status tables.

[33mcommit a3b832e2d348715f59ab28eb94d39822e2b2971d[m
Author: Nisha <nisha.gopalakrishnan@oracle.com>
Date:   Wed Mar 18 09:32:26 2015 +0530

    BUG#20106553: ALTER TABLE WHICH CHANGES INDEX COMMENT IS
                  NOT LONGER INPLACE/FAST OPERATION.
    
    Analysis:
    
    ALTER TABLE operations involving only changes to the INDEX
    comment is no longer INPLACE for MyISAM tables and requires
    index rebuild for InnoDB.
    
    The patch for bug fix BUG#19779365 reflects the modified
    index comment using ALTER TABLE operation by index rebuild
    which cannot be [1;31mperf[mormed using INPLACE algorithm. Hence
    ALTER TABLE operation involving changes only to the INDEX
    comment using INPLACE algorithm reports an error.
    
    Fix:
    
    Changing the index comment using ALTER TABLE operation
    is marked as a fast/INPLACE operation.

[33mcommit 02855476a86c801f03c60ad79ec47ee8a69de354[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Mar 9 17:37:26 2015 +0100

    Bug#20575529 PERFORMANCE_SCHEMA.GLOBAL_VARIABLES MYSTERY DEPRECATION
    WARNING
    
    The following query
      SELECT * from [1;31mperf[mormance_schema.global_variables;
    displays a deprecation warning related to the global variable 'sql_log_bin',
    because of special logic implemented for this very variable for backward
    compatibility reasons.
    
    See the resolution of
      Bug#67433 Using SET GLOBAL SQL_LOG_BIN should not be allowed
    for details.
    
    This fix filters out explicitly variable 'sql_log_bin'
    from table [1;31mperf[mormance_schema.global_variables because:
    - this variable is not intended to be global,
    - there is no need to preserve backward compatibility
    for this table since it is new, unlike the existing
    table INFORMATION_SCHEMA.global_variables.
    
    (cherry picked from commit 56f6154ed5ae5cb51f658000d4af2725d2129f55)

[33mcommit dbe3bd6a5e1cd0a9cec7f265c9c83d65a6079ff3[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Mar 9 17:40:04 2015 +0100

    Bug#20559828 SHOW GLOBAL VARIABLES WHERE NOT DEPRECATED IN EMBEDDED
    
    Before this fix, the command:
      SHOW GLOBAL VARIABLE WHERE ...
    printed a deprecation warning, inviting to use the [1;31mperf[mormance_schema tables
    instead.
    
    This warning was incorrect, and caused confusion for the embedded build.
    
    There are two separate changes that are independent:
    
    First, the syntax
      SHOW GLOBAL VARIABLE WHERE ... [LIKE ...]
    should be changed to simply
      SHOW GLOBAL VARIABLE [like ...]
    abandonning the WHERE clause syntax.
    This applies to all builds, including embedded.
    
    Secondly, the query
      SHOW GLOBAL VARIABLE [like ...]
    might be rewritten using the new [1;31mperf[mormance_schema tables instead.
    This is optional, and only applies to builds that include the
    [1;31mperf[mormance_schema (hence does not apply for the embedded build).
    
    The warning printed mixed the two changes.
    
    The deprecation only applies to the first part,
    dropping support for the WHERE clause in SHOW STATUS / SHOW VARIABLES.
    
    With this fix, the proper deprecation warning is now printed,
    and it is also printed for embedded builds.
    
    (cherry picked from commit 7323b2abfb98105f76336b81d1e010a0c0036c45)

[33mcommit aee6c5647735414325446eb82c260feb127bad9f[m
Author: Daogang.qu <bill.qu@oracle.com>
Date:   Wed Mar 11 11:03:05 2015 +0800

    Bug #19451053  CRASH AFTER DIRECT INSERT INTO MYSQL.GTID_EXECUTED TABLE
    
    When gtid_mode=on and binary log is off, the server automatically
    inserts the transaction's GTID into mysql.gtid_executed within
    the transaction. But if the GTID has already been inserted into
    the table by an explicit INSERT statement, the server crashes.
    
    After the fix, the server does not crash in above case. Push a
    warning to client if user is modifying the gtid_executed table
    explicitly. Ignore the duplicate key error and log a warning
    for it when writing transaction owned GTID into gtid_executed
    table implicitly within the transaction, we did not push a
    warning to client in the case, since it is slave SQL thread
    or worker sometimes.
    
    @ sql/binlog.cc
    We invoke the warn_on_modify_gtid_table(...) in two places to
    decrease the iterations to table list for improving [1;31mperf[mormance.

[33mcommit a29137ffdb16e9036e40e38e29580670ae91c475[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Mar 9 17:37:26 2015 +0100

    Bug#20575529 PERFORMANCE_SCHEMA.GLOBAL_VARIABLES MYSTERY DEPRECATION
    WARNING
    
    The following query
      SELECT * from [1;31mperf[mormance_schema.global_variables;
    displays a deprecation warning related to the global variable 'sql_log_bin',
    because of special logic implemented for this very variable for backward
    compatibility reasons.
    
    See the resolution of
      Bug#67433 Using SET GLOBAL SQL_LOG_BIN should not be allowed
    for details.
    
    This fix filters out explicitly variable 'sql_log_bin'
    from table [1;31mperf[mormance_schema.global_variables because:
    - this variable is not intended to be global,
    - there is no need to preserve backward compatibility
    for this table since it is new, unlike the existing
    table INFORMATION_SCHEMA.global_variables.

[33mcommit 7a1965aaab13591f173053f1492c5bcc77df08cc[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Mar 9 17:40:04 2015 +0100

    Bug#20559828 SHOW GLOBAL VARIABLES WHERE NOT DEPRECATED IN EMBEDDED
    
    Before this fix, the command:
      SHOW GLOBAL VARIABLE WHERE ...
    printed a deprecation warning, inviting to use the [1;31mperf[mormance_schema tables
    instead.
    
    This warning was incorrect, and caused confusion for the embedded build.
    
    There are two separate changes that are independent:
    
    First, the syntax
      SHOW GLOBAL VARIABLE WHERE ... [LIKE ...]
    should be changed to simply
      SHOW GLOBAL VARIABLE [like ...]
    abandonning the WHERE clause syntax.
    This applies to all builds, including embedded.
    
    Secondly, the query
      SHOW GLOBAL VARIABLE [like ...]
    might be rewritten using the new [1;31mperf[mormance_schema tables instead.
    This is optional, and only applies to builds that include the
    [1;31mperf[mormance_schema (hence does not apply for the embedded build).
    
    The warning printed mixed the two changes.
    
    The deprecation only applies to the first part,
    dropping support for the WHERE clause in SHOW STATUS / SHOW VARIABLES.
    
    With this fix, the proper deprecation warning is now printed,
    and it is also printed for embedded builds.

[33mcommit f4c37f7aea732763947980600c6882ec908a54a0[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Mon Feb 2 12:26:28 2015 +0200

    WL#6860 Binlogging XA-prepared transaction
    
    ChangeLog
    ---------
    Fri Mar  6 17:44:42 EET 2015
    
    Resolving conflicts at merging to 5.7.
    
    Fri Mar  6 13:59:13 EET 2015
    
    - fixing innodb transaction freeing that was missed out in a case of
      1phase xa through mysqlbinlog applier.  Restructuring an if to
      comply to innodb style, as earlier requested by Marko.  Fixing an
      assert, not dealing directly with the WL per him as well.
    
    Thu Mar  5 08:50:38 EET 2015
    
    - Compilation in innodb got broken on some platforms with the last patch. Fixed now.
    
    Wed Mar  4 19:13:52 EET 2015
    
    - XA COMMIT ONE PHASE at replaying from binlog made
      binlog.binlog_xa_prepared_disconnect to cause failures in other
      tests masquerading to bug19502202. Yet the reason turned out to be
      missed re-attachment of storage engine trans context, that matches
      previous detachment.
      Fixed and the test is augmented to catch the failure alone.
      Small changes are done to innodb.cc and handler.cc.
    
    Tue Mar  3 21:35:28 EET 2015
    
    - fixing more non-deterministic cases that PB2 reveals
    
      1.  main.xa: I left disconnect to run wo/ generally needed synchronisation
                   with connection that runs XA recover. I did not like to contrain
                   the test with P_S dependency. Added warnings and todo, if the warning
                   will come true;
      2.  rpl_xa_survive_crash_debug: found a reason of result mismatch, fixed *it*, but
                                      the test fails sporadically. I suspect --error 2013;
      3.  sys_vars.innodb_support_xa_func: fixed
    
    - ending cleanup requested by Dmitry at today's review of xa.cc and sql_plugin.*
      changes. Hunks from the latter pair of files are relocated onto xa.*.
    - adding support and test for mysqlbinlog recovery for XA-COMMIT-ONE-PHASE;
    
    Tue Mar  3 16:59:14 EET 2015
    
    - determinism in binlog_xa_prepared_disconnect (gtid on/off), and
      rpl_xa_survive_crash_debug (binlog format, synchronisation)
    - fixes to Bug20608551 (LiBing's contribution).
    
    Tue Mar  3 11:59:43 EET 2015
    
    - Changes are done to support "mysqlbinlog | mysql | mysqld" recovery,
      making couple of log_event.cc static functions to turn to public into
      sql_plugin.cc,h and applier_reset_xa_trans() is defined in xa.cc.
      binlog_xa_prepared.test is extended to cover the case;
    - XA COMMIT/ROLLBACK vs GTID_NEXT testing is added;
    - Bug20616249 fixes are incorporated with the WL patch;
    - cleanup in done in xa.cc,h
    
    Fri Feb 27 18:30:28 EET 2015
    
    - Incorrect ordering of rollback to binlog and SE is fixed. XA-rollback
      is written first now. That must be the most probable reason of bug20616249;
    - set @@gtid_next and following XA-commit,rollback worked incorrectly in few ways:
      a. mysql.gtid_executed on slave missed records
      b. set @@gtid_next could not be set to another "manual" value 'cos
         of uncleared status of previous assignment done to the first logging phase
         of XA START..PREPARE
      Tests are extend to cover b.
    - Crash simulation in the middle of XA-rollback is added to prove the existing
      policy of survival of the prepared XA is not gone, as well as to prove
      correct logging ordering (see above).
    
    Tue Feb 24 12:46:33 EET 2015
    
    Updating few test result files:
     - XA basic framework main.xa complained about mismatch
       expectedly 'cos of having disconnect of a prepared transaction;
     - binlog_xa_prepared.test is slightly cleaned not to depend
       on preceeding tests that may contribute to binlog which is
       printed out to show new valid logging pattern.
    Fixing compilation on win;
    
    Todo: rpl_gtids_table_disable_binlog_on_slave seems to have caught
    mysql.gtid_executed updating by two-phased logged XA, under investigation.
    
    Mon Feb 23 21:24:48 EET 2015
    
    Fixing failed tests on hundson.
    
    i_binlog.binlog_loose_XA_trans.test  did not expect new XA-START query, corrected;
    binlog.binlog_xa_prepared_disconnect failed 'cos of a previous test, its
      correctness verification clause is refined to account that;
    
    rpl.rpl_gtids_table_disable_binlog_on_slave did not expect two-phase
      logging, GTID_NEXT handler did not let to change the value after
      XA-PREPARE which must be done each time a "manual" GTID_NEXT
      prepands XA-START.  check_super_outside_prepared_trx_outside_sf() is
      introduced to refine a former GTID_NEXT check function.  Also a
      minor bug in updating GTID_EXECUTED is fixed when XA COMMIT is
      invoked from another connection (through "recovery" interface).  The
      test was extensively cleaned from all way using (incl copy-pasting)
      integer constants.
    
    compilation on embedded is fixed.
    
    Fri Feb 20 20:06:24 EET 2015
    
    Addressing Shiv's final notes dealing with cleanup
    and undoing a result file.
    
    Fri Feb 20 13:22:11 EET 2015
    
    Cleanup is do to
     - replacing xid methods with more appropriate, adding a new m_is_binlogged
       to the XID_STATE constructor initializer list;
     - dismantling st_replace_native_trx_args;
     - restoring help--no-win results.
    
    Thu Feb 19 19:05:49 EET 2015
    
    --xa_survive_disconnect option is removed;
    changes around m_native_trx_ptr that is turned into
    THD::ha_data.ha_ptr_backup to provide one-to-many association THD to
    
    SE (multiple engines in on XA transaction) rather than 1-to-1;
    removing unneeded tests, renaming in remained;
    fixing rpl_trx_boundary_parser_warning (announced but slipped from
    yesterday patch);
    
    various consmetics: empty new lines for coding standard (announced but
    slipped from yesterday patch); renaming, few cosmentics
    
    Wed Feb 18 20:08:20 EET 2015
    
    Cosmetics requested by Luis and Dmitry: renaming, adding empty line as separator throughout
    the patch,
    rpl_trx_boundary_parser_warning references to explicit log_pos
    are elminated 'cos the test started failing due to FD's grown one byte by the fixes.
    
    todo: sort out need of --xa_survive_disconnect option, raised by Luis. Upon that I'll take
    care of tests that are new-option sensitive.
    
    Wed Feb 18 09:43:15 EET 2015
    
    A bug found out in that updating slave_relay_log_info table at XA_prepare handling
    ended up in locking the table for following transactions.
    It's fixed with refining flush_info() conditions to chose the file repo branch
    for the method invocation in this XA_prepare case.
    
    Fri Feb 13 20:25:36 EET 2015
    
    Addressed
    
     - the final Marko's notes (tests related)
     - few notes from Dmitry
     - renaming a new transaction_ctx member
    
     - cleanup in trans_xa_prepare() as well as removed open questions
       to refer to new reported bug.
       In the bug report it's suggested to correct the error code and specify details
       of prepare failure.
       We won't automatically add XA ROLLBACK in this unlikely case.
    
    Thu Feb 12 14:36:29 EET 2015
    
    xa_prepared_binlog_off is fixed in the assert part
    which could not run correctly due to log_bin is OFF in this test.
    Cosmetic changes in few tests, and picking up Marko's assert.
    
    Fri Feb  6 18:13:37 EET 2015
    
    Refining the last commit's solution to syncronize the exiting connection's
    THD::cleanup with a follower connection through SELECTing from P_S.threads.
    That makes the test available wo/ DBUG, but requirs P_S.
    
    xa.h's start_recovery_xa() is made of two arguments. The new one
    carries m_is_binlogged value.
    
    Fri Feb  6 12:06:57 EET 2015
    
    Changes in tests motivated by
    1.  It turned that there's no SQL way of learning by the killer or by
        an external connection that a prepared trx of one that is supposed
        to be gone is available.  Methods based on `show processlist' or
        SELECT from I_S do *not* work!  The exiting connect is not in the
        result list, but its THD may not have been passed through
        cleanup() method.
        I worked it around with dbug_sync. Here and in other place incl other tests.
    2.  gtid asserts added to main.xa_prepared_binlog_off
        and binlog.binlog_prepared_disconnect
    
    XID::m_is_binlogged flagging is fixed. In the previous vesion the member
    may be left uninitialized.
    
    Thu Feb  5 11:44:41 EET 2015
    
    Fixing some tests (not full addressing yet) per Shiv's notes;
    making 2nd of two ser_buf_size declarations compilable everywhere,
    the issue was spotted by Dmitry.
    da
    todo: address the rest of sticking out notes, by Marko and Shiv,
          in the following commit.
    
    Wed Feb  4 22:22:26 EET 2015
    
    opt_xa_prepared_rollback_at_disconnect is renamed to
    opt_xa_survive_disconnect;
    due to semantics got inverted, its value had to be inverted in sources;
    innodb cleanup is done;
    binlog_xa_prepared_do_and_restart.inc refactored to introduce
    another sourced include, extended;
    tests renaming is done per reviewers and the renaming above.
    
    Wed Feb  4 14:40:20 EET 2015
    
    Asserting the new and old trx properties in
    innodb_replace_trx_in_thd. It's also revealed a flaw in arguments
    passing (fixed: see changes around get_slave_trx_orig());
    fixed visibility in control_events.h per Dmitry's request, and
    compilation of static const XID_t::ser_buf_size;
    [1;31mperf[mected binlog_xa_prepared_do_and_restart.inc to invoke
    a new
      extra/binlog_tests/binlog_xa_prepare_connection.inc
    to initiate connections with certain properties.
    
    Tue Feb  3 15:38:43 EET 2015
    
    Fixing Marko's style complains, extending tests per his suggestion.
    An dbug-sync assert was found in innodb, explained and worked around.
    Potential non-determinism of XA RECOVER in tests like
    binlog_xa_prepared_disconnect is eliminated.
    
    Mon Feb  2 12:25:28 EET 2015
    
    Cleanup, merge with trunk.
    
    Addressed remained notes by Marko.
    Futher extension to xa_prepared_binlog_off and binlog_xa_prepared_disconnect
    is done to disconnect prepared XA with the server shutdown.
    
    Thu Jan 29 15:01:48 EET 2015
    
    Replaced --sleep 1 in binlog_xa_prepared and eliminated the same race
    in rpl_xa_old_logging;
    
    Found an issue and corrected XA COMMIT ONE PHASE. It's fixed
    with making XA_prepare_log_event to record the value of ONE_PHASE option.
    It case the new bool member is true the event commits the current
    group.
    Added tests for this case (rpl_xa_old,new_logging).
    
    Fixed compilation issue due to a incomplete renaming done in innodb;
    
    Extended rpl_xa_old,new_logging with random XID generation and made
    more XA transaction to disconnect.
    
    --
    TODO:
    
    Self-review, and
    hand to reviewers to finish their work.
    
    Tue Jan 27 22:35:06 EET 2015
    
    Addressing all Marko's notes.
    Adding up 2pc deregistration into the "read-only" XA prepared
    branch of innobase_close_connection() (Marko, plz assess);
    Extending testing base with adding more tests to
    extra/binlog_tests/binlog_xa_prepared.test, a common source for either
    --skip-log-bin and --log-bin top-level test files of
    xa_prepared_binlog_off and binlog_xa_prepared_disconnect.
    
    There're already notes from Dmitry who left me them on IM,
    to be addressed in tomorrow patch.
    
    TODO:
    1. replace --sleep 1 made in binlog_xa_prepared.test
       to avoid race of connection close and XA COMMIT from another connection.
    2. eliminate the same race in rpl_xa_old_logging
    3. Address Dmitry's comments and get it fully reviewed.
    
    Tue Jan 27 10:10:44 EET 2015
    
    After --skip-log-bin support opened by the previous patch an issue was revealed
    in that "read_only" prepared XA transation cleanup was not conducted properly
    to assert in innodb
         assert ( rw_trx_list.len >= n_prepared_trx).
    The "read-only" trx must've been rolled back and get to proper state.
    This patch corrects that issue.
    
    Few review notes (by Marko) are addressed as well. These include inlines
    and white space consmetics.
    
    Fri Jan 23 14:30:43 EET 2015
    
    The final notes are cleared.
    
    Fri Jan 23 12:44:04 EET 2015
    
    Addressed Neha's documentation and cleanup related comments.
    
    Thu Jan 22 12:51:15 EET 2015
    
    Addressed all points by Neha (related to WL7440).
    
    Wed Jan 21 17:05:33 EET 2015
    
    Merging with libbinlogevent is done (uint4korr() replaced).
    
    Wed Jan 22
    
    This patch merges few WL:s pushed to the trunk
    as well addresses few notes made by reviewers.
    The external WL that caused changes are wl7440, wl7592, trx_boundary_parser.
    
    This patch does not fix two items present on RB:7755 atm:
    
    TODO/FIXME: trx_t::is_recovered life time to settle down.
    
    The WL6860 agenda
    -----------------
    
    The WL addresses long time standing limitation of XA support
    in the server. When the server runs with replication (binary log) turn on
    prepared XA transaction had to be rolled back at disconnection
    Bug#11745231 (bugs.mysql.com/12161) contains the whole story of complains.
    
    This worklog adds support for XA-transactions to make them sustaining
    connection close or server restart without any harm to replication.
    For backward compatibility a new server option/global-read-only-var is introduced:
    --xa_prepared_rollback_at_disconnect
    to have the default value to comply with the rollback "old" behavior.
    
    The Worklog patch consists of
    
    - binary logging extension to write prepared XA and its Commit or Rollback decision
      as separate group of events into the binary log.
      That makes XA-binlogging possibly interleaving yet without any harm to
      data consistency after replaying on the slave.
    
    - slave applier extension to handle XA-prepared and its termination (Commit or Rollback) event
    
    - extension to XA recovery implementation in that connection closing leaves
      a prepared XA in the transaction cache as well as specially marked in
      Innodb. Such prepared XA can be discovered and terminated as the user
      wishes, as well as by the slave applier
    
    - extension to handlerton interface to add up interface allowing
      attach and detach a SE "internal" transaction from the server level transaction
      handle
    
    - augmentment to connection close logics in Innodb as well as
      changes to maintain disconnected transaction's state sane to survive
      the server restart.
    
    Conflicts:
            sql/xa.h

[33mcommit d0fae771666d16e85ae24be805864ae1a0c40e30[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Mar 4 09:57:08 2015 +0100

    Fix for Bug#20408733:
    
     ScanOperations allocate too much memory for receive buffers
    
    A scan operation, both a plain single table scan, and a
    'query scan' used by pushed join, stores the result set
    in a buffer. This maximum size of this buffer is calculated
    and preallocated before the scan operation is started.
    
    This buffer may consume considerable amount of memory, in
    some cases we have observed a 2Gb buffer footprint in a test
    executing 100 parallel scans. This was for a tiny 2-node,
    non-mt config (2 fragments), and the memory consumption will scale
    linearly with more fragments
    
    There are several root causes for this problem:
    
    1. Result rows are 'unpacked' to full NdbRecord format before they
       are stored in the buffer. If only some of the table columns are
       selected from a table, there will be lots of empty (wasted) space
       in the buffer.
    2. Due to the 'unpacked' buffer format, varchar/varbinary columns has
       to be allocated for the max size defined for the columns.
    3. The 'BatchByteSize' and 'MaxScanBatchSize' is not taken into consideration
       as a limiting factor when calculating max buffer size.
    4  As buffer size is scaled by 'BatchSize', the problem became worse
       with 7.2 where the default was raised from 64 to 256
    
    This patch refactors the NdbReceiver class to buffer result rows
    in its 'packed' format. The NdbReceiver internal 'class NdbReceiverBuffer'
    is introduced in order to better seperate NdbReceiver handling of
    buffered (packed) and non buffered result set.
    
    Buffered scan result rows are now not unpacked until they are
    made the 'current row', either by NdbReceiver::getNextRow(), or
    NdbReceiver::getRow(<row>).
    
    The NdbReceiver code which convert from the transporters 'packed'
    format, into either row format described with a NdbRecord,
    or into NdbRecAttr values, are refactored into several
    NdbReceiver::unpack<foo>() methods. The same code is then
    used to unpack either a buffered or unbuffered result row.
    
    NdbReceiver::result_bufsize() has been added to calculate the
    size of the (packed) result buffer which has to be allocated.
    This calculation now also takes the config variables
    'BatchByteSize' and 'MaxScanBatchSize' as a limiting factor
    of the required buffer size.
    
    Unpacking of NdbRecAttr data from either a NdbScanOperation or
    a NdbQueryOperation has also been streamlined: Previously the
    'dataPtr' for each RecAttr were fetched from NdbReceiver and then
    'unpacked' one by one. The new method
    NdbReceiver::get_AttrValues(NdbRecAttr* rec_attr_list) leaves this
    operation entirely to NdbRecord, and as an addition side effect
    makes the same unpack code reusable inside NdbReceiver.
    
    In addition a general cleanup of the NdbReceiver class has been
    [1;31mperf[mormed, some of them are:
    
    - Removed the code for handling the old NdbRecaAttr variant of
      NdbReceiver::execKEYINFO20(). That code relied on
      NdbReceiver::m_rows[] to have been allocated and set up, and m_rows[]
      is not allocated *anywere* - So that code is efficiently defuncted.
      Checked back to 7.0, without finding aby trace of that.
    
    - Removed 'useRec' argument to NdbReceiver::init(). The same
      info can be deducted by checking if the later NdbRecord* arg to
      NdbReceiver::do_setup_ndbrecord() '!= NULL'. Furthermore,
      setting 'useRec' in ::init(), and not specifying a NdbRecord in
      ::do_setup_ndbrecord(), would set the NdbReceiver into an
      inconsistent state.
    
    - Cleaned up the 'class NdbRecord' decl itself by removing
      unused, and redundant member variables. Mainly due to the
      same info now being available from 'class NdbReceiverBuffer'.

[33mcommit 052d342d16723eb126991a11d2ea9916f9fc9407[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Feb 19 16:09:01 2015 +0100

    BUG#18360716: ERRORS IN MTS WORKER THREADS REPORTED WRONGLY FOR ANONYMOUS TRANSACTIONS
    
    Before this patch, error messages from MTS would use an empty string
    when referring to the GTID of an anonymous transaction. The correct
    way to refer to anonymous transactions is to use the text "ANONYMOUS".
    Empty string is generally used when referring to no GTID at all
    (e.g., when GTID_EXECUTED is empty); a single transaction that does
    not have a GTID is different from no transaction.
    
    In order to prevent similar bugs in the future, we also simplify the
    locking code for Gtid_specification::to_string so that it is very easy
    to use the existing function that generates the string for "ANONYMOUS".
    
    This is already tested by rpl_[1;31mperf[mschema_execute_status_by_worker, so
    we can just update the result file and don't need to write a new test.
    
    @sql/rpl_rli_pdb.cc
    - Use the new, simplified version of
      Gtid_specification::to_string.  This ensures that we
      use "ANONYMOUS" for anonymous transactions, rather than
      just "".
    
    @sql/rpl_gtid.h
    - Improve some (related) comments.
    
    @mysql-test/suite/rpl/r/rpl_[1;31mperf[mschema_applier_status_by_worker.result
    - Update result file.
    
    @mysql-test/suite/rpl/t/rpl_[1;31mperf[mschema_applier_status_by_worker.test
    - Add bug reference.

[33mcommit d133fbc1b7d9060c5d296ea15149a446a938b7c2[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Tue Feb 17 09:46:15 2015 +0100

    WL#6378: New Data Dictionary. Fix failing test [1;31mperf[mschema.transaction_gtid.
    
    The test [1;31mperf[mschema.transaction_gtid executes transactions and checks
    that the GTID field of
    [1;31mperf[mormance_schema.events_transactions_[current|history] is as
    expected.
    
    The following statements did not add any row to these tables before
    WL#6378, but changed in WL#6378 so that they add a row to these
    tables:
    
    - TRUNCATE
    - SELECT stored_function()
    - SET AUTOCOMMIT
    
    These statements were used in the test, and thus the change in the
    worklog caused the test to fail with result mismatch.
    
    We fix the TRUNCATE case by executing it on a different connection and
    using a WHERE clause when reading the tables so that it only selects
    rows from the connection where the transactions we are checking are
    executed.
    
    We fix the SELECT stored_function() case in the same
    way. Additionally, we simplify the SELECT and use a .inc file instead
    of a stored function.
    
    We fix the SET AUTOCOMMIT case by switching the order between SET
    AUTOCOMMIT and clearing the tables, so that the tables are cleared
    after SET AUTOCOMMIT.
    
    @mysql-test/include/execute_*_sync_point.inc
    - Allow sourcing a file rather than executing a statement on the
      auxiliary connection.
    
    @mysql-test/suite/[1;31mperf[mschema/include/reset_transaction_gtid.test
    - Use the auxiliary connection to clear the tables rather than
      the connection where transactions are executed.
    - Since we use the correct connection, no need to set GTID_NEXT.
    
    @mysql-test/suite/[1;31mperf[mschema/include/show_transaction_gtid.test
    - Avoid using a stored function.
    - Document the file.
    
    @mysql-test/suite/[1;31mperf[mschema/t/transaction_gtid.test
    - Use have_debug_sync.inc, not have_debug.inc.
    - Remove the now unused stored functions.
    - Invoke reset_transaction_gtid.inc after SET AUTOCOMMIT.

[33mcommit 42dfb943e363c7a94a6f6c0cb28197e09532b29d[m
Merge: 1ebd7de50a2 74d2dfb9cc4
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Feb 17 02:07:15 2015 +0100

    Merge branch 'mysql-5.6' into mysql-5.7
    
    NULL merge.
    
    Conflicts:
            mysql-test/suite/[1;31mperf[mschema/t/ortho_iter.test
            sql/sql_acl.cc

[33mcommit 74160058148d3e267ec5f7059c3832dd4531db76[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Mon Feb 16 18:27:07 2015 +0100

    WL#7589: Follow-up patch requested by QA. We now disable [1;31mperf[mormance_schema digests
    properly in the test.

[33mcommit ca02877576b4c8a434cc93fb3af9dbc9e52154f1[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Feb 13 16:00:24 2015 +0100

    Bug#20455184: Assertion failed: join_cond in optimizer.cc
    
    The problem query [1;31mperf[morms a NATURAL LEFT JOIN between a derived table
    and a regular table, before applying a NATURAL LEFT JOIN with a
    second regular table. The first natural join has no common columns in
    the two tables, so there is no generated join condition. In such case,
    store_top_level_join_columns() (line 8541) will add TRUE as join
    condition to the inner table of the outer join, since an outer join
    should, by convention, contain a non-empty join condition.
    
    But the criterion for adding a join condition is:
    
        if (table_ref_2->outer_join &&
            !table_ref_1->join_cond() && !table_ref_2->join_cond())
          table_ref_2->set_join_cond(new Item_int((longlong) 1,1)); // Always true.
    
    As we see, it also checks whether the join condition of the outer table
    is non-empty, which is not the case here, since a WHERE clause of
    the derived table has been promoted to become the "join condition"
    of this table. Hence, the TRUE condition is not appended to the join,
    and subsequent code fails to recognize this as a proper outer join,
    which eventually causes an assertion.
    
    The fix is to modify the test to exclude the reference to
    table_ref_1->join_cond(), since this is not related to the outer join.

[33mcommit 42bac3be4a1a5e315c0de8314de96500c4d0a9a8[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Mon Feb 9 02:35:13 2015 +0100

    WL#6629 Performance Schema, Status Variables: MTR fixes: embedded
    
    - main.plugin: Suppress deprecation warnings that only apply to non-embedded
    - [1;31mperf[mschema.show_sanity: LOCKED_CONNECTS

[33mcommit 010443059a660f2ed94d499c2a65f03a464f23ce[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Sun Feb 8 23:50:52 2015 +0100

    WL#6629 Performance Schema, Status Variables: Misc. MTR fixes
    
    Miscellaneous testcase fixes:
    - audit_log.audit_log_debug_bugs
    - sys_vars.sql_log_bin_basic
    - [1;31mperf[mschema.memory_aggregate_32bit
    - [1;31mperf[mschema.show_sanity (NDB)

[33mcommit e65c7cd523d0435f640e8e7435646d7ed164ba06[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Fri Feb 6 12:41:27 2015 +0000

    Post-push fix for BUG#19982543
    
    Fixed a result of a [1;31mperf[mormance schema test case.
    
    Wrapped P_S mutex_key uses with HAVE_PSI_INTERFACE.

[33mcommit b086fdac74f7e55259e487d383f2ebd7aa4128a1[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Feb 6 16:22:17 2015 +0100

    Bug#19678930: WRONG STACK SIZE CALCULATION LEADS TO
                  STACK OVERFLOW IN PINBOX ALLOCATOR
    
    The problem was that the allocator used by the lock-free container
    implementations misused alloca() when scanning for objects which
    could be safely deallocated. The code tried to calculate how much
    memory could safely be allocated on the stack for qsort based
    pre-sort, but these calculations were not accurate.
    
    This could manifest itself as stack overflow errors with high
    number of concurrent connections to the server. (e.g. 8000
    connections with default 256Kb stack size).
    
    This patch fixes the problem by removing the use of alloca
    and replacing qsort approach by a linear scan through all
    pointers (pins) owned by each thread. We remove each
    such pointer from the list of pointers to objects the current
    thread has marked as unused (purgatory). At the end we have
    a list of pointers to objects not in use by any thread. Since
    there are few active (non-null) pointers at any given time,
    this turns out to be quite efficent.
    
    The patch also fixes a bug in the code determining when
    scanning for unused objects should be [1;31mperf[mormed. Instead
    of doing this scan only at certain intervals (for [1;31mperf[m reasons),
    the old code did this scan almost always.
    
    Micro-benchmarks show that these two changes give a
    a clear [1;31mperf[mormance improvement over the old approach.
    
    Some #include directives in the LF code was also changed
    so that they only use <> for system headers. This reduces
    the likelyhood of conflicts between system headers and
    MySQL headers with generic names (e.g. hash.h).
    
    The patch also reduces usage of thread-local storage and is a
    step towards implementing WL#6817.
    
    Based on patch written by Dmitry Lenev.

[33mcommit bc70e43d575ba37e711696e138269fa9c5462a3b[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Fri Nov 28 01:08:05 2014 +0100

    WL#7083 step 8.2. Tests: Fix existing test cases.
    
    @suite/innodb/t/innodb_bug60049.test
    - The error is expected after dropping mysql system table
      innodb_table_stats, so we can safely suppress it, because
      mysql.innodb_table_stats is dropped in the test.
    
    @mysql-test/suite/[1;31mperf[mschema/r/threads_mysql.result
    - Display the compression thread info when gtid_mode is OFF, since
      MySQL server starts the compression thread even if gtid_mode is OFF
      in the wl7083.
    
    @mysql-test/suite/rpl/t/rpl_gtids_table_disable_log_slave_updates.test
    - Remove a possible race from the test (FLUSH LOGS may already have
      signalled complete_compression when the wait starts). Instead,
      simplify it to avoid using debug sync points when forcing a
      gtid_executed table compression.

[33mcommit bb20908369174f8371c9c00446db21e00b9260e6[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Jan 29 22:50:09 2015 +0100

    WL#7083 step 3.3. GTID_MODE settable: Make SET GTID_NEXT safe for concurrent SET GTID_MODE.
    
    Now that gtid_mode is dynamic, we must reinforce 'set gtid_next' so that
    it is safe against concurrent modifications of gtid_mode. Thus, 'check
    gtid_next' and 'set gtid_next' must be done atomically while holding
    global_sid_lock.rdlock. This is not possible using the common
    ON_CHECK/ON_UPDATE functions in sys_vars; instead we need to implement a
    specialized Sys_vars::session_update member function.
    
    - Move logic for setting thd->variables.gtid_next away from log_event.cc
    and sys_vars.cc and into gtid_acquire_ownership_single.
    
    - Rename gtid_acquire_ownership to set_gtid_next.
    
    - Move logic for checking that gtid_next is compatible with gtid_next
    into set_gtid_next. This was previously in both
    Gtid_log_event::do_apply_event, gtid_reacquire_ownership_if_anonymous,
    and check_gtid_next.
    
    - Make set gtid_next work correctly when there are concurrent set
    gtid_mode, i.e., hold global_sid_lock during all critical operations
    without releasing it between.
    
    - Remove unused, ifdef'ed-out code for checking compatibility between
    gtid_next and gtid_next_list.
    
    @sql/log_event.cc
    - Call set_gtid_next instead of setting the value directly.
    
    @sql/rpl_gtid.h
    - Replace gtid_acquire_ownership_single by set_gtid_next and add a comment.
    
    @sql/rpl_gtid_execution.cc:set_gtid_next
    - Replace gtid_acquire_ownership_single by set_gtid_next.
    - Make set_gtid_next check compatibility with gtid_mode while holding global_sid_lock.
    - Clarify logic for acquiring and releasing locks, use 'lock_count' to keep track of which locks are held at any point.
    
    @sql/rpl_gtid_execution.cc: other changes
    - Update comment
    - Make gtid_reacquire_ownership_if_anonymous use set_gtid_next instead of checking and setting gtid_next directly.
    - Add assertion in gtid_pre_statement_checks to ensure the invariant that nothing should be owned when a statement begins, if gtid_next=automatic.
    
    @sql/sys_vars.cc, @sql/sys_vars.h
    - Remove check function for gtid_next. Instead make the update function use set_gtid_next, which will [1;31mperf[morm the appropriate checks.
    - Rename Sys_var_gtid_specification -> Sys_var_gtid_next since this is now very specific to gtid_next.
    
    @mysql-test/suite/binlog/t/binlog_gtid_errors.test
    - Now that sanity checks for gtid_next are done in the update function rather than the check function, some error codes change.
    
    @mysql-test/suite/sys_vars/t/gtid_next_basic.test
    - Add tests to check that gtid_next can be set to DEFAULT but not NULL.

[33mcommit 6dc19dc78c35c75202669989e8baaa6894d4b6bf[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Thu Feb 5 14:16:43 2015 +0530

    - Bug#20422255: SOME RQG TESTS FAIL WITH TIMEKEEPER TIMEOUT (STATUS 110) SINCE 2015-01-06
    
      - After switching to use InnoDB as default SE (WL#6737) there is increase in
        debug execution time for some of the test-cases.
      - Good-part is release-execution time has been reduced (we saw 25% improvement
        for the test-case mentioned in the bug)
      - Increase in debug time came from the extra debug check that InnoDB [1;31mperf[morms
        so in short nothing wrong with the increase time.
      - But in general course given that we need to run pb2 on daily basis it would
        be good to reduce the debug-execution-time. If not for all cases at-least
        for tables created by Optimizer.
      - With that aim we evaluated the code and found that buf_validate is top-most
        costly function in the case mentioned and probably in all such cases.
        To suppress cost of this we have turned it off for temp-tablespace
        (of-course with some configurability.)
    
      Reviewed by: Sunny Bains <sunny.bains@oracle.com>
      RB: 7934

[33mcommit 3dc07ac0020301baf82bf29088a1effd9183fb9d[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Tue Feb 3 22:11:12 2015 +0000

    BUG#20456932: P_S REPLICATION_GROUP_MEMBERS TABLE IS MISSING MEMBER_PORT COLUMN
    
    Performance schema replication_group_members table was missing
    member_port column, without this column it was not possible for
    external utilities to know which MySQL servers are members of the
    group.
    
    The above problem was fixed by adding the member_port column to
    [1;31mperf[mormance_schema.replication_group_members table.
    Also, to follow other replication tables names, member_address
    column was renamed to member_host.

[33mcommit eda45b182941b149933e866fab6247494dd9a437[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Tue Feb 3 22:09:01 2015 +0000

    BUG#20455917: GROUP REPLICATION APPLIER CHANNEL IS NOT LISTED ON ALL REPLICATION P_S TABLES
    
    Group Replication applier channel was not being listed on all
    replication [1;31mperf[mormance schema tables, what was causing some status
    values not to be accessible.
    This was happening because Group Replication applier channel doesn't
    have a regular Master_info object, since it doesn't have a IO
    thread. Applier data comes from group communication toolkit and not
    from a point-to-point connection from a master.
    
    On this patch we ensure that Group Replication applier channel is
    listed on all replication [1;31mperf[mormance schema tables by adding the
    channel with a fake master hostname "<NULL>".

[33mcommit f8d205a3d2d3043e2d62a4e90f5c5d89ada1c07b[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Feb 2 11:22:20 2015 +0200

    Fix determinism in alter_table_stage_progress.test
    
    The [1;31mperf[mormance_schema.events_stages_history_long contains events from
    all threads and it can be polluted from previous tests that were
    executed.
    
    Thus use [1;31mperf[mormance_schema.events_stages_history instead which only
    contains stages from the currently executing thread.
    
    But by default only the last 10 events are displayed in
    [1;31mperf[mormance_schema.events_stages_history and the events this test is
    looking for (LIKE 'stage/innodb/alter table%') could be flushed down the
    drain before it gets a chance too peek them. Thus increment the default
    value of 10 to 1000 via alter_table_stage_progress-master.opt (the limit
    is a readonly option and cannot be increased with SET GLOBAL).
    
    Reviewed-by:    Marko (via IM)

[33mcommit 6f99d663bbad00ff72935901ef9937438f8a44d8[m
Author: Tiago Jorge <tiago.jorge@oracle.com>
Date:   Fri Jan 30 16:19:35 2015 +0000

    BUG#20445758 - GROUP REPLICATION PLUGIN SHOULD NOT PASS INTERNAL POINTERS TO P_S TABLES
    
    Description
    ============
    
    Group Replication stats and status are available to users trough
    [1;31mperf[mormance schema tables.
    
    Those [1;31mperf[mormance schema tables are defined on server, and when user
    selects them the tables will ask the needed data to plugin using the
    plugin interface.
    
    The problem is that some of the returned info from plugin are internal
    pointers that caller can modify, to avoid that all data must be returned
    as a copy instead of internal pointers.
    
    Solution
    ============
    
    All returned and default values to P_S tables now belong
    to the Group Replication plugin, which returns them via output pointer
    values, making a snapshot of the current values.

[33mcommit 8ef56039b73fc4b8d38ea0a9e41dc55d849b5fa9[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Nov 13 13:20:31 2014 +0100

    WL#7592 step 12. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix GTIDs in P_S tables.
    
    In this patch we correct the GTID shown in the GTID columns of
    [1;31mperf[mormance_schema.events_transactions_current and
    [1;31mperf[mormance_schema.events_transactions_history.
    
    Background:
    The GTID is changed during the lifetime of a transaction as follows:
    
    - On a master, the transaction is "AUTOMATIC" while executing. Only when
      it commits is it assigned a GTID of the form UUID:NUMBER.
    
    - On a slave, the transaction is assigned a GTID before it starts to
      execute.
    
    - When GTID_MODE = OFF, transactions use the special GTID "ANONYMOUS"
      rather than "UUID:NUMBER".
    
    This was not reflected correctly in the GTID column of these
    [1;31mperf[mormance_schema tables. Among other things, the history table could
    contain 'AUTOMATIC'. This should never happen since it makes it
    impossible to identify the transaction.
    
    This issue is not directly related to WL#7592. However, it showed up as
    a test failure in [1;31mperf[mschema.transaction after a refactoring that was
    part of WL#7592. Therefore, we fix it in order to make tests pass for
    WL#7592.
    
    @mysql-test/include/execute_at_sync_point.inc
    - New auxiliary test file.
    
    @mysql-test/suite/[1;31mperf[mschema/include/reset_transaction_gtid.inc
    - Auxiliary test file used by the new test.
    
    @mysql-test/suite/[1;31mperf[mschema/include/show_transaction_gtid.inc
    - Auxiliary test file used by the new test.
    
    @mysql-test/suite/[1;31mperf[mschema/r/transaction.result
    - Update result file because test file changed.
    
    @mysql-test/suite/[1;31mperf[mschema/r/transaction_gtid.result
    - Result file for new test case.
    
    @mysql-test/suite/[1;31mperf[mschema/t/transaction.test
    - Since we need a more elaborate testing of what GTID value shows up in
      the P_S table, moved it to a different file.
    
    @mysql-test/suite/[1;31mperf[mschema/t/transaction_gtid.test
    - New test file to verify the new behavior.
    
    @sql/binlog.cc
    - Move the code for setting GTID for P_S away from this place and into
      generate_automatic_gtid.
    - Introduce a debug_sync point used by the new transaction_gtid.test.
    
    @sql/handler.cc
    - Set the GTID for P_S when the transaction starts.
    
    @sql/rpl_gtid.h
    - Forward declaration of new function.
    
    @sql/rpl_gtid_execution.cc
    - Set the GTID for P_S also for anonymous transactions. This is for the
      case when the GTID is specified by SET GTID_NEXT or
      Gtid_log_event::do_apply_event.
    - Add new auxiliary function gtid_set_[1;31mperf[mormance_schema_values to call
      the P_S macro MYSQL_SET_TRANSACTION_GTID with the correct arguments.
    
    @sql/rpl_gtid_state.cc
    - Set the GTID for P_S when generating automatic GTID.
    
    @sql/transaction.cc
    - Set the GTID for P_S when the transaction starts.
    
    @storage/[1;31mperf[mschema/pfs.cc
    - Remove pfs->m_gtid_set.
    
    @storage/[1;31mperf[mschema/pfs_events_transactions.h
    - Remove pfs->m_gtid_set, since it is not needed.
    
    @storage/[1;31mperf[mschema/table_events_transactions.cc
    - Remove pfs->m_gtid_set, since it is not needed.
    - Always rely on Gtid_specification::to_string to generate a correct
      string.
    - Update/clarify a comment.

[33mcommit 5ff65df0595094fda32eb4fdc32a39a8d7bd8433[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:02:16 2014 +0100

    WL#7592 step 1. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Small simplifications.
    
    In this patch we just make a few very small simplifications to improve
    DBUG output, add assertions, etc. These are not directly related to the
    worklog, but were improvements made as part of debugging the feature.
    
    This does not change behavior of the server, so it is not possible to
    have a test case for this.
    
    The following changes have been done:
    - Assert that Gtid.set() is only used with a valid sidno and gno.
    - Assert that gtid_acquire_ownership is only called when gtid_next is
      set correctly.
    - Add DBUG_ENTER to some functions.
    - Improve DBUG_PRINT output.
    - Encapsulate all places where THD::owned_gtid is modified.
    - Make Gtid.set() assert that sidno and gno are valid.
    - Gtid.is_null() and Gtid.empty() seem to have been added concurrently
      by different patches. Both have the same purpose, so we unify them
      into Gtid.is_empty(). Also added assert ensure that gno>0 if and only
      if sidno>0; this is consistent with the protocol for THD::owned_gtid
      and with all other places that use this struct.
    - Move forward declarations for functions defined in rpl_rli_pdb.cc from
      rpl_slave.cc to rpl_rli_pdb.h.
    - Make it possible to use rpl_reconnect.inc also when
      rpl_default_connections.inc has not been used previously.
    - Make rpl_init.inc autodetect the number of servers correctly also in
      the case when
      $rpl_topology=='none'.
    - Improve some comments.
    - Remove Xid_log_event::is_valid, which was left behind by mistake
      after WL#7440.
    
    @mysql-test/include/rpl_default_connections.inc
    - Make it possible for mysql-test/include/rpl_reconnect.inc to know
      whether this script has been called.
    
    @mysql-test/include/rpl_init.inc
    - Make rpl_init.inc autodetect that only one server is needed if
      $rpl_topology=='none'
    
    @mysql-test/include/rpl_reconnect.inc
    - Make this script work also if rpl_default_connections.inc has not been
      used.
    
    @mysql-test/include/show_rpl_debug_info.inc
    - Add GTID_MODE.
    
    @sql/binlog.cc
    - Rename Gtid::empty to Gtid::is_empty.
    - Improve DBUG output.
    - Add comment to clarify where FLUSH LOGS is implemented.
    
    @sql/handler.cc
    - Rename Gtid::is_null to Gtid::is_empty.
    
    @sql/log_event.cc
    - Remove strange multiplication by worker id in debug sleep.
    - Add DBUG_ENTER in User_var_log_event::do_apply_event.
    - Fix some comments.
    - Remove Xid_log_event::is_valid, which was left behind by mistake
      after WL#7440.
    
    @sql/log_event.h
    - Use builtin true/false instead of custom defined ones.
    
    @sql/rpl_binlog_sender.cc
    - Improve DBUG output.
    - Add a comment.
    
    @sql/rpl_context.cc
    - Use THD::owned_gtid_set only inside #ifdef HAVE_GTID_NEXT_LIST.
    
    @sql/rpl_gtid.h
    - Make Gtid.set() assert that sidno and gno are valid.
    - Replace Gtid::is_null() and Gtid::empty() by new function
    Gtid::is_empty().
    - Correct comments for return status for from Gtid_set::Gtid_set and
      Gtid_set::add_gtid_encoding.
    - Correct comment for Gtid_state::get_automatic_gno.
    
    @sql/rpl_gtid_execution.cc
    - Add assertions in gtid_acquire_ownership_single.
    - Remove useless return value from skip_statement().
    - Move call to skip_statement() out from DBUG_RETURN(), since function
      calls inside DBUG_RETURN will make the debug trace look wrong if the
      called function uses DBUG_ENTER/DBUG_RETURN too.
    
    @sql/rpl_gtid_state.cc
    - Change group to gtid in debug printout.
    - Add missing semicolon after ifdef-ed out line.
    - Use THD::clear_owned_gtid, instead of duplicating the code inside
      clear_owned_gtid.
    - Rename Gtid::is_null to Gtid::is_empty.
    
    @sql/rpl_rli_pdb.cc
    - Correct DBUG_ENTER text.
    - Use int64 instead of longlong for logical timestamps (it is int64
      everywhere else).
    - Use existing symbolic constant instead of hard-coded number.
    
    @sql/rpl_rli_pdb.h
    - Moved these forward declarations to the file where they belong.
    
    @sql/rpl_slave.cc
    - Move two forward declarations to the appropriate header file.
    - Rename Gtid::empty to Gtid::is_empty.
    - Remove Gtid_specification::clear. Now using set_automatic instead.
    
    @sql/sql_base.cc
    - Do not call decide_logging_format from DBUG_RETURN. Doing that causes
      the debug trace to look like decide_logging_format was called after
      the return statement, i.e., one level up in the call stack from where
      it was actually called.
    
    @sql/sql_class.cc
    - Compile out owned_gtid_set since it is only used by the
      compiled out code for gtid_next_list.
    - Use Gtid::clear instead of clearing the values manually.
    
    @sql/sql_class.h
    - Compile out owned_gtid_set since it is only used by the
      compiled out code for gtid_next_list.
    - Use Gtid::clear instead of setting the value directly.
    
    @sql/sys_vars.cc
    - Add DBUG_ENTER/DBUG_RETURN to update_gtid_next.
    
    @storage/[1;31mperf[mschema/pfs.cc
    - Remove Gtid_specification::clear. Now using set_automatic instead.
    
    @storage/[1;31mperf[mschema/table_replication_execute_status_by_worker.cc
    - Make worker->currently_executing_gtid use type==AUTOMATIC_GROUP to
      indicate that no GTID is executing, rather than type==GTID_GROUP,
      sidno=0, gno=0.
    
    @libbinlogevents/include/control_events.h
    - Remove comment that was completely wrong.
    
    @libbinlogevents/src/control_events.cpp
    - Remove explanation of what GTIDs are (better do that in the manual
      and/or where GTIDs are defined).

[33mcommit f46329044f8618212923bdf52e15d5b464201edc[m
Author: binsu <bin.x.su@oracle.com>
Date:   Thu Jan 8 11:10:20 2015 +0800

    Bug#20125466 - ASSERT IN FIL_IO BUF_READ_PAGE_LOW ROW_UPDATE_FOR_MYSQL
    
    After a cascade update, we will update the statstics for referenced tables.
    And we don't acquire any lock as we want better [1;31mperf[mormance. So there
    could be some 'DROP TABLE' thread which would marks all the root page numbers
    of the referenced table's indexes as FIL_NULL. So the updater thread
    could read an root page with number of FIL_NULL and read the page out of range.
    
    We don't have MDL locks for referenced tables in the server layer now,
    so we only protect the conflicts of cascade update and 'DROP TABLE' by
    our dict_operation_lock. But this is released before the statistics updated.
    Considering the [1;31mperf[mormance issue, we could skip the lock but do a
    double check when the statistics updater accesses the index.
    
    Reviewed-by: Jimmy <jimmy.yang@oracle.com>
    RB: 7735

[33mcommit acb1193b3753dad0931d8eef45e8a348823bc9c2[m
Merge: 20a884833f6 6ca9b51d2f7
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Jan 15 11:52:15 2015 +0200

    Merge commit 'e90f0f0' into mysql-trunk-wl5889
    
    * commit 'e90f0f0':
      WL#7868: InnoDB: Improvements around flushing for proper [1;31mperf[mormance
    
    Conflicts:
            storage/innobase/log/log0log.cc

[33mcommit 6ca9b51d2f749b10b9de3fcf3c0b15a056a4df1c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jan 15 13:16:38 2015 +0900

    WL#7868: InnoDB: Improvements around flushing for proper [1;31mperf[mormance
    
    1: Implementing improvements to the adaptive flushing algorithm
    
    1-a: should consider the distribution about the oldest LSN of the pages
    in the
    end of the flush_list
    
    1-b: should consider flushing balance for each buffer pool instances
    
    2: Setting a thread priority for the page_cleaner (if the platform
    allows and
    authorized to set)
    
    3: Proper flush waiting when the max modified LSN age is around
    max_modified_age_sync
    
    3': should block the checkpoint LSN overwritten
    
    Reviewed-by: Sunny Bains <Sunny.Bains@oracle.com>
    Reviewed-by: Shaohua Wang <shaohua.wang@oracle.com>
    
    RB: 7444 (4881 for bzr)

[33mcommit efb386b21fb79ff39841962f0428eb808d634033[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Jan 13 08:25:26 2015 +0100

    Bug #20281249 USE MALLOC_ALLOCATOR IN PRIORITY_QUEUE
    
    Currently the container used internally in class Priority_queue
    must be default-constructible.
    Extend Priority_queue, so that we can use Malloc_allocator and track memory
    usage in [1;31mperf[mormance schema.

[33mcommit 3f4abb52fbfe9118fc94a4b60876c16e5aa2e0e0[m
Author: sneha modi <sneha.modi@oracle.com>
Date:   Tue Jan 13 08:20:59 2015 +0100

    WL#6747: InnoDB: make fill factor settable
    
    We changed the hard-coded merge threshold (BTR_CUR_PAGE_COMPRESS_LIMIT
    (50%))
    to be settable for each index.
    
    * merge threshold : If the data amount in the page becomes below the
    threshold
    when deleting row or updating to shorter row, attempts to merge with
    neighbor page.
    
    Even if the page was merged,
    the page become near 100% filled and might cause split page soon.
    If the threshold is set lower, the merged page farther from 100% and
    might not cause
    split page soon. Reducing opportunity for page merge/split benefit
    [1;31mperf[mormance.
    
    New syntax: "MERGE_THRESHOLD=" specifying in comment clause of DDL is
    parsed
    (capital sensitive; should be capital letter)
    
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>
    
    RB: 7434 (5154 for bzr)

[33mcommit 521789e9f05bc62438223fa67d45b271401efd3c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Tue Jan 13 15:30:43 2015 +0900

    WL#6747: InnoDB: make fill factor settable
    
    We changed the hard-coded merge threshold (BTR_CUR_PAGE_COMPRESS_LIMIT
    (50%))
    to be settable for each index.
    
    * merge threshold : If the data amount in the page becomes below the
    threshold
    when deleting row or updating to shorter row, attempts to merge with
    neighbor page.
    
    Even if the page was merged,
    the page become near 100% filled and might cause split page soon.
    If the threshold is set lower, the merged page farther from 100% and
    might not cause
    split page soon. Reducing opportunity for page merge/split benefit
    [1;31mperf[mormance.
    
    New syntax: "MERGE_THRESHOLD=" specifying in comment clause of DDL is
    parsed
    (capital sensitive; should be capital letter)
    
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>
    
    RB: 7434 (5154 for bzr)

[33mcommit 82fdbe9ede04af7392565a149c4e2542f51aea16[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jan 9 15:59:29 2015 +0100

    Bug #20281249 USE MALLOC_ALLOCATOR IN PRIORITY_QUEUE
    
    Currently the container used internally in class Priority_queue
    must be default-constructible.
    Extend Priority_queue, so that we can use Malloc_allocator and track memory
    usage in [1;31mperf[mormance schema.

[33mcommit 384e07f2a8e780db80382597b35f98bb43c1ad3f[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Jan 8 12:23:07 2015 +0100

    WL#8190: Refactor low-level thread handling
    
    This patch refactors the low-level code for handling OS threads.
    It unifies the API for creating and joining threads so that there
    no longer are separate functions for Windows and non-Windows
    platforms.
    
    This also makes sure that thread creation on Windows is always
    tracked properly by [1;31mperf[mormance schema.
    
    The patch replaces macros with static inline functions or typedefs
    where appropriate and does general refactoring of thread handling
    and mysys initialization code.

[33mcommit 84b4c29e00694cf24bed6b390ec4afff354017b0[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jan 7 12:20:56 2015 +0200

    Account memory allocated in handler0alter.cc
    
    WL#5889 Add InnoDB events to Performance Schema's Event Stage table
    
    Add handler0alter.cc to the list of files that use UT_NEW() or
    UT_NEW_NOKEY(). This way memory allocated from that file will show up as
    'memory/innodb/handler0alter' rather than 'memory/innodb/other' in
    [1;31mperf[mormance_schema.memory_summary_global_by_event_name.event_name.

[33mcommit 120c81d7b94b7b9810adfa8c7f10e8e67876a997[m
Merge: bf1b8b5f033 e6582a81c7c
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Tue Jan 6 08:01:37 2015 +0530

     WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for
     MySQL Optimizer
    
     Currently Optimizer uses MyISAM as default Storage Engine for creating
     intermediate temporary table. In order to dependency on MyISAM we have
     already enabled InnoDB as option for this. With this WL we have also
     tunned it to make its [1;31mperf[mormance onpar with MyISAM.
    
     Reviewed by: Yasufumi Kinoshita (yasufumi.kinoshita@oracle.com)
     Reviewed by: Bin Su (bin.su@oracle.com)
     RB: 7216

[33mcommit a86c6024677943f4ecaa4075e45e25678517c61e[m
Author: David.Zhao <david.zhao@oracle.com>
Date:   Mon Jan 5 16:56:19 2015 +0800

    WL#7420 Geometry Collection Support
    
    1. Simplify set operation result if it's a geometry collection (GC) or multi-geometry
       that has one component only, by directly returning the component.
    2. Simplify GIS function handling of GC arguments when both arguments have one
       component only by using the single components directly instead.
    3. Improve [1;31mperf[mormance of GC component merge algorithm, using Boost.Geometry.Rtree;
    4. Improve [1;31mperf[mormance of GIS relation checks and set operations given GC components,
       using Boost.Geometry.Rtree
    4. Correctly handle GC arguments that are empty or nested.
    
    Bugs fixed in this worklog:
    
    BUG#20085563
    BUG#20111542
    BUG#20187460
    BUG#20202913
    BUG#20211491
    BUG#20211639 - Boost Geometry bug
    BUG#20266956

[33mcommit 5a473a621d4149b4163c396998557ee759f8313f[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Nov 28 14:57:54 2014 +0100

    Bug#19505175 REGRESSION IN Q21 OF DBT3 TEST FOR WL7339
    
    WL#7339 started to use more correct records per key estimates from
    InnoDB. This caused the cost estimate for the original query plan
    to become four times higher than previous, which caused another
    query plan to be selected.
    
    The new selected query plan does a table scan on the orders table.
    This table has 1.5 million records. On this table we have the
    following condition:
    
      orders.o_orderstatus = 'F'
    
    This is used for calculating the condition filter effect for this
    table. With the current guestimate for equality conditions of 0.005,
    this caused the estimated number of partial rows to be produced from
    this table to be only 7500. In reality, 730.000 rows were produced.
    
    The cause for this very wrong estimate is that the o_orderstatus
    column only contains three distinct values and almost half of the
    records have the value 'F'.
    
    To make the condition filter produce a better (more conservative)
    estimate for cases like this and to reduce the likelihood of
    similar regressions, the fix for this problem is to increase the
    condition filter constant for equality estimates to 0.1. With this
    we estimate that 1 of 10 records will pass the filter (instead of
    only 1 out of 200).
    
    Changes in tests:
    
    @ mysql-test/r/compress.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/derived.result
       Mostly changes to filtered estimates. One change in query plan.
       This query now gets the same plan as it had before WL#6635.
    @ mysql-test/r/ds_mrr-big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/eq_range_idx_stat.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/explain_for_connection_rqg_json.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_for_connection_rqg_trad.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_other.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_small.result
       Mostly changes to filtered estimates. One query has
       changes to its query plan: changes from using FirstMatch
       to do materialization.
    @ mysql-test/t/filter_single_col_idx_small.test
       Added test to verify that the filter estimate for basic
       filter constants is not less than one row
    @ mysql-test/r/greedy_optimizer.result
       Mostly changes to filtered and cost estimates. Eight queries
       has changes in query plans. Four of these changes back to
       the query plan they had before WL#6335 was pushed. The four
       last had similar changes in query plans as for when WL#6635 was
       pushed.
    @ mysql-test/r/greedy_search.result
       Mostly changes to filtered estimates. Two queries has
       changes to its query plan. One of these are returned to
       what it was before WL#6635 was pushed. The other has
       similar changes to as what was introduced by WL#6335.
       The number of partial query plans for some queries are
       increased. With the exception of one of these, all
       new numbers are below what it was before WL#6335.
       The one the has a very high increas in the number of
       partial query plans, changes from 22201 to 5799004.
       Before WL#6635 this query considered 735518 partial
       query plans.
    @ mysql-test/r/group_by.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap_hash.result
       One change in query plan. The new plan is more similar
       to what the query plan was before WL#6635 was pushed. It has
       the same join order but uses table scan on the second
       table instead of ref access as it did before WL#6635.
    @ mysql-test/r/index_merge_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_intersect_dml.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_myisam.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_none.result
       Changes in filtered, rows and estimates in explain output.
    @ mysql-test/r/innodb_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bkaunique.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_nojb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_none.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/named_pipe.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/negation_elimination.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/order_by_all.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_icp_mrr.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_none.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/partition_locking.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/partition_pruning.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr_cost.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/shm.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl_compress.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_sj_all.result
      Mostly changes to filtered, rows and cost estimates. Three queries
      has plan changes. Two of these returns to what the plan was before
      WL#6635. The last has changes that makes the plan look more like
      it was before WL#6635 but not identical.
    @ mysql-test/r/subquery_sj_all_bka.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Five of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_mat.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Six of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_nosj.result
      Changes to filtered and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka_nixbnl.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bkaunique.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subselect_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/type_temporal_fractional.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/wl6711_heap_to_disk.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/innodb/r/innodb_lock_wait_timeout_1.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/general2_no_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/general2_ps_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_no_prot.result
      Changes in filtered and rows estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_ps_prot.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_no_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_myisam.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/[1;31mperf[mschema/r/batch_table_io_func.result
      Changes in filtered estimate in explain output.
    @ sql/item.h
      Change the value for the condition filter constant COND_FILTER_EQUALITY
      from 0.005 to 0.1.
    @ unittest/gunit/item_filter-t.cc
      Change in unit test for condition filter for IN lists:
      Reduced from having six values in the IN list to four
      values. The reason is that with six values in the
      IN list the calculated condition filter will be larger
      than 0.5 and then rounded down to 0.5.

[33mcommit 5d3fe2bed1cd6ca2dc3a1534b34671ee01b6cedd[m
Author: Namit Sharma <namit.sharma@oracle.com>
Date:   Fri Nov 28 19:09:46 2014 +0530

    Bug#20083366 - RPL.RPL_PERFSCHEMA_APPLIER_STATUS FAILS IN TEST ASSERTION
    
    Problem:
    An assert condition to check COUNT_TRANSACTIONS_RETRIES in
    rpl.rpl_[1;31mperf[mschema_applier_status failed. The reason for this was improper
    clean up by previous test - rpl_mts_database_transaction_retry.test.
    It leaves COUNT_TRANSACTIONS_RETRIES value in [1;31mperf[mormance_schema.replication_applier_status = 4 .
    This has a potential to cause assert conditions on this status in subsequent running tests to fail.
    
    Fix:
    Restart the server at the end of the test rpl.rpl_mts_database_transaction_retry
    thereby clearing value of COUNT_TRANSACTIONS_RETRIES.

[33mcommit b43e7c82bdce4ca188facc097539d228394d9f85[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Dec 16 21:02:48 2014 +0100

    Bug#20032861 DROP DATABASE PERFORMANCE_SCHEMA DROPS IT ON ALL SQL NODES
    
     - Suppress the warning which occurs(on the other mysqld) when mysql_upgrade runs and tries to create
      the [1;31mperf[mormance_schema database(which hasn't been dropped because it has local tables).
     - this is the same supression as ndbinfo_upgrade.test

[33mcommit 60b559444866783696001d4eda78b0bb965e7aac[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Mon Dec 15 09:45:02 2014 +0000

    BUG#19634753 WL1697: IF SQL_SLAVE_SKIP_COUNTER > 0, IO_THREAD(S) ARE
                 NOT ABLE TO START
    
    Problem:
    =======
    
    sql_slave_skip_counter global variable with a value greater than 0 is
    not allowing IO threads to start.
    
    Analysis:
    ========
    
    sql_slave_skip_counter is a global variable that affects SQL thread
    behavior only.
    
    When using MSR, it is allowed to start only one SQL thread if the value
    of sql_slave_skip_counter is greater than zero.
    
    The slave startup is [1;31mperf[morming the sql_slave_skip_counter check even
    if only IO threads were asked to be started up.
    
    Fix:
    ===
    
    Only [1;31mperf[morms the sql_slave_skip_counter check during slave startup
    if the server was asked to start a SQL thread.

[33mcommit f24d63e37257b9a202c63e920a55cbc926114580[m
Author: david <david.zhao@oracle.com>
Date:   Mon Dec 1 17:38:48 2014 +0800

    Bug#20073459 GIS PERFORMANCE ISSUE: TOO MANY REALLOCS
    
    Geometry data in practical use cases often are at least hundreds of bytes, so
    we should use more aggressive allocation increments i.e. this version
    
             int String::reserve(size_t space_needed, size_t grow_by)
    
    to reserve memory space for GIS data.
    This function is dedicated for GIS data, it allocates memory by bigger
    chunks, should be called by GIS code. Now when calling this function, we grow
    by 512 bytes normally, and only grow by smaller steps if we know for sure we
    are reaching the end of the geometry data.
    
    We should not use the int String::reserve(size_t space_needed) version as some
    of the GIS code does currently, otherwise we would be reallocating the geometry
    memory buffer multiple times incrementally for a single geometry when
    generating/producing the geometry data piece by piece, which is a lot of
    overhead.
    
    Since this is a [1;31mperf[mormance enhancement, no test cases are added. Existing
    GIS tests can verify the correctness of this change.

[33mcommit 5422990495d27b96f1305946d2c98a24de2122a2[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Nov 17 11:48:22 2014 +0100

    Bug#19912474 - MYSQL_EMBEDDED FAILS WHEN LINKER SELECTS WRONG VERSION OF
    HANDLER CONSTRUCTOR
    
    Before this fix, class handler contained members defined conditionally
    with #ifdef HAVE_PSI_TABLE_INTERFACE.
    
    Even when the members defined like this are private,
    changing the binary layout of the class causes build issues
    when linking code compiled with/without the [1;31mperf[mormance schema
    instrumentation.
    
    This was found while building the embedded server,
    but the same issue also exists when compiling a server
    and storage engine plugins with different options.
    
    The fix is to always define the attributes needed
    by the [1;31mperf[mormance_schema table io instrumentation,
    even when these attributes are not used.
    
    A consequence is that the structure PSI_table_locker_state
    is now always defined in psi.h

[33mcommit b96785c675e9162e3c56d10ee9a3a8af06c09c4e[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu Nov 13 13:56:46 2014 +0100

    Enable the ndbcluster test suite for MySQL Server with NDB
    
     - Change the current logic in mtr.pl which detects and enables testing
       of ndbcluster to automatically turn on testing of the 'ndbcluster'
       suite when it's been detected that
      1) This is MySQL Server(no ndb-Y.Y.Y in version)
      2) ndbcluster is supported(i.e MySQL Server compiled with NDB).
      3) Full ndbcluster testing has not been requested(with --ndb|--include-ndbcluster)
      4) ndbcluster has not been skipped(with --skip-ndb[cluster])
    
     - Add the new 'ndbcluster' suite and implement basic.test
    
     - Intention of the 'ndbcluster' suite is the [1;31mperf[morm basic integration check
       of MySQL Server compiled with NDB.
    
     Example test of MySQL Server with NDB:
      $bld_trunk_with_ndb> cmake ../trunk -DWITH_NDBCLUSTER_STORAGE_ENGINE=1
      $bld_trunk_with_ndb> make
      $bld_trunk_with_ndb/mysql-test> ./mtr
       <snip>
      Checking supported features...
      - enabling ndbcluster(for integration checks)
      <snip>
      ndbcluster.basic                            [ pass ]
      <snip>
    
      Alternatively:
    
      $bld_trunk_with_ndb/mysql-test> ./mtr basic
      $bld_trunk_with_ndb/mysql-test> ./mtr ndbcluster.basic
      $bld_trunk_with_ndb/mysql-test> ./mtr --ndb
       ^run all ndb* suites in addition to the new 'ndbcluster'

[33mcommit 0b5c5bfe76b3a0a76faa6507684d9ebf60343d3a[m
Merge: 1b8f2017d57 4f1953af7fa
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Nov 6 19:37:49 2014 -0800

    bug#19846392
    Merge from 7.3 to 7.4
    
    Avoid segv when closing session factory with sessions still active
    
    ClusterConnectionImpl.java
      Cleanly shut down transactions in progress when close is called
        for each ndb, close the transaction if it is not active
          if the transaction is active, throw an exception to the user
        for each ndb, close (delete) the underlying Ndb object
        release each NdbRecord
        close (delete) the Ndb that manages NdbRecords
      Prevent new NdbRecord objects from being created while close is in progress
    
    ClusterTransactionImpl.java
      Prevent new operations from starting while close is in progress
    
    DbImpl.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    DbImplForNdbRecord.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    Bundle.properties
      Add message "No more operations can be [1;31mperf[mormed while this Db is closing"
      Add message "Cannot close a transaction while it is active."

[33mcommit 4f1953af7facffff639fb57fe8f5756189428550[m
Merge: 1029255fcc1 451fddc1339
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Nov 6 19:26:37 2014 -0800

    bug#19846392
    Merge from 7.2 to 7.3
    Avoid segv when closing session factory with sessions still active
    
    ClusterConnectionImpl.java
      Cleanly shut down transactions in progress when close is called
        for each ndb, close the transaction if it is not active
          if the transaction is active, throw an exception to the user
        for each ndb, close (delete) the underlying Ndb object
        release each NdbRecord
        close (delete) the Ndb that manages NdbRecords
      Prevent new NdbRecord objects from being created while close is in progress
    
    ClusterTransactionImpl.java
      Prevent new operations from starting while close is in progress
    
    DbImpl.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    DbImplForNdbRecord.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    Bundle.properties
      Add message "No more operations can be [1;31mperf[mormed while this Db is closing"
      Add message "Cannot close a transaction while it is active."

[33mcommit 451fddc133951a52c1faa9249af36ab82ae4c11b[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Nov 6 16:49:40 2014 -0800

    Avoid segv when closing session factory with sessions still active
    
    ClusterConnectionImpl.java
      Cleanly shut down transactions in progress when close is called
        for each ndb, close the transaction if it is not active
          if the transaction is active, throw an exception to the user
        for each ndb, close (delete) the underlying Ndb object
        release each NdbRecord
        close (delete) the Ndb that manages NdbRecords
      Prevent new NdbRecord objects from being created while close is in progress
    
    ClusterTransactionImpl.java
      Prevent new operations from starting while close is in progress
    
    DbImpl.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    DbImplForNdbRecord.java
      Maintain a "closing" flag set during cluster connection close
      Maintain a reference to the current transaction
      Prevent close of transaction if transaction is active
    
    Bundle.properties
      Add message "No more operations can be [1;31mperf[mormed while this Db is closing"
      Add message "Cannot close a transaction while it is active."

[33mcommit 24f10455d9a9ed87bab35a693f3b2f226a089750[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Nov 6 18:32:52 2014 +0900

    Follow up of
    revno: 9202
    Bug#19803497 : SIGNAL 11 IN INNODB.INNODB_BUFFER_POOL_RESIZE_DEBUG
    
    [1;31mperf[mschema.sxlock_func test should skip to check instruments that depend on compiling options.
    And the new intrdouced "wait/synch/sxlock/innodb/buf_chunk_map_latch" also should be excluded,
    because it is debug build only.

[33mcommit c60ac300ad2e5d1396694fb9f642b78b10bc3c93[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Nov 4 10:28:26 2014 +0800

    Bug#19424075    WRITE/SYNC REDO LOG BEFORE FLUSH THREAD CACHE TO BINLOG
    
    According to the crash recovery logic of mysql server, we only
    need to guarantee that write/sync prepared transaction to InnoDB
    redo log before writing to binary log. And prepared records of
    transactions were not committed into InnoDB redo log in a group.
    
    To improve [1;31mperf[mormance, write/sync prepared records of transactions
    to InnoDB redo log in a group right before writing to binary log
    during binlog group commit flush stage.

[33mcommit db7b0c188577ef4eaef4fec5e4f66e85f6a3559d[m
Author: matthias.leich@oracle.com <>
Date:   Fri Oct 31 16:31:41 2014 +0100

    Fix for the new test [1;31mperf[mschema.bad_option which
    - failed on WIN systematic
    - was introduced by the push to mysql-trunk
      Revno 9158 matthias.leich@oracle.com 2014-10-30

[33mcommit fc23bee27cde59c0085c81f2143e7eb229e9b053[m
Merge: 813a0266f00 0e50255a218
Author: matthias.leich@oracle.com <>
Date:   Thu Oct 30 19:27:30 2014 +0100

    Fix for Bug #12430653 PERFSCHEMA.BAD_OPTION_* TESTS FAILS UNDER HIGH LOAD
    1. Unite
       - all BAD_OPTION_* tests which tended to sporadic failures
       - all remaining BAD_OPTION_* tests which have the same
         architecture like the weak one before
       into the new test PERFSCHEMA.BAD_OPTION
    2. Fix the weakness which caused the sporadic failures in PB.
    3. Remove the old PERFSCHEMA.BAD_OPTION_* TESTS
    4. Correct [1;31mperf[mschema/t/disabled.def

[33mcommit 0e50255a218467e12a91e213d3f7e7ff1154fff2[m
Author: matthias.leich@oracle.com <>
Date:   Thu Oct 30 18:38:54 2014 +0100

    Fix for Bug #12430653 PERFSCHEMA.BAD_OPTION_* TESTS FAILS UNDER HIGH LOAD
    1. Unite
       - all BAD_OPTION_* tests which tended to sporadic failures
       - all remaining BAD_OPTION_* tests which have the same
         architecture like the weak one before
       into the new test PERFSCHEMA.BAD_OPTION
    2. Fix the weakness which caused the sporadic failures in PB.
    3. Remove the old PERFSCHEMA.BAD_OPTION_* TESTS
    4. Correct [1;31mperf[mschema/t/disabled.def

[33mcommit 25cc59a1fedd8f9f73aa0fd898454427c4aff59e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Oct 16 13:23:24 2014 +0200

    Fix for bug#18352514
    
       STRUCT MEMBERS IN MT-SCHEDULER ARE NOT CACHELINE ALIGNED AS ASSUMED
    
    A lot of [1;31mperf[mormance tuning has been [1;31mperf[mormed inside the
    multi threaded scheduler to optimize the cache behavior of its
    internal data structures.
    
    Sub-members in these structures has been placed such that thread local
    members doesn't 'overflow' onto a cacheline possible being accessed by
    another thread. Where required, extra padding bytes has been inserted
    to isolate cachelines owned, or shared, by different threads.
    Thus avoiding entire cache line to be invalidated if another thread
    write into a cacheline not entirely owned by itself.
    
    Micro benchmarking has previously proved that such
    optimization improved the [1;31mperf[mormance several percent.
    
    This entire optimization depends on that the global
    'struct thr_repository' instance itself starts at
    a cache line aligned base address *and* that the compiler
    doesn't rearrange or add extra padding to the scheduler
    struct.
    
    It turns out that the above prerequisits are not guaranteed
    or checked by the code. Thus these cacheline optimization
    has only worked when the global thr_repository instance by coincidence
    ended up being cacheline aligned. Furthermore, on 64bit build the compiler
    also added extra padding words in the 'struct thr_safe_pool' such that
    it broke our own attempt to pad it to a cacheline aligned size.
    
    This fix ensures that 'g_thr_repository' is constructed '(via placement new)
    on a cacheline aligned address. Furthermore, asserts are added in the
    constructors to verify cacheline aligned adresses where such are assumed
    by design.

[33mcommit 9c8b7f32405f6fa727e78256bf84f84afa4839ae[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Oct 16 12:29:19 2014 +0200

    Fix for bug#19661543
    
       RACE CONDITION BETWEEN ::PERFORMRECEIVE AND ::UPDATE_CONNECTIONS()
    
    Fix a regression introduced by WL#3860 (ATC patches). That WL
    removed the TransporterFacade::theMutexPtr which implictly
    provided protection between concurent calls of ::[1;31mperf[mormReceive()
    and ::update_connections().
    
    Fix is in two parts:
    
    Part1, TransporterReceiverWatchdog.patch:
    
    Adds a DEBUG watchdog which will assert that there are no
    concurrent calls to ::[1;31mperf[mormReceive() and ::update_connections().
    Doesn't fix anything, except guarding against reintroducing the
    same problem later.
    Running MTR tests with this patch will crash rather rapidly on >= 7.3
    Part1 is pushed 7.1 ->
    
    Part2: The real fix:
    This patch moves ::update_connections() calls inside do_poll()
    where it is now done (serialized) in a loop together with
    ::[1;31mperf[mormReceive(). Part2 will be added to the merge 7.3 ->

[33mcommit a2c23cc807f08f7e60686eefbb946946d03c2752[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Oct 15 15:38:26 2014 +0300

    WL#5889 Add InnoDB events to Performance Schema's Event Stage table
    
    Add an (empty) array with InnoDB stage events and register it with
    [1;31mperf[mormance schema at InnoDB startup.

[33mcommit 456f8275043a347fee79ab7b2846d2512ebda26c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Oct 13 14:22:16 2014 +0200

    Merge of Bug #19582925 ENHANCED DETECTION AND DUMP OF CORRUPT OR UNSUPPORTED MESSAGES
    
    Test of io state is kept in outside loop of
    TransporterRegistry::unpack() to not introduce to much [1;31mperf[mormance
    regression.

[33mcommit 7b899ac00c5d6a18b5b8eda661885f6f1afc502a[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 8 10:31:13 2014 +0300

    Bug#19632776 Code gets incorrectly optimized away by gcc
    
    As reported in https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=764220
    it is possible that gcc incorrectly optimizes away some code
    when a function that takes a pointer or reference as a parameter
    is declared as attribute((const)) or attribute((pure)).
    
    The function affected was page_zip_rec_needs_ext().
    
    As a preventive measure, we will remove the potentially problematic
    attributes from all functions that take pointers or references that
    they are dereferencing. Functions that [1;31mperf[morm pointer arithmetics
    without dereferencing the pointers should be safe: page_offset() is
    an example.
    
    While we are at it, remove also some attribute((nonnull)).
    This attribute is dangerous, because it does not always generate
    a warning when NULL may be passed, but it may optimize away code
    for handling the NULL case. We wanted this attribute for the sake
    of the warnings, not for the optimizations.
    
    rb#6940 approved by Vasil Dimov and Jimmy Yang

[33mcommit 10ea32adfd3f88e3324da04e210bae912f49f3c0[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Oct 2 11:54:56 2014 +0300

    Bug#19724300 REMOVE TABLE LOOKUPS FROM THE CHANGE BUFFER
    
    When exporting a tablespace, InnoDB needs to merge all buffered changes to
    that tablespace. However, the API for that is using the table_id, and we are
    [1;31mperf[morming unnecessary table lookups in the change buffer. The lookups should
    never fail (there is dead code), because MDL will be preventing a concurrent
    ALTER/TRUNCATE/DROP TABLE while the export operation is pending.
    
    ibuf_get_table(): Remove.
    
    ibuf_merge(): Replace table_id with space_id.
    
    ibuf_contract(): Call ibuf_merge_pages() directly.
    
    ibuf_merge_in_background(): Renamed from ibuf_contract_in_background().
    Replace table_id with space_id.
    
    rb#6890 approved by Vasil Dimov

[33mcommit d2dfd0f30e1be9b29f603cc1ec2aa22ea6227005[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Oct 1 21:46:31 2014 +0200

    Bug#19050141 SHOW EVENTS IN PERFORMANCE_SCHEMA GIVES ACCESS DENIED
    
    Before this fix, SHOW EVENTS in the [1;31mperf[mormance_schema database
    returned an access denied error.
    
    Technically, the error is actually expected, because no user
    can be granted the EVENT_ACL privilege on the [1;31mperf[mormance schema.
    
    From an ease of use point of view,
    and considering that an exception for the INFORMATION_SCHEMA
    is already implemented, it is desirable to relax the check,
    and return an empty result set instead of an access denied error
    in this case.
    
    This fix implements an exception for the PERFORMANCE_SCHEMA
    in SHOW EVENTS, for consistency with existing behavior.

[33mcommit e3488700a7466ddbaa99de90601ded54d922ea15[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Sep 25 14:47:20 2014 +0300

    Re-apply most part of r8740, which was reverted in r8741.
    
    We will not rename the tests in --suite=innodb yet, because
    some of the test names may occur in QA tracking systems,
    and the tests are tracked by name.
    
    Bug#19458208 73623: REMOVE REDUNDANCY FROM INNODB TEST FILE NAMES
    
    Most files were renamed by the following Bourne Again shell (bash)
    script that removes the test suite name prefix and replaces - with _
    in test names:
    
    for s in mysql-test/suite/*innodb*
    do
            S="${s##*/}"
            for t in "$s/t/$S"[-_]*.test
            do
                    ST="${t%/t/*}"
                    St="${t#*/t/}"
                    TT="${t#*/t/$S?}"
                    T="${St%.test}"
                    U="$(echo "${TT%.test}"|tr - _)"
                    bzr mv $ST/t/{$T,$U}-master.opt
                    bzr mv $ST/r/{$T,$U}.result
                    bzr mv $ST/t/{$T,$U}.test
            done
    done
    
    Some further cleanup was [1;31mperf[mormed manually. Some suites contained
    tests that started with innodb_, not with the full suite name.
    
    Approved by Sunny Bains and Erlend Dahl

[33mcommit fcbffd3ca50326dda632e000bfd225dd19440ac0[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Sep 25 13:56:14 2014 +0300

    Fix a compilation error when [1;31mperf[mormance schema is disabled.

[33mcommit 34cf824221ba7de885b0076212b96506cbd7454a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 22 15:14:12 2014 +0200

    Bug#19658933 CRASH IN MY_VALID_MBCHARLEN_UTF8 AT STRINGS/CTYPE-UTF8.C:5883
    
    Under load, the server could crash while [1;31mperf[morming
      SELECT * from [1;31mperf[mormance_schema.events_waits_current
    while attempting to populate the OBJECT_TYPE column.
    
    The issue was that the code in table_events_waits_common::read_row_values()
    populates the colum value from m_row.m_object_type,
    but this member is not initialized in every code path in ::make_row().
    
    Fixed the code by:
    - checking m_row.m_object_type_length instead,
      to follow the general coding pattern used
    - adding a missing initialization in
      table_events_waits_common::make_metadata_lock_object_columns(),
      for the case when the metadata lock version has changed.

[33mcommit 79d68956771feb644c8fd7b02d7e9c8eaf9b7300[m
Merge: 8a079f268a3 be1222375d1
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Fri Sep 19 15:30:02 2014 +0530

    WL#1697: Multisource replication
    
     This patch implements the Multisource feature for MySQL Replication.
     Through this feature, a slave could connect to multiple masters and start
     replicating from all of them simulatenously. conflicts handling is left
     to application.
    
     Important points (See WL#1697, for detailed description)
     =================
     0. MSR is supported only for TABLE type slave repository.
     1. Introduced the concept of slave channel and a FOR CHANNEL clause to
        for replication commands.
     2. Having a FOR CHANNEL clause to a the replication command would
         act on that slave CHANNEL only.
     3. A default channel(which is an empty string- "") always exist.
     4. If FOR CHANNEL clause is not present, then by default it
        would act on all channels unless where the command doesn't make sense.
        For following commands, without providing FOR CHANNEL clause would act
        on all channels.
             -START SLAVE,
             -STOP SLAVE,
             -SHOW SLAVE STATUS,
             -FLUSH RELAYLOGS,
             -RESET SLAVE .
      5. CHANGE MASTER, SHOW_RELAYLOG_EVENTS, wait_until_sql_after_gtids(),
         master_pos_wait() error out if a channel name is not specified and
         channels are greater than 1
      6. Since default channel always exist, backward compatibility in commmands is
         supported. A user need not worry about FOR CHANNEL clause at all.
      7. Replication [1;31mperf[mormance schema tables are also supported for MSR
      8. All SHOW VARIABLES display values of  default channel only.
         For other channels, PFS tables have to be reffered.
      9. It is integrated with GTIDs and MTS.
      10. Error messages in the server log and user session have a
         "for channel <channel_name>" to display the error on which channel
         has occured.
      11. All slave global variables act on all channels.

[33mcommit 4f81455952a67b8fc35c9481a0e4ae7db5b49fd6[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Sep 15 14:37:31 2014 +0200

    Repush of WL#7544 after 7.4.1 clone tag.
    
    ------------------------------------------------------------
    revno: 4458
    revision-id: mikael.ronstrom@oracle.com-20140915090314-4feki0sr3fcmsflr
    parent: mikael.ronstrom@oracle.com-20140912185859-6417bvvey4ih4blu
    committer: Mikael Ronstrom <mikael.ronstrom@oracle.com>
    branch nick: push_wl7544
    timestamp: Mon 2014-09-15 11:03:14 +0200
    message:
      WL#7544: Optimisations of the NDB API receiver thread in various manners. 1) Optimising method to unpack field data from TRANSID_AI signal, 2) optimised handling of trp_client locking check, 3) reorganised code in NDB API receiver thread for better [1;31mperf[mormance and more readability, 4) Optimisation signal receive handler

[33mcommit c81405fd95aa83d89d20abc7195d76caab7f3df2[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Mon Sep 15 11:03:14 2014 +0200

    WL#7544: Optimisations of the NDB API receiver thread in various manners. 1) Optimising method to unpack field data from TRANSID_AI signal, 2) optimised handling of trp_client locking check, 3) reorganised code in NDB API receiver thread for better [1;31mperf[mormance and more readability, 4) Optimisation signal receive handler

[33mcommit 3454f9a36df2c03c3ea095ee6e2b0cabc2164ae2[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 10 13:13:53 2014 +0200

    Fix for Bug#19552283 :
    
      Transporters receiveBuffers might be reset while
      another thread read them (race)
    
    There is a potential race between a thread
    receiving data with TransporterRegistry::[1;31mperf[mormReceive,
    while another thread (async) initiate disconnect of the
    transporter by calling Transporter::doDisconnect(). The
    later method will call TCP_Transporter::disconnectImpl()
    which clears the receive buffer.
    
    A typical case where this happens is ::doDisconnect() being
    called from the thread running TransporterRegistry::start_clients_thread().
    Simultaneously another thread might 'do_poll()' which [1;31mperf[mormReceive()
    on the same transporter.
    
    This fix moves 'receiveBuffer.clear()' out of ::disconnectImpl()
    and into a new method Transporter::resetBuffers() which is now
    called from ::do_connect(). At this point The transporter has
    been trough a report_disconnect() synched with [1;31mperf[mormReceive().
    Furthermore, it is known to be in a DISCONNECTED state, which
    implies that it can't be seen as connected again by
    ::[1;31mperf[mormReceive() as this also has to be synched.
    
    This also introduce a relaxing of in which states buffered
    data can be read & unpacked from the transporters: Any received
    data will now be available in the receivebuffers as long as
    the transporter as available in 'm_has_data_transporters'.
    The patch takes advantage of this by simplifying the check
    of connection states in ::[1;31mperf[mormReceive().
    
    The patch also removes the 'virtual' decl. of
    Transporter::doDisconnect(). This was a leftover from the
    time before (the virtual) Transporter::disconnectImpl()
    was introduced.
    
    Lots of comments are added in order to better explain
    the concurrency controll and restrictions between
    ::pollReceive(), ::[1;31mperf[mormReceive() and ::update_connections().
    ******
    Fix for bug#19559313, call getPerformStateString() with a nodeId arg instead of a state argument

[33mcommit 4717a4e54408960388eed0e2572d6c635fe0ab2c[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Sep 10 19:27:49 2014 +1000

    WL#6835 - For reads make the enter/exit of user threads statement based.
    This is required for [1;31mperf[mormance. The trx_t::mutex enter/exit for every
    row incurs a 3-4% overhead.

[33mcommit 4bd317e244a9549d54684ec314f77eca8d80118c[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Sep 4 11:14:23 2014 +0200

    BUG#19535945 FAILED CREATE PARTITIONED TABLE LEAKS TABLE INSTRUMENTATION
    
    Before this fix, particular illegal CREATE TABLE statements would:
    - fail to create the table, as expected
    - and yet still create the table statistics in the [1;31mperf[mormance schema
    
    This in the long term causes instrumentation leaks,
    consuming space in [1;31mperf[mormance schema buffers for tables
    that do not exist.
    
    The problem is located in ha_create_table(),
    where one code path:
    - creates instrumentation with PSI_TABLE_CALL(get_table_share)
    - creates the table itself in the storage engine
    - fails to cleanup on failures, with PSI_TABLE_CALL(drop_table_share).
    
    The fix adds the missing call to
      PSI_TABLE_CALL(drop_table_share)
    for this error path.

[33mcommit bdd9d86f0752f32dad467e1ea33cce5ecdce703e[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Mon Sep 1 13:52:58 2014 +0200

    WL#7315 Optimizer cost model: main memory management of cost constants
    
    Follow-up patch: Fix compile failure in unit tests when compiling
    without [1;31mperf[mormance schema. The fix is to remove the PSI_mutex_key
    object from being included in the unit tests. This is no
    longer needed since the unit test no longer needs to link in
    ha_resolve_by_name().

[33mcommit 161bc22f3b370848f8e68277c6dbdfa3753a5732[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Aug 29 11:02:14 2014 +0300

    Bug#19458208 73623: REMOVE REDUNDANCY FROM INNODB TEST FILE NAMES
    
    Most files were renamed by the following Bourne Again shell (bash)
    script that removes the test suite name prefix and replaces - with _
    in test names:
    
    for s in mysql-test/suite/*innodb*
    do
            S="${s##*/}"
            for t in "$s/t/$S"[-_]*.test
            do
                    ST="${t%/t/*}"
                    St="${t#*/t/}"
                    TT="${t#*/t/$S?}"
                    T="${St%.test}"
                    U="$(echo "${TT%.test}"|tr - _)"
                    bzr mv $ST/t/{$T,$U}-master.opt
                    bzr mv $ST/r/{$T,$U}.result
                    bzr mv $ST/t/{$T,$U}.test
            done
    done
    
    Some further cleanup was [1;31mperf[mormed manually. Some suites contained
    tests that started with innodb_, not with the full suite name.
    
    Approved by Sunny Bains

[33mcommit 98db68cc17b8e08b8a1f2d9f3e80fcc1d82b13fa[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Thu Aug 28 15:52:07 2014 +0530

    WL#1697: Multisource replication
    
    Fix valgrind errors
    
      - Corrected the way to delete the channel list
      - Fixed the declaration of key_cond_slave_worker_hash
    
     - rpl_multi_source_[1;31mperf[mschema fails on valgrind because memory is not
       reclaimed for received_transaction_set when multiple channels exist.
     - In this patch memory is reclaimed after every read row

[33mcommit 1fa87df0d25c718bd3b33c2128ffb7f2ea216729[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Mon Aug 25 10:57:15 2014 +0200

    Bug#18404381: REMOVE UNNEEDE CMAKE CHECKS AND #IFDEFS IN 5.7.5
    
    Post-push fix: Fix broken build on Solaris with InnoDB memcached and
    without [1;31mperf[mormance schema.

[33mcommit 70a8dac744aeeeb27c1d3326740801194988478d[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Fri Aug 22 14:07:31 2014 +0530

    WL# 7583 - THD transitional string refactoring.
    This patch does transitional refactoring of string
    types in the THD class. This will ease on the migration to
    the new strings framework. A number of places in the
    server uses char* instead of const char*. A gradual
    transformation of the server code is required to use
    const char* and use const references to pass objects
    around functions. This will enable the smooth transition
    of the server code to use the new string framework classes.
    
    This changeset is responsible for changing the type of
    db name of the Statement class from a raw char* to
    LEX_CSTRING. This results in set of cascade changes across
    several files. The following data members are modified as
    part of this patch:
          1. sp_name
               LEX_CSTRING m_db
          2. Alter_table_ctx
              const char *db
              const char *table_name
              const char *alias
              const char *new_db
              const char *new_alias
          3. Foreign_key
              LEX_CSTRING ref_db
              LEX_CSTRING ref_table
          4. Statement
              LEX_CSTRING m_db
          5. Table_ident
              LEX_CSTRING db
              LEX_CSTRING table
          6. TABLE_LIST
              const char *db, *table_name, *alias
              LEX_CSTRING view_db
              LEX_CSTRING view_name
          7. Trigger
              LEX_CSTRING m_db_name
              LEX_CSTRING m_subject_table_name
          8. user_var_entry
          LEX_CSTRING m_catalog
    
    The functions which are modified as part of this patch include:
          1. create_embedded_thd
          2. check_embedded_connection
          3. acl_getroot
          4. check_routine_access
          5. acl_authenticate
          6. check_access
          7. mysql_table_grant
          8. get_table_grant
          9. acl_notify_htons
          10. log_loaded_block
          11. decide_logging_format
          12. net_after_header_psi
          13. Event_job_data::execute
          14. Events::update_event
          15. Events::drop_schema_events
          16. filesort
          17. write_schema_op_to_binlog
          18. injectApplyStatusWriteRow
          19. ndb_binlog_thread_func
          20. handler::ha_external_lock
          21. Item_field::fix_fields
          23. Item_func_sp::itemize
          24. Item_func_database::val_str
          25. File_query_log::write_slow
          26. set_thd_db
          27. Query_log_event::Query_log_event
          28. Query_log_event::do_apply_event
          29. Load_log_event::do_apply_event
          30. Log_event::get_db
          31. handle_slave_io
          32. handle_slave_sql
          33. Current_schema_tracker::store
          34. db_load_routine
          35. lock_db_routines
          36. sp_drop_db_routines
          37. sp_load_for_information_schema
          38. sp_head::execute
          39. sp_head::execute_trigger
          40. sp_instr_stmt::exec_core
          41. mysql_admin_table
          42. Alter_table_ctx::Alter_table_ctx
          43. MYSQL_AUDIT_NOTIFY_CONNECTION_CONNECT (macro)
          44. MYSQL_AUDIT_NOTIFY_CONNECTION_CHANGE_USER (macro)
          45. list_open_tables
          46. open_table
          47. make_cache_key
          48. Query_cache::invalidate
          49. THD::~THD
          50. thd_binlog_filter_ok
          51. db
          52. set_db
          53. reset_db
          54. copy_db_to
          55. Table_ident::Table_ident
          56. Table_ident::change_db
          57. mysql_open_cursor
          58. mysql_change_db_impl
          59. write_to_binlog
          60. get_default_db_collation
          61. mysql_create_db
          62. mysql_alter_db
          63. mysql_rm_db
          64. mysql_change_db_impl
          65. backup_current_db_name
          66. mysql_change_db
          67. mysql_opt_change_db
          68. mysql_upgrade_db
          69. mysql_derived_prepare
          70. Sql_cmd_handler_open::execute
          71. db_is_default_db
          72. mysql_load
          73. write_execute_load_query_log_event
          74. mysql_load
          75. all_tables_not_ok
          76. dispatch_command
          77. mysql_execute_command
          78. mysql_parse
          79. add_table_to_list
          80. plugin_load
          81. Prepared_statement::set_db
          82. Prepared_statement::prepare
          83. Prepared_statement::reprepare
          84. Prepared_statement::swap_prepared_statement
          85. Prepared_statement::execute
          86. mysql_rename_tables
          87. do_rename
          88. close_cached_connection_tables
          89. mysqld_show_create_db
          90. store_create_info
          91. view_store_create_info
          92. List_process_list::operator()
          93. Fill_process_list::operator()
          94. make_table_list
          95. fill_schema_table_by_open
          96. get_all_tables
          97. make_schema_select
          98. get_trigger_table
          99. initialize_information_schema_acl
          100. mysql_rm_table_no_locks
          101. mysql_alter_table
          102. add_table_for_trigger
          103. udf_init
          104. mysql_create_view
          105. mysql_make_view
          106. mysql_drop_view
          107. get_table_category
          108. open_table_def
          109. TABLE_LIST::prepare_view_securety_context
          110. TABLE_LIST::prepare_security
          111. init_one_table
          112. new_nested_join
          113. TABLE_LIST::get_db_name
          114. TABLE_LIST::get_table_name
          115. Table_trigger_dispatcher::Table_trigger_dispatcher
          116. Trigger::create_from_dd
          117. Trigger::execute
          118. Trigger::parse
          119. Trigger::get_db_name
          120. Trigger::get_subject_table_name
          121. Trigger_creation_ctx::create
          122. Trigger_loader::load_triggers
          123. Trigger_loader::drop_all_triggers
          124. my_tz_init
          125. initialize_[1;31mperf[mormance_schema_acl
          126. ParserTest::parse
          127. ACL_internal_schema_registry::register_schema
          128. mysql_routine_grant
          129. Log_to_csv_event_handler::log_slow
          130. sp_exist_routines
          131. create_file
          132. Statement::Statement
          133. end_connection
          134. prepare_new_connection_state
          135. mysql_ha_hash_get_key
          136. init_lex_with_single_table
          137. innobase_get_foreign_key_info
          138. ha_myisammrg::append_create_info
          139. initialize_peformance_schema_acl.
    A new helper function to_lex_cstring(const char*) has been
    introduced which converts a const char* null-terminated
    string to LEX_CSTRING type.

[33mcommit 73188dee6c4b7976f7bc0fdf428f316264302396[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Aug 20 13:08:27 2014 +1000

    WL#6835 - There main fixes:
    
    1. Fix the test case, DEADLOCK is a full rollback. The COMMIT following the
       UPDATE is su[1;31mperf[mluous.
    
    2. Remove the blocking from the trx_t::hit_list if it is rolled back during
       the record lock enqueue phase
    
    3. When the trx_t::state == TRX_STATE_FORCED_ROLLBACK, return DB_FORCED_ABORT
       on COMMIT/ROLLBACK requests.

[33mcommit d6d6df9cf3b62d0bec30cf1ed6ce943d6c535224[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Aug 14 21:37:12 2014 +0200

    Bug#19419463 ASSERT FROM->M_BYTE_COUNT == 0 AT SQL_DIGEST_STORAGE::COPY IN SQL_DIGEST.H
    
    Before this bug, a select from table
    [1;31mperf[mormance_schema.events_statements_current
    could cause a failed assert in debug mode.
    
    The assert did fail under load, when reading the digest of a query
    that is concurrently starting.
    
    Note that tables
    - [1;31mperf[mormance_schema.events_statements_history
    - [1;31mperf[mormance_schema.events_statements_history_long
    are not affected by this bug, as only reading from a live query
    can expose the race condition found.
    
    The root cause is that the copy is doing a dirty read by design,
    but the code is not robust to handle data changing unexpectedly.
    
    Fixed sql_digest_storage::copy to be more robust.

[33mcommit fdca8c6cf2e650469a8b3f4d65e5788d61d7801c[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Aug 12 13:48:18 2014 +0300

    Bug#19233510 [ERROR] INNODB: IGNORING THE REDO LOG DUE TO MISSING
    MLOG_CHECKPOINT
    
    InnoDB crash recovery expects the redo log to be empty after the
    latest checkpoint, or to contain a MLOG_CHECKPOINT marker.
    
    Sometimes, a MLOG_CHECKPOINT marker was incorrectly being omitted when
    [1;31mperf[morming a log checkpoint.
    
    We must only omit the marker when [1;31mperf[morming a shutdown, to avoid
    hitting an infinite loop.
    
    This bug used to mostly occur in tests that invoke extra checkpoints,
    such as those related to WL#6501 (TRUNCATE TABLE) and WL#7277
    (bulk load for index creation).
    
    rb#6330 approved by Jimmy Yang

[33mcommit a5ecc38f44abb66aa2024c70e37d1f4aa4c8ace9[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Aug 11 10:43:11 2014 +0300

    Bug#19330255 WL#7142 - CRASH DURING ALTER TABLE LEADS TO
    DATA DICTIONARY INCONSISTENCY
    
    The server crashes on a SELECT because of space id mismatch. The
    mismatch happens if the server crashes during an ALTER TABLE.
    
    There are actually two cases of inconsistency, and three fixes needed
    for the InnoDB problems.
    
    We have dictionary data (tablespace or table name) in 3 places:
    
    (a) The *.frm file is for the old table definition.
    (b) The InnoDB data dictionary is for the new table definition.
    (c) The file system did not rename the tablespace files yet.
    
    In this fix, we will not care if the *.frm file is in sync with the
    InnoDB data dictionary and file system. We will concentrate on the
    mismatch between (b) and (c).
    
    Two scenarios have been mentioned in this bug report. The simpler one
    first:
    
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    The files were not yet renamed in the file system.
    2a. The server is killed, without making a log checkpoint.
    3a. The server refuses to start up, because replaying MLOG_FILE_RENAME2
    fails.
    
    I failed to repeat this myself. I repeated step 3a with a saved
    dataset. The problem seems to be that MLOG_FILE_RENAME2 replay is
    incorrectly being skipped when there is no page-redo log or
    MLOG_FILE_NAME record for the old name of the tablespace.
    
    FIX#1: Recover the id-to-name mapping also from MLOG_FILE_RENAME2
    records when scanning the redo log. It is not necessary to write
    MLOG_FILE_NAME records in addition to MLOG_FILE_RENAME2 records for
    renaming tablespace files.
    
    The scenario in the original Description involves a log checkpoint:
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    2. A log checkpoint and a server kill was injected.
    3. Crash recovery will see no records (other than the MLOG_CHECKPOINT).
    4. dict_check_tablespaces_and_store_max_id() will emit a message about
    a non-found table #sql-ib22*.
    5. A mismatch is triggering the assertion failure.
    
    In my test, at step 4 the SYS_TABLES root page (0:8) contains these 3
    records right before the page supremum:
    * delete-marked (committed) name=#sql-ib21* record, with space=10.
    * name=#sql-ib22*, space=9.
    * name=t1, space=10.
    space=10 is the rebuilt table (#sql-ib21*.ibd in the file system).
    space=9 is the old table (t1.ibd in the file system).
    
    The function dict_check_tablespaces_and_store_max_id() will enter
    t1.ibd with space_id=10 into the fil_system cache without noticing
    that t1.ibd contains space_id=9, because it invokes
    fil_open_single_table_tablespace() with validate=false.
    
    In MySQL 5.6, the space_id from all *.ibd files are being read when
    the redo log checkpoint LSN disagrees with the FIL_PAGE_FILE_FLUSH_LSN
    in the system tablespace. This field is only updated during a clean
    shutdown, after [1;31mperf[morming the final log checkpoint.
    
    FIX#2: dict_check_tablespaces_and_store_max_id() should pass
    validate=true to fil_open_single_table_tablespace() when a non-clean
    shutdown is detected, forcing the first page of each *.ibd file to be
    read. (We do not want to slow down startup after a normal shutdown.)
    
    With FIX#2, the SELECT would fail to find the table. This would
    introduce a regression, because before WL#7142, a copy of the table
    was accessible after recovery.
    
    FIX#3: Maintain a list of MLOG_FILE_RENAME2 records that have been
    written to the redo log, but not [1;31mperf[mormed yet in the file system.
    When [1;31mperf[morming a checkpoint, re-emit these records to the redo
    log. In this way, a mismatch between (b) and (c) should be impossible.
    
    fil_name_process(): Refactored from fil_name_parse(). Adds an item to
    the id-to-filename mapping.
    
    fil_name_parse(): Parses and applies a MLOG_FILE_NAME,
    MLOG_FILE_DELETE or MLOG_FILE_RENAME2 record. This implements FIX#1.
    
    fil_name_write_rename(): A wrapper function for writing
    MLOG_FILE_RENAME2 records.
    
    fil_op_replay_rename(): Apply MLOG_FILE_RENAME2 records. Replaces
    fil_op_log_parse_or_replay(), whose logic was moved to fil_name_parse().
    
    fil_tablespace_exists_in_mem(): Return fil_space_t* instead of bool.
    
    dict_check_tablespaces_and_store_max_id(): Add the parameter
    "validate" to implement FIX#2.
    
    log_sys->append_on_checkpoint: Extra log records to append in case of
    a checkpoint. Needed for FIX#3.
    
    log_append_on_checkpoint(): New function, to update
    log_sys->append_on_checkpoint.
    
    mtr_write_log(): New function, to append mtr_buf_t to the redo log.
    
    fil_names_clear(): Append the data from log_sys->append_on_checkpoint
    if needed.
    
    ha_innobase::commit_inplace_alter_table(): Add any MLOG_FILE_RENAME2
    records to log_sys->append_on_checkpoint(), and remove them once the
    files have been renamed in the file system.
    
    mtr_buf_copy_t: A helper functor for copying a mini-transaction log.
    
    rb#6282 approved by Jimmy Yang

[33mcommit 96be324f31b92cbcc364478b753da46407689230[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sat Aug 9 10:16:20 2014 +0800

    WL#6964  MTS: Support SLAVE_TRANSACTION_RETRIES in MTS mode
    
    DESCRIPTION
    ===========
    This patch implemented slave_transaction_retries feature on multi-threaded slave.
    Workers can retry the transactions being applied automatically if any temporary error
    is encountered.
    
    Basically, the behaviour of retrying transaction on multi-threaded slave is same
    to the behavior of retrying transaction on singled-threaded slave. Definitions of
    temporary error are same, and sleep policies are same too.
    
    DESIGN
    ======
    - How To Retry
      The worker reads the transaction's event from relay logs and applies them.
      But it just reads and applies the events from the first one to the one on which
      temporary error happens. The following events are still in job queue, so they
      are not read from relay logs.
    
    - How To Know Events' Positions of The Transaction
      When coordinator dispatches each event to a worker, it stores the event's position
      (relay log name and offset) into Slave_job_item structure together with the event.
      To minimize the [1;31mperf[mormance impact, relay log name is not stored as a string. As
      you know, relay log's extension is an auto-increment number. So we just store the
      number as a integer into Slave_job_item.
    
      Workers can construct the real relay log name by using relay_log_basename server
      variable and the number.
      NOTE: Workers may construct a wrong relay log name if users have changed
            relay log base name and events of the transactions are in the relay log with
            old basename. In this case, slave will stop with an error.

[33mcommit 18d3540818f5965966ab9dff9db35480c40131a5[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Aug 5 16:29:07 2014 +0200

    Revert fix merged in from 5.6.20 that does not work as is in MySQL Cluster
    Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS, SHOW PROCESSLIST, SHOW BINLOGS
    
    Reverted revisions are:
    
    ------------------------------------------------------------
    revno: 2876.552.146
    revision-id: venkatesh.duggirala@oracle.com-20140718110044-ivyb4xygx4q59puv
    parent: ashish.y.agarwal@oracle.com-20140717135640-3m6s2n57s1th3vom
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6.20-release
    timestamp: Fri 2014-07-18 16:30:44 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Post push fix (removing try-catch)
    ------------------------------------------------------------
    revno: 2876.552.16 [merge]
    revision-id: venkatesh.duggirala@oracle.com-20140509042350-ni6xx9khsej94hl9
    parent: venkatesh.duggirala@oracle.com-20140508124746-8de4t4utydx15x2k
    parent: venkatesh.duggirala@oracle.com-20140509042215-kukwf135nfyy9yy2
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6
    timestamp: Fri 2014-05-09 09:53:50 +0530
    message:
      Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
      SHOW PROCESSLIST, SHOW BINLOGS
    
      Fixing post push test failure (MTR does not like giving
      127.0.0.1 for localhost incase of --embedded run, it thinks
      it is an external ip address)
        ------------------------------------------------------------
        revno: 2875.596.13
        revision-id: venkatesh.duggirala@oracle.com-20140509042215-kukwf135nfyy9yy2
        parent: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
        committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
        branch nick: mysql-5.5
        timestamp: Fri 2014-05-09 09:52:15 +0530
        message:
          Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
          SHOW PROCESSLIST, SHOW BINLOGS
    
          Fixing post push test failure (MTR does not like giving
          127.0.0.1 for localhost incase of --embedded run, it thinks
          it is an external ip address)
    ------------------------------------------------------------
    revno: 2876.552.15 [merge]
    revision-id: venkatesh.duggirala@oracle.com-20140508124746-8de4t4utydx15x2k
    parent: mithun.c.y@oracle.com-20140508092217-2vyrjxxtzf18r0ho
    parent: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
    committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
    branch nick: mysql-5.6
    timestamp: Thu 2014-05-08 18:17:46 +0530
    message:
            Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
            SHOW PROCESSLIST, SHOW BINLOGS
    
            Merge from mysql-5.5 ( Fix is changed from
            mysql-5.5's patch)
    
            Problem:  A deadlock was occurring when 4 threads were
            involved in acquiring locks in the following way
            Thread 1: Dump thread ( Slave is reconnecting, so on
                          Master, a new dump thread is trying kill
                          zombie dump threads. It acquired thread's
                          LOCK_thd_data and it is about to acquire
                          mysys_var->current_mutex ( which LOCK_log)
            Thread 2: Application thread is executing show binlogs and
                           acquired LOCK_log and it is about to acquire
                           LOCK_index.
            Thread 3: Application thread is executing Purge binary logs
                           and acquired LOCK_index and it is about to
                           acquire LOCK_thread_count.
            Thread 4: Application thread is executing show processlist
                           and acquired LOCK_thread_count and it is
                           about to acquire zombie dump thread's
                           LOCK_thd_data.
            Deadlock Cycle:
                 Thread 1 -> Thread 2 -> Thread 3-> Thread 4 ->Thread 1
    
            The same above deadlock was observed even when thread 4 is
            executing 'SELECT * FROM information_schema.processlist' command and
            acquired LOCK_thread_count and it is about to acquire zombie
            dump thread's LOCK_thd_data.
    
            Analysis:
            There are four locks involved in the deadlock.  LOCK_log,
            LOCK_thread_count, LOCK_index and LOCK_thd_data.
            LOCK_log, LOCK_thread_count, LOCK_index are global mutexes
            where as LOCK_thd_data is local to a thread.
            We can divide these four locks in two groups.
            Group 1 consists of LOCK_log and LOCK_index and the order
            should be LOCK_log followed by LOCK_index.
            Group 2 consists of other two mutexes
            LOCK_thread_count, LOCK_thd_data and the order should
            be LOCK_thread_count followed by LOCK_thd_data.
            Unfortunately, there is no specific predefined lock order defined
            to follow in the MySQL system when it comes to locks across these
            two groups. In the above problematic example,
            there is no problem in the way we are acquiring the locks
            if you see each thread individually.
            But If you combine all 4 threads, they end up in a deadlock.
    
            Fix:
            Since everything seems to be fine in the way threads are taking locks,
            In this patch We are changing the duration of the locks in Thread 4
            to break the deadlock. i.e., before the patch, Thread 4
            ('show processlist' command) mysqld_list_processes()
            function acquires LOCK_thread_count for the complete duration
            of the function and it also acquires/releases
            each thread's LOCK_thd_data. Instead of it, Now it will take
            a copy of THDs from global_thread_list and [1;31mperf[morm traversal
            (on copied THDs) only after releasing  LOCK on LOCK_thread_count.
            During traversal(on copied THDs), removal from global_thread_list is
            blocked using another mutex LOCK_thd_remove such that
            THD copied are valid during traversal(otherwise remove destroys THD).
    
            Now the new locking order after this patch is:
            LOCK_thd_remove -> LOCK_thd_data -> LOCK_log ->
            LOCK_index -> LOCK_thread_count
        ------------------------------------------------------------
        revno: 2875.596.12
        revision-id: venkatesh.duggirala@oracle.com-20140508124301-9kqzomeujeucnxcu
        parent: mithun.c.y@oracle.com-20140508091953-qafkm6r1dhycdjyh
        committer: Venkatesh Duggirala<venkatesh.duggirala@oracle.com>
        branch nick: mysql-5.5
        timestamp: Thu 2014-05-08 18:13:01 +0530
        message:
          Bug#17283409  4-WAY DEADLOCK: ZOMBIES, PURGING BINLOGS,
          SHOW PROCESSLIST, SHOW BINLOGS
    
          Problem:  A deadlock was occurring when 4 threads were
          involved in acquiring locks in the following way
          Thread 1: Dump thread ( Slave is reconnecting, so on
                        Master, a new dump thread is trying kill
                        zombie dump threads. It acquired thread's
                        LOCK_thd_data and it is about to acquire
                        mysys_var->current_mutex ( which LOCK_log)
          Thread 2: Application thread is executing show binlogs and
                         acquired LOCK_log and it is about to acquire
                         LOCK_index.
          Thread 3: Application thread is executing Purge binary logs
                         and acquired LOCK_index and it is about to
                         acquire LOCK_thread_count.
          Thread 4: Application thread is executing show processlist
                         and acquired LOCK_thread_count and it is
                         about to acquire zombie dump thread's
                         LOCK_thd_data.
          Deadlock Cycle:
               Thread 1 -> Thread 2 -> Thread 3-> Thread 4 ->Thread 1
    
          The same above deadlock was observed even when thread 4 is
          executing 'SELECT * FROM information_schema.processlist' command and
          acquired LOCK_thread_count and it is about to acquire zombie
          dump thread's LOCK_thd_data.
    
          Analysis:
          There are four locks involved in the deadlock.  LOCK_log,
          LOCK_thread_count, LOCK_index and LOCK_thd_data.
          LOCK_log, LOCK_thread_count, LOCK_index are global mutexes
          where as LOCK_thd_data is local to a thread.
          We can divide these four locks in two groups.
          Group 1 consists of LOCK_log and LOCK_index and the order
          should be LOCK_log followed by LOCK_index.
          Group 2 consists of other two mutexes
          LOCK_thread_count, LOCK_thd_data and the order should
          be LOCK_thread_count followed by LOCK_thd_data.
          Unfortunately, there is no specific predefined lock order defined
          to follow in the MySQL system when it comes to locks across these
          two groups. In the above problematic example,
          there is no problem in the way we are acquiring the locks
          if you see each thread individually.
          But If you combine all 4 threads, they end up in a deadlock.
    
          Fix:
          Since everything seems to be fine in the way threads are taking locks,
          In this patch We are changing the duration of the locks in Thread 4
          to break the deadlock. i.e., before the patch, Thread 4
          ('show processlist' command) mysqld_list_processes()
          function acquires LOCK_thread_count for the complete duration
          of the function and it also acquires/releases
          each thread's LOCK_thd_data.
    
          LOCK_thread_count is used to protect addition and
          deletion of threads in global threads list. While show
          process list is looping through all the existing threads,
          it will be a problem if a thread is exited but there is no problem
          if a new thread is added to the system. Hence a new mutex is
          introduced "LOCK_thd_remove" which will protect deletion
          of a thread from global threads list. All threads which are
          getting exited should acquire LOCK_thd_remove
          followed by LOCK_thread_count. (It should take LOCK_thread_count
          also because other places of the code still thinks that exit thread
          is protected with LOCK_thread_count. In this fix, we are changing
          only 'show process list' query logic )
          (Eg: unlink_thd logic will be protected with
          LOCK_thd_remove).
    
          Logic of mysqld_list_processes(or file_schema_processlist)
          will now be protected with 'LOCK_thd_remove' instead of
          'LOCK_thread_count'.
    
          Now the new locking order after this patch is:
          LOCK_thd_remove -> LOCK_thd_data -> LOCK_log ->
          LOCK_index -> LOCK_thread_count

[33mcommit 1bd41d2c6326d82af0ffd843a23a51de9ddf3454[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Mon Aug 4 14:29:44 2014 +0800

    Bug #19351967 CENTROID() AND COVEXHULL() ACCEPTS GEOMETRYCOLLECTION CONTAINING INVALID POLYGON
    Check for ring validity for polygons of geometry collections given to centroid, but not for convexhull and envelope, because in the latter two gis functions,
    we don't rely on polygon ring being valid, we can compute the logically correct result for any polygons.
    And also for envelope we don't do so for [1;31mperf[mormance since they will be used by rtree index code of storage engines.

[33mcommit febe1236e040784dde0263e16d3481f2fb1724fa[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sun Aug 3 11:08:23 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    Avoids to generate mysqld-debug.dmp during the test.
    IO thread can exit always when SRV_SHUTDOWN_EXIT_THREADS, if page cleaner already exited.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for [1;31mperf[mormance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      optimal [1;31mperf[mormance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit f46daa74eee57b2102303ece3246787bd6f40792[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Sat Aug 2 02:08:16 2014 +0200

    Fixed a build break when compiling without the [1;31mperf[mormance schema

[33mcommit ff659570dd24733c162fd260d52dc23eee56cbdf[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jul 31 11:18:11 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    There are race between io threads and page cleaners to exit only when abort initialize InnoDB.
    Especially, Windows native AIO is weak for the race.
    IO threads should not exit during page cleaners active, in accurate.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for [1;31mperf[mormance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      optimal [1;31mperf[mormance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit 95284c56ee334c3c288f2c2f098e80d3ab561808[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Jul 30 15:16:17 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Update [1;31mperf[mschema .result test files after the addition of one more event.

[33mcommit a6e4d5b95dee422316b521b796ab194d08c65d67[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jul 30 00:45:17 2014 +0200

    WL#7698 PERFORMANCE SCHEMA, REDUCE MEMORY USAGE FOR TABLE IO / TABLE LOCK
    
    Revised new system variables:
    - [1;31mperf[mormance_schema_max_index_stat
    - [1;31mperf[mormance_schema_max_table_lock_stat

[33mcommit 99cc9410fb58f583fe3070237854f2befe841ef2[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Tue Jul 29 16:04:43 2014 +0200

    WL#5630: Allow WITHOUT VALIDATION on EXCHANGE PARTITION
    
    post-push test fix.
    
    [1;31mperf[mormance schema digest is affected by new tokens in sql_yacc.yy/lex.h.

[33mcommit ca25011abb142c193c8a58de1222f2e971e016bc[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sat Jul 26 03:22:05 2014 +0900

    Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
    From 5.7, page cleaner is multi-threaded for [1;31mperf[mormance scalability.
    But it is not used during shutdown and recovery phases.
    
    It should be multi-threaded during shutdown and recovery phases for their
    optimal [1;31mperf[mormance.
    
    * The previous problem was fixed. (releasing the event objects was too early)
    
    Approved by Sunny in rb#5465

[33mcommit b38414c39da4cc8b2e27f57dc9c80c89b1aede91[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Thu Jul 24 21:44:34 2014 +0400

    WL#7976: Deprecate skip-innodb in 5.6, remove in 5.7.
    
    Follow-up patch to fix new test failures:
    
      - [1;31mperf[mschema.no_threads failed on Windows only.
    
        The problem was that the number of P_S-thread-instances
        has increased when InnoDB was enabled. The test limited
        that number by 10, which was not enough on Windows.
    
        The fix is to increase the limit to 20.
    
      - rpl_myisam_recovery
    
        skip-innodb option was missed.

[33mcommit c4597498d6d9d424dba5bde1422ec45581222e1a[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Wed Jul 23 12:17:37 2014 +0800

    st_distance [1;31mperf[mormance optimization: given a geometry collection,
    compact its points/multipoints into one multipoint object and linestrings/multilinestrings into one multilinestring object
    before calling BG::distance for better [1;31mperf[mormance, because BG::distance has O(NlogN) complexity for calculating distance of such objects.

[33mcommit cdb6ddc9c35bba01d9d3a20cf25eac570b5e5bf3[m
Author: Alexander Nozdrin <alexander.nozdrin@oracle.com>
Date:   Tue Jul 22 16:48:41 2014 +0400

    WL#7293: Make sure that when 32-bit connection-IDs roll over, still in-use
    IDs are not re-issued
    
    Committed on behalf of Jon Olav Hauglid <jon.hauglid@oracle.com>.
    
    With this patch the currently used thread IDs are maintained in a sorted
    list so that it can be checked that each new connection is assigned an ID
    which is not currently in use. This prevents reuse of active IDs in cases
    where the 32 bit counter wraps around. The counter is kept at 32 bit as it
    is part of the client protocol.
    
    The patch makes code more consistent by using the my_pthread_id typedef
    instead of various uint/ulong variants.
    
    The patch also changes the thread_created counter to be incremented using
    atomics rather than protected by a separate mutex. The list of active THDs
    is changed from map to a sorted array.
    
    Combined this gives no reduced connect/disconnect [1;31mperf[mormance even with the
    added processing required to assign unique IDs.

[33mcommit 6306226d27ee7284a069fde7f58308087cb6a5a7[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Tue Jul 22 16:24:07 2014 +0530

    WL#1697: Multisource Replication - Testframework
    
    Patch2
    =======
    This patch [1;31mperf[morms a small refactoring of the mtr replication framework
    in order to enable multi-source replication in the framework.
    
    In this patch, we add a parameter to disable invokation of rpl_sync.inc
    from test framework files that would otherwise use it.
    
    Patch done by Sven Sandberg!

[33mcommit 7520ea7cab84ff212a2bc361bddcd2cfdc803f4a[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Tue Jul 22 16:15:15 2014 +0530

     WL#1697: Multisource Replication - Testframework
    
     Patch 1
     =======
     This patch [1;31mperf[morms a small refactoring of the mtr replication framework
     in order to enable multi-source replication in the framework.
    
     In this patch, we change the internal represenation of the topology:
    
    - Before this patch, we used $rpl_master_list to represent the topology.
      It had the format:
    
      <number>{N}
    
      where the i'th <number> is the master of server i, or space if
      server i does not have a master.
    
      <number> is padded with spaces to $rpl_server_count_length
      characters, to ease string manipulation.
    
    - After this patch, we use $rpl_connection_list to represent the
      topology. This variable has the format:
    
      (m<NUMBER>s<NUMBER>)*
    
      where each element represents a single master-slave connection;
      m<NUMBER> is the master and s<NUMBER> is the slave.
    
      <NUMBER> is right-padded with spaces to $rpl_server_count_length
      characters, to ease string manipulation.
    
      - change the following files:
          - rpl_change_topology.inc
            - generate rpl_connection_list correctly
            - use rpl_connection_list to iterate over slaves
          - rpl_for_each_slave.inc (rename to rpl_for_each_connection)
            - iterate over rpl_connection_list instead of rpl_master_list
          - rpl_generate_sync_chain.inc
            - change logic for computing the master of a given server so
              that it uses $rpl_connection_list instead of $rpl_master_list
          - rpl_init.inc
          - rpl_reset.inc
            - use rpl_for_each_connection / rpl_for_each_server
              instead of parsing rpl_master_list
    
      Patch done by sven.sandberg@oracle.com

[33mcommit 330ecbaddd067d1919e8f52082e1ba92934557af[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Jul 18 14:05:52 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Followup to:
    vasil.dimov@oracle.com-20140711135727-3jtss2fq6c7diyhb
    adjust mtr result files after lifting the limits of
    [1;31mperf[mormance_schema_max_memory_classes.

[33mcommit e6c6a8f8b0d43efb31819a814ba7a31ded96e925[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Jul 17 15:37:31 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Followup to:
    vasil.dimov@oracle.com-20140711135727-3jtss2fq6c7diyhb
    adjust mtr result files after lifting the limits of
    [1;31mperf[mormance_schema_max_memory_classes.

[33mcommit 359e77b1e8fe9ad77d7b00993307b12f569fde2a[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Jul 11 16:57:27 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Increase [1;31mperf[mormance_schema_max_memory_classes:
    max from 256 to 1024
    default from 250 to 1024
    
    InnoDB will add about 100-200 new entries.
    
    Approved by:    Marc Alff (via email)

[33mcommit 40ec5373c044547a66d5456b15d61553de8f3401[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jul 10 10:46:28 2014 +0200

    Bug#19142753 ASSERT: MODE !=LOCK_X || LOCK_TABLE_HAS(THR_GET_TRX(THR), INDEX->TABLE, LOCK_IX)
    
    WL6742-Improve InnoDB SELECT COUNT(*) [1;31mperf[mormance by using handler::records()
    introduced new functionality in InnoDB.
    
    handler::records can now fail in ways never seen by the server before.
    
    This improved things on the InnoDB side:
    kevin.lewis@oracle.com-20130626194505-muocxuem7bd2wiw8
      bug#16802288 - FAILING ASSERTION: PREBUILT->SQL_STAT_START ||
      TRX->STATE == TRX_STATE_ACTIVE
    
      Add error handling for DB_DEADLOCK and DB_LOCK_TABLE_FULL to
      handler::records() for COUNT(*).  Also rollback transaction when
      handler::records receive DB_DEADLOCK, DB_LOCK_TABLE_FULL or
      DB_LOCK_WAIT_TIMEOUT.
    
    However: thd_mark_transaction_to_rollback() isn't enough.
    The optimizer needs to abort immediately for the cases mentioned above.
    
    Fix: improve error handling for all calls to handler::records()
    Add a new public function ha_records() which does the error checking,
    make handler::records() protected.

[33mcommit 16827fcee42fdc668523375b42194a0cfb10b1bd[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Jul 9 15:33:20 2014 +0900

    Revert the following change, because the regression was innocent about the Bug#11755438:Bug#47213 fix now.
    
    ------------------------------------------------------------
    revno: 8279 [merge]
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Wed 2014-06-25 05:40:00 +0200
    message:
      Disable memory barrier only for Intel CPU, because [1;31mperf[mormance regression was observed at some conditions for Intel CPU.
      follow up for Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL

[33mcommit 74422f3b69a57fb9b2b7fe0e5d3c92a375fca246[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Fri Jul 4 12:43:02 2014 +0530

    Bug#18757964:RPL.RPL_CIRCULAR_FOR_4_HOSTS FAILS WITH RESULT
    MISMATCH NON DETERMINISTIC RESULTS
    
    Problem:
    ========
    rpl_circular_for_4_hosts  failing sporadically in pb2
    regression runs. Test lead to non deterministic results in
    case of lower [1;31mperf[mormance. If the lock of the table will be
    later than the inert in next session then the result may
    differ from the expected one.
    
    Analysis:
    ========
    This bug is clearly reproducible in windows with MTS
    and --slave-parallel-type=logical_clock.
    Test script has circular replication like 1->2->3->4->1.
    As part of test they do following two steps.
    
    1) Insert value 6 in the autoinc column on server_3 (and
    prevent it from replicating further using
    SQL_SLAVE_SKIP_COUNTER on server_4). Due to the
    auto_increment_offset setting, the autoinc value 6 is
    normally generated on server_2. When we later insert a row
    on server_2,we thus cause a duplicate key error on server_3.
    
    2) Reconfigure the topology to like this 1->2->4->1,2->3.
    On server 4 change master is executed by taking Server 3's
    Exec_master_log_pos and Master_Log_File. On Server 3
    Exec master position changes as per the order in which
    workers execute the following parallel transactions.
    
    use `test`; INSERT INTO t1(b,c) VALUES('B',2)
    use `mtr`; INSERT INTO test_suppressions (pattern) VALUES
    ( NAME_CONST('pattern',_latin1'Slave SQL.*Duplicate entry
    
    Example:
    Normal case:
    "Server 3 pos_c is ------3639"
    "Server 3 file_c is -----slave-bin.000001"
    
    slave-bin.000001 3554 Query  4 3639 COMMIT
    slave-bin.000001 3639 Query  2 3732 BEGIN
    slave-bin.000001 3732 Intvar 2 3764 INSERT_ID=6
    slave-bin.000001 3764 Query  2 3876 use `test`; INSERT
    INTO t1(b,c) VALUES('B',2)
    slave-bin.000001 3876 Query  2 3961 COMMIT
    slave-bin.000001 3961 Query  3 4047 BEGIN
    slave-bin.000001 4047 Query  3 4294 use `mtr`; INSERT INTO
    test_suppressions (pattern) VALUES ( NAME_CONST
    
    Error case:
    "Server 3 pos_c is ------4372"
    "Server 3 file_c is -----slave-bin.000001"
    
    slave-bin.000001 3554 Query  4 3639 COMMIT
    slave-bin.000001 3639 Query  2 3732 BEGIN
    slave-bin.000001 3732 Intvar 2 3764 INSERT_ID=6
    slave-bin.000001 3764 Query  2 3876 use `test`; INSERT
    INTO t1(b,c) VALUES('B',2)
    slave-bin.000001 3876 Query 2 3961  COMMIT
    slave-bin.000001 3961 Query 3 4047  BEGIN
    slave-bin.000001 4047 Query 3 4294  use `mtr`; INSERT INTO
    test_suppressions (pattern) VALUES ( NAME_CONST('pattern',
    _latin1'Slave SQL.*Duplicate entry .6. for key .PRIMARY.*
    Error_code: 1062' COLLATE 'latin1_swedish_ci'))
    slave-bin.000001 4294 Query 3 4372  COMMIT
    
    Because of this incorrect position in change master
    INSERT B gets missed in 4 & 1.
    
    Fix:
    ===
    Since the main intention is to generate duplicate key
    error on Server 3 to make test script more deterministic
    moved the mtr add suppression statement to a place bit
    ahead where we do a proper sync up of all servers.

[33mcommit 84fca39d685db285fea4b88f3b11a496fdea40b1[m
Author: Ritheesh Vedire <ritheesh.vedire@oracle.com>
Date:   Thu Jul 3 13:13:48 2014 +0530

    WL#1697: Multisource Replication.
    
     This patch implements the Multisource feature for MySQL Replication.
     Through this feature, a slave could connect to multiple masters at
     the same time and start replication.
     The present patch contains only the code for the feature.
    
     Important information for this feature. (See WL#1697, for full description)
     ==========================================================================
     1. MSR is supported only for TABLE type slave repositories.
     2. Having a FOR CHANNEL to the replication command would
        act on that CHANNEL only.
     3. If FOR CHANNEL clause is not present, then by default it
        would act on all channels. START SLAVE, STOP SLAVE, SHOW SLAVE STATUS,
        FLUSH RELAYLOGS, RESET SLAVE etc unless where the command doesn't make sense.
     4. CHANGE MASTER, SHOW_RELAYLOG_EVENTS, master_pos_wait(), gtid_set_wait()
        error out if a channel name is not specified.
     5. The folloing patch implements all the commands for MSR
     6. replication [1;31mperf[mormance schema tables are also supported for MSR
     7. All SHOW STATUS VARIABLES act on default channel only (which is "",
        this always exists, not settable and deleted by the user)

[33mcommit 010811852fb66d5bccd42ebd84c263aba0157f9c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Wed Jul 2 22:07:50 2014 +0400

    WL#1159 "Allow multiple locks in GET_LOCK()".
    
    This patch is based on contribution from Konstantin Osipov.
    
    The goal of this patch is to allow user to acquire multiple
    user-level locks in the same connection via serie of calls to
    GET_LOCK() function. Also it makes possible to take the same
    lock twice in connection (i.e. locks became recursive).
    GET_LOCK() no longer implicitly releases the previous lock held
    by the connection. To release all locks in connection one can
    use new RELEASE_ALL_LOCKS() function.
    
    The above is achieved by deleting all old code which implements
    user-level locks, and forwarding user-level lock requests to the
    metadata locking subsystem. A new metadata type was introduced
    for user-level locks - "user". Instances of "USER" locks are
    mutually exclusive.
    
    Possible deadlocks between multiple locks taken in different order,
    as well as between user-level locks and metadata/waits for table
    flushes are detected and resolved using the MDL deadlock detector.
    Waits for user-level locks are preferred as victim over waits
    for locks typically acquired by DDL, but waits for locks typically
    acquired by DML are preferred over waits for user-level locks.
    
    Wait for user-level lock which is aborted due to deadlock is
    reported as ER_USER_LOCK_DEADLOCK error. No transaction rollback
    happens in this case.
    
    MDL subsystem was extended to support the old behavior when waits
    for user-level locks are automagically aborted if connection which
    requested them disconnects. However now this situation is handled
    similarly to KILL statement rather than to lock wait timeout.
    
    Since IS_USED_LOCK() function returns id of connection which owns
    the lock the MDL API had to be extended with capability to query
    lock owner.
    
    Note that with old implementation when a wait for user-level lock was
    aborted due to query or connection being killed GET_LOCK() function
    always returned NULL and statement using this function might have
    succeeded or failed with ER_QUERY_INTERRUPTED error, depending if
    statement tried to do anything else after calling GET_LOCK().
    With new implementation statement which called GET_LOCK() and was
    killed will always fail with ER_QUERY_INTERRUPTED due to slightly
    different KILL error handling by MDL subsystem.
    
    Since MDL subsystem imposes limits on the length of key used to
    identify objects new implementation introduces limit on user-level
    lock name length. New limit is 64 characters.
    ER_USER_LOCK_WRONG_NAME error is emitted when one of functions
    accepting user-level lock name as argument gets name which is
    longer than 64 character. Also behavior is changed to return the
    same error for NULL or empty ('') lock name.
    Also new implementation will always convert lock name to utf8 from
    its original charset and [1;31mperf[morm case-insensitive comparison.
    
    Existing test coverage for user-level lock functionality has been
    extended and moved from main.func_misc to main.user_lock test.
    
    New rpl.rpl_user_lock test covers replication of statements using
    user-level lock functions.
    
    Some other existing tests had to be adjusted to the new user-lock
    behavior including rpl.rpl_err_ignoredtable test. The latter was
    additionally fixed to really test what it was intended to test.
    
    Tests in [1;31mperf[mschema test suite which used mutex used in user-level
    lock implementation to test aggregation of wait events were changed
    to use different mutex. Tests which used user-level locks to test
    aggregation for memory events we adjusted to new implementation
    memory usage. Coverage for new MDL namespace was added to
    [1;31mperf[mschema.mdl_func test.
    
    Unit tests covering MDL API/code changes were added.

[33mcommit 1acb93c14f1cf0b500e282fba74326ed6d805fe1[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Jul 2 10:25:56 2014 +0200

    Traditional record of [1;31mperf[mschema.pfs_upgrade*

[33mcommit fb8d1568bd1807c5d8e9bd43001571c3ad67955c[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Jul 1 15:27:26 2014 +0200

    Remove su[1;31mperf[mluous -master.opt file for the removed test case ndb_mt_recv.

[33mcommit b2adab2f07608231dc9793c4894c653dda9d6f9c[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Jul 1 08:19:52 2014 +0530

    - Follow-up [1;31mperf[mormance fix for WL#7682.
    
      Intrinsic Table: Optimization to use dict_table_t for caching row_id/trx_id
      instead of innodb_session_t
    
      With WL#7682 we introduced intrinsic tables. Given that these tables has
      session scope row_id and trx_id needed for these tables were cached in
      innodb_session_t (session structure cached in thd).
    
      Unfortunately, accessing these ids through session is [1;31mperf[mormance costly as
      it involves lookup into map using table name and loading of thd that in turn
      causes invalidation the CPU cache.
    
      We observed a regression upto 35% with Wisconsin-DB queries.
    
      So we are moving back to original idea of caching these ids as part of
      dict_table_t.
    
      Approved by: Jimmy (rb#5881)

[33mcommit bb3af066b944f5ea7d964149d7e36cedfa1680f2[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Jun 30 11:10:33 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Replace ut_realloc() with a [1;31mperf[mormance schema aware mechanism.
    For now it just does free()+malloc().
    
    Redirect malloc()/realloc()/free() calls from fts0blex.cc and
    fts0tlex.cc to ut_malloc()/ut_realloc()/ut_free().
    
    Manage st_innobase_share::idx_trans_tbl::index_mapping with
    ut_realloc()/ut_free() instead of my_realloc/my_free().

[33mcommit 2d2a17e275b00028c5168704728c45d035ea5fd2[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Fri Jun 27 08:12:27 2014 +0800

    Bug #18963555   TAKING WRITE LOCK WHILE ADDING A GTID INTO GTID SET IS NOT GOOD TO PERFORMANCE
    
    In Gtid_state::update_on_commit(), it takes global_sid_lock.wrlock()
    while adding a GTID into gtid set, which is not good to [1;31mperf[mormance.
    
    Optimize it to take global_sid_lock.rdlock() and lock a mutex for
    the given SIDNO while adding the GTID into gtid set for improving
    [1;31mperf[mormance.

[33mcommit 15c9d80019a5337c4e28fe3a50abe510e9b5c452[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Jun 27 01:43:12 2014 +0200

    Build cleanup, when compiling without the [1;31mperf[mormance schema

[33mcommit 85892b3255a96a07c21e01f628abfc2afddd994e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Jun 26 09:42:07 2014 +0200

    Part 2 with fix for 'Node Restart' part of Bug#19053226
    
    AUTOTEST FAIL: TESTSRBANK -N MIX -L 300 -R 15 T1
    
    This will also fix the AutoTest 'testSRBank -n NR ...'
    However, there are still more failures in the 'Mix'
    part of the test where we fails to start after a Node Restart
    followed by a System Restart before the NR has completed.
    ... will likely open another bug report to track that issue.
    
    The root cause for the failing 'Node Restart' (this fix) was
    that the different 'Validate' methods in the Bank class
    did not differentiate between permanent (NDBT_FAILED)
    and temporary (NDBT_TEMPORARY) failures. Thus, a node restart
    during validation of Bank consistency caused Bank::[1;31mperf[mormMakeGLs()
    to return NDBT_FAILED if it were in the first part where it check
    consistency of the 'GL'
    
    This fix catch temporary errors from ::startTransaction(),
    ::execute() and ::nextResult(), and handle these by
    skipping the Validate part for this ::[1;31mperf[mormMakeGLs round.

[33mcommit 0c6e277e00fddd30688227da7cbcbfa912927c24[m
Merge: 6944bdb1a42 5f280dd21dd
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Jun 25 05:40:00 2014 +0200

    Disable memory barrier only for Intel CPU, because [1;31mperf[mormance regression was observed at some conditions for Intel CPU.
    follow up for Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL

[33mcommit 12421fdcfcae6534d1df3f531588d286d67363b0[m
Merge: 45875a3c913 518f1d0cb7d
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Tue Jun 24 14:25:42 2014 +0200

    Bug #19048563 : PERFORMANCE REGRESSION IN UPDATE OPS CAUSED BY TRUNK REVNO 8242 (BUG#11755438)
    
    Some of the added memory barrier (internal of spin loops of mutex/rw_lock) was too expensive. Removed.
    This is partial reverting of the fix for Bug#11755438.
    
    ------------------------------------------------------------
    revno: 8242 [merge]
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Thu 2014-06-19 07:36:42 +0200
    message:
      Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL
    
      Because of difference about memory ordering, some critical flags of mutex/rw_lock might be missed to read on non-Intel CPUs.
      Even for Intel-CPUs, the explicit memory barrier instruction might cause positive effects for [1;31mperf[mormance.
    
      Approved by Kevin Lewis in rb#5466
    ------------------------------------------------------------

[33mcommit a26cd1a88d6aba91afe0df774ef7870a8771280f[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Jun 20 11:02:36 2014 +0200

    Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
    From 5.7, page cleaner is multi-threaded for [1;31mperf[mormance scalability.
    But it is not used during shutdown and recovery phases.
    
    It should be multi-threaded during shutdown and recovery phases for their
    optimal [1;31mperf[mormance.
    
    Approved by Sunny in rb#5465

[33mcommit 09eefae20693bbfd254c3dfa4f513bbd47165383[m
Merge: 65baf80a7fc 19b3f570fcd
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jun 19 07:36:42 2014 +0200

    Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL
    
    Because of difference about memory ordering, some critical flags of mutex/rw_lock might be missed to read on non-Intel CPUs.
    Even for Intel-CPUs, the explicit memory barrier instruction might cause positive effects for [1;31mperf[mormance.
    
    Approved by Kevin Lewis in rb#5466

[33mcommit 56328cc0c3756395e78bf36abf2ad3ece5a9c42b[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jun 18 12:11:47 2014 +0200

    Bug#18991366 SAFE_HASH::MUTEX (MF_KEYCACHES.C) IS NOT INSTRUMENTED
    
    Before this fix, the rwlock used in the SAFE_HASH implementation
    was not instrumented for the [1;31mperf[mormance schema.
    
    With this fix, the rwlock is now instrumented.
    
    Also fixed the misleading variable name.

[33mcommit 14c09f5647079bdfae13505d622ec4d97f9a2997[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jun 18 07:23:21 2014 +0200

    Bug#18900309 PERFORMANCE SCHEMA MEMORY INSTRUMENTATION IGNORES SETUP TABLES
    
    This is a [1;31mperf[mormance bug, in pfs_memory_alloc_v1().
    
    The recently added [1;31mperf[mormance schema instrumentation for memory
    does not honor:
    - the per instrument enabled flag in table setup_instruments
    - the consumers in table setup_consumers
    
    As a result, memory allocation is always instrumented,
    even when the instrumentation is supposed to be turned off.
    This leads to un necessary statistics collection,
    and to [1;31mperf[mormance overhead.

[33mcommit 6073c2319c6fc655c010a31e422aa54498f4ca70[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Thu Jun 12 18:28:31 2014 +0530

    Bug #18806829 OPENING INNODB TABLES WITH MANY FOREIGN KEY REFERENCES IS
    SLOW/CRASHES SEMAPHORE
    
    Problem:
    
    There are 2 lakh tables - fk_000001, fk_000002 ... fk_200000.  All of them
    are related to the same parent_table through a foreign key constraint.
    When the parent_table is loaded into the dictionary cache, all the child table
    will also be loaded.  This is taking lot of time.  Since this operation happens
    when the dictionary latch is taken, the scenario leads to "long semaphore wait"
    situation and the server gets killed.
    
    Analysis:
    
    A simple [1;31mperf[mormance analysis showed that the slowness is because of the
    dict_foreign_find() function.  It does a linear search on two linked list
    table->foreign_list and table->referenced_list, looking for a particular
    foreign key object based on foreign->id as the key.  This is called two
    times for each foreign key object.
    
    Solution:
    
    Change the linked lists table->foreign_list and table_referenced_list to
    std::set structures table->foreign_set and table->referenced_set.
    
    rb#5673 approved by Vasil.

[33mcommit 717880f9976a47a2d4f254bfd20464c3d66fc08e[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Jun 6 15:26:51 2014 -0700

    Add suggestion to user to maximize [1;31mperf[mormance of updates
    
    API-documentation/Session:
      * To efficiently [1;31mperf[morm read/modify/write, the user should find the object,
      * modify the fields to be updated, set the fields *not* to be updated
      * to undefined, and call session.update on the object.

[33mcommit 238ba132b8032933c54917f4c96f4bdb2eb547a3[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Jun 5 21:28:01 2014 +0200

    WL#7802 PERFORMANCE SCHEMA, BATCH TABLE IO
    
    Added support to report table io events by batch,
    in the [1;31mperf[mormance schema instrumentation.

[33mcommit df5fa3f7dbe309c3def11c5eb61d011df139ab96[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 12:13:54 2014 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
    - The jties tests intentionally checks more or less all permutations of
        const result/parameter types in order to see what is supported
        by C++ vs. Java. Tthis causes warnings about "type qualifiers ignored
        on function return type" (which is turned on by -Wextra in maintainer.cmake).
        Avoid the su[1;31mperf[mluous warnings by turning them off.

[33mcommit 3c5b4b69302ed978d8960ba9b946eddb1c0cf006[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 12:13:01 2014 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - "type qualifiers ignored on function return type [-Wignored-qualifiers]"
     - caused by "const" on function return types which are always considered const
     - fix by removing the su[1;31mperf[mluous const

[33mcommit f45c834dffb420110a0eab33d4690e1b9ba85010[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 11:41:50 2014 +0200

    Adapt pushed joins to WL#6016
    
     1) The call to ha_make_pushed_join() was moved to a place where
       "semijoin dups elimination" had not yet been [1;31mperf[mormed and
       thus the AQP code failed to detect that such part of the
       query should not be pushed. Fix by moving the call to ha_make_pushed_join()
       even later in the optimizer(although that code is in sql_select.cc).
       This is more or less the exact same place as in 5.6 based code.
    
     2) The join type is now determined earlier and serves in the current
        design as the "primary source of the type". Thus the AQP code
        will see join types of JT_RANGE and JT_INDEX_MERGE which can be treated
        the same way as JT_ALL. In long run the two new join types
        can each have separate implementations to avoid the checks to see
        if "quick" is used or not since that is now determined by the join type.
        - JT_RANGE/INDEX_MERGE have to have "quick" set
        - JT_ALL can have "quick" set

[33mcommit 887aebeb4ac41c78bbc590cb9ee110a6e2452440[m
Author: kevin.lewis@oracle.com <>
Date:   Wed Jun 4 15:14:17 2014 -0500

    Bug#17713871-ASSERT ERROR == DB_SUCCESS, COMMIT_CACHE_REBUILD(),
    ALTER TABLE, PRBLEM RENAMING
    
    In order to add even more uniqueness to the temporary filename, this patch replaces the LSN in the filename with a more unique another number.  It is just a static global number that is initialized to a random distributed 32-bit number using ut_time() and ut_crc32().  It is then incremented atomically for each temporary file name assigned.  This should not cause [1;31mperf[mormance regression since an atomic_increment is nothing compared to a OS file rename.
    
    Approved by Marco in RB#5541

[33mcommit daea9a4fe12ef152b54b0b1668cbbaafa674f02f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jun 4 13:27:08 2014 +0200

    Bug#18759597 MISSING ROWS ON WHERE .. IN QUERY WITH VARIABLES AND CONCAT
    
    cmp_item_sort_string_in_static::store_value() assumed that its input
    string could not change. It turns out it can.
    
    In this case we stored 'm' 'n' 'a' 'l' 'xm' in column one of each row.
    We sort the data to get 'a' 'l' 'm' 'n' 'xm'
    
    Then the expression is analyzed again, and Item_func_concat::tmp_value
    changes value. So when we do lookup, we search in 'a' 'l' 'm' 'n' 'm'
    
    The binary-search failed, since the array of rows is no longer
    properly sorted.
    
    Fix: Save string results locally in
    cmp_item_sort_string_in_static::store_value()
    This is done only once during the prepare phase, so it should not have
    any negative [1;31mperf[mormance impact.
    
    After doing that change, we see that the classes
    cmp_item_sort_string and cmp_item_sort_string_in_static are nearly identical.
    We can remove them both, and move all member functions into cmp_item_string instead.
    
    Also: remove some C-style casts, introduce new template function down_cast instead.
    
    Where/why is it analyzed again?
    The range optimizer does some analysis and ends up calling Item::save_in_field_no_warnings
    which ends up calling Item_func_concat::val_str() again.
    Looking at the source code of that function, we see there are many
    ways that tmp_value can change.

[33mcommit 8275f0c1011fe619a8beb1db46607526e85241ef[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu May 29 20:13:29 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Start with a prototype for accounting new-like allocations.
    
    Usage of the provided accounting methods is:
    
    new int;       --replacedby--> UT_NEW(int);
    new Foo;       --replacedby--> UT_NEW(Foo);
    new Foo(args); --replacedby--> UT_NEW(Foo(args));
    delete x;      --replacedby--> UT_DELETE(x);
    new Foo[8];    --replacedby--> UT_NEW_ARRAY(Foo, 8);
    delete[] arr;  --replacedby--> UT_DELETE_ARRAY(arr);
    
    Output in [1;31mperf[mormance_schema looks like this:
    
    mysql> SELECT * FROM memory_summary_global_by_event_name WHERE event_name LIKE '%/innodb/%'\G
    *************************** 1. row ***************************
                      EVENT_NAME: memory/innodb/btr0btr
                     COUNT_ALLOC: 80
                      COUNT_FREE: 80
       SUM_NUMBER_OF_BYTES_ALLOC: 1675
        SUM_NUMBER_OF_BYTES_FREE: 1675
                  LOW_COUNT_USED: 0
              CURRENT_COUNT_USED: 0
                 HIGH_COUNT_USED: 1
        LOW_NUMBER_OF_BYTES_USED: 0
    CURRENT_NUMBER_OF_BYTES_USED: 0
       HIGH_NUMBER_OF_BYTES_USED: 25
    *************************** 2. row ***************************
                      EVENT_NAME: memory/innodb/dict0dict
                     COUNT_ALLOC: 3
                      COUNT_FREE: 0
       SUM_NUMBER_OF_BYTES_ALLOC: 624
        SUM_NUMBER_OF_BYTES_FREE: 0
                  LOW_COUNT_USED: 0
              CURRENT_COUNT_USED: 3
                 HIGH_COUNT_USED: 3
        LOW_NUMBER_OF_BYTES_USED: 0
    CURRENT_NUMBER_OF_BYTES_USED: 624
       HIGH_NUMBER_OF_BYTES_USED: 624
    ...
    *************************** 6. row ***************************
                      EVENT_NAME: memory/innodb/other
                     COUNT_ALLOC: 0
                      COUNT_FREE: 0
       SUM_NUMBER_OF_BYTES_ALLOC: 0
        SUM_NUMBER_OF_BYTES_FREE: 0
                  LOW_COUNT_USED: 0
              CURRENT_COUNT_USED: 0
                 HIGH_COUNT_USED: 0
        LOW_NUMBER_OF_BYTES_USED: 0
    CURRENT_NUMBER_OF_BYTES_USED: 0
       HIGH_NUMBER_OF_BYTES_USED: 0
    6 rows in set (0.00 sec)

[33mcommit 810de5cecbebaf2ea0d8784d2decbbc184c9553d[m
Merge: 64a39949165 d9943287566
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed May 14 10:44:57 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
    
      There are specific use-cases where-in modules needs a temporary table for
      stagging data for a while and doesn't really care about transaction semantics.
    
      Given the use-case we have designed a special type of temporary table that
      are light weight (no-undo, no-transactional semantics, no-locking,
      no-doublewrite-buffer, operational even in read-only, etc....) and are
      optimized for [1;31mperf[mormance.
    
      These tables will come into existence for a while
      (for example: during query exeuction) for stagging a data of subquery
      and so shortlived. Being temporary table they would continue to reside
      in shared temporary tablespace.
    
      Approved by: rb#4870 (Sunny + BinSu)

[33mcommit cc2a5250d8bb11d2f237e2ff5d08b7d8e24b2bdc[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue May 13 10:00:20 2014 +0530

    Bug #18723872  FK: ASSERTION: *CURSOR->INDEX->NAME == TEMP_INDEX_PREFIX
    
    This is a regression caused by the fix for Bug #11758237 INSERT ON DUPLICATE
    KEY UPDATE SOMETIMES WRITES BINLOG POSITION INCORRECT.  The trx->error_state
    will be modified when a thread is suspended.  But logic of placing just the gap
    locks on unique secondary indexes without actually [1;31mperf[morming the insert
    depends on the value of trx->error_state.  So the value of trx->error_state is
    restored for insert into every index.  Refer to rb#3196 for the original fix.
    
    approved by Marko over IM.

[33mcommit d9cb2a9a14d7b0cd741e0e400c1ae51fc23269a3[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu May 8 08:02:59 2014 +0200

    Bug#18675099: WINDOWSCACHE.CMAKE MUST BE UPDATED FOR VS2013
    
    WindowsCache.cmake hardcodes results for CMake configure checks so they
    don't have to be [1;31mperf[mormed on Windows (as this is very slow).
    
    The problem was that the cache file had several values which were no
    longer correct. The consequence that we were using Windows specific
    workarounds in cases where it was no longer needed.
    
    This patch removes the outdated values from the cache file so that the
    checks are actually made. It also reorganizes the cache file so that it
    matches config.h.cmake, removes duplicate entries, removes dead
    entries and adds missing entries. The patch also removes some
    unneeded Visual Studio version checks.

[33mcommit fcad58710b5ee23cf5384bdecd956469c2be9be4[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu May 8 11:01:48 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Directly invoke appropriate version of row_search_for_xxxx
        This helps in saving some computation that otherwise seems to cost
        on [1;31mperf[mormance.

[33mcommit e942cd2612a7a8cf6bdf4a5b56a618eeb939050d[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed May 7 14:09:15 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Removed redundant initialization to fix [1;31mperf[mormance drop.

[33mcommit 2dddb4d95f35eba754b3499d855d2143da2b3479[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed May 7 09:31:50 2014 +0200

    Fix the failing ndb_big_addnode.test which started failing after
    increasing ThreadConfig to 4 LDM threads.
    
    Similar changes has already been [1;31mperf[mormed on the similar
    'non-big' ndb_addnode.test

[33mcommit 170f30b0dbe0f4fb200089765523839c39751a37[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon May 5 21:06:32 2014 -0700

    Add two new classes, DBTransactionContext and DBSessionImpl.
    These will free the JavaScript code from the start/prepare/execute
    cycle of Ndb and NdbTransaction, eventually allowing operations
    to be [1;31mperf[mormed with fewer scheduled async calls and therefore
    less overhead.

[33mcommit 99714eed97ec737c920f972962a755dce8326707[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue May 6 11:03:37 2014 +0300

    Bug#18704384 REMOVE DICT_IND_COMPACT() AND SIMPLIFY PAGE_CREATE()
    
    page_create_low(): Use low-level byte access to initialize the page,
    instead of invoking the high-level methods.
    
    page_dir_get_nth_slot(): Add parenthesis to help the compiler [1;31mperf[morm
    constant folding, when calling the macro with a constant n.
    
    dict_ind_compact: Remove.
    
    rb#5322 approved by Kevin Lewis and Sunny Bains

[33mcommit 0111249e608be6aa808548922adaa458a8c346e9[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Apr 30 20:29:32 2014 -0700

    This change alllows Ndb::startTransaction() to run as a sync call in the
    main JS thread if we suspect that it will not block.  As a proof of
    concept, this allows you to measure the [1;31mperf[mormance with this change;
    but it should not be used as-is, because the guess could be wrong,
    which would cause the main thread to block waiting for network I/O.

[33mcommit c676cfd1b023fe9f096096c16791af7002347e1a[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 30 12:32:08 2014 +0200

    ndb_backup_rate.test checks that we are able to recover normal
    operation after a 'Redo log full' situation.
    
    This is done by configuring a slow 'DiskCheckpointSpeed=1M', and
    letting the test program produce lots of logging by inserting and
    deleting rows while waiting for a 'Redo log full' error.
    
    However, when running with debug compiled binaries, the insert + delete
    [1;31mperf[mormance could be so slow that we are unable to fill the redo log.
    Thus this test often failed in the 'mix-debug' part of the test.
    
    This fix disable the test when run with debug binaries.
    NOTE: It was already disabled for Valgrind for the same reasons.

[33mcommit 65b3474c13bc5b17fa9d801e835e794ecf1c1ef5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Apr 25 16:48:09 2014 +0300

    Bug#18651616 UT_AD(0) IN FIL_NAMES_WRITE() IN
    IBUF_MERGE_OR_DELETE_FOR_PAGE(), DROP TABLE
    
    A debug assertion can fail in fil_space_lookup() when a page I/O
    completion thread is [1;31mperf[morming a change buffer merge on a
    single-table tablespace while DROP TABLE is executing.
    
    The fix is to remove the debug assertion and to never skip writing a
    MLOG_FILE_NAME record due to space->stop_new_ops. There is no possible
    race condition thanks to the interlocking of space->stop_new_ops and
    space->n_pending_ops.
    
    fil_space_free_low(): Update a comment.
    
    fil_names_write(): Remove the debug assertion and expand the comment.
    
    rb#5244 approved by Yasufumi Kinoshita

[33mcommit 6574d5da5753ebeb4d0375a57c176713428e722a[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Thu Apr 24 14:09:12 2014 +0530

    BUG#18484649 - STRICT MODE + CURSOR FETCH INTO WRONG TYPE,
                   ASSERTION IN PROTOCOL::END_STATEMENT
    
    BACKGROUND:
    An assertion failure in 5.7 in Protocol::end_statement()
    occurred when we try to fetch a large value (int) which
    cannot fit into a variable (tiny int) declared inside
    stored routine through a cursor. However, it works fine
    in 5.6.
    Also, This assertion failure happens only in debug builds,
    We do not get any assertion failure/crash on release builds,
    Because asserts are not active on release builds.
    
    ANALYSIS:
    On analysis, it is found that error comes in
    Materialized_cursor::fetch() function. But its return type
    is void. Therefore, the calls to upper level functions assumed
    everything went OK with it (however, it resulted in error),
    so it resulted in incorrect value of err_status in
    sp_head::execute() function.
    The documentation for
    virtual bool execute(THD *thd, uint *nextp) = 0;
    function in sp_instr.h says
    
        @return Error status.
    
    In this case execute() did not [1;31mperf[morm as documented - it has
    reported an error but still returned false.
    
    FIX:
    The fix is to set err_status variable correctly (to true if
    it is known that some error has occurred or the query was
    killed i.e thd->killed is set).
    The return type of function Materialized_cursor::fetch() is
    modified from void to bool, so that it reports the error to
    upper level functions correctly. This fix results in err_status
    variable inside sp_head::execute() to be set correctly
    after the execution of statement and [1;31mperf[morm according to
    the documentation.

[33mcommit 83eb58476c024a501b06f608c4e5eb3e8f440cb0[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Apr 17 14:52:26 2014 +0200

    Bug#18259356 - PROCESSLIST_ID FOR SLAVE SQL THREAD IS MISSING
    
    For historical reasons, background replication threads are
    assigned a connection_id(), and displayed in:
    - SHOW PROCESSLIST
    - table INFORMATION_SCHEMA.PROCESSLIST.
    
    In the [1;31mperf[mormance schema, these threads are also displayed,
    but with a NULL PROCESSLIST_ID.
    
    Having the same thread displayed:
    - with a PROCESSLIST_ID (in the information_schema)
    - without a PROCESSLIST_ID (in the [1;31mperf[mormance_schema)
    is inconsistent, and is a bug.
    
    With this fix, the ID assigned in the server code
    is also given to the instrumentation,
    so that column PROCESSLIST_ID for replication threads
    is correctly populated in table [1;31mperf[mormance_schema.threads.

[33mcommit f0c5773e87f297297bd48d2c46029ca9b921c2fd[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Apr 11 12:19:46 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - During shutdown don't demand even if buf-fix-count > 0 continue to flush it.
        This is safe given that it is for temporary tablespace only (where-in we
        can avoid flush completely on shutdown) + user threads are already done
        [1;31mperf[morming the action.

[33mcommit 8b7a460cbe5f104ea130c0be4c03e0773fbe21ff[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Apr 1 19:20:03 2014 +0300

    Fix Bug#71708 70768 fix [1;31mperf[m regression: high rate of RW lock creation
    and destruction
    
    Lazily create dict_table_t::stats_latch the first time it is used.
    It may not be used at all in the lifetime of some dict_table_t objects.
    
    Approved by:    Bin (rb:4739)

[33mcommit 4a2b168ffc95ebcd95f711087093e51476dab81a[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Mon Mar 31 09:03:38 2014 +0200

    Bug#18364815: OPTIMIZER PREFERS TABLE SCAN WHEN USING "IN"
                  WITH VALUE OF DIFFERENT TYPE
    
    Item_func_in checked whether there were items of different type
    in the IN clause and would automatically reject the use of
    range access in that case. However, this is too restrictive
    since a great deal of type conversions are automatically done
    in MySQL, e.g. conversion of quoted numbers to integer.
    "col IN (1, "2")" is therefore [1;31mperf[mectly legal but used to be
    rejected.
    
    The fix is to remove the automatic rejection and let the range
    optimizer make the decision on its own since it already
    [1;31mperf[morms this check on its own (see comparable_in_index())
    anyway.

[33mcommit 2c49837e2180fb16ac45756c65e1bd935919336d[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Mar 26 00:51:12 2014 +0000

    Bug #17461576   NDB REPLICATION : STOP SLAVE WHEN MASTER EPOCH DECLINES
    
    Fix which monitors the epochs received from the immediately
    upstream master, determined by server_id column == server_id metadata.
    
    Specifically, an observed epoch decline will have different results
    depending on the state of the Slave SQL thread.
    
      1.  Just after a RESET SLAVE statement
          No effect
      2.  Just after a START SLAVE statement
          A warning is produced
      3.  None of the above.
          The Slave SQL thread will stop.
    
    The intention with 1) is to enable warning free setting.
    The intention with 2) is to warn when a Slave is being positioned on
    a previously applied epoch.
    The intention with 3) is to stop the Slave when there is some system
    malfunction resulting in the re-application of an existing epoch.
    
    Testing is [1;31mperf[mormed using error insertion on the upstream master.
    
    A new testcase, ndb_rpl_slave_replay is added to the ndb_rpl suite.

[33mcommit cdae3c3a69919f43efb805ab829629263b43752d[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Mar 25 12:36:56 2014 +0100

    Bug#18363910 SKIP MAIN.PLUGIN_LOAD AND MAIN.PLUGIN_LOAD_OPTIONS ON WINDOWS EMBEDDED
    
    The tests plugin_load and plugin_load_options will pre-load the plugin
    *before* the test is started, so the 'skip this on windows' logic is executed too late.
    
    Plugin initialization fails because the embedded server is compiled without
    [1;31mperf[mormance schema, and the plugin is compiled with [1;31mperf[mormance schema.
    In this case, mysql_mutex_register has two different implementations,
    breaking the "one-definition-rule". The trick of mapping PSI_server to
    PSI_noop works on most platforms, but not windows.
    
    Solution: remove the mutex registration code, the mutex was unused anyways.

[33mcommit df88218c130bd7abe556424b10fa5afcf298254c[m
Author: Evgeny Potemkin <evgeny.potemkin@oracle.com>
Date:   Fri Mar 21 14:18:24 2014 +0400

    Bug#18335908: SERVER CRASH IN MAIN.JOIN_CACHE TEST
    Join cache could link to a previous join cache in order to provide better
    [1;31mperf[mormance. Linking is done by join cache's constructor. However, when
    later allocation of buffer for join cache fails, the join cache is dropped,
    but link is left as is. This leaves an uninitialized join cache available
    to a prev join cache. When server starts fetching records it tries to get
    data from that dropped join cache through a valid prev join cache object
    and this leads to server crash.
    Now when a join cache's initialization fails and prev cache is available
    the link to the join cache being dropped is cleared.

[33mcommit eee11a50dae271d91587271d4f7592febc1620cc[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Mar 17 21:55:00 2014 -0700

    Add test case for issue where a TableMapping created from a literal mapping
    could not be used to [1;31mperf[morm operations (fixed in bzr 686).

[33mcommit d70d23160f1405cedd89a77dd8f2edf846028a7f[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Wed Mar 12 04:00:22 2014 +0100

    Removed [1;31mperf[mschema.mdl_func since the test is fixed. Updated rpl_gtid_stress_failover with correct bug number

[33mcommit d620c25aef9254d47031ebf63c7a2a88e2ef46f1[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun Mar 9 19:33:46 2014 +0800

    Bug#17932935 CALLING IS_SEMI_SYNC_SLAVE() IN EACH FUNCTION CALL
                 HAS BAD PERFORMANCE
    
    Semisync master Binlog_transmit_observer needs to know if the
    dump thread is from a semisync slave. Because the logic is
    different between semisync slave and normal slave. So
    is_semi_sync_slave() is called in each Binlog_transmit_observer
    function. is_semi_sync_slave() reads the user variable
    'rpl_semi_sync_slave' through checking the dump thread's user
    variable hash table. That is very slow and has remarkable impact
    on semisync replication [1;31mperf[mormance.
    
    the user variable 'rpl_semi_sync_slave' is never changed after
    it is initialized. So we optimized the code as below:
    * is_semi_sync_slave() is only called in
      repl_semi_binlog_dump_start() when starting the dump thread.
      And then its value is stored in a pthread_key.
    
    * Other Binlog_transmit_observer functions just get it value
      through reading the pthread_key.

[33mcommit da539823da56b63d546d4a406cde4831bc27f98d[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Mar 7 09:16:57 2014 +0100

    WL#7578 Don't use ndb_schema_share from ack_schema_op
    
     - The schema event handler part of the binlog thread has received
       an event(someone inserted into mysql.ndb_schema) and it should
       [1;31mperf[morm the event and reply with an ack.
     - Even if ndb_schema_share pointer should be NULL, the function
       must reply. Especially since the reply does not use the ndb_schema_share
       pointer but rather just writes to mysql.ndb_schema.
     - Should mysql.ndb_schema not exist at this time, the write will of course fail
       but then all the other mysqld(s) wil also notice the same problem
     - Also, it was wrong to check "ndb_schema_share" without taking the
       "ndb_schema_share_mutex"

[33mcommit 565d20b44f24fcc855dc616164d87b03cfad10bc[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Mar 6 12:15:07 2014 +0100

    Bug#17606098: DEADLOCK INVOLVING SRV_MONITOR_THREAD AND LOCK_THD_DATA
    
    This deadlock involved LOCK_thd_data and InnoDB's trx_sys mutex.
    It could occur if InnoDB code (e.g. the srv_monitor_thread) called
    thd_security_context() while having trx_sys locked and at the same time
    the server tried to notify InnoDB about a deadlock (which involves calling
    mysql_lock_abort_for_thread() while holding LOCK_thd_data).
    This could lead to deadlock if mysql_lock_abort_for_thread() lead to
    trx_allocate_for_mysql() being called inside InnoDB.
    
    This very rarely happens - the problem was found with RQG and is very
    difficult to reproduce.
    
    This patch solves the problem by splitting LOCK_thd_data so that a
    separate new mutex LOCK_thd_query protects the query string
    accessed by thd_security_context().
    
    The patch also strengthens the protection of the query string by
    enforcing that it can only be set by the owner thread and that this
    requires locking of LOCK_thd_query. Reading the query string can
    be done by the owner thread without holding LOCK_thd_query but
    other threads reading the query string have to have LOCK_thd_query
    locked.
    
    This also solves a separate problem where other threads could
    read the query string while it was being deleted by the owner thread.
    
    Finally, the patch updates the [1;31mperf[mormance schema mutex
    heuristics - including correcting an issue introduced by WL#6369.

[33mcommit 2a1975757944f5d17e5ffcc719d52d051e38720d[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Mar 3 13:53:59 2014 +0100

    WL#7082 -  Move permanent transformations from JOIN::optimize () to JOIN::prepare ().
    As one single patch.
    
    semijoin, outer-join-to-inner, parenthesis-removal,
    join-condition-to-where-clause are moved from JOIN::optimize() to
    JOIN::prepare(), which is renamed to select_lex::prepare().
    
    Old approach for WHERE/HAVING conditions:
    at end of preparation, save copies of conditions in
    sl->prep_where/having, then allow oneself to trash sl->where/having in
    optimization; in optimization make sure to keep in sync
    sl->where/having with join->conds/having. At start of next execution,
    recreate sl->where/having from the "prep_" copies.
    New approach:
    At end of preparation, sl->where/having is considered frozen. In
    optimization, make trashable copies of it, and use only those
    copies. Ditch them at end of execution.
    
    Some functions like mysql_select() are made to use select_lex->where;
    in the pre-patch situation select_lex->where was passed as argument
    AND the function assumed that this argument was
    ==select_lex->where... this change makes code simpler.
    
    Made some functions which use JOIN to rather use, and belong to, SELECT_LEX:
    record_join_nest_info
    simplify_joins
    convert_subquery_to_semijoin
    resolve_subquery
    flatten_subqueries.
    They try to use JOIN as little as reasonably possible.
    
    Moved JOIN::prepare() to select_lex, and simplified its signature
    (arguments can be found in select_lex).
    Made setup_conds() member of select_lex, with less arguments.
    Removed arguments from setup_ref_array().
    
    Simplified setup_wild(), more JOIN members are made private,
    JOIN::join_list removed, reset_nj_counters goes to select_lex.
    
    JOIN::table_list is now used only in optimization/execution.
    
    JOIN_TAB::on_expr_ref, JOIN/select_lex::where/conds/having are
    renamed, some getters/setters are added.
    
    Simplified select_lex::first_cond_optimization: rename member to be
    more specific, and removed argument in make_join_statistics().
    
    I remove some hacks which came in previous PS fixes (see the WL for
    bug numbers), because they become su[1;31mperf[mluous.
    
    Changes in opt trace tests: trace blocks for
    semijoin/outer-join-to-inner move from the join_optimization block to
    the join_preparation block. In ps-specific tests, where the shown
    trace is of EXECUTE, it implies that now the trace starts with the
    transformation already done (by PREPARE).
    
    See the WL text for overview of goal and code changes.
    
    Additional details are in these commit comments.
    
    @  mysql-test/suite/opt_trace/include/bugs.inc
    
    bug was fixed long ago
    
    @  mysql-test/suite/opt_trace/r/bugs_no_prot_all.result
    
    Also fixed a bad number for the query block where the view is merged
    ("in_select#").
    
    @  mysql-test/suite/opt_trace/r/general2_no_prot.result
    
    fixed trace of view merging/materialization
    
    @  mysql-test/suite/opt_trace/r/general_ps_prot_all.result
    
    Around line 7866, we see:
    "original_condition": "(1 and (`t2`.`s` = 'c') and (`t6`.`d` = `f1`()))",
    "steps": [
      {
        "transformation": "equality_propagation",
    -    "resulting_condition": "(1 and multiple equal('c', `t2`.`s`) and multiple equal(`f1`(), `t6`.`d`))"
    +    "resulting_condition": "(1 and (`t6`.`d` = `f1`()) and multiple equal('c', `t2`.`s`))"
    A multiple equality is not created anymore for t6.d=f1(). I think this
    is ok. Creation of such item requires f1() to be a constant (see
    check_simple_equality() which looks for field=constant), and f1()
    should not be considered constant, it is a stored function which may
    return a different value for each row of t2.
    
    @  mysql-test/t/sp.test
    
    Tests added along the way, when fixing bugs in the prototype
    
    @  mysql-test/r/subquery_all.result
    Result changes in EXPLAIN: they come from the can_skip_order change in sql_union.cc:
    we used to pass a NULL pointer to prepare(), now we instead empty the list in select_lex;
    this has the advantage of showing the optimization in the query printed by EXPLAIN;
    and this alternative technique looks ok because remove_redundant_subquery_clauses
    does it too.
    
    @  sql/item.h
    
    removed one hack ("real_items" argument), added chop_ref
    
    @  sql/item_cmpfunc.cc
    
    removed one hack ("real_items" argument)
    
    @  sql/item_cmpfunc.h
    
    removed one hack ("real_items" argument)
    
    @  sql/item_subselect.cc
    
    About the removal of "// did we changed top item of WHERE condition":
    - in-to-exists is, as before, a permanent transformation
    - pre-patch, Item_in_subselect::fix_fields() would be passed &JOIN::conds as "ref"
    argument, so when it changes the condition (injects outer=inner
    equality in subquery's WHERE), it changes *ref, which changes
    JOIN::conds, but because it is a permanent transformation, it also
    needs to manually "keep in sync" select_lex->where.
    - post-patch, fix_fields() operates on &select_lex->where_cond, and
    JOIN::where_cond is not "alive" yet (it starts its life in
    JOIN::optimize()), so no manual syncing is needed.
    
    Likewise, no manual syncing of having_for_explain is needed.
    
    @  sql/sql_base.cc
    
    No manual syncing needed (see comment of item_subselect.cc).
    
    @  sql/sql_delete.cc
    
    Don't pass "conds", just use select_lex->where_cond as input. When we
    want to optimize the condition, we make a copy of it.
    
    @  sql/sql_lex.cc
    
    Part of the end-of-prepare job of
    st_select_lex::fix_prepare_information() has moved to
    the start-of-optimize get_optimizable_conditions().
    One real_item() is removed, in this move.
    
    @  sql/sql_optimizer.cc
    
    Get trashable copies at start of JOIN::optimize() and use only them.
    Removed dead code in #ifdef.
    Transformations move to JOIN::prepare(), and the horror of "let's
    update prep_where because we did permanent transformations in
    JOIN::optimize()", is gone - this saves some copying and memory allocations.
    In simplify_joins(), the part:
              /* If join condition has a pending rollback in THD::change_list */
              join->thd->change_item_tree_place(table->join_cond_ref(), &conds);
    was useless: "conds" is a local variable, &conds could never be found
    when change_item_tree_place() searches. So I replace "conds" by an
    Item** passed in argument. In a test file I added some queries which
    used to break (lack a rollback) due to this useless code.
    In record_join_nest_info(), prep_join_cond; the corresponding job is
    now in get_optimizable_join_conditions().
    In replace_subcondition(), removed useless manual syncing.
    At the end of flatten_subqueries(), same.
    
    @  sql/sql_prepare.cc
    
    use setup_fields_with_no_wrap, makes less code lines.
    In reinit_stmt_before_use(), don't recreate select_lex->where_cond, it is
    already good (== made of permanent items); only need to clean up those
    items, to make them ready for reusal.
    
    @  sql/sql_resolver.cc
    
    JOIN::prepare now operates only on select_lex->where, not JOIN::conds
    which is now reserved for JOIN::optimize.
    JOIN::prepare does transformations at its end.
    Some functions now done at "prepare" time are moved to this file.
    
    @  sql/sql_union.cc
    
    We work around an oddity of IN->EXISTS (whose effects were only a
    strange WHERE in the trace, when I started trusting
    select_lex->where_cond instead of always passing conds=NULL
    to JOIN::prepare() of the fake select lex).
    
    @  sql/sql_update.cc
    
    When we start optimizing UPDATE, we get trashable conditions.
    
    @  sql/sql_view.cc
    
    Fixed a bad number for the query block where the view is merged
    ("in_select#"), this is visible in bugs_no_prot_all.result file.
    
    @  sql/table.h
    
    In TABLE_LIST, m_join_cond becomes the permanent condition,
    m_optim_join_cond a trashable copy created at start of optimization.
    Moved all optimization-only members to one place.

[33mcommit 152b4c8441ecfe9857225225de41aab476d1b0f7[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Fri Feb 28 16:42:26 2014 +0400

    Follow-up for WL#7306 "Improve MDL [1;31mperf[mormance and scalability by
    implementing lock-free lock acquisition for DML".
    
    Fix for mdl_sync-t unit test failures under ASAN/Valgrind and on
    Windows.
    
    Ensure that Mock_error_handler error handler gets removed before
    THD object, for which it was installed, is destroyed.

[33mcommit 4993303a08424a75e6d2121825bdd7ac7263362d[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Feb 28 01:50:48 2014 +0100

    Bug#18324285 PERFORMANCE OVERHEAD IN CONNECT DISCONNECT WITH PERFORMANCE
    SCHEMA
    
    This fix is a [1;31mperf[mormance improvement.
    
    1)
    
    When a thread connects, reset of all per thread statistics
    are now delayed until a statistic is actually collected.
    
    This lazy initialization benefits workloads with very short lived sessions,
    for which instrumentation is disabled.
    
    2)
    
    When a thread disconnect, the per thread statistics are aggregated
    to a parent only for statistics that actually collected data.
    
    This optimization benefits workloads with very short lived sessions,
    for which instrumentation is disabled.
    
    3)
    
    For the statement instrumentation,
    reset of an individual event_name statistic is also now delayed
    until a statistic is actually collected.
    
    This benefit all workloads, because all workloads only contain a few
    types of statements (SELECT, INSERT, UPDATE, DELETE, ...),
    from the very long list of statements supported in MySQL.
    
    Only statements for event names actually executed are aggregated on
    disconnect.
    
    4)
    
    The memory footprint of internal memory buffers is reduced,
    by removing some attributes reserved for future use,
    that were in fact not used.
    
    In particular, statistics for mutexes, rwlocks and conditions
    now need less memory.

[33mcommit 0c60d406c00c598a7c63eb7b962af0b89e432a06[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Feb 27 23:17:23 2014 +0400

    WL#7306 "Improve MDL [1;31mperf[mormance and scalability by implementing lock-free
    lock acquisition for DML" and fix for bug #18077895 "WL7305 PUSH (7249)
    CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP UP TO -8%".
    
    The idea of this task is to change the acquisition of unobtrusive locks on
    the fast path, which currently involves acquisition of MDL_lock::m_rwlock,
    some checks, increment of the packed counter and release of m_rwlock, to
    a single atomic compare-and-swap operation.
    Similarly the release of the lock on the fast path becomes single atomic
    compare-and-swap (in absence of obtrusive locks and assuming we are not
    releasing the last lock for this MDL_lock object) instead of acquisition
    of MDL_lock::m_rwlock, decrement of the packed counter, some checks and
    m_rwlock release.
    As result these operations become at least twice cheaper than their
    old versions which has a nice effect on [1;31mperf[mormance/scalability.
    
    Acquisition/release of locks on the slow path (i.e. unobtrusive locks in
    presence of obtrusive locks and obtrusive locks) still has to use the old
    approach involving locking/unlocking MDL_lock::m_rwlocks and checks of
    MDL_lock::m_granted/m_waiting bitmaps/lists.
    
    This patch implements the above idea by [1;31mperf[morming the following
    three transformations:
    
    I)   MDL_lock::m_fast_path_granted_count is replaced with an atomic
         MDL_lock::m_fast_path_state member, which in the ideal case of
         "fast path" acquisition/release is checked and changed using CAS
         without holding any mutexes.
    II)  Since we would like to check in the same atomic CAS operation that
         MDL_lock object was not destroyed, its m_is_destroyed member is
         replaced by a IS_DESTROYED bit flag in the m_fast_path_state
         packed counter.
    III) Similarly, since we also would like to check in the same atomic CAS
         that there are no granted or pending obtrusive locks, we have to
         add a HAS_OBTRUSIVE bit flag in the m_fast_path_state, while
         keeping MDL_lock::m_obtrusive_locks_granted_waiting_count.
         This flag should be set when we are about to try acquiring an obtrusive
         lock and cleared once the last granted or pending obtrusive lock goes
         away.
    
    
    Most of the remaining changes in this patch are necessary in order to fix
    bug #18077895 "WL7305 PUSH (7249) CAUSES SYSBENCH OLTP_RO 1 THREAD TO DROP
    UP TO -8%".
    
    This bug manifested itself as a slowdown for workloads involving 1 connection
    in cases when there were many concurrent connections to the same server in the
    past or there were many dormant connections at the same time as 1 active
    connection.
    
    In such scenarios the release of a metadata lock meant that MDL_lock became
    unused, was removed from lock-free hash with all lock objects and we
    tried to return it back to allocator. The latter operation involved
    scanning pins for all current and past connections, which became fairly
    expensive in this scenario.
    
    This patch solves this problem by avoiding releasing MDL_lock objects
    and removing them from the hash once they are no longer used. Instead we
    keep unused objects in MDL_map and start their eviction only if their
    number passes certain threshold and the ratio of unused/total lock objects
    is big enough. We evict random unused objects so on average objects
    which are used more often will stay in the hash and rarely used objects
    will go away.
    
    The above idea is implemented by:
    
    a) Introducing a new HAS_SLOW_PATH flag in the MDL_lock::m_fast_path_state
       member, which indicates if there any tickets in MDL_lock::m_granted
       and m_waiting lists or we are about try to add one. Thanks to this
       flag, it is possible to distinguish between used and unused MDL_lock
       objects in atomic compare-and-swap operations used to implement fast
       path acquisition and release of locks.
    b) Changing code which releases locks to avoid removing unused MDL_lock
       objects from the hash and deleting them afterwards. Instead we
       atomically increment the newly introduced MDL_map::m_unused_lock_objects
       counter. Similarly, on the first acquisition of lock for MDL_lock which
       was previously unused we atomically decrement this counter.
    c) In cases when the increment of the MDL_map::m_unused_lock_objects counter
       exceeds the threshold value and the unused/total objects ratio is high
       enough, we try to reduce the number of unused objects. We look-up a random
       unused object in MDL_map, mark it as destroyed, remove it from the hash and
       return it back to allocator. As a consequence MDL_map::remove() method
       has became MDL_map::remove_random_unused().
    d) To support the change described in c), a new lf_hash_random_match()
       function was introduced which allows us to efficiently find a random
       object which matches certain condition in LF_HASH.
    e) Also to support the change described in c), a new PRNG was added to
       MDL_context class. This PRNG is used as a source for randomness for
       look-ups of random unused objects.
    
    Unit tests were added covering handling of unused MDL_lock objects and
    for the new lf_hash_random_matches() function.
    
    
    Finally, this patch fixes a violation of the pinning protocol, which was
    introduced by WL7305 and which occured when the MDL subsystem failed
    to look up MDL_lock object in lock free hash. The LF_HASH documentation
    was updated to reflect the need to call lf_hash_search_unpin in this case.

[33mcommit 2b0a9b1193ff07a0ca9302a92df710e9fd3cf383[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Mon Feb 24 13:38:40 2014 +0100

    Bug#18136628: RANGE OPTIMIZER: IMERGE_LIST_OR_TREE() ERROR DUE
                  TO SHALLOW COPY
    
    imerge_list_or_tree() [1;31mperf[morms the following OR operation in
    the range optimizer: "<list_of_index_merges> OR <sel_tree>",
    e.g
    
    Index merge list:
       (pred1_idx1 OR pred1_idx2)    <- an index merge
       AND
       (pred2_idx1 OR pred2_idx2)    <- another index merge
    OR
    sel_tree:
       pred3_idx1
    
    as follows: 'sel_tree' is first ORed with
    "(pred1_idx1 OR pred1_idx2)" and then with
    "(pred2_idx1 OR pred2_idx2)" by calls to tree_or(). However,
    tree_or() is allowed to modify its parameters, and because both
    index merges should be ORed with the same 'sel_tree' predicate,
    imerge_list_or_tree() makes a copy of it for each call to
    tree_or(). This is done to avoid the following situation:
    
    Index merge list:
       (col1 < 10 OR <something>)
       AND
       (col1 = 5 OR <something>)
    OR
    sel_tree:
       col1 > 9
    
    imerge_list_or_tree() does:
    1) Perform "col1 < 10 OR col1 > 9" => always true, 'sel_tree'
       is marked accordingly
    2) Perform "col1 = 5 OR true" => always true since 'sel_tree'
       was marked as true. This is obviously incorrect and was
       caused by step 1) modifying 'sel_tree'. The remedy is to
       copy 'sel_tree'.
    
    Without copying 'sel_tree', the range would therefore be:
    
       (TRUE OR <something>) AND (TRUE OR <something)
    => TRUE
    
    While the correct range is:
    
       (TRUE OR <something>)
       AND ((col1 = 5 OR col1 > 9) OR <something>)
    => ((col1 = 5 OR col1 > 9) OR <something>)
    
    The problem in this bug was that while imerge_list_or_tree()
    correctly made a copy of 'sel_tree' before each OR, the copy
    it made was shallow. Results from step 1) therefore still
    affected the copy of 'sel_tree' used in step 2.
    
    The fix is to change the SEL_TREE ctor to make a deep
    copy. The ctor is not used anywhere else, so this only
    affects imerge_list_or_tree()

[33mcommit 25781c154396dbbc21023786aa3be070057d6999[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Feb 24 14:00:03 2014 +0530

    Bug #17604730 ASSERTION: *CURSOR->INDEX->NAME == TEMP_INDEX_PREFIX
    
    Problem:
    
    When INSERT ... ON DUPLICATE UPDATE or REPLACE statements are used, then
    after encountering a DB_DUPLICATE_KEY error, we continue to process all
    the unique secondary indexes to place the necessary gap locks.  The
    problem is in the following scenario:
    
    1. The table has one primary index, one unique secondary index and
       one non-unique secondary index.
    2. The INSERT ... ON DUPLICATE UPDATE ... is executed on the table.
    3. Insert into the clustered index reported DB_DUPLICATE_KEY.  This
       error information is saved.  We proceed to take gap locks in all
       unique secondary indexes.
    4. Insert into the unique secondary index reported DB_LOCK_WAIT.
    5. Step 4 is repeated from a higher layer row_ins().  When this is
       done, the earlier error information saved in step 3 is lost.
    6. Next instead of taking just gap locks or skipping non-unique
       secondary indexes, because of loss of information regarding the
       error already saved, an actual insert is [1;31mperf[mormed on the non-unique
       secondary index.  This triggers the assert.
    
    Solution:
    
    Save the error information in a non-local location so that it is not lost.
    
    rb#4723 approved by Kevin.

[33mcommit 9b3b0f7d5f76e11b5709e53cc6aa1c0217e875d8[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Fri Feb 21 00:20:39 2014 +0000

    Bug #16693068     7.1.18 TO 7.1.22-27 UPGRADE NOT POSSIBLE IF LARGE NUMBER OF TABLES PRESENT
    
      The Dbdih::getInstanceKey() function is used to determine which LDM instance
      is responsible for storing a table fragment, based on the table fragment's
      log_part_id, which is set at table creation time, and must be stable between
      initial restarts of a node.
    
      The DbDih::getInstanceKey() function should return a value between 1 and
      NDBMT_MAX_BLOCK_INSTANCES-1 inclusive, identifying which LDM worker array entry
      should be used for a given fragment.  Value 0 refers to the LDM Proxy instance
      in ndbmtd, and so is not used.
      However getInstanceKey() has a bug where it can return numbers between 1 and
      NDBMT_MAX_BLOCK_INSTANCES inclusive.  This exceeds its range.
    
      Where it returns NDBMT_MAX_BLOCK_INSTANCES as a result, this causes an
      array out-of-bounds exception when mapping an instance key to a receiving
      thread, and this can have various bad effects.
    
      This bug is not generally observed as getInstanceKey() uses the log_part_id
      to determine the instance number, and recent code changes keep log_part_ids
      far below values which can return NDBMT_MAX_BLOCK_INSTANCES.
    
      Older code stored unlimited log part ids in the fragment definition,
      and used modulo division by the number of LDM instances at 'runtime'
      to get the correct log part.
    
      More recent code stores log part ids modulo the current number of running
      LDM instances at table creation time, and continues to determine fragment
      instance keys (LDM instance numbers) based on modulo division.  It's not
      that clear exactly what problem was solved here, but it happens to hide the
      getInstanceKey() bug.
    
      The number of running LDM instances is always < NDBMT_MAX_BLOCK_INSTANCES -1,
      so normal code does not hit the bug in getInstanceKey()
    
      During an upgrade we can load table definitions on disk from an older version
      with non-modulo log part numbers.  This can expose the getInstanceKey() bug
      which has various symptoms (hangs, crashes).
    
      To solve this, this patch [1;31mperf[morms modulo division of log_part_ids by the running
      node's number of LDM instances when loading a potentially 'old' (unbounded) fragment
      definition from disk.  This extends the guarantee that fragment log_part_ids will
      be < NDBMT_MAX_BLOCK_INSTANCES - 1 to cover the upgrade scenario so that the
      getInstanceKey() bug is not exposed.  This fix is aligned with the previous modification
      which stores these 'modulo log part ids'.
    
      To further clarify the new situation, getInstanceKey() is
      modified to no longer [1;31mperf[morm modulo division on the
      log_part_id as it is no longer required.
      Additionally, getInstanceKey() is modified to require that
      the resulting instance key is in range.
      Further, existing code which sets a fragment's log_part_id
      is modified to require that the log part id is within range
      (0 to NDBMT_MAX_BLOCK_INSTANCES -2 inclusive).
      A new define NDBMT_MAX_WORKER_INSTANCES is defined as
      NDBMT_MAX_BLOCK_INSTANCES - 1, in which terms the range of the
      log_part_id is 0 to NDBMT_MAX_WORKER_INSTANCES -1 inclusive.
    
      Performing %= num_ldm_instances on the log_part_id should be
      safe for non-initial restarts, as those are only allowed when
      the number of log parts is not changed.
    
      One downside of storing log parts in their 'modulo' form is that an
      increase in the number of log parts cannot remap existing fragments
      to the new log parts over an initial restart.
      That problem was introduced by the original change to store modulo
      log parts and is considered out of scope here.
    
      testUpgrade is extended to cover the scenario required here :
       - Many tables
       - Many fragments/table
       - System restart upgrade rather than node restart upgrade
    
      An execution of this upgrade test is added to the upgrade-tests
      suite for running in Autotest.

[33mcommit 08fe4fd3727e1d508a32bc3f6b4a25db2426c7c3[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Feb 18 16:08:52 2014 -0800

    Remove the error handling in jscrund_mysqljs backend which duplicates
    work [1;31mperf[mormed in the jscrund frontend.

[33mcommit 98ddfb78e2e841ff5d9cfa7771031e93baa5ca8a[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Feb 18 10:11:42 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    When binlog is off, report @@GLOBAL.GTID_PURGED from executed_gtids,
    since @@GLOBAL.GTID_PURGED and @@GLOBAL.GTID_EXECUTED are always
    same, so we did not save gtid into lost_gtids for every transaction
    for improving [1;31mperf[mormance.
    Optimize conditions of saving gtid into table when binlog is off.
    Optimize code.

[33mcommit b3002c9c00c492d2255f0563aa24d9f1fea6e14d[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Jan 28 22:42:55 2014 +0100

    Bug#18024455 NUMBER OF ROWS IN PERFORMANCE SCHEMA TABLES
    
    Before this fix, the estimated number of rows for a [1;31mperf[mormance schema
    table was a hard coded value 1000 for most tables.
    
    This value was intended only as a way to convey to the optimizer
    the information that a table has "many" rows,
    as opposed to none (0) or just one (1),
    which can be optimizer differently.
    
    This innacurate row count has no negative effects,
    but is still visible in:
    - the output of EXPLAIN
    - the information_schema
    
    In particular, it causes confusion, as the number of rows claimed
    in EXPLAIN is inconsistent with the number or rows really used.
    
    This fix implements a ::get_row_count() method for every [1;31mperf[mormance_schema
    table, which computes a more accurate number of rows,
    based on sizing parameters used when allocating memory for each table.

[33mcommit defb87dde56414a6973cb510be2322e8b7f8341b[m
Merge: 901d27f74e2 5b11ccc40ce
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Sat Jan 25 17:15:37 2014 +0530

    Bug#17319380 ERROR 1406 (22001): DATA TOO LONG FOR COLUMN
    'PROCESSLIST_STATE' FOR SLAVE SQL
    Problem:
    SQL thread's idle state message (Slave has read all relay
    log; waiting for the slave I/O thread to update it) is 75
    chars length. But [1;31mperf[mormance_schema.threads
    processlist_state column is defined as char(64). When SQL
    thread is instrumented and 75 chars state message is
    inserted into this varchar(64) column. Server will generate
    either warning/error depends on sql_mode empty or
    'strict_all_tables'.
    Fix: Changing all the state messages which are greater than
    64 chars in leangth into a *less than 64 chars* state message.
    
    1) "Slave has read all relay log; waiting for the slave I/O
    thread to update it" (75 chars state message) is changed
    into "Slave has read all relay log; waiting for more updates"
    (54 chars state message)
    
    2) "Master has sent all binlog to slave; waiting for binlog
    to be updated" (69 chars state message) is changed into
    "Master has sent all binlog to slave; waiting for more
    updates" (61 chars state message).
    
    3) "Making temporary file (create) before replaying LOAD DATA
    INFILE." (65 chars state message) is changed into "Making
    temporary file (append) before replaying LOAD DATA INFILE"
    (64 chars state message).
    
    4) "Making temporary file (append) before replaying LOAD DATA
    INFILE." (65 chars state message) is changed into "Making
    temporary file (append) before replaying LOAD DATA INFILE."
    (64 chars state message).

[33mcommit e6c8ea008e4feaecbe3882f554b8d97a8e443b70[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Jan 24 09:55:48 2014 +0200

    Bug#18116588 UNNECESSARY HASH LOOKUP IN FIL_NODE_CREATE()
    
    The InnoDB function fil_node_create() is always being invoked after
    fil_space_create().
    The call to fil_space_create() succeeds if it creates a fil_space_t object.
    The function fil_node_create() is taking a space_id of the created
    fil_space_t object and [1;31mperf[morming a hash table lookup, even though the object
    was just created.
    
    fil_space_create(): return fil_space_t*, or NULL if the operation fails.
    fil_node_create(): Replace the space_id parameter with fil_space_t*.
    
    rb#4424 approved by Kevin Lewis and Jimmy Yang

[33mcommit 4745b919a125d0da3959214f36a9bc5b49f113c7[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sat Jan 18 17:59:51 2014 +0900

    Bug#17666170 : BTR_PCUR_RESTORE_POSITION CAN TRY OPTIMISTIC RESTORATION FOR BACKWARD CURSOR
    
    btr_pcur_restore_position_func() can try optimistic restoration also for backward cursor.
    (can skip tree search from root block)
    This is optimization for btr_pcur_move_backward_from_page() [1;31mperf[mormance.
    
    Approved by Marko Mäkelä, Mattias Jonsson in rb#3841

[33mcommit 7003cf91834842d1bbd3a0d638551871ce5122e3[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Jan 17 11:31:41 2014 -0800

    The node-mysql driver by default constructs an Error object in order
    to get a stack that includes the user's call to Query. This can be
    a [1;31mperf[mormance bottleneck.
    
    mysql_service_provider.js
      set the driver property "trace" to false by default
    
    MySQLConnectionPool.js
      use the connection property mysql_trace to initialize the driver property

[33mcommit 1935b1bfe448eb8769ffb05f90ea10115030f558[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Fri Jan 17 19:06:22 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Resolve possible deadlock between 'RESET MASTER' thread and transactions' threads.
    The deadlock is dug out by sysbench [1;31mperf[mormance test.
    Add lock to prevent from compressing gtid table while reseting the gtid table.

[33mcommit 47a732ecb21971345d470d34af09ac9bf54aaa7c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Fri Jan 10 11:53:41 2014 +0400

    WL#7305 "Improve MDL scalability by using lock-free hash".
    
    The main benefit of this patch is that it opens the way for
    implementing WL7306 which brings significant improvement to
    MDL [1;31mperf[mormance/scalability in some scenarios.
    
    The basic idea behind this patch is to change the MDL_map
    implementation to use LF_HASH instead of a partitioned HASH container,
    where each partition is protected by individual mutexes.
    
    Nice results of such a change:
    
    - Since on systems with atomic support LF_HASH is lock-free,
      MDL_map_partition::m_mutex and potential concurrency bottleneck
      associated with it was removed.
    - For the same reason it doesn't make sense to partition LF_HASH.
      So we returned back to the scheme with one hash for the whole
      MDL_map and removed the MDL_map_partition class and the
      mdl_locks_hash_partitions start-up parameter.
    - Thanks to the fact that LF_HASH is integrated with LF_ALLOCATOR
      and uses per-thread hazard pointers to avoid objects in the
      hash from being deleted immediately after they were looked up, we
      were able to get rid of all MDL_map/MDL_lock machinery responsible
      for reference counting (i.e. MDL_lock::m_ref_usage/m_ref_release/
      m_version).
    - We also no longer need the MDL_map_partition::m_unused_locks_cache
      as LF_ALLOCATOR has its own mechanism for caching objects which are
      expensive to create/destroy.
    
    To support the above changes the following additional steps were taken:
    
    - Since it is tricky to use LF_HASH with objects of different types
      stored in LF_ALLOCATOR, to support these changes we had to get rid
      of MDL_object_lock/MDL_scoped_lock dichotomy. This was done by moving
      out their differences to a MDL_lock_strategy structure which is
      referenced from the MDL_lock object by pointer.
    - To make it easier to use LF_HASH with non-trivially copyable objects
      (such as MDL_lock) a new callback "initialize" was added to it. This
      callback allows finishing of initialization of the object provided by
      LF_ALLOCATOR and set element key from the object passed as parameter to
      lf_hash_insert.
      Also LF_HASH was extended to support a user-provided hash function
      such as MurmurHash3 used in the MDL subsystem.
    - LF_HASH and LF_ALLOCATOR initialization functions were extended to be
      able to accept callback functions used in them as explicit parameters.
    - lf_alloc_direct_free() was fixed to call destructor callback before
      doing my_free() on memory belong to object being freed.
    
    Also the following user visible change was made --
    --metadata_locks_cache_size and --metadata_locks_hash_instances startup
    options and corresponding system variables were declared as deprecated
    as they now have no effect.

[33mcommit 7dd4661b07a3a9a22c5f67be6d4d305e071ed870[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Jan 10 12:52:48 2014 +0900

    WL#6642 - Follow up fix, buf_dblwr_flush_buffered_writes() opportunity should not be reduced.
    
      * WL#6642 - InnoDB: multiple page_cleaner threads
    
      * 'innodb_page_cleaners' option is added to [1;31mperf[morm flushing dirty pages
      * and keeping free pages for each buffer pool instance in parallel.
      * Increasing the value of the options increases scalability of the activity.
    
      * This patch was originally written by Inaam Rana,
      * and refactored by Yasufumi Kinoshita.
    
      * rb#3004 Approved by Kevin Lewis and Marko Makela

[33mcommit b794018f488c0485bd64916aa36acc327c16d7ae[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jan 8 10:56:56 2014 +0100

    Bug#17993294 MORE THAN ONE STAGE NAMED "STAGE/SQL/INIT"?
    
    Before this fix, the very first stage executed within a statement was
    "stage/sql/init".
    
    For statements and stages instrumentation, the instrumented code
    that creates this stage looks like:
    
      MYSQL_START_STATEMENT(...);
      THD_STAGE_INFO(thd, stage_init);
    
    The older SHOW PROFILE instrumentation, however,
    uses a hard coded name for the first proc_info state, "starting".
    See PROFILING::start_new_query().
    
    Using THD_STAGE_INFO(thd, stage_init) at the beginning of a statement is
    incorrect for two reasons:
    - it is incompatible with the previous SHOW PROFILE instrumentation,
    - it collides with a different stage named "init", already used as part of some
      statements execution, causing confusion.
    
    This fix defines a "stage/sql/starting" stage,
    which is now the very first stage executed after MYSQL_START_STATEMENT().
    
    Bug#18035404 "WAITING TO GET READLOCK" STAGE IS NOT USED IN 5.6 ANY MORE
    
    Before this fix, stage "stage/sql/Waiting to get readlock"
    in table [1;31mperf[mormance_schema.setup_instruments was unused in the code.
    
    This fix removes the orphaned instrument.

[33mcommit c722778c7c56bf41500a90e2b3a90896f963ea4a[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun Jan 5 21:14:21 2014 +0800

    WL#6630 Semisync separate acks collector
    
    FEATURE
    =======
    Before this feature, acknowledgments were received in dump threads. After
    sending an acknowledgment request, dump threads needed to receive the
    acknowledgment immediately. So the following events were delayed and
    delaying time was depended on the network situation.
    
    All slave connections are based on TCP which is duplex. It means we
    can send binary events and receive acknowledgments simultaneously.
    In this way, binary events can be sent without delay.
    
    This worklog implement above feature.
    
    DESIGN
    ======
    * Start/Stop ack receive thread
      The thread is controlled automatically by semisync master.
    
      - It is started automatically when enabling semisync master through
        SET rpl_semi_sync_master_enabled = ON
    
      - It is stopped automatically when disabling semisync master through
        SET rpl_semi_sync_master_enabled = OFF
    
    * Show ack receive thread status to [1;31mperf[mormance_schema.threads
    
      - Ack receive thread status can be showed through querying
        [1;31mperf[mormance_schema.threads table, when semisync master is on(ack thread is
        up).
    
      - PROCESSLIST_STATE contents
        1. Waiting for semi-sync slave connection.
        2. Waiting for semi-sync ACK from slave.
        3. Reading semi-sync ACK from slave.

[33mcommit 5aad1c4ed57fb8fe1b701584d5c6a3eda43ae806[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Dec 27 22:46:12 2013 +0900

    WL#6642 - InnoDB: multiple page_cleaner threads
    
    'innodb_page_cleaners' option is added to [1;31mperf[morm flushing dirty pages
    and keeping free pages for each buffer pool instance in parallel.
    Increasing the value of the options increases scalability of the activity.
    
    'innodb_page_cleaners'
    Variable Scope   | Global
    Dynamic Variable | No
    Type             | numeric
    Default          | 1
    Range            | 1 .. 64
    
    This patch was originally written by Inaam Rana,
    and refactored by Yasufumi Kinoshita.
    
    rb#3004 Approved by Kevin Lewis and Marko Makela

[33mcommit 7c93988c58e72bff4d4218f8b12e8b8cbbf8adbf[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Fri Dec 27 10:17:59 2013 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    1. Correct the behavior of the transaction commit when saving gtid into the gtid table. We just need to commit the transaction when compressing the gtid table, reseting the gtid table, and fetching gtids from the gtid table.
    2. Fix bugs caused by XA transaction.
    3. Remove an assertion as we generate transaction's gtid and save it into table on the top of XA_END. So the thd->owned_gtid.sidno is assigned in XA_PREPARE and XA_COMMIT phase.
    4. Set default value of system variable 'executed_gtids_compression_period' to 1000, as we got a good [1;31mperf[mormance with the value in [1;31mperf[mormance test.
    5. Add test cases to verify that XA transactions' gtids are stored into gtid table and can be reported from global.gtid_executed correctly.

[33mcommit 412ad42b5992f0c216a09de963827751d4671d3f[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Dec 20 11:07:12 2013 -0600

    A recent change for Bug 16249481 introduced compiler warnings on
    Windows 64 builds and a compiler error on Windows 32 builds.
    This patch contains typecasting in a few places to eliminate the
    WIN64 warnings along with a few nonfunctional cleanup changes.
    
    The WIN32 compile error was fixed previously with a normal assert
    in os0atomic.h.  This patch enhances that fix by;
    1) Rearrange macros so that they are in the order by lint, ulint,
       uint32, uint64.
    2) Deleted some unused macros.
    3) Added C++-style casts for the Windows versions of atomic macros.
       Used static_cast<> as a first choice, but used reinterpret_cast<>
       where the Windows VS2010 compiler required it.
    4) The return value for the win32 versions of os_atomic_increment_uint32
       and os_atomic_decrement_uint32() were not returning the correct value.
       They were returning the original value.  To be consistent with these
       macros/functions on other OSes, they should return the new value.
       This had no affect since the return value is not currently being checked.
    5) HAVE_ATOMIC_BUILTINS_64 was not being defined for WIN64 as intended
       because of an error.  The code used #ifndef _WIN32 to denote WIN64, but
       actually, on WIN64, WIN32 is also defined.  So this patch changes it to
       #ifdef _WIN64.  The macro that this activated is used by 64-bit monitor
       integers, so it is not critical InnoDB code, but may improve [1;31mperf[mormance
       on WIN64 some intances.
    
    Patch approved on RB#4181 by Marko

[33mcommit bd6771b70a783fd194b8eb0b116370929b03d5c3[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Fri Dec 20 12:28:17 2013 +0100

    BUG#17827018: SLAVE WITH GTID_MODE=OFF LOSES GTID OF GTID-TRANSACTION IN RELAY LOG
    BUG#17813449: SLAVE WITH GTID_MODE=ON GENERATES A GTID FOR ANONYMOUS TRANSACTIONS IN RELAY LOG
    
    Background:
    In replication, all transactions that have a GTID are supposed to be
    replicated with the same GTID, and all transactions that don't have a
    GTID are supposed to be replicated without a GTID. If the current
    GTID_MODE prevents preserving GTIDs or non-GTIDs in this way, an error
    should be generated.
    
    Problem 1:
    If a transaction was replicated to the slave and stored in the relay
    log but not executed, and then the slave was restarted with the
    opposite GTID_MODE, then the transaction would be executed and
    assigned a wrong GTID:
     1.1. (BUG#17813449) If the transaction was anonymous and the slave
          changed from GTID_MODE=OFF to ON, then a new GTID was generated
          for the transactions.
     1.2. (BUG#17827018) If the transaction was a GTID-transaction and the
          slave changed from GTID_MODE=ON to OFF, then the transaction
          would lose its GTID.  If the slave was built in debug-mode, an
          assertion was raised.
    The correct behavior in both cases is to generate an error and not
    execute the transaction.
    
    Problem 2:
    The server would generate an error message when it started with
    gtid_mode=off and there exists GTIDs in some old binary log.
    
    Problem 3:
    Gtid_log_event::do_apply_event did not check if the event was an
    Anonymous_gtid_log_event. So an Anonymous_gtid_log_event would be
    applied like a Gtid_log_event and thus it would set a wrong gtid on
    the transaction.
    
    Problem 4:
    Implicit commits inside a transaction is disallowed in
    GTID-transactions, and generate an error.  However, the check for this
    was too strong: it generated an error also if
    gtid_mode='anonymous'. That will cause replication errors in NEW->OLD
    once the server generates Anonymous_gtid_log_events.
    
    Fix 1.1:
    The slave applier thread has to use gtid_next=anonymous when it
    executes transactions from an old relay log. However, the slave only
    knows that it executes an old relay log by the absence of
    Gtid_log_event before a transaction. So the solution is:
     1.1.1. Set thd->variables.gtid_next.type to the new value
            NOT_YET_DETERMINED_GROUP when executing a
            Format_description_log_event originating from another server,
            and:
             1.1.1.1. In Gtid_log_event::do_apply_event, silently convert
                      this value to the correct GTID (this gets executed
                      for relay logs generated with GTID_MODE=ON).
                      Generate an error if gtid_mode=off.
             1.1.1.2. In gtid_pre_statement_checks, silently convert this
                      value to ANONYMOUS (this gets executed for relay
                      logs generated with GTID_MODE=OFF).  Generate an
                      error if gtid_mode=on. (This error fixes the bug.)
     1.1.2. Setting NOT_YET_DETERMINED_GROUP must be done both for
            parallel slave workers and for the legacy SQL thread; thus
            both in rpl_rli.cc and rpl_rli_pdb.h.
     1.1.3. In order for the NOT_YET_DETERMINED_GROUP flag to be correctly
            converted to ANONYMOUS_GROUP in RBR, we need to call
            gtid_pre_statement_checks for row events even if opt_bin_log
            is off.
     1.1.4. The change in row events would generate an assertion for
            setting the diagnostics area twice, because
            gtid_pre_statement_checks was called before
            mysql_reset_thd_for_next_command. Thus, we move the call
            to gtid_pre_statement_checks to just after
            mysql_reset_thd_for_next_command in
            Rows_log_event::do_apply_event
     1.1.5. When using mysqlbinlog --skip-gtids, the user expects a client
            that replays the output to generate new GTIDs. However, with
            this change, the client is 'too smart'; when it processes the
            Format_description_log_event it sets gtid_next to
            NOT_YET_DETERMINED, and later when it executes a statement
            without having seen any GTID (as the GTIDs are filtered out by
            --skip-gtids), it will force the transaction to be
            anonymous. So when --skip-gtids is given, mysqlbinlog now
            prints a SET GTID_NEXT='AUTOMATIC' statement, just after the
            BINLOG base64 statement.
    
    Fix 1.2:
     1.2.1. Remove the assertion: It is [1;31mperf[mectly possible that the
            applier thread reads a Gtid_log_event when gtid_mode is not
            off.
     1.2.2. Add a check to ensure error is generated when trying to
            execute a Gtid_log_event when gtid_mode=off.
    
    Fix 2:
    This error message should never be generated, so just remove it.
    
    Fix 3:
    In Gtid_log_event::do_apply_event, check if
    spec.type==ANONYMOUS_GROUP, if yes set gtid_next='anonymous'. If
    gtid_mode=on, generate an error.
    
    Fix 4:
    Generate error only if gtid_next is a GTID.

[33mcommit 2aaa1064cc1f461755f2a660c64a1c832cede636[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Dec 19 13:59:01 2013 +0100

    Fix for bug#17973819  NEED TO ADAPT TO MONOTONIC TIMERS NOT ALWAYS BEING MONOTONIC
    
    Also a followup fix for bug#17647637, NDBTICK_CURRENTMICROSECOND() MUST BE MONOTONIC
    
    This fix handle that Monotonic timers seems to have bugs on several platforms
    which might result in the monotonic clock doing small jumps back in time.
    This is normally due to im[1;31mperf[mect syncing of the clock between multiple
    CPU cores.
    
    Such small backticks are not really harmfull for our scheduler and watchdog
    algorithms, so we make the backtick protection less strict in this patch.
    However, we will still assert that the backtick is less than 10ms in the
    NdbTick_Elapsed() calculation.
    
    We removed redundant checks for backticks in several 'timediff'
    methods as such a check already exist in NdbTick_Elapsed() which is
    used in the same place.
    
    We also removed some assert('not backtick') where they are used
    in the same place where NdbTick_Elapsed() was called for the
    same reason as above.

[33mcommit fba37ef976d4c24d9f94c286b9dfad2fb46cfe5b[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Wed Dec 18 14:36:19 2013 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    
    1. Create a mysql system table named gtid_executed, a row in the table can
    contain an interval of several GTIDs, all having the same SID and consecutive
    GNOs as following:
    
      CREATE TABLE gtid_executed(
        sid CHAR(36) NOT NULL,       -- Source ID
        gno_start BIGINT NOT NULL,   -- First GNO of interval
        gno_end BIGINT NOT NULL,     -- Last GNO of interval
        PRIMARY KEY(sid, gno_start)  -- PK on interval start
      ) ENGINE = InnoDB;
    
    
    2. Store gtids into the gtid_executed table always if gtid_mode is enabled as
    following:
      INSERT INTO gtid_executed VALUES (SID, GNO, GNO)
      - If binlog is disabled and gtid_mode is enabled, store gtids from master's
    transactions, relayed BINLOG transactions and 'SET @@SESSION.GTID_NEXT'
    statement into gtid_executed table right before transaction prepare.
      - If binlog is enabled and gtid_mode is enabled, store GTIDs into both binlog
    and gtid_executed table.
    
    
    3. The gtid_executed table would be filled with singleton interval as we cannot
    use UPDATE instead of INSERT, because any concurrently executing transactions
    would conflict on the update. So we propose to create a FOREGROUND thread (we
    can not create a BACKGROUND thread as we have to create THD object to open
    gtid_executed table) and introduce a user variable
    'executed_gtids_compression_period' to control how often the thread should be
    woken up to compress the gtid_executed table regularly.
      - To the user variable 'executed_gtids_compression_period', if value is 0,
    the thread never wakes up, if value is 1, wakes up every transaction. If value
    is 1000, wakes up every 1000 transactions.
      - We also added the thread's state info (suspending, compressing gtid_executed
    table) into [1;31mperf[mormance_schema.threads table, so that user can monitor it and
    tune the user variable 'executed_gtids_compression_period' (The thread is in
    suspended mode most of the time, wakes up only when it needs).
      - THe thread runs always, user cannot kill it as we didn't add it into
    PROCESSLIST.
      - The thread opens a transactional context to execute the following within a
    single transaction on an non-autocommit mode:
        - Read each row by the PK in increasing order, delete consecutive rows from
    the gtid_executed table and fetch these deleted gtids at the same time.
        - Store compressed intervals from these deleted gtids into the gtid_executed
    table.
    
    
    4. Report @@GLOBAL.GTID_EXECUTED and @@GLOBAL.GTID_PURGED from both binlog and
    gtid_executed table as following:
      - GLOBAL.GTID_EXECUTED = all gtids of gtid_executed table;
      - GLOBAL.GTID_PURGED = GLOBAL.GTID_EXECUTED - (GTID_EXECUTED_BINLOG -
    GTID_PURGED_BINLOG);
      (GTID_EXECUTED_BINLOG: gtid_set of gtids is ever logged in binary logs.
       GTID_PURGED_BINLOG  : gtid_set of gtids is purged from binary logs.)
    
      - GLOBAL.GTID_EXECUTED is initialized with all gtids of gtid_executed table
    during server restarting. Every transaction's gtid is added into the
    GLOBAL.GTID_EXECUTED and gtid_executed table if gtid_mode is enabled after
    server restarts.
      - GLOBAL.GTID_PURGED is initialized with gtids from gtid_executed table and
    binlogs and precomputed as above during server restarting. After server
    restarts, added purged gtids into GLOBAL.GTID_PURGED when purging logs
    (regardless binlog is enabled or not), and added every transaction's gtid into
    GLOBAL.GTID_PURGED if binlog is disabled and gtid_mode is enabled.
      - A previous log event with GTID_EXECUTED_BINLOG (GTID_EXECUTED -
    GTID_ONLY_IN_TABLE) set is created when rotating binlog, so GTID_ONLY_IN_TABLE
    variable is maintained when binlog is enabled. GTID_ONLY_IN_TABLE is initialized
    with difference gtids from gtid_executed table and the latest binlog during
    server restarting. GTID_ONLY_IN_TABLE will never change since starting server.
    (we don't need maintain GTID_ONLY_IN_TABLE when binlog is disabled)
      - If gtid_executed_table is empty, add all gtids in GTID_EXECUTED_BINLOG into
    gtid_executed table during server restarting (Handle the upgrade case, and the
    case that a slave is provisioned from a backup of the master and the slave is
    cleaned by RESET MASTER and RESET SLAVE before this.).
      - The gtid_executed table is also initialized with gtid_purged when user is
    initializing it through SET GLOBAL gtid_purged. (It is possible to update the
    value of gtid_purged, but only by adding GTIDs to those already listed, and only
    when gtid_executed is unset—that is, on a new server.)
    
    
    5. The gtid_executed table is reset when resetting master.
    
    
    6. Gtid_table_persistor class
       It manages all operations on the gtid table.
    
       - m_count
         Count the append rows of the table.
    
       - save(Gtid *gtid)
         Insert the gtid into table.
    
       - save(Gtid_set *gtid_set)
         Store gtid set into the table.
    
       - compress()
         Compress intervals into consecutive GNOs in the table.
    
       - reset()
         Delete all rows from the table.
    
       - fetch_gtids_from_table(Gtid_set *gtid_set)
         Fetch gtids from the table and store them into gtid set.

[33mcommit fd3c1aa33587372b33591ff4e65f0c694fe380c1[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Dec 17 19:04:00 2013 +0200

    Fix Bug#17958039 PERFSCHEMA.SIZING_GROWTH FAILING WITH QC ENABLED ON PB2
    
    This is a followup to vasil.dimov@oracle.com-20131203163459-tlkyqdq93jysk9z0.
    
    After adding a per-table rwlock, [1;31mperf[mormance schema needs a minor tweak. It
    affects the size of the internal buffer used when instrumenting rwlocks and
    will affect the server only if the rwlock instrumentation is used with lot
    of tables. The symptoms without this tweak can be to see the rwlock_lost
    counter going up (as per Mark Alff).
    
    Reviewed by:    Marc Alff (rb:4184)

[33mcommit 20e12a891fae7d95b63e57f53618fd77aef31ca8[m
Author: Tarique Saleem <tarique.saleem@oracle.com>
Date:   Mon Dec 16 17:54:43 2013 +0530

    skipped the test [1;31mperf[mschema.sizing_growth for query_cache enabled as it failing on daily and weekly platforms. Logged a Bug #17958039 for tracking this issue until that time.

[33mcommit 51aabf4ebaaed9445c5bb78c16898876db904d5f[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Thu Dec 12 21:55:53 2013 +0400

    Follow-up patch for WL#7304 "Improve MDL [1;31mperf[mormance and
    scalability by implementing "fast-path" for DML locks".
    
    Unit test class for MDL-related tests now saves original
    error hook before installing custom one and restores it
    after test execution.

[33mcommit 8f261a2a7842441c5998e59866aeda0bba1bd945[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Dec 6 11:43:30 2013 -0800

    iBug 17885485 - SETPARTITIONKEY() FAILS AFTER PREVIOUS TRANSACTION FAILS
    
    After a failed commit, the session transaction state becomes idle
    but the cluster transaction is not cleaned up. A subsequent setPartitionKey
    fails because there is still a cluster transaction.
    
    SessionImpl:
      change commit to fail the transaction on errors
      change executeCommit to call flush with the commit flag
      change flush to allow the caller to executeCommit instead of executeNoCommit
        followed by executeCommit
        serendipity: this has a slight [1;31mperf[mormance benefit
    
    TransactionErrorSetPartitionKeyTest
      Add four tests for setPartitionKey after a failure to insert a duplicate record
        commit: this is the original bug report
        flush/commit: flush the failed change and then commit the transaction
        flush/rollback: flush the failed change and then roll back the transaction
        rollback: roll back the failed change

[33mcommit 5724f608ffc89b2f5a11fffe40b533979354bd95[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Dec 4 08:56:07 2013 +0200

    Followup to vasil.dimov@oracle.com-20131203163459-tlkyqdq93jysk9z0:
    
    fix compilation failure when [1;31mperf[mschema is enabled.
    
    The key is called dict_table_stats_latch_key in mysql-5.6, but
    dict_table_stats_key in mysql-trunk.

[33mcommit 1496cc312e678e0c9da3d7f8c2a9ae575a6b184c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Tue Dec 3 14:07:19 2013 +0400

    WL#7304 "Improve MDL [1;31mperf[mormance and scalability by implementing 'fast-path'
    for DML locks".
    
    Since typical user workload consists mostly of DML statements it makes sense
    to improve [1;31mperf[mormance/scalability by optimizing MDL subsystem for such type
    of statements.
    
    This patch implements "fast-path" for metadata locks acquired by DML
    statements. Acquisition/release of DML lock (S, SH, SW, SR locks) are
    converted into counter increment/decrement (under protection of
    MDL_lock::m_rwlock) instead of more complex code involving list
    manipulation (at expense of acquisition/release of locks typical for DDL
    statements).
    
    Such a step reduces size of critical section associated with
    MDL_lock::m_rwlock and increases scalability/[1;31mperf[mormance in benchmarks
    involving DML workload. Particularly, benchmarking of draft patch
    implementing this idea shown that it provides at least 10% [1;31mperf[mormance
    improvement in single-table OLTP_RO/POINT_SELECT SysBench tests.
    
    Details
    =======
    
    We split all lock types for each of MDL namespaces in two sets:
    
    A) "Unobtrusive" lock types
       1) Each type from this set should be compatible with all other
          types from the set (including itself).
       2) These types should be common for DML operations.
    
    We optimize acquisition and release of locks of this type by avoiding
    complex checks and manipulations on m_waiting/m_granted sets/lists and
    replacing it with a check of and increment/decrement of integer counters.
    We will call the latter type of acquisition/release "fast path".
    
    2) "Obtrusive" lock types
       1) Granted or pending lock of those type is incompatible with some
          other lock (including itself).
       2) Not common for DML operations
    
    These locks have to be always acquired in the old fashion - involving
    manipulations with m_waiting/m_granted sets/lists, i.e. using "slow path".
    Moreover in the presence of active/pending locks from "obtrusive" set we
    have to acquire even locks of "unobtrusive" type using "slow path".
    
    -------
    
    For GLOBAL/COMMIT and SCHEMA namespaces (i.e. namespaces with lock
    represented by MDL_scoped_lock class):
    
    "Unobtrusive" locks set consists of IX lock type.
    "Obtrusive" locks set consists of S and X lock types.
    
    For all other namespaces (i.e. represented by MDL_object_lock class):
    
    "Unobtrusive" locks set consists of S, SH, SR and SW locks.
    "Obtrusive" locks set consists of SU, SNW, SNRW and X.
    
    -------
    
    To implement the above MDL_lock object got two new members:
    
    - MDL_lock::m_obtrusive_locks_granted_pending_count - number of granted
      or pending locks of "obtrusive" types. Necessary to quickly verify that
      we can grant "unobtrusive" locks without further checking.
    
    - MDL_lock::m_fast_path_granted_count - packed counter of number of
      granted locks of specific "unobtrusive" type which were granted using
      fast-path algorithm and not using "slow path" (e.g. for MDL_object_lock
      we use 20-bit chunks of this counter to represent number of S/SH, SR and
      SW locks acquired).
    
    The above two members are still protected by MDL_lock::m_rwlock lock.
    
    Essentially this patch replaces:
    
    1) addition/removal of "unobtrusive" lock to MDL_lock::m_granted list during
       lock acquisition/release with and incrementing/decrementing of corresponding
       part of m_fast_path_granted_count.
    2) check for granted or pending locks which conflict with "unobtrusive" lock
       is replaced with check on m_obtrusive_locks_granted_pending_count counter.
    
    We still allocate MDL_ticket objects for requests which are satisfied using
    fast path algoritm, but we mark them using MDL_ticket::m_is_fast_path member,
    so we know that such ticket can be released in simplified fashion.
    
    In order for deadlock detection algorithm to work properly we need to do so
    called "materialization" of fast path tickets for thread which is about to
    start waiting. This process clears MDL_ticket::m_is_fast_path flag, add ticket
    to appropriate m_granted list and decrement corresponding
    m_fast_path_granted_packed_count counter under protection of MDL_lock::m_rwlock.
    We also have to do this when acquiring "obtrusive" locks and locks for the
    threads with open HANDLERs.
    
    -------
    
    Unit tests for MDL subsystem were extended to cover scenarios important for
    the changes described above.
    Also this patch changes [1;31mperf[mschema.mdl_func test to make it robust against
    line number changes in MDL code.

[33mcommit b3b0a645fc3b93278f9e1cc312efaba50fe4f9e2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Nov 25 11:02:29 2013 +0200

    Bug#17543588 PERFORMANCE REGRESSION (9%) FOR DBT-3 QUERY 21
    
    This is an unexpected regression from an earlier fix:
    
    Bug#16852278 SIMPLIFY RECORD COMPARISONS
    
    The reason of this regression is that memcmp() can be slower than a for loop.
    If memcmp() is a library call, there is an overhead involved,
    which we can avoid by comparing the first few bytes with a loop.
    The built-in memcmp() of GCC on x86 and AMD64 (repz cmpsb) is known to
    [1;31mperf[morm worse than the GNU libc library function:
    
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43052
    
    cmp_data(): Use a for loop to compare some first bytes.
    Then invoke memcmp() for any remaining common bytes if needed.
    
    innodb.cmake: Pass -fno-builtin-memcmp when compiling rem0cmp.cc
    with GCC on x86 or AMD64.
    
    rb#3931 approved by Jimmy Yang

[33mcommit 0d2a88c18228c5e96a98f45058fd8c45b5ddd83c[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Nov 21 10:21:30 2013 +0530

    BUG#17242996: ATOMIC/X86-GCC.H:MAKE_ATOMIC_CAS_BODY64 POTENTIAL
                  MISCOMPILATION BUG
    
    Analysis:
    --------
    The make_atomic_cas_body64 implementation on IA32 with GCC but
    without GCC builtins might get miscompiled due to a wrong
    constraint.
    
    make_atomic_cas_body64 implementation uses CMPXCHG8B with a
    single memory operand as input/output operand. However the
    constraint '+' was not used to indicate the same due to
    issue reported by Bug#11746008. Hence it could result in
    using two different memory locations, thus accessing an
    uninitialized memory with probable runtime crash.
    
    Fix:
    ---
    The assembly atomic implementation (x86-gcc.h) is not used on
    Windows and Solaris. The oldest version of GCC supported by
    Linux variants on any platform is GCC 4.1.2 which supports
    builtin atomics. On MAC, GCC versions higher than 4.1.2 or
    Clang is used which supports builtin atomics as well.
    
    Hence this patch removes assembly atomic implementation
    (x86-gcc.h) and relies on the GCC builtins. Upon using
    an older unsupported versions of GCC(A warning is flagged),
    we may see some [1;31mperf[mormance regressions since we fallback
    to rw locks.

[33mcommit 2ec981d5f4d32a20a5993f44ba0ce8d56441cba0[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Nov 19 12:09:52 2013 +0100

    Bug#17813333 PSI KEYS WERE INCORRECTLY PASSED TO FUNCTION SET_PSI_KEYS
    
    Before this fix, the mutex
      'wait/synch/mutex/sql/MYSQL_RELAY_LOG::LOCK_sync'
    was not properly instrumented for the [1;31mperf[mormance schema.
    
    In particular, no wait time was recorded for this mutex,
    and no mutex instance was instrumented,
    because the incorrect key was used when instrumenting the code.
    
    The root cause is a simple mismatch in the parameters order,
    in the call to relay_log.set_psi_keys().
    
    This fix uses the proper keys when calling set_psi_keys()
    for the relay log.

[33mcommit d44e17a087b5f6900c50ef603cc7dc10b529e84e[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Nov 18 23:26:47 2013 +0100

    WL#7152 PERFORMANCE SCHEMA, EXTRACT DIGEST
    
    Refactoring: moved all the digest related code
    - from the [1;31mperf[mormance schema
    - to sql layer in the server

[33mcommit 395abb82fadee167197fd0d611872f5ce4b6a4f2[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Sat Nov 16 00:40:12 2013 +0100

    Bug#17752288 TRANSACTION MTR TESTS FAIL
    
    Remove [1;31mperf[mschema.transaction_nested_events from experimental collection

[33mcommit 9b2b79df3485531129675e9389c266170727f9c1[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Fri Nov 15 17:16:02 2013 -0600

    BUG#17799147 PERFORMANCE SCHEMA: REFINE GTID HANDLING
    
    Improved method for capturing the GTID for transaction events
    recorded by the Performance Schema.
    
    Added GTID test cases to [1;31mperf[mschema.transaction
    
    Fixed CRLF/LF discrepancies in handler.cc and sql_cache.cc

[33mcommit 373e19504debd275a1f2897f2a251cc274fc74d3[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Thu Nov 14 23:00:49 2013 +0100

    Bug#17752288 TRANSACTION MTR TESTS FAIL
    
    Disable query cache for  [1;31mperf[mschema.transaction_nested_events

[33mcommit 62d0eb5109000e10ee1b3848cf924bdf659a4f67[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Mon Nov 11 19:26:08 2013 -0800

    jscrund:  If node is run as "node --expose-gc jscrund ..." then a full GC will be [1;31mperf[mormed between tests.

[33mcommit 1e16af5eed0626587b8b49a652fca7ef6f09a9cf[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Tue Nov 12 00:00:03 2013 +0100

    Bug#17752288 TRANSACTION MTR TESTS FAIL
    
    Moved [1;31mperf[mschema.transaction_nested_events to the experimental
    collection until intermittent failure resolved

[33mcommit b78629ac9ff4fa0985da16b8b3fd776b0f8c57c1[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 15:35:42 2013 +0100

    ndb_waiter
     - remove the unnecessary WIN32 define
     - remove su[1;31mperf[mluous variables

[33mcommit 202f0f08574d90444983df06ecfeefab0a3c8f3d[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Nov 11 13:03:29 2013 +0100

    Bug#17750252 USAGE OF LOCALTIME IS UNSAFE IN THREADED CODE
     - usage of localtime is not safe since some platforms are using a static
       buffer shared between all threads for storing the result of the conversion.
     - replace localtime() with localtime_r()+stackbuf
     - add call to tzset() in ndb_init() to initialize the time conversion information
       once.  This loads information about current timezone as well as  during which periods
       daylight saving calculations should be [1;31mperf[mormed.

[33mcommit 4d46b7560a4d91c85d10ef68ee349e4b1b4a7e17[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Nov 8 20:58:48 2013 +0100

    Bug#17766582 PERFORMANCE SCHEMA OVERHEAD IN PFS_LOCK
    
    This fix is a general cleanup for code involving atomic operations in the
    [1;31mperf[mormance schema, to reduce overhead and improve code clarity.
    
    Changes implemented:
    
    1)
    
    Removed 'volatile' in the code.
    The C 'volatile' keyword has nothing to do with atomic operations,
    and is confusing.
    
    This is a code cleanup.
    
    2)
    
    Added missing PFS_cacheline_uint32 to atomic counters,
    to enforce no false sharing happens.
    
    This is a [1;31mperf[mormance improvement.
    
    3)
    
    Modified optimistic locks, for clarity.
    
    Pattern before:
      pfs_lock lock;
      m_lock.begin_optimistic_lock(&lock);
      m_lock.end_optimistic_lock(&lock);
    
    Pattern after:
      pfs_optimistic_state lock;
      m_lock.begin_optimistic_lock(&lock);
      m_lock.end_optimistic_lock(&lock);
    
    The new type, pfs_optimistic_state, better reflects that a state information
    is used in a begin / end section.
    This provides better typing, for type safety.
    
    Adjusted all the code accordingly.
    
    4)
    
    Modified pfs_lock allocation, for [1;31mperf[mormances.
    
    Pattern before:
      m_lock.is_free();
      m_lock.free_to_dirty();
      m_lock.dirty_to_allocated();
      total: 0+1+2 = 3 atomic operations
    
    Note that free_to_dirty() could fail even for free locks,
    because the CAS can use an old version number,
    making the code attempt again another record.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.free_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      total: 2+1 = 3 atomic operations.
    
    Now the code is garanteed to detect free records,
    because free_to_dirty() does an atomic load then a CAS.
    
    Adjusted all the code accordingly.
    
    5)
    
    Modified pfs_lock deallocation, for [1;31mperf[mormances.
    
    Pattern before:
      m_lock.allocated_to_free();
      Total 2 atomic operations.
    
    Pattern after:
      m_lock.allocated_to_free();
      Total 1 atomic operation.
    
    6)
    
    Modified record updates, for [1;31mperf[mormances.
    
    Pattern before:
      m_lock.allocated_to_dirty();
      m_lock.dirty_to_allocated();
      Total 4 atomic operations.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.allocated_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      Total 2 atomic operations.
    
    Adjusted all the code accordingly.
    
    7)
    
    Modified record peek, for reliability.
    
    Pattern before:
      m_lock.is_populated()
      used a dirty read, causing spurious missing records
    
    Pattern after:
      m_lock.is_populated()
      uses an atomic load, for correctness.
    
    This change is expected to resolve spurious test failures,
    where some records in [1;31mperf[mormance schema tables are sometime missing,
    os some statistics (when computed on the fly) are sometime under evaluated.

[33mcommit 0efa750c9abb12f4a7eefc67cf571f2770f7cb83[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Nov 8 09:36:41 2013 +0100

    WL#1509 Pack values of non-sorted fields in the sort buffer
    
    Fix [1;31mperf[mormance regression when sorting small/empty input tables.

[33mcommit ca0a17724cf5f979a2af028e30db5bf8ab435166[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Nov 2 15:42:38 2013 -0700

    In jscrund null adapter, add option to measure [1;31mperf[mormance of udebug.log() messages

[33mcommit 6d0e6b9903c27de5131e4b678f59d140d8a5fc25[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Nov 1 15:43:47 2013 +0100

    WL#7260: Split LOCK_thread_count
    
    After the improvements made in WL#6606, [1;31mperf[mormance testing has
    shown that the major bottleneck for connect/disconnect [1;31mperf[mormance
    now is the LOCK_thd_count mutex.
    
    This patch increases the connect/disconnect [1;31mperf[mormance by
    splitting LOCK_thd_count so that it no longer protects
    several different structures/variables. LOCK_thd_count
    is now only used to protect the list of connections (THDs).
    
    Changes made to existing usage of LOCK_thd_count:
    
    LOCK_thread_cache is introduced to protect the thread cache used
    by the default connection handler (one thread per connection).
    
    Synchronization during startup of signal handler thread is now
    done using LOCK_start_signal_handler.
    
    Synchronization during shutdown of main thread connection
    listening is now done using LOCK_socket_listener_active.
    
    The global thread_id counter is now incremented using atomics,
    rather than being protected by LOCK_thd_count.
    
    THD::current_linfo is now protected by THD::LOCK_thd_data rather
    than LOCK_thd_count.
    
    max_used_connections is now reset under protection of
    LOCK_connection_count rather than LOCK_thd_count.

[33mcommit ec552cce563ff98f3e7a8e3821771bc1c1cb4a54[m
Merge: 4096ca111da 8a04a4d7453
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Oct 29 11:53:37 2013 +1100

    WL#7290 - Refactor the mini-transaction code.
    
    Convert all the #defines into enums, improve the dynarr [1;31mperf[mormance.
    Use proper interfaces and improve extensibility. Add #define wrappers
    for existing calls to the old mtr functions, this was done to keep
    the changes to the minimum.
    
    Approved by Yasufumi Kinoshita and Kevin Lewis rb#3555.

[33mcommit 8cdb42fa6b18faea0070de78a3697b2e9c66ca07[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Oct 25 15:04:32 2013 +0530

    Bug #17446761: INNODB.INNODB_WL6560_DEBUG ,INNODB-LOG-FILE-SIZE GET WRONG
    ERROR FOR MYSQLD_CMD
    
    
     - To test recovery for any crash or failure during server start-up phase
       RECOVERY_CRASH technique is used.
    
      - This technique use to invoke exit(3) which in turn invokes cleanup action.
        Cleanup action at this stage can lead to error or deadlock as the parallely
        started rollback thread might be in half-cooked state when it is forced to
        cleanup.
    
      - Ideally RECOVERY_CRASH should simply kill the server instantenously to
        stimulate actual crash where-in there would no cleanup action involved.
        Fixed RECOVERY_CRASH for the same using _exit() so that exit handlers
        are not called which [1;31mperf[morms cleanup action.
    
       Patch approved by: Sunny (rb#3681)

[33mcommit 06d7a95dee1f51de2f73b25912089d0ac62073f2[m
Author: prabakaran thirumalai <prabakaran.thirumalai@oracle.com>
Date:   Thu Oct 24 22:26:12 2013 +0530

    WL#6407 Code reorganization to avoid race condition between
    server main thread and the kill server thread
    
    New class Global_THD_manager is added which encapsulates
    global_thread_list and synchronization primitives related to
    this data structures such as LOCK_thread_count and COND_thread_count.
    It provides method to add/remove/find/count/do_func/traverse
    global_thread_list and guards it with LOCK_thread_count internally.
    
    Removed ready_to_exit flag and joining signal_hand thread
    with the main mysqld thread during server shutdown. Removed
    kill_server_thread and calling clean_up() only from the
    main mysqld thread. For Windows, shutdown handler thread
    is joined with main mysqld thread during server shutdown.
    
    Removed assert in storage/[1;31mperf[mschema/pfs_lock.h which relaxes
    double free when ready_to_exit flag is set. Also enabled
    commented out code in [1;31mperf[mormance schema cleanup.

[33mcommit 828092e513fe7870fbccc25921e58d8a27294720[m
Merge: b6bb36538b7 2665e90489b
Author: Tanjot Uppal <tanjot.uppal@oracle.com>
Date:   Thu Oct 24 15:02:05 2013 +0530

    Null merge from 5.6 to trunk for Backporting the skipping logic for the below [1;31mperf[mschema tests, with Query Cache Enabled.
    
    added:
      mysql-test/include/have_QC_Disabled.inc
    modified:
      mysql-test/suite/[1;31mperf[mschema/t/aggregate.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_a.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_a_no_h.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_a_no_u.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_a_no_u_no_h.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_h.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_u.test
      mysql-test/suite/[1;31mperf[mschema/t/event_aggregate_no_u_no_h.test
      mysql-test/suite/[1;31mperf[mschema/t/nesting.test
      mysql-test/suite/[1;31mperf[mschema/t/view_table_io.test

[33mcommit 402fcb0a17406d3aeca052204f1fb75762558726[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Oct 17 19:05:54 2013 +0900

    WL#7050 - InnoDB: Refactor redo log write code for better [1;31mperf[mormance
    
    - This is rewrite of log_write_up_to() to improve its [1;31mperf[mormance in case where innodb_flush_log_at_trx_commit = 2.
    
    In log_write_up_to():
    
    * Remove wait mode. We always wait with one exception. And that is when doing log
    sync from master thread. It makes that synchronous as well because that happens
    only once per second.
    
    * Because we only have one log group therefore we don't need two flush_events.
    * Remove unnecessary fields like written_to_some_lsn, written_to_all_lsn.
    * If only write is requested we don't have to acquire the log_sys::mutex after we
    release it. We currently do that only to do event handling but event handling is
    really only needed in case where flush is requested i.e.: a thread should be
    waiting on the event iff it is interested in flushing. Writes are serialized under
    log_sys::mutex.
    
    This patch was originally written by Inaam Rana.
    
    rb#2389 Approved by Sunny and Yasufumi
    
    ===========
    
    Adjustment for [1;31mperf[mormance was done therough inherited rb#3373
    
    - optimize log_write_up_to() more
            * remove the second log_sys->mutex obtain also for "innodb_flush_log_at_trx_commit = 1" path
            * remove unnecessary ut_memcpy. (because log_group_write_buf() is protected by log_sys->mutex)
            * remove dirty-read from flush_to_disk=true case. (to avoid regression at some cases)
              (to keep current arbitration for write/fsync contention between log and data file)
            * fix wrong handling of O_DSYNC
    - revive log_buffer_sync_in_background(). (because it needs to be used)

[33mcommit 5d9734847b311f38dee5663dd81471aa8e566077[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Oct 15 14:18:09 2013 +0530

    Bug #17159662 WASTED WORK IN METHOD MY_XML_ERROR_POS()
    
    Problem: The execution of the loop at line 553 does not [1;31mperf[morm
    any useful work when s[0] is '\n'.
    Fix: Fix would be to iterate variable 's' from end and break when
    s[0] is '\n' thus saving some cpu.

[33mcommit 32d2d37176487d6e86da395f26528a564f834835[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Mon Oct 7 17:29:00 2013 +0530

    Bug #16630410 NDB:LCP FRAGMENT SCAN WATCHDOG SHOULD BE CONFIGURABLE
    
    The LCP fragment scan watchdog periodically checks for lack of
    progress in a fragment scan [1;31mperf[mormed as part of a local checkpoint.
    The watchdog was hard-coded to shut down the node if there was no
    progress for 60 seconds.
    
    Added a configuration parameter LcpScanProgressTimeout to set the
    maximum time for which the local checkpoint can be stalled before
    the LCP fragment scan watchdog shuts down the node. A parameter
    value of 0 disables the LCP fragment scan watchdog.
    
    Added a unit test to set the parameter value to 0 and verify that
    the LCP fragment scan watchdog is disabled.

[33mcommit 50a9d039be58b99b2d9b5276ed456f9c0dfe2dd1[m
Merge: 2f355baaaad 2a81ab58a51
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Sun Oct 6 23:51:04 2013 +0530

    Bug#17514406- MISMATCH BETWEEN SLAVE_WORKER_INFO AND
                  REPLICATION_EXECUTE_STATUS_BY_WORKER
    
    Problem:
    =======
    The Slave_worker class has an atribute called "id"
    that starts indexing from 0. But this is not used
    by the table mysqld.slave_worker_info. This table
    instead uses Rpl_info::internal_id which starts
    indexing with 1. The [1;31mperf[mormance schma table for
    worker used Slave_worker::id and hence the bug.
    
    Fix:
    ===
    Changed [1;31mperf[mormance schma table for worker to use
    Rpl_info::internal_id.

[33mcommit 548ed174c75c409b7fce4e7c94f2beb9e1dd078c[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Oct 4 21:30:18 2013 +0200

    Bug#17310878 PFS OVERHEAD ON FREQUENT CONNECT/DISCONNECT
    
    This fix is for MySQL 5.7
    
    This fix is a [1;31mperf[mormance improvement.
    
    The issue was that the call to PSI_THREAD_CALL(delete_current_thread)(),
    which is executed each time a thread disconnects,
    was placed inside a critical section involving LOCK_thread_count.
    
    The fix is to [1;31mperf[morm the same call sooner,
    before entering the LOCK_thread_count critical section.
    
    Serializing all calls to pfs_delete_current_thread_v1() was the major
    cause of [1;31mperf[mormance overhead, for frequent connect/disconnect.

[33mcommit 5ac2d3a55a152ce75788ee4be317803164ce067f[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Oct 1 18:55:07 2013 +0200

    Bug#17539520 PERFORMANCE_SCHEMA THREAD CREATION PERFORMS TOO MUCH INITIALIZATION
    
    This fix is a [1;31mperf[mormance improvement.
    
    A lot of code present in create_thread() initialized the per thread internal
    buffers that support:
    - table events_waits_current
    - table events_stages_current
    - table events_statements_current
    
    Executing this code to fully initialize every member for every row is in
    fact un necessary, and the code has been removed.
    
    Only the initialization of the default NESTING_EVENT_ID and
    NESTING_EVENT_TYPE for the thread waits and stages is required,
    and the overall initialization of these columns has been cleaned up.
    
    As a related fix,
    PFS_events_waits::m_thread was redundant with PFS_events::m_thread_internal_id,
    and has been removed.
    
    Logic in table_events_waits_common::make_row() that was in fact specific to
    only two sub classes has been moved to
    - table_events_waits_current::make_row()
    - table_events_waits_history::make_row()

[33mcommit c060922aca6c352c27ea04722ad06167fe83a8c9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Oct 1 14:03:24 2013 +0200

    Fix for bug#15907515
    
      RECEIVER THREAD COULD BLOCK/BUSY WAIT WHILE HOLDING RECEIVER MUTEX
    
    This fix removes the waiting for more job buffers to become available
    inside [1;31mperf[mormReceive() (or actually: mt_checkDoJob() called from it).
    This was bad as the receive mutex was held during this wait.
    Furthermore, mt_checkDoJob() was also sleep-loop waiting for
    job buffers to be freed. Which could end up as a busy wait if
    the receive thread was running under RealTime priority !!
    
    ::[1;31mperf[mormReceive() will now instead return with a 'full' status
    to the receive thread without further waiting/retry. The receive mutex
    will then be unlocked and the receiver thread will yield() on the
    'thr_job_queue_head::m_waiter' condition. Later, 'm_waiter' will
    be signaled, and the receive thread awakened, when more job
    buffers are available. (After being consumed by the worker thread.)
    
    Furthermore this fix also removes a bug where a negative return
    from the initial check_job_buffer() call in the receive thread,
    prevented the call of ::[1;31mperf[mormReceive(). Thus, ::[1;31mperf[mormReceive()
    did not call TCP_Transporter::doReceive() which would ofload
    received data from OS TCP-buffers into local 'ReceiveBufferMemory'.
    This could probably cause premature communication blockage, or at
    least the configured ReceiveBufferMemory to not fully being used to
    ofload the OS TCP buffer as intended.
    
    NOTE: 'check job_buffer outcome' is not relevant for
    ::[1;31mperf[mormReceive() until *after* it has done a ::doReceive() on
    all TCP transporters having avail data.

[33mcommit 53ce8a6f09f3344059d9b09a0013ef6ae2b543dd[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Mon Sep 30 18:15:28 2013 -0500

    BUG#17267760 SPURIOUS CRASH IN LF_HASH, WITH TRUNCATE_STRESS TEST
    
    Restored i_[1;31mperf[mschema.truncate_stress to experimental status

[33mcommit 2a81ab58a51d236f7156a1315da627581803e8d6[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Sat Sep 28 03:47:18 2013 +0530

    Bug#17514406- MISMATCH BETWEEN SLAVE_WORKER_INFO AND
                  REPLICATION_EXECUTE_STATUS_BY_WORKER
    
    Problem:
    =======
    The Slave_worker class has an atribute called "id"
    that starts indexing from 0. But this is not used
    by the table mysqld.slave_worker_info. This table
    instead uses Rpl_info::internal_id which starts
    indexing with 1. The [1;31mperf[mormance schma table for
    worker used Slave_worker::id and hence the bug.
    
    Fix:
    ===
    Changed [1;31mperf[mormance schma table for worker to use
    Rpl_info::internal_id.

[33mcommit d0b83b28988c3bdff5a4ac9d4a4112bd574e1d5a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Sep 27 12:02:22 2013 +0200

    Bug#17503130 TABLE PERFORMANCE_SCHEMA.TABLE_HANDLES BECOMES EMPTY AFTER A
    BIG WORKLOAD
    
    Before this fix, the implementation of table [1;31mperf[mormance_schema.table_handle
    would appear empty after the server executed some code,
    causing spurious failures in test cases like mdl_func.
    
    The failure was spurious, because the same test did work after a server
    restart.
    
    The root cause is that the logic to check for locks in
    table_table_handles::make_row() is broken, mismatched locks are used.
    
    It works only by accident after a server restart,
    because the PFS_table::m_lock and the PFS_table_share::m_lock happen to be
    initialized to the same initial value.
    
    The fix is to use the proper lock for end_optimistic_lock().

[33mcommit 0d55a0761c9c7b8edeea4b9231ad931fa74fdb3e[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Sep 26 14:04:57 2013 +0200

    Bug#17507853 PREVENT FALSE SHARING OF CPU CACHE LINE FOR SENSITIVE COUNTERS
    
    This fix is a code cleanup to avoid risks of [1;31mperf[mormance degradations.
    
    In the [1;31mperf[mormance schema code, several internal counters are highly
    sensitive, because they are used very often, and used with atomic
    operations.
    
    For these counters, it is critical that the counter is allocated on a CPU
    cache line, and that no other variable is allocated in the same memory area,
    which can end up in the same CPU cache line.
    
    This fix introduce PFS_cacheline_uint32 and PFS_cacheline_uint64,
    to enforce this property.
    
    Al internal counters related to allocation internal objects have been
    changed to use PFS_cacheline_uint32/64.
    
    Whether CPU cache line collision did happened in the past,
    causing [1;31mperf[mormance degradations, is unknown, so this fix may not improve
    current [1;31mperf[mormance.
    
    What this fix does however, is to make sure that cache line collisions are
    now impossible, to prevent any regressions when using:
    - different compilers
    - different compiler options
    - different CPUs
    
    Also, the allocation algorithm for PFS_account, PFS_user and PFS_host have
    been changed to follow the mainline code.
    
    PFS_scan has been removed, this way of [1;31mperf[morming allocations is no longer
    needed.

[33mcommit 2842462367583c32e6da619041df8966d1ee39f9[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Sep 25 12:27:11 2013 +0300

    Bug#17492672 REPLACE MEM_ALLOC() WITH UT_MALLOC()
    
    In InnoDB, the functions mem_alloc() and mem_free() are wrappers for
    mem_heap_alloc() and mem_heap_free(). They create an anonymous
    InnoDB memory heap for the single user object.
    
    Because the functions are defined inline, their invocation will generate
    quite a bit of code.
    
    It is questionable if there is any benefit of using mem_alloc() instead of
    ut_malloc(). There clearly is some CPU and memory overhead. The only possible
    benefit could be the extra debugging info, which is the subject of
    Bug#16924719 69422: small [1;31mperf[mormance impact with heap block debugging info
    in release builds
    
    ut_malloc_low(): Remove the always-true parameter and rename to
    ut_malloc().  Remove ut_mem_null_ptr. Use a simple assertion also with
    the InnoDB memory heaps.
    
    ut_zalloc(): New function, to replace mem_zalloc().
    
    mem_alloc(): Replace with ut_malloc().
    mem_free(): Replace with ut_free().
    
    mem_alloc2(): Replace with ut_malloc().
    We will no longer determine the true allocated size of the block,
    but instead use the requested size only.
    
    os_mem_alloc_large(): Remove an #elif branch that contained non-compiling code.
    (The third argument of ut_malloc_low() had been removed a long time ago.)
    
    Do not check if the pointer is NULL before calling ut_free(), because ut_free()
    will check that. The caller-side check was needed for mem_free().
    
    rb#3397 approved by Kevin Lewis

[33mcommit 44154397b91e8b50406fdae5c91544e856074a45[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Sep 25 00:03:01 2013 +0200

    Bug#17504345 - PERFORMANCE OVERHEAD, THREAD AGGREGATE ON DISCONNECT
    
    Before this fix,
    
    1)
    
    On thread connect, the [1;31mperf[mormance schema needs to evaluate if the thread is
    instrumented or not.
    
    The logic used did look up the content of [1;31mperf[mormance_schema.setup_actors
    for the connecting account, every time.
    
    2)
    
    On thread disconnect, the [1;31mperf[mormance schema needs to aggregate statistics
    for parent users / account / hosts or globally, based on statistics
    collected by the disconnecting thread.
    
    The logic did [1;31mperf[morm this aggregation all the time, even when nothing was
    collected during the thread life time, for example when the [1;31mperf[mormance
    schema is disabled.
    
    With this fix,
    
    1)
    
    A new attribute PFS_account::m_enabled is created,
    that saves the result of the setup_actors look up.
    
    Whenever the content of the setup_actors table is changed,
    the code re evaluates PFS_acount::m_enabled for every account,
    to reflect new rules in setup_actors.
    
    When a thread connects, the corresponding PFS_account::m_enabled flag is
    used to decide if the thread is to be instrumented, if found.
    
    This avoids looking up setup_actors on every connection,
    the look up only happen the first time a given account connects.
    
    This flag is internal, there are no visible end user changes.
    
    2)
    
    A new attribute PFS_thread::m_aggregate_on_disconnect is created,
    that indicates if aggregation during disconnect is needed.
    
    When the [1;31mperf[mormance schema is disabled, this flag is always false.
    
    When the [1;31mperf[mormance schema is enabled, and in particular:
    - when the 'global_instrumentation' consumer is enabled
    - when the 'thread_instrumentation' consumer is enabled
    - when the 'instrumented' column in table thread is enabled
    this flag PFS_thread::m_aggregate_on_disconnect is re evaluated,
    for every running thread if necessary.
    
    The m_aggregate_on_disconnect flag is only set when a thread can collect per
    thread statistics.
    
    During disconnect, aggregation is done only when the flag is set.
    
    The net effect of this change is that aggregation on disconnect only happens
    when necessary, saving CPU when no per thread stats are ever collected.
    
    This flag is internal, there are no visible end user changes.

[33mcommit 0078c465bdbfc3bb03f1e3624ecdd8acf1d669ff[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 23 13:12:00 2013 +0200

    Bug#17493868 IMPROVE PERFORMANCE_SCHEMA MEMORY INSTRUMENTATION
    
    Before this fix,
    
    1)
    
    The memory instrumentation code could spent time [1;31mperf[morming
    operations in struct PFS_memory_stat
    
    2)
    
    Memory allocated internally by the [1;31mperf[mormance schema
    was not reported by the [1;31mperf[mormance schema memory instrumentation.
    
    With this fix,
    
    1)
    
    A new member, PFS_memory_stat::m_used, is added.
    This flags makes the code path much shorter.
    
    For cases when no memort instrumentation is enabled (the default),
    there are no stats to maintain.
    
    Even when the memory instrumentation is used, it is very unlikely that a
    given session has statistics for every kind of memory instrument, so most
    statistics will be unused.
    
    This is a [1;31mperf[mormance improvement.
    
    2)
    
    A new instrument is added, named
    "memory/[1;31mperf[mormance_schema/internal_buffers".
    The [1;31mperf[mormance schema now reports its own memory usage as well.

[33mcommit 9065096c6bdc3e17d54e4d661c5a8062da3c1832[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Mon Sep 23 16:19:24 2013 +0530

    Bug#17440991- THREAD_ID IN REPLICATION RELATED P_S TABLES
                  IS NOT THE INTERNAL PFS THREAD_ID
    
    Problem:
    =======
    The thread_id fields in the [1;31mperf[mormance schema tables show
    the PFS_thread::m_thread_internal_id. But the replication
    related Performance schema tables show THD::thread_id.
    The two can be different numbers for a given thread.
    
    Analysis:
    ========
    THD::thread is the connection_id. For background threads
    (threads not associated with a user connection), these are
    not incremented. Performance schema instruments background
    threads as well and hence maintains a different counter,
    PFS_thread::m_thread_internal_id.
    
    Fix:
    ===
    Modify replication related P_S tables to use
    PFS::m_thread_internal_id instead of THD::thread_id.

[33mcommit 31be19d9a6f7d9c244a750c1b9f17dcb362821d8[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Sep 20 11:47:15 2013 +0200

    Bug#14771682 PERFORMANCE SCHEMA LEAKS MEMORY ON SHUTDOWN
    
    Currently, memory allocated for the [1;31mperf[mormance schema is never freed,
    which causes valgrind warnings.
    
    To resolve completely the issue, two changes are needed:
    
    1)
    Resolve bug#56666 Race condition between the server main thread and the
    kill server thread,
    which is the scope of WL#6407.
    
    2)
    Fix the function cleanup_[1;31mperf[mormance_schema() itself to free all memory used
    by the [1;31mperf[mormance schema.
    
    This fix implements the second part only.
    
    Code is commented out using #ifdef HAVE_WL6407,
    while waiting for part 1).
    
    The function cleanup_[1;31mperf[mormance_schema() has been tested (with #define
    HAVE_WL6407), which exposed some leaks, which have been all fixed.
    
    Executing unit tests under valgrind also exposed additional issues,
    all fixed in the unit tests.

[33mcommit 095cbf2aeb3aad69ec1c7d6b3a1b71b550c395bc[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Sep 19 10:55:34 2013 +0200

    Bug#17478068 COMPILING THE PERFORMANCE SCHEMA WITH ONLY SOME
    INSTRUMENTATIONS
    
    Before this fix, building the server could be done:
    - with the [1;31mperf[mormance schema, including ALL available instrumentation
    - without the [1;31mperf[mormance schema.
    
    The problem is that given the growing list of available instrumentations,
    this is less and less practical.
    
    In particular, to isolate a functional or [1;31mperf[mormance issue,
    it can be desirable to build only with a particular instrumentation.
    
    This fix introduce the following cmake options:
    - DISABLE_PSI_MUTEX
    - DISABLE_PSI_RWLOCK
    - DISABLE_PSI_COND
    - DISABLE_PSI_FILE
    - DISABLE_PSI_TABLE
    - DISABLE_PSI_SOCKET
    - DISABLE_PSI_STAGE
    - DISABLE_PSI_STATEMENT
    - DISABLE_PSI_SP
    - DISABLE_PSI_IDLE
    - DISABLE_PSI_STATEMENT_DIGEST
    - DISABLE_PSI_METADATA
    - DISABLE_PSI_MEMORY
    which can be used to not build with a specific instrumentation
    
    When a specific instrumentation is not built:
    - Instrumentation points added in the server code,
      which uses macros like PSI_MUTEX_CALL,
      do not make calls to the [1;31mperf[mormance schema, saving CPU.
    - The [1;31mperf[mormance schema does not allocate memory for the instrumentation,
    - The thread aggregation code does not maintain statistics for the
      instrumentation, as all statistics are 0, to save CPU.
    
    Note that code like PFS_mutex_class is still compiled,
    but effectively dead.
    The goal is to reduce the memory and CPU footprint,
    not to eliminate completely symbols from the binary.
    
    Note that the table schema is unchanged.
    Tables that depend on a specific instrumentation,
    such as [1;31mperf[mormance_schema.mutex_instances,
    will be empty when building with DISABLE_PSI_MUTEX=ON,
    but the table still exists.
    
    These options are intended to be used primarily by maintainers,
    and as such comes with the following limitation:
    
    The test suite has _not_ been fixed to have each test check for additional
    pre conditions, such as:
    - instrumentation X must be available
    - instrumentation Y must be available
    to make the tests suite pass no matter what the combination of compiling option
    is used.
    Adding these tests would make the test suite hard to maintain, for no additional
    value.
    
    The only test expected to pass is [1;31mperf[mschema.start_server_nothing,
    which verifies that the server is still functional even with missing
    instrumentation.

[33mcommit b8cd19c32d1e39921ecc1707f0f295e77e73360c[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Sep 4 13:36:12 2013 +0200

    Bug#17406891 - CRASH IN MDL INSTRUMENTATION, WHEN THREAD IS NOT
    INSTRUMENTED
    
    In pfs_rebind_table_v1(),
    the [1;31mperf[mormance schema instrumentation for table handles
    could crash, when assigning a table handle to a non instrumented thread.
    
    With this fix,
    for table [1;31mperf[mormance_schema.table_handles,
    column OWNER_EVENT_ID is set to 0
    when the thread that owns a table handle is not instrumented.

[33mcommit 5702c71c858c8c0b31c6df0a7f963c9f34b80866[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Sep 4 11:50:13 2013 +0200

    Bug#17400029 - UNDERCOUNTED FREE OPERATIONS IN PERFORMANCE SCHEMA MEMORY
    INSTRUMENTATION
    
    In the [1;31mperf[mormance schema memory instrumentation,
    statistics collected about FREE operations could be under evaluated,
    leading to a perceived memory leak.
    
    The problem was that when a memory block was counted as allocated,
    it was not always counted as freed, because the logic in
    memory_free_v1() and memory_realloc_v1() tested too many flags.
    
    During a call to memory_free_v1(),
    the value of:
    - the global_instrumentation consumer
    - the thread_instrumentation consumer
    - the thread.INSTRUMENTED column
    - the presence of thread instrumentation
    
    may affect *how* free statistics are collected,
    for example accounted per thread or globally,
    but it should not affect *if* free statistics are collected.
    
    With this fix, there is no logic affecting *if* free statistics
    are collected.
    Every memory block counted as allocated is then counted as freed,
    when invoking the instrumentation for the free (or realloc) operation.
    
    This fix also renamed the memory instrumentation apis.

[33mcommit 500571c85f71d0ec74adeaa2912507238bec0797[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Sep 3 11:25:45 2013 +0200

    Bug#17399658 MISSING PSI_MEMORY_KEY WHEN BUILDING PLUGINS WITHOUT THE
    PERFORMANCE SCHEMA
    
    This fix is a build issue.
    
    When compiling without the [1;31mperf[mormance schema,
    the declaration and definition of some PSI_memory_key for the memory
    instrumentation did not match.
    
    The declaration was always present (both with and without the [1;31mperf[mormance
    schema), as intended.
    
    The definition was only present when compiling with the [1;31mperf[mormance schema,
    leading to unresolved symbols when compiling without.
    
    The fix is to always declare and define every keys.
    
    Also, found and fixed a my_alloc() / free() mismatch.

[33mcommit 4d275c89954685e2ed1b368812b3b5a29ddf9389[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Thu Aug 22 11:38:26 2013 +0530

    Bug#11844915 - Hang in THDVAR mutex acquisition
    
    Analysis:
    --------------
    Storage for session system variables (THDVAR) from plugins were
    allocated lazily. On the first access to any of such variables
    memory for all of them was allocated and values were copied from
    global variables. This was done under protection of
    LOCK_global_system_variables mutex.
    
    The problem might occur when such first access happens from within
    InnoDB storage engine if at this point some internal InnoDB locks
    are also held. We can end-up in deadlock situation when the thread
    owning these locks is waiting for LOCK_global_system_variables mutex
    and thread owning LOCK_global_system_variables waits (maybe
    indirectly through several other threads) for one of these InnoDB
    locks.
    
    According to InnoDB team this doesn't happen in the current code,
    as InnoDB tries to avoid accessing session variables while holding
    internal locks, but this has happened in the past.
    
    Since this also might happen in future when code changes, we would
    like to make our code more future proof by avoiding this problem.
    
    Fix:
    --------------
    Instead of allocating storage for session system variables (THDVAR)
    lazily, modified code to populate session system variables on
    creation of thread for the connection. So now, to accesses these
    variables from plugins, mutex "LOCK_global_system_variables" need
    not be a acquired.
    
    Since, innodb is built-in storage engine, all the THDVARs of innodb
    are populated at this time with other THDVARs. So now, accessing
    these THDVARs from innodb layer never acquires mutex
    "LOCK_global_system_variables".
    
    Note that since connection object preparation happens in the connection
    thread nowadays the [1;31mperf[mormance impact of such a change should be
    acceptable.
    
    For dynamically loaded plugins situations stays the same as before.
    If plugin was loaded after connection was created then memory for
    THDVARs of this plugin will be allocated on the first access to them
    and LOCK_global_system_variables mutex will be acquired. So it is a
    good idea for plugins to avoid holding internal locks while accessing
    to such variables.
    
    OTOH, since in practice most of connections will be created after the
    plugin load and thus will have memory for plugin's THDVARs allocated
    at connection creation time the probability of deadlock occurring for
    dynamic plugins is greatly reduced by this patch as well.
    
    Also, while populating THDVARs, we copy all TDHVARs and then
    iterate through ALL vars using hash "bookmark_hash". In this loop,
    now we are only checking whether var is of type string with flag
    MEMALLOC or not. If yes then strdup the value. If we do not have
    any vars of this type then we just iterate through all the vars. To
    avoid this, one more hash is introduced to hold the reference of
    variable of type STRING with flag MEMALLOC only. If this hash has
    any elements then only we iterate through them otherwise no.

[33mcommit e965e3a8db9dc9ffc4e41ff2ac53ef8c042f61ef[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Aug 19 20:02:29 2013 +0200

    Bug#17325229 - CLIENT.C FAILS TO COMPILE WHEN HAVE_PSI_INTERFACE IS NOT DEFINED ON WINDOWS
    
    Fixed a build break when compiling without the [1;31mperf[mormance schema on windows
    platforms.

[33mcommit f4d4b1823f4f25e107feae7a352c152ca5246db8[m
Author: Vamsikrishna Bhagi <vamsikrishna.bhagi@oracle.com>
Date:   Fri Aug 16 14:16:45 2013 +0530

    WL#6982 Make plugin column in mysql.user non-null
    
    Since the introduction of the post-4.1 authentication method,
    the choice of authentication method was [1;31mperf[mormed on the length
    of the password hash alone, thus enabling easy change of the
    authentication methods through a password change.
    
    This has become a problem with the introduction of other
    authentication methods in 5.5. We needed a special value (empty)
    of the authentication plugin column and a set of special cases
    throughout the code to support the backward compatible behavior.
    
    Since in 5.6 the pre-4.1 password hash is to be deprecated this
    worklog aims to remove this backward compatible layer and simplify
    the authentication code by removing the special cases.
    
    Now the native authentication methods will be subject to the same
    rules as the other authentication methods.

[33mcommit 0a0dcc2e30e5a1341c817f333198683363507729[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri Aug 16 10:47:40 2013 +0530

    WL#3656:PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    - Post push fix.
    - Fixed sporadic failure in
        rpl_[1;31mperf[mschema_execute_status_by_coordinator.test

[33mcommit 51180cc85c670fc4ce0b64cc1efe467c00957d65[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Wed Aug 14 19:21:46 2013 +0530

    BUG#16498740 - CREATE/ALTER: INCORRECT ERROR MSG. SEPARATE
                   UNSUPPORTED OPS FROM OTHER FAILURES
    
    BACKGROUND:
    CREATE or ALTER table can fail due to unsupported options
    OR if the user forgot to specify something in the command.
    In this particular case in the bug report, ALTER table failed
    because the user forgot to specify tablespace for ndb during
    create table time.
    While [1;31mperf[morming alter table, the current error message said
    "doesn't have this option" which can be misleading to the user.
    The given option is supported but the user forgot to specify
    tablespace during create time.
    Instead of misleading error message we can print a useful error
    message to the user.
    
    ANALYSIS:
    On failure of Alter table, HA_WRONG_CREATE_OPTION was invoked
    which mapped to ER_ILLEGAL_HA in handler::print_error.
    This change was done in Bug#13840553.
    ER_ILLEGAL_HA can be misunderstood by user that storage engine
    (in this case ndbcluster) does not support altering column
    storage from main memory to disk whereas the problem is
    user forgot to specify tablespace for ndb during create table.
    
    FIX:
    To fix this bug, a new handler error HA_MISSING_CREATE_OPTION
    is added and it is returned when ALTER fails which is then
    mapped to newly introduced error code ER_MISSING_HA_CREATE_OPTION
    in handler::print_error() function.

[33mcommit 0d157ac755a93289d350bad500c77a8a00209887[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Wed Aug 14 02:44:56 2013 +0530

    WL#3656:PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Post push fix.
    Fixed valgrind failure in rpl_[1;31mperf[mschema_connect_status.test

[33mcommit a836753aef1d94c797caa626f89c9800368f9e8c[m
Merge: a13786fe47b 8c7c5b92db8
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Aug 13 08:46:32 2013 +1000

    WL#6044 - InnoDB: Policy-based mutex
    
    Create a common interface for all InnoDB mutex types. This change gets rid
    of the distinction between "os_fast_mutex" and the InnoDB home-brew mutex
    implementation.
    
    Create several classes of mutexes (more to be added later).
    
     1. OSBasicMutex   - POSIX or Windows CRITICAL_SECTION
     2. OSTrackedMutex - Same as #1 but we track the acquire/release
     3. TTASFutexMutex - Where available on Linux, uses futexes instead of condvars
     4. TTASEventMutex - The old home-brew implementation
     5. SpinMutex      - Spin only version that uses TTAS
    
    #3 should get rid of an existing [1;31mperf[mormance issue around condvar broadcasts
    that results in a thundering herd problem. Also, we can now set the mutex type
    to any one of the above for individual mutexes in the code.
    
    Rewrite the os_event_t code, use OSBasicMutex as the underlying mutex type.
    The code uses static polymorphism (C++ templates) instead of inheritance. This
    is to avoid the vptr overhead. This is mainly relevant only for mutexes that are
    currently used for the blocks. For very large buffer pools, e.g., 64G this will
    reduce the mutex overhead significantly. On Linux where futexes are available
    the mutex overhead can be as low as sizeof(lock_word_t).
    
    Rewrite the UNIV_SYNC_DEBUG code. Move all PFS key defines and declarations
    to sync0sync.h and sync0sync.cc.
    
    rb#1913 Approved by Jimmy Yang and Yasufumi Kinoshita.

[33mcommit 48072bbfab37493aff59391e799313b98064b5dc[m
Author: Tanjot Uppal <tanjot.uppal@oracle.com>
Date:   Sat Aug 10 00:09:08 2013 +0530

    Made i_[1;31mperf[mschema.truncate_stress experimental till the Bug#17267760 is fixed

[33mcommit 206707fd65bf7239e2d90a2597997b74bff90dc2[m
Merge: d95cabdc738 81d0397fb97
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Aug 9 21:26:03 2013 +0900

    - WL#6326 : InnoDB: fix index->lock contention
    
    The acquisition of node pointer page latches is protected by index->lock latch.
    
    Before WL#6326, index->lock protected all node pointer pages either in S or X
    mode, and no individual block->lock were acquired on node pointer pages.
    
    After WL#6326, node pointer pages are protected by individual block->lock
    S-latch or X-latch. The acquisition of node pointer page latches is covered by
    index->lock, for preventing deadlocks.
    
    This WL# depends on WL#6363
    
    (rb#1099 : Approved by Kevin and Jimmy)
    
    
    - WL#6363 : implement SX-lock for rw_lock
    
    InnoDB internally uses rw-lock implementation to keep consistency of internal
    resources. Basically the rw-lock has 2 types S-lock (shared) and X-lock (exluded).
    The fix adds the new type SX-lock (shared excluded) for room to optimize
    concurrency and improve scalability more.
    
    At least, S-lock and X-lock behave same, and compatible for current code. So,
    nothing changed by only this fix as it is. (no functional/[1;31mperf[mormance changes
    for users)
    
    The new state SX-lock will be used by the future work. (e.g. WL#6326: InnoDB:
    fix index->lock contention)
    
    new state of rw_lock: SX-lock
          | S|SX| X|
        --+--+--+--+
         S| o| o| x|
        --+--+--+--+
        SX| o| x| x|
        --+--+--+--+
         X| x| x| x|
        --+--+--+--+
    
    (rb#1098 : Approved by Inaam and Kevin)

[33mcommit b59dbcf032eab3258a6a11165199e587fbb2a2ae[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Aug 8 15:50:08 2013 +0200

    Test scripts cleanup, when building without the [1;31mperf[mormance schema

[33mcommit 119c1f774fe19ef5eaf6f7c25efc64392fd695ab[m
Author: Rohit Kalhans <rohit.kalhans@oracle.com>
Date:   Wed Aug 7 18:48:46 2013 +0530

    WL#6314: Intra-schema multi-threaded slaves
    
    1. Fixed rpl_[1;31mperf[mschema_execute_status_by_worker test failure after merge to trunk.
    2. Fixed main.mysqlbinlog which failed due to change in binlog format after this
       WL.

[33mcommit b4bf2d5bb511d08a17fec540463ce020fdb322ba[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Wed Aug 7 10:03:18 2013 +0530

    WL#6606 - Offload THD and network initialization to worker
              thread.
    Initialization of THD and vio/net initialization happens
    in the acceptor thread that accepts the connection. THD and
    network initialization involves acquiring locks, memory
    allocation of various structures and system calls which are
    compute-bound as well as tasks that may block. The acceptor
    thread is an event loop that waits for new connection events
    from clients. To maximize the number of connections that can
    be handled per unit of time, the acceptor thread should
    spend as much of its time listening for new connections.
    This means thd and vio/net initialization should be
    offloaded from the accept event loop and delegated to
    worker threads that handle the client connections.
    This worklog provides a generic framework which offloads
    THD initialization and net/vio initialization to worker
    threads for all types of communication channels (shared
    memory, named pipes and sockets) that clients connect with
    server.
    In addition, this worklog refactored the existing
    interfaces of the struct scheduler_functions into an object
    oriented API, refactored and moved code related to
    connection handling and its management into a separate
    directory and files that contain implementations of specific
    related functionality. This resulted in removal of unnecessary
    #defines, modularity, better code clarity and readability in
    addition to [1;31mperf[mormance improvements made in the worklog.
    As result of changes in this worklog, the follow bugs have
    been fixed:
    Bug#12951536 - THD INITIALIZATION TOO EXPENSIVE FOR
                   ACCEPT() THREAD.
    Bug#12951595 - TOO MUCH NETWORK INITIALIZATION DONE
                   IN ACCEPT() THREAD.
    Bug#12951605 - ACCEPT() SOCKET GETS TOO MUCH OF FCNTL()S.
    
    User Visible Changes:
    The system variables "bind_address", "thread_handling",
    ""thread_cache_size" and status variables "threads_cached",
    "Slow_launch_threads" are no longer visible in embedded
    server mode (where they have no effect).

[33mcommit af26a07c054598a5804d534447c6814cb3cdb6b1[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Jul 30 13:53:56 2013 +0200

    Fixed build break,
    when compiling without the [1;31mperf[mormance schema

[33mcommit d3f4978056fd36abfcd48624e1e3964a8ad12933[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Jul 29 15:57:40 2013 +0200

    Fixed spurious warnings about [1;31mperf[mormance_schema tables,
    when not using the [1;31mperf[mormance_schema.

[33mcommit 86b01057eee3558e371effb4d906ebe2f86de392[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Sat Jul 27 09:09:13 2013 +1000

    WL#6578 - Create a new std:vector like class to handle read view trx IDs.
    Add infrastructure for reading the transaction time from the server
    session instance. Bypass RW code paths when a transaction is RO. Some minor
    code rearrangement in btr0sea.cc, after doing some [1;31mperf[m tests. Seems like
    some instruction cache misses due to jumps (gotos). Make some hash functions
    const correct.

[33mcommit 7da044c4004b8254abe77f36d37b3689bcc44e04[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jul 17 09:26:45 2013 +0200

    Bug#17164720 ASSERT IN READ_TOKEN , STORAGE/PERFSCHEMA/PFS_DIGEST.H
    
    Before this fix,
      SELECT * from [1;31mperf[mormance_schema.events_statements_current
    could cause an assertion failure when computing the DIGEST / DIGEST TEXT columns.
    
    The root cause is a race condition, that happen under load.
    The thread computing the digest reads data from a buffer,
    which is also conccurently updated when the target session is executing a statement.
    
    The fix sanitizes data when reading from the digest storage buffer,
    and when data that does not fit the expected format is found,
    a NULL digest and digest text is displayed.

[33mcommit 6d74dc4fa379fab063612b113c5ae851ae49dff6[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Tue Jul 16 16:17:21 2013 +0800

    Fix Bug#17057168 LARGE PERFORMANCE REGRESSION FOR INNODB
    GEOMETRY/SPATIAL INDEX LOOKUP
    
    For the new datatype DATA_GEOMETRY, comparison of this data always
    return 0. This caused it can't find the correct key range of the index
    on this data type, so the query on this type column will always do full
    index scan. That's why we got this [1;31mperf[mormance regression.
    The solution is: Since we still use still use BLOB as underlying
    datatype, so we should compare geometry data following the compare BLOB
    way.
    In this patch, I also fixed the occasionally failure of test case
    i_innodb.innodb_bug15963619. According to Krunal's suggestion, I
    increased the restart wait time from 10 to 60.
    
    rb#2890 Approved by Jimmy.

[33mcommit 51f17f92512e578f268cddf5ac0d5cb45cf0db6f[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri Jul 12 19:12:21 2013 +0530

    BUG#17154390 - CRASHING WHEN EXECUTING SELECT ON
                   NEW PFS TABLES WITHOUT REPLICATION
    
    Problem:
    ========
    There is a test in [1;31mperf[mschema suite which tests the same.
    But the issue was not caught because mtr starts the server
    with server-id=1. Now the code says:
    if(server-id!=0)
      init_slave();
    Since server-id=1, init_slave is called and active_mi is
    declared and thus dereferencing an attribute of active_mi
    is fine.
    
    When server is started manually, server-id=0. Hence,
    init_slave() is not called, so active_mi is not declared.
    So, de-referencing an attribute of active_mi lead to a
    server crash.
    
    Fix:
    ===
    Fixed the issue by checking for NULL before dereferencing.

[33mcommit 3e0c928538af3684c86c66df04791a8764276e4a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Jul 8 11:01:53 2013 +0200

    Bug#17041705 DO NOT BUILD PERFORMANCE SCHEMA FOR EMBEDDED
    
    Before this fix, the [1;31mperf[mormance schema storage engine was build for the
    embedded library.
    
    There is no [1;31mperf[mormance instrumentation when building for embedded,
    this is dead code.
    
    The fix is to introduce a new option to the MYSQL_ADD_PLUGIN cmake macro,
    named NOT_FOR_EMBEDDED.
    
    When this flag is set,
    - the plugin is not built for the embedded server,
    - the plugin is conditionally included (#ifndef EMBEDDED_LIBRARY)
      in the sql/sql_builtin.cc generated file.
    - the plugin is not in the list of libraries used to build libmysqld
    
    Now, because some instrumentation can be expanded at compile time,
    in particular with PSI_XXX_CALL macros when building code,
    the option RECOMPILE_FOR_EMBEDDED is also added to plugins that
    can contain references to the server itself (define MYSQL_SERVER).
    
    This affects the blackhole and the federated engine.
    
    Last, when executing tests with MTR, when using the --embedded server option,
    FRM files for [1;31mperf[mormance schema tables could still be present on disk,
    which affects queries on the information_schema,
    causing spurious ER_UNKNOWN_STORAGE_ENGINE warnings.
    
    The root cause was that the installation process,
    when using mysqld --bootstrap to install a new database,
    was always creating [1;31mperf[mormance schema tables (and FRM files),
    because the [1;31mperf[mormance schema plugin was always loaded.
    
    mysqld --bootstrap has been relaxed a bit,
    to allow for not always loading the [1;31mperf[mormance schema storage engine.
    
    The MTR script, when installing a database for an embedded test,
    now installs a database without [1;31mperf[mormance_schema tables.

[33mcommit 73bd6da140dbaf3a8a3cade023f7ad0445d70d0a[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Thu Jul 4 23:44:54 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Implemented following PS tables for replication monitoring:
    1) replication_connection_configuration
    2) replication_connection_status
    3) replication_execute_configuration
    4) replication_execute_status
    5) replication_execute_status_by_coordinator
    6) replication_execute_status_by_worker
    
    In this patch:
    fixed rpl_[1;31mperf[mschema_* tests failing on pb2 WL branch

[33mcommit f4b19eef5bf7e6b48215aeddd1a1dac68003f909[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Thu Jul 4 19:19:28 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Implemented following PS tables for replication monitoring:
    1) replication_connection_configuration
    2) replication_connection_status
    3) replication_execute_configuration
    4) replication_execute_status
    5) replication_execute_status_by_coordinator
    6) replication_execute_status_by_worker
    
    In this patch:
    Addressed comments from Marc
    - fixed windows build failure on pb2.
    - added #include "my_global.h" in all added .cc
    - worked on  more tests in [1;31mperf[mschema suite
    - applied patch for Bug#17041705

[33mcommit 9de6393e1f8aa95d31b0f0192fb3acdcb76a26da[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Wed Jul 3 21:03:17 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Implemented following PS tables for replication monitoring:
    1) replication_connection_configuration
    2) replication_connection_status
    3) replication_execute_configuration
    4) replication_execute_status
    5) replication_execute_status_by_coordinator
    6) replication_execute_status_by_worker
    
    In this patch:
    Fixed rpl_[1;31mperf[mschema_execute_status_by_coordinator.test
    faling on PB2 WL branch

[33mcommit daea07347f35e5dfa35a82690bca4368f95090f3[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Wed Jul 3 17:44:50 2013 +0530

    Bug#16957721: VALIDATE_PASSWORD_LENGTH ALLOWED TO HAVE
                  BELOW VALIDATE_PASSWORD PARAMETER VALUE
    
    Description: Value of validate_password_length variable
                 is set in following manner:
                 max(validate_password_length,
                   (validate_password_number_count +
                     validate_password_special_char_count +
                     (2*validate_password_mixed_case_count)))
    
                 While this is done each time value of
                 any of the above mentioned variable is
                 changed, same check should be [1;31mperf[mormed at
                 the time of plugin installation which was
                 missing and hence it was possible to set
                 validate_password_length to a value which
                 does not satisfy above mentioned criteria.
    
                 This patch fixes the issue by introducing a
                 check at plugin installation time as well.

[33mcommit 8435f64548a8a2c34c5dee0b8ff9ef8dbf7efe1a[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Tue Jul 2 16:52:38 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Implemented following PS tables for replication monitoring:
    1) replication_connection_configuration
    2) replication_connection_status
    3) replication_execute_configuration
    4) replication_execute_status
    5) replication_execute_status_by_coordinator
    6) replication_execute_status_by_worker
    
    In this patch:
    fix to handle failure of test
    [1;31mperf[mschema.information_schema on pb2 WL branch.
    The issue was: In case MTS(mi->rli->workers) was not set,
    the get_row_count() function returned a uninitialized
    value.

[33mcommit 30edb3d95265df818e120282aee7c8ce8e504bc3[m
Merge: 743c7bd025f ab7c7d236ec
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Thu Jun 27 12:23:19 2013 +0300

    Bug #16996656: UNIQUE OPTION PREFIXES NOT DEPRECATED IN 5.5+\
    
    trunk version : stop accepting long command line option prefixes.
    Test cases updated to specify the full command line option names.
    [1;31mperf[mormance schema test on amibguous options prefixes shortened
    the wl6978 test on deprecation warning shortened to remove
    server side test and test only client side.

[33mcommit 1aeb6612c90923f20002187a6f3e0c9bddaa5955[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Wed May 29 16:28:39 2013 -0700

    mysql-js add support for type converters
    
    MySQLConnection.js:
      add adapter parameter to newResultObject and getFields to [1;31mperf[morm type conversions
    
    MySQLConnectionPool.js:
      add typeConverterMap with key column type and value converter object
      add type converters for timestamp and date column types
    
    MySQLDictionary.js:
      check type converter map for each column and store in column.typeConverter.mysql
    
    DBTableHandler.js:
      store column.typeConverter in field.typeConverter at initialization
      add adapter parameter to newResultObject and getFields to [1;31mperf[morm type conversions

[33mcommit 6f5f19b338543277a108a97710de8dd59b9dbb60[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed May 29 14:50:35 2013 +0300

    Bug#16852278 SIMPLIFY RECORD COMPARISONS
    
    cmp_data(): Common inline function for comparing two fields.
    Compare the common prefix with memcmp().
    
    Assert that the fields being compared cannot be stored externally. Only clustered index leaf page records can contain externally stored fields, and the clustered index records should differ already in the PRIMARY KEY columns, which cannot be stored externally.
    
    page_cmp_dtuple_rec_with_match(): Remove, and call
    cmp_dtuple_rec_with_match() instead, on user records only.
    
    cmp_rec_rec_with_match(): Remove the unused parameter matched_bytes.
    Ignore the initial value of *matched_fields (assume that it is 0).
    
    page_cur_search(): Remove the constant parameter PAGE_CUR_LE.
    
    Remove support for byte-level prefix granularity in searches.
    
    There were 2 users of this information: the adaptive hash index, and
    prefix searches (cmp_dtuple_is_prefix_of_rec(), called for match_mode
    == ROW_SEL_EXACT_PREFIX).
    
    The byte-level granularity was only available for VARBINARY columns,
    and before rb#2350 Bug#16723431 REMOVE SRV_LATIN1_ORDERING, for the
    default collation latin1_swedish_ci. Prefix searches should never
    supply a partial-field prefix, due to an assertion in
    row_sel_convert_mysql_key_to_innobase().
    
    The adaptive hash index is unlikely to benefit much from the
    byte-level granularity either. (No [1;31mperf[mormance penalty was observed
    when testing rb#2350, and the keys did include latin1_swedish_ci.)
    
    btr_cur_t: Remove up_bytes, low_bytes.
    btr_search_t: Remove n_bytes.
    buf_block_t: Remove n_bytes, curr_n_bytes.
    
    dtuple_fold(), rec_fold(): Remove n_bytes.
    
    ut_pair_min(), ut_pair_cmp(): Remove.
    
    page_cur_search_with_match(), page_cur_try_search_shortcut():
    Remove iup_matched_bytes, ilow_matched_bytes.
    
    cmp_dtuple_rec_with_match_low(), cmp_dtuple_rec_with_match():
    Remove matched_bytes.
    
    Remove the normalization of the comparison result to -1 or 1.
    Instead, return negative or positive, and adjust the callers.
    
    btr_search_check_guess(), page_cur_search_with_match(), page_validate(),:
    Allow a wider range of return values from cmp_dtuple_rec_with_match().
    
    row_merge_blocks(): Allow a wider range of return values from
    cmp_rec_rec_simple().
    
    ib_like_t: Remove the unused and unsupported operations SUFFIX,
    SUBSTR, REGEXP.
    
    eval_cmp_like(), eval_cmp(): Simplify the code.
    
    pars_like_rebind(): Remove the unsupported ib_like_t operations.
    
    dtuple_coll_cmp(), cmp_data_data(), cmp_dtuple_rec(),
    cmp_dtuple_rec_with_match_low(), cmp_rec_rec(),
    cmp_rec_rec_with_match(), cmp_rec_rec_simple(), cmp_dfield_dfield(),
    innobase_mysql_cmp(), cmp_whole_field(), cmp_data(),
    cmp_dfield_dfield_like_prefix(), row_merge_tuple_cmp(): Return
    negative or positive instead of -1 or 1.
    
    dtype_get_pad_char(): Merge to ib_col_set_value() and cmp_data().
    
    cmp_decimal(): Refactored from cmp_decimal(). Compare two DATA_DECIMAL
    fields. This should only be needed for table definitions before MySQL 5.1.
    MYSQL_TYPE_NEWDECIMAL is compared as binary strings.
    
    cmp_debug_dtuple_rec_with_match(): Remove. This is virtually identical
    to its only caller. There should not be any inconsistency between the
    different comparison functions, because all of them should use cmp_data().
    
    rb#2490 approved by Jimmy Yang

[33mcommit 42499d9394bf103a27d63cd38b0c3c6bd738a7c7[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri May 24 00:08:22 2013 +0300

    WL#6871 cleanup:
    
    Remove support for byte-level prefix granularity in searches.
    
    There were 2 users of this information: the adaptive hash index, and
    prefix searches (cmp_dtuple_is_prefix_of_rec(), called for match_mode
    == ROW_SEL_EXACT_PREFIX).
    
    The byte-level granularity was only available for VARBINARY columns,
    and before rb#2350 Bug#16723431 REMOVE SRV_LATIN1_ORDERING, for the
    default collation latin1_swedish_ci. Prefix searches should never
    supply a partial-field prefix, due to an assertion in
    row_sel_convert_mysql_key_to_innobase().
    
    The adaptive hash index is unlikely to benefit much from the
    byte-level granularity either. (No [1;31mperf[mormance penalty was observed
    when testing rb#2350, and the keys did include latin1_swedish_ci.)
    
    btr_cur_t: Remove up_bytes, low_bytes.
    btr_search_t: Remove n_bytes.
    buf_block_t: Remove n_bytes, curr_n_bytes.
    
    dtuple_fold(), rec_fold(): Remove n_bytes.
    
    ut_pair_min(), ut_pair_cmp(): Remove.
    
    page_cur_search_with_match(), page_cur_try_search_shortcut():
    Remove iup_matched_bytes, ilow_matched_bytes.
    
    cmp_dtuple_rec_with_match_low(), cmp_dtuple_rec_with_match():
    Remove matched_bytes.
    
    PageCur::searchShortcut(): Remove up_match_bytes, low_match_bytes.

[33mcommit 1a1b7473b1a00d186fa67e982342f4f00b03e963[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri May 17 13:36:39 2013 +0300

    WL#6871 page creation optimization.
    
    page_dir_get_nth_slot(): Add parenthesis to help the compiler [1;31mperf[morm
    constant folding, when calling the macro with a constant n.
    
    page_create_low(): Use low-level byte access to initialize the page,
    instead of invoking the high-level methods.

[33mcommit be6e9921e6a23ade921d453b3e5b9b0df4582d6c[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Fri May 17 10:19:03 2013 +0200

    ndb - bug#16817928
    
    In dropTableGlobal(DropTableCascadeConstraints == 0) make sure to [1;31mperf[morm all
    checks before starting to [1;31mperf[morm actual drops.
    
    Otherwise the it might have dropped some indexes and/or FK and then return an error.
    
    That behaviour confused the black-art-ninja-code in ha_ndb_ddl_fk.cc that makes ndb
    become bug compatible wrt foreign_key_checks. The result of that confusion was that
    a FK was getting dropped twice, and the second time one got 4238 (Trigger Not Found)
    
    After this patch, if dropTableGlobal return 21080, no drops of anything will have been [1;31mperf[mormed.

[33mcommit ea1826f75d283a6a07d3dc8d3fe87307b4d194eb[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri May 17 11:47:21 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
    Changed table names.
    Changed File names and class names accordingly.
    
    Updated tests in [1;31mperf[mschema suite.
    
    TODO:
    
      * Fix an issue identified in table: replication_execute_status_by_executor.
      * Test that MTS works correctly after delaying destructing few objects.
      * Write mtr test cases for features in WL.

[33mcommit f6ad65b0534d569e81d5b4df3e3657fb206c6619[m
Author: Shivji Kumar Jha <shivji.jha@oracle.com>
Date:   Fri May 10 23:29:56 2013 +0530

    WL#3656- PERFORMANCE SCHEMA table for SHOW SLAVE STATUS
    
     Implemented following PS tables for replication monitoring:
       1) replication_connection_config_by_channel
       2) replication_connection_status_by_channel
       3) replication_execute_config_by_channel
       4) replication_execute_status_by_channel
       5) replication_execute_status_by_coordinator
       6) replication_execute_status_by_executor
    
     Updated tests in [1;31mperf[mschema suite accordingly.
    
     TODO:
         - Some fields in the above tables have hardcoded values.
         - Extend tables to contain multiple rows.
         - Write mtr scrpits to test the printed values.

[33mcommit ddd573420c4010d9376c1ef94a46f5948179a762[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Fri May 3 13:18:15 2013 +0200

    Bug #14095855 FOREIGN_KEY_CHECKS SHOULD APPLY TO NDB TABLES
    This is a patch for supporting toggling on/off
    of foreign key checking in ndb.
    Innodb supports
    
    set foreign_key_checks = 0;
    
    to immediately disable all foreign key checking, even
    if set inside an ongoing transaction. All operations
    prior to the setting will have foreign key constraints
    checked and all operations after will not. For Innodb
    these checks are done immediately, but for Ndb the setting
    
    set ndb_deferred_constraints = 1;
    
    will defer the foreign key checks until commit time.
    The setting of foreign_key_checks = 0; inside a transaction
    will only affect operations executed after the change,
    operations done before will cause foreign keys to be checked.
    
    The setting of foreign_key_checks = 0 will of course also
    disable any defined cascade of deletes or updates.
    
    The implementation disables Ndb's internal triggers for
    foreign key checking inside TUP so it will also have a
    [1;31mperf[mormance improvement during restoring backups with
    foreign key checks disabled.

[33mcommit 1892fbc1c974ee3ab71efde31f217b0cec91d945[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Thu May 2 13:22:56 2013 +0300

    wl#6244 testprg
    This patch introduces a rudimentary testFK program with following testcases
    - CreateDrop
      create/drop random FKs
    
    - CreateDropWithData
      create random FK's
      Fill table
      Drop FK's
    - CreateDropDuring
      one thread does create/dop random FKs
      one thread does insert/delete on table
    - CreateDropError TODO
      [1;31mperf[morm create/drop FK with error insert and verify
      no resource leakages
    - Basic1
      create random FK's
      one thread run mixed DML on table
    - Basic5
      create random FK's
      five threads run mixed DML on table
    - Basic55
      create random FK's
      ten threads run mixed DML on table (with two different transaction profiles)

[33mcommit 266514b36a70e45825e8f90cfd9b24704763bbf7[m
Author: Tanjot Uppal <tanjot.uppal@oracle.com>
Date:   Tue Apr 16 10:52:31 2013 +0530

    Disabling the test [1;31mperf[mschema.dml_esms_by_digest till Bug #16626404 is fixed, as this test is failing regularly on weekly-trunk

[33mcommit 26aaa43d35911c286f99cff46a8fce464b4fc3c4[m
Author: magnus.blaudd@oracle.com <>
Date:   Fri Apr 12 15:18:03 2013 +0200

    Bug#16360660 UNNECESSARY ASSERTION FAILURE ON SIGNALLOGGERMANAGER::LOG()
    
     - assert fired when ndbd or ndbmtd was started with the environment variable NDB_SIGNAL_LOG
      containing a string specifying unknown blocks
     - NDB_SIGNAL_LOG format s either empty which means to log all blocks or a semicolon
      separated list of blocks which names the blocks for which  logging should be [1;31mperf[mormed.
    
     - Modified the code for NDB_SIGNAL_LOG more failsafe by skipping unknown blocks, skipping if signal
      log file can't be opened.
     - Renamed a few variables to make code more readable.
     - Marked function getParameter as static
     - Added comments and fixed layout
     - Moidfied the failing assert to be done in two steps, first verify the argument bno and then the index
      into logModes array.

[33mcommit 0b36e0573bd2c0038350d067c85752debd7488d6[m
Author: Tanjot Uppal <tanjot.uppal@oracle.com>
Date:   Fri Apr 12 13:00:41 2013 +0530

    Remove the below tests from the disable list:
    i_binlog.binlog_unsafe_fulltext_plugin
    i_innodb.innodb_bug16097753
    i_main.partition_big_innodb
    i_main.partition_big_myisam
    [1;31mperf[mschema.hostcache_ipv4_max_con
    [1;31mperf[mschema.hostcache_ipv6_max_con

[33mcommit 2b20f2981de602d5c9f5bf4851e12b0f36b4547f[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Apr 11 12:36:05 2013 -0500

    Bug #16609256 - INLINE FFLUSH CALL CAUSES DROP IN SYSBENCH PERFORMANCE
    
    The patch for 11763660 expanded the macro for UT_DBG_PANIC which was
    defined as abort(). This macro is part of every call to ut_a(), ut_ad()
    and uterror.  So this expanded about 7500 lines of code.  The result
    was a [1;31mperf[mormance drop because of a generally larger codebase.
    
    So this patch replaces UT_DBG_PANIC with a function called ut_panic().
    This patch flushes both stderr and stdout as a precaution before calling
    abort.  The previous patch added fflush(stderr) to make sure that all
    error messages were written before crashing.
    
    Approved by Marko in http://rb.no.oracle.com/rb/r/2290/

[33mcommit 510dd48bf510dc0a3bda9e62cede698325d05fdd[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Apr 5 00:21:47 2013 -0500

    WL6742-Improve InnoDB SELECT COUNT(*) [1;31mperf[mormance by using
           handler::records()
    
    As a first step toward implementing WL6605 (Improve InnoDB SELECT
    COUNT(*) [1;31mperf[mormance), where a known record count is adjusted by
    transaction deltas, this worklog implements the handler::record()
    call in InnoDB to return the record count to the optimizer instead
    of making it count records itself.
    
    This patch improves the cost of an in-memory table scan by about 20%
    since record data does not need to be returned to the optimizer via
    the handler interface.  It effectively pushes down the counting effort
    from the server into InnoDB, reducing a lot of memcpy calls and data
    conversions.
    
    Just as before, only records visible to the current transaction are
    counted. There is a new testcase in this patch to prove that the
    correct count(*) is returned even when other transactions of each
    isolation level are in various stages of activity.
    
    Approved by Marko in http://rb.no.oracle.com/rb/r/1693/
    QA approval by Amit in http://wl.no.oracle.com/worklog/?tid=6742

[33mcommit 4daf627a660a066f3e31a2e2678f3f6b258c64bc[m
Author: Akhila Maddukuri <akhila.x.maddukuri@oracle.com>
Date:   Wed Apr 3 11:25:53 2013 +0530

    WL#6733 Migrate [1;31mperf[mschema MTR test suite to run with innodb storage engine

[33mcommit 6106ce004def06d8280b82112b1cc1b69c2b957e[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Apr 2 15:27:34 2013 +0200

    ndbapi
    
     - remove su[1;31mperf[mluous DBUG printouts

[33mcommit 51fc47fc791a7ef13abd26e352f6c85cd840254b[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Wed Mar 20 08:38:27 2013 -0700

    Change the contract for [1;31mperf[mtest
    Make the begin and createBatch calls async with a callback
      this allows other implementations (e.g. sql) to [1;31mperf[morm some setup

[33mcommit 31826708b1b929b5192d5f599c748fd41759f9bf[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Wed Mar 20 13:43:54 2013 +0530

    Bug #16078466:EXPLAIN CRASH IN FIELD_ITERATOR_TABLE_REF::
                  GET_OR_CREATE_COLUMN_REF
    
    Analysis:
    ---------
    Server crashes on a debug build when:
    a) Update and delete operation is [1;31mperf[mormed using an
    invalid column as the join clause within a stored
    procedure or function and
    b) When the stored procedure or function is invoked a
    second time.
    
    In case of SP/SF, the lex object associated with the
    instructions are cached when the SP/SF is first invoked.
    The flag 'is_join_columns_complete' which is part of the
    lex object indicates that the resolution of the join
    columns is complete. This flag is set when the SP/SF is
    first invoked though the resolution is not complete
    since the column in the joining clause does not exist.
    
    Since the lex object is cached, when the SP/SF is invoked
    the second time, it is assumed that the resolution is
    complete and the server crashes while trying to access
    the join column during debug assert.
    
    Note: On a release build, it gives an appropriate error.
    
    Fix:
    ---
    Mark the lex object as broken when there is an error which
    forces the lex object to be re-created during the subsequent
    invocation of the SP/SF.

[33mcommit 794beca3d9b396475ebfa3bf6be39635df05e759[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Mar 19 00:40:06 2013 +0000

    Bug #16316828   WL#5092 'REDUCED BINLOG SIZE FOR RBR' CAUSES NDB CONFLICT RESOLUTION FAILURES
    
    Root of problem was that HTON_NO_BINLOG_ROW_FORMAT was ignored when
    writing RB Binlog events, causing binlog events to be missing relevant
    info affecting the conflict detection testcases.
    
    Fix to Binlogging code has been pushed to mysql-5.6 + mysql-trunk, cherry
    picked back into mysql-5.6-cluster-7.3.
    
    This patch modifies the conflict detection code to be more strict about
    having the necessary columns to [1;31mperf[morm conflict detection.
    
    The existing conflict detection testcases are modified to check the
    behaviour is as expected.
    
    A new testcase is added to check the original problem with
    binlog-row-image=minimal.

[33mcommit 945e26015322be76405bd4c4b67cb061d262debd[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Mar 18 15:55:23 2013 +0100

    Followup fix for Bug#16491986 AVOID EXTRA READTUPLE() WHEN UPDATE IS EXECUTED AS DELETE+REINSERT
    
    That fix introduced a bug where all PK columns was not always read
    by the readTuple() prior to the delete. Thus random values was
    used to produce the key of the tuple to be deleted - Sometimes
    causing the tuple expecting to conflict with our update
    to be deleted instead of the correct tuple.
    
    This fix ensures that 'm_pk_bitmap_p' is always a part
    of the read_set whenever delete+reinsert us used
    to [1;31mperf[morm the update.

[33mcommit 78b968f080071057a20f11e31f21220415634179[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Mar 14 19:00:59 2013 -0700

    Change stats API for better [1;31mperf[mormance
    Avoid accessing Arguments array
    Avoid polymorphic function calls

[33mcommit fff2546f770120bc5c7c553c394ad44db72e1362[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Mar 14 17:04:20 2013 -0700

    [1;31mperf[mtest: jscrund reads connection properties from jscrund.config

[33mcommit cde46f571dfa27c9580e907bc1f67e883099ec0d[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Mar 13 14:41:40 2013 +0100

    Bug#16484617 COMPUTEHASH() REQUIRES 64-BIT ALIGNED MEMORY
    
     - reverse the logic so that mallcoc is [1;31mperf[mormed before the buffer is aligned

[33mcommit 146eae07ac6e9b98811a0107003a2529907bf08a[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Mar 13 14:20:48 2013 +0100

    Bug#16475372: TEST FOR 13641256 IN I_MAIN.QUERY_CACHE IS UNSTABLE ON PB2
    
    This patch rewrites the test case for Bug#13641256 to
    use DBUG_EXECUTE_IF() rather than rely on [1;31mperf[mormance schema.
    
    The patch also fixes a debug only problem noticeable when
    running query cache tests with the --debug MTR option.

[33mcommit 61a465368982e3efd24478d5b45504dac6c2a1aa[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Mar 11 12:53:14 2013 +0100

    Fix for Bug#16020945 HA_NDBCLUSTER ADAPTION TO 5.6 RBWR
    
    RBWR (Read Before Write Removal) previously relied on
    the optimizer calculating the 'read_set' for which rows
    requiring to be read as part of an update or delete
    operation. The handler::table_flags() returned
    HA_PRIMARY_KEY_REQUIRED_FOR_DELETE which caused the
    optimizer to add primary key columns to the read_set
    for update and delete operations.
    
    The problem with this approach is that the optimizer
    has insufficient information about how the handler
    will [1;31mperf[morm the read & update/delete operation at
    this stage. A conservative strategy was thus used which
    in several cases added the primary key column even when
    not required. This later prevented RBWR to 'remove' the
    read where this would have been possible.
    
    This fix removes returning HA_PRIMARY_KEY_REQUIRED_FOR_DELETE
    from ha_ndbcluster::table_flags(). Thus the RBWR optimizer
    will only see the read_set required by the runtime to evaluate
    any conditions and the new updated values. Any additional
    columns required by ha_ndbcluster in order to update or delete
    the rows and BLOB columns are now added to read_set inside
    the handler after RBWR has been decided.

[33mcommit 65c147bf5a98ca8b311c161651050e5c66795ac3[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Mar 1 13:24:46 2013 +1100

    WL#6044 - Spin before calling the system mutex "enter". This is to test if it
    makes any difference in [1;31mperf[mromance. Last set of results indicate that the
    Sys mutexes on Linux 1K OLTP RW Sysbench tests show regression compared to
    mysql-trunk.

[33mcommit 6e5022ecf5303ff6f8aa73705e6b27ef9a337983[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Feb 28 14:52:21 2013 +0100

    ndbcluster
    
     - remove su[1;31mperf[mlous semicolon at end of line

[33mcommit 5477f00935d10a2a12adc6f2c59d5b0baa369d11[m
Merge: 1e6af3993f3 d2bb5f41115
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Feb 26 14:29:19 2013 +0530

    - WL#6469: Optimizing CREATE/DROP [1;31mperf[mormance for temporary tables
      goal: optimize temp-table ddl [1;31mperf[mormance.
      how: temp-table lifetime is limited to connection/server lifetime
      and so lot of actions like redo logging (needed for recovery),
      writing metadata to SYSTEM tables (needed for re-loading table
      on re-start) can be avoided.
      Post Optimization create/drop [1;31mperf[mormance of innodb-temp-table is
      comparable with memory-temp-table.
    
      Review: rb#1544
      Approved by: Sunny + Jimmy + Michael

[33mcommit 45aa6b1cdea29c37778931c751247a470f3fa9d4[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Feb 19 11:12:16 2013 +0100

    Bug#13944392 PERFORMANCE OVERHEAD IN MY_HASH_SORT_BIN
    
    This fix is a [1;31mperf[mormance improvement.
    
    To sort strings according to character set and collation rules,
    the character set code implements dedicated hash functions.
    
    The implementation of these hash functions was too complex,
    causing un necessary memory access with pointer indirections,
    causing CPU overhead when computing a string hash.
    
    This cause hash functions in general to appear in various
    [1;31mperf[mormance benchmarks, for example with oprofile,
    or when benchmarking the [1;31mperf[mormance schema.
    
    The [1;31mperf[mormance schema is affected in particular for:
    - the table instrumentation (a LF_HASH by table schema + table name is
      used),
    - the file io instrumentation (a LF_HASH by file name is used).
    
    Any other code using a hash on a string is affected also.
    
    The root cause of the problem is the code itself.
    
    Typically, code looks like:
    
    hash_function( ulong *nr /* INPUT and OUTPUT parameter */)
    {
      /* loop in the string */
      for (...)
      {
        *nr = complex expression ( *nr );
      }
    }
    
    The problem is that there is no need to constantly read from and write to
    the location of the nr pointer inside the loop, which causes a lot of memory
    indirections.
    
    The code has been changed to look like:
    
    hash_function( ulong *nr /* INPUT and OUTPUT parameter */)
    {
      ulong tmp;
    
      tmp = *nr; /* read INPUT parameter */
    
      /* loop in the string */
      for (...)
      {
        /* [1;31mperf[morm computation locally */
        tmp = complex expression ( tmp );
      }
    
      *nr = tmp; /* write OUTPUT parameter */
    }
    
    This change improves both clarity, and efficiency.
    
    Every character set hash function is affected,
    as all functions used the same coding pattern.

[33mcommit 4370c9caa51b2a0c85aebd01d3edf056a7bc1eea[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Sat Feb 16 15:15:33 2013 -0800

    This patch implements pooling of Ndbs in NdbConnectionPool.
    Pooling is controlled by connection properties ndb_session_pool_min
    and ndb_session_pool_max.
    With this change, ab results for [1;31mperf[mtest/webserver2 (one session per request)
    come into line with [1;31mperf[mtest/webserver (one session for all) results.

[33mcommit 1c290b295852fe5c1e8b5f42b6db7e6ffa54f6e9[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Feb 15 12:07:23 2013 +1100

    WL#6044 - Remove mutex_list_print_info(), we don't keep a global list anymore.
    If required, some policy can implement it. Adding and removing from the global
    mutex list has a cost. Also, the [1;31mperf[mormance schema should be used for such
    things now.

[33mcommit 38b6607916a1a269496b258fa6c110453ec6ef46[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Feb 13 11:34:36 2013 +0100

    ndb
     - remove unused functions and member variables from Parser class
     - hardcode the breakOnInvalidArg since Parser was always created with
      that argument set to "true"
     - remove su[1;31mperf[mluous class ParseInputStream whos functionality is no longer used
     - this should fix compilation errors for compilers refusing access to private
      member variables in member instances

[33mcommit 6983522a4e98c336ed0393429c9049bb1e0cb34b[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Feb 8 17:40:59 2013 +1100

    Bug#1624555 - PATCH FOR BUG#14729365 CAUSE RO PERFORMANCE REGRESSION FOR 8 CORE HOST(S)
    
    Use a system specific random indexer in the fuzzy counter class. This helps in
    reducing the cache coherency impact when multiple threads try and update
    the same counter.
    
    Note: This patch uses THD::thread_id if a "fast" random indexer is not available
    for that OS. However, for srv_stats.n_rows_read this "fast" part is moot because
    a static thread specific identifier should be faster than a function call. Given
    that the [1;31mperf[mormance tests were done with the OS specific indexer I've decided
    to leave that as is for now. We should revisit this.
    
    Approved by Jimmy Yang, rb#1950

[33mcommit a15d3848cb620d5fb49f07d953737f8771d644a3[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Feb 5 12:13:56 2013 +0200

    Implement "WL#6752 Online rename index - InnoDB part" (rb:1831)
    
    Extend InnoDB ALTER capabilities with a RENAME INDEX.
    
    The rename consists of 3 phases:
    
    1. Renaming in InnoDB data dictionary (SYS_INDEXES)
    2. Renaming in InnoDB data dictionary cache (the dict_table_t/dict_index_t
       objects)
    3. Renaming in InnoDB persistent stats storage
    
    If the ALTER TABLE has gone via the 'rebuild' path (new clustered index has
    been built), then 1. and 2. are noops since the new table is created with
    the correct new names of the indexes. Then we only handle the rename in
    persistent stats storage via the newly added function alter_stats_rebuild(),
    called from ha_innobase::commit_inplace_alter_table().
    
    If the ALTER TABLE has gone via the 'no-rebuild' path, then we [1;31mperf[morm the
    steps as follows:
    1. Call the newly added function rename_indexes_in_data_dictionary() from
       commit_try_norebuild(), called from
       ha_innobase::commit_inplace_alter_table()
    2. Call the newly added function rename_indexes_in_cache() from
       commit_cache_norebuild(), called from
       ha_innobase::commit_inplace_alter_table().
    3. Call the newly added function alter_stats_norebuild() from
       ha_innobase::commit_inplace_alter_table().

[33mcommit 807ee642e371136982eea9b4edfea1d53af14f16[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Fri Jan 25 17:40:10 2013 -0800

    Work in [1;31mperf[mtest; bug fix in MySQLConnection so that webserver test will run.

[33mcommit 0d601533629055e6c74531644a40ea32f559842e[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Jan 24 12:58:30 2013 -0800

    [1;31mperf[mtest: add find test & webserver test.

[33mcommit 560e4c374e05a6fa463d4840506e109cbf2d56dc[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Jan 23 22:05:59 2013 -0800

    This commit adds the simple insert benchmark script in [1;31mperf[mtest/insert.js
    plus support for float and double columns.

[33mcommit b979c7706f37cc92678b5b7b14d2f58570cbbd6a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Jan 16 17:38:07 2013 +0100

    Bug#16175473 TUNING, USE STATIC CALLS FOR THE PERFORMANCE SCHEMA INSTRUMENTATION
    
    This change is a [1;31mperf[mormance tuning improvement.
    
    Before this fix, a line of code instrumented as:
      PSI_MUTEX_CALL(start_mutex_wait)(...);
    would always be compiled using a indirect call, using a function pointer:
      PSI_server->start_mutex_wait(...);
    
    This is expected when building code for dynamic plugins,
    but inefficient when building code inside the server itself.
    
    With this fix, the same line of code is now compiled as either
      PSI_server->start_mutex_wait(...);
    or
      pfs_start_mutex_wait_v1(...);
    depending of whether a dynamic or static call is needed.
    
    The net result is that instrumentation calls for the server code itself
    can now be compiled staticly.
    
    The server code has been adjusted to use static calls for the instrumentation
    interface, which affected the server initialization code.
    As part of this change, helper functions are defined to wrap access to thread
    local storage, so that detecting invalid TLS usage is easier.

[33mcommit f5a8fba01afb954fa38bfa0de1ce34df0a74c422[m
Author: viswanatham gudipati <viswanatham.gudipati@oracle.com>
Date:   Fri Dec 21 11:10:51 2012 +0530

    Migration:
    Parts : modified the result file of partition_exch_qa_2.test
    [1;31mperf[mschema: copied the new result files from wl tree: mysql-trunk-wl5656

[33mcommit 9085f1c59bdeda50f7d92f139969fbdc7b8a9dc8[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Dec 18 14:04:12 2012 +0100

    Add MCP for problem with HA_HAS_OWN_BINLOGGING flag semantics
    which has changed after patch which fixed [1;31mperf[mormance_schema + gtid

[33mcommit 9b314b51b645baa6f5cc7257b9af4c758a0eca0d[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Dec 13 16:56:33 2012 -0800

    Add query to mysql-js connector
    Most of the work is done in Query.js
    The user view is via a query object obtained from a Session,
      given a table name, constructor, or instantiated domain object
    Query uses a builder pattern to construct a where clause
    Query is then executed to give an array of results
    
    Add a test suite integraltypes to test the query functionality
    
    This is an incomplete implementation. It supports query where the
      primary key or unique key are compared equal, allowing a primary
      or unique key lookup to be [1;31mperf[mormed.
    
    Change the spi signature of key builder functions to take key values
      instead of an object with keys in it. This change is to allow query
      to use the same spi to query (and delete).

[33mcommit 499242ab0214e8fce069119edd4eaaf23ce4b55a[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Mon Dec 3 13:06:36 2012 +0530

    BUG#14059662 - DDL ON EVENT SCHEDULER MEMORY LEAK WITH
                   SKIP-GRANT-TABLES ENABLED
    
    BACKGROUND:
    When MySQL server is started with --skip-grant-tables option
    Event Scheduler is disabled,i.e the event scheduler is rendered
    nonoperational.
    At low level, Event_queue and Event_scheduler object are not
    instantiated.
    When an event DDL like Create Event or Alter Event is [1;31mperf[mormed
    during this time, it is added in mysql.event table and appropriate
    operations either create or update are done respectively.
    Then memory is allocated for the event queue element to be
    created, But it is not put in the event queue since event queue
    object has not been instantiated in the beginning itself.
    
    While doing server shutdown, when it cleans up scheduler's resources
    The memory for the Event Queue Element which is created is not
    deallocated, destructor for it is not called and it hogs the memory
    without deleting the element. Valgrind reported memory leak
    because of this issue.
    
    FIX:
    As a fix for this bug, A Check has been added in Create and
    Update event code. If the Event Scheduler is in DISABLED state,
    Memory for the event queue element is not allocated.
    We are allowed to create the event even when the scheduler is
    DISABLED.
    Later When the server is restarted again without skip-grant-tables,
    it picks up the event from mysql.event table and starts executing the
    event normally.

[33mcommit d755488a9a083b39c7a7224186649f80e9db3073[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Nov 29 17:57:42 2012 +0100

    Fixed miscellaneous doxygen formatting errors.
    Improved [1;31mperf[mormance schema doxygen comments.

[33mcommit 41330c89b423b0158636096ce25c6f78946fd903[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Nov 26 17:07:04 2012 +0200

    Bug#15863023 SUPPORT IN-PLACE EXTENSION OF VARCHAR COLUMN (WL#6554)
    
    The WL#5534 ALTER TABLE API introduces the flag
    ALTER_COLUMN_EQUAL_PACK_LENGTH for extending a VARCHAR column so that the
    character set and collation remains the same.
    
    This flag is NOT set when the column is being extended from up to than 255
    bytes to at least 256 bytes.
    This is [1;31mperf[mect for InnoDB, because when the maximum length is less than 256
    bytes, the current length will be stored in one byte. When the 256-byte boundary
    is not being crossed, we can extend the VARCHAR in-place, by just updating
    the data dictionary.
    
    rb:1547 approved by Jimmy Yang, Dmitry Lenev

[33mcommit f19b8329be6cddc32878e5a6f8518d1ca0f024e2[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Nov 22 13:18:17 2012 +0100

    Fix for Bug#15907515 RECEIVER THREAD COULD BLOCK/BUSY WAIT WHILE HOLDING RECEIVER MUTEX
    
    Note: This fix require the fix for bug 15907122 as 'baseline'.
    
    This fix removes the waiting for more job buffers to become available
    inside [1;31mperf[mormReceive() (or actually: mt_checkDoJob() called from it).
    
    Instead [1;31mperf[mormReceive() will now return with a 'full' status
    to the receive thread, which will unlock the receive mutex,
    and start a conditional wait for more job buffers to become available.
    
    Furthermore this fix also removes the check_job_buffer() *before*
    [1;31mperf[mormReceive() such that we will now doReceive() of any pending
    TCP data into our local receiveBuffers even if the job buffers are full.

[33mcommit d51c8cd4920788826637983ed1b83a2922b8e788[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Nov 15 12:26:04 2012 +0530

    - removed univ_unlikely as it was added for [1;31mperf[mormance improvement but it didn't achieved needed goals

[33mcommit d106b4b6d48932905c52ae234ab19571ff0a5288[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Wed Nov 14 11:55:46 2012 +0530

    BUG#12395512: NULL BYTE IN GROUP_CONCAT()'S SEPARATOR CLAUSE
                  BREAKS MYSQLDUMP
    
    Analysis:
    ---------
    When the view definition contains special character
    in the SEPARATOR clause of the GROUP_CONCAT aggregate
    function, the 'mysqldump' utility creates invalid
    view definition.
    
    Hence the reload of such a dump file results in syntax
    error and the view is not re-created.
    
    While creating the view definition, character escaping is
    not [1;31mperf[mormed on the string specified in the SEPARATOR
    clause. This results in the creation of the invalid view
    definition.
    
    Fix:
    ---
    Performed character escaping on the string specified in the
    SEPARATOR clause of the GROUP_CONCAT aggregate function
    while creating the view definition.
    
    Other issues fixed by this patch:
    --------------------------------
    a) The string literal for SEPARATOR clause contains incorrect
    value when the charset defined by 'character_set_client' and
    'character_set_connection' are different.
    b) The string literal for SEPARATOR clause contains incorrect
    value in I_S.VIEWS table when the 'character_set_client' is
    different from system charset(UTF8).

[33mcommit a61c2175e25c11a43fdfbc2b541127fd1530b961[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Nov 7 13:07:36 2012 +0100

    Traditional record of [1;31mperf[mschema.pfs_upgrade*

[33mcommit 6f76166f17c1c5e6dc233215d03fb93bd33b9b85[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Nov 7 20:18:53 2012 +1100

    WL#6047 - Increment the global rows read counter when prebuilt is destroyed.
    This is to reduce cache-coherency ([1;31mperf[mormance) issues around the counter.

[33mcommit 83fabe6c30461bf86fa5bd80c76ba2e1d65de629[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Fri Nov 2 22:41:40 2012 +0400

    WL#6561 "Deprecate and remove support for .sym files (custom symlink
    implementation)".
    
    Starting from Vista/Server 2008 a native symlinking mechanism is
    supported by Windows (through MKLINK command). Since starting from
    5.6 we don't plan to support Windows XP/Server 2003 this makes MySQL
    Server implementation of symbolic links for Windows (based on custom
    .sym files) redundant. Therefore it is a good idea first to deprecate
    and then completely remove code implementing custom symbolic links.
    
    Note that having this custom implementation around is a bad idea not
    only from code complexity view point, but it also creates [1;31mperf[mormance
    problems in some scenarios and is cause behind some bugs.
    
    This patch is the second step in this WL which completely removes this
    custom implementation. To do this it:
    * Removes USE_SYMDIR macro and all code withing #ifdef USE_SYMDIR.
    * Replaces usage of my_disable_symlinks with my_enable_symlinks
      to facilitate the next step.
    * Removes my_use_symdir global variable. In places where it is used
      to check if DATA/INDEX DIRECTORY clause should be supported we use
      my_enable_symlinks instead. Uses my_enable_symlinks to store value
      of --symbolic-links start-up option.
    
    After this change --symbolic-links option doesn't have any effect on
    Windows. But it still controls behavior for DATA/INDEX DIRECTORY
    clauses for MyISAM and Archive tables on Unix platforms.

[33mcommit eb4d333417a9eea6308b2de18ba5ac15a6661e4e[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Wed Oct 31 12:34:31 2012 +0100

    Worklog #6073: Remove INSERT DELAYED: re-recorded [1;31mperf[mschema on 64 bit.

[33mcommit e91d6fea9a11932fed3c35118005aa1f3a5d7f0f[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Tue Oct 30 10:31:31 2012 +0100

    Bug#11754168 - PARTS OF INDEX_MERGE_INNODB.TEST ARE DISABLED
                   DUE TO EXPLAIN DIFFS
    
    When a huge amount of rows are updated, InnoDB's #records
    estimates go crazy for a while because indexes are full of
    delete-marked rows. InnoDB does not understand that
    delete-marked rows are deleted so they are counted. To
    improve testing, the CREATE TABLE - INSERT sequence is
    modified to not update rows. The result is that a bunch of
    tests now use index_merge where table scan used to be
    [1;31mperf[mormed.
    
    There are no differences to the tests other than the way the
    tables are created and populated.

[33mcommit ab018ab7ac92d9ec65fd7fa8693ded6f318239d4[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Oct 29 08:39:07 2012 +0100

    Traditional recrod of [1;31mperf[mschema.digest* .result file

[33mcommit 65ef3042f1b85f1c1fd854715fa817f2f61f3dee[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Thu Oct 18 10:38:01 2012 +0200

    Bug #14730856:  UNION PARSING ERROR
    
    MySQL for some reason rejected a [1;31mperf[mectly standard SQL syntax for
    query blocks within parentheses. (A query block is a 'select' expression
    without 'union'.) Simplest example of rejected syntax is (SELECT 1)
    UNION SELECT 2, while we always allowed SELECT 1 UNION (SELECT 2).
    The syntax is even allowed by bison, the error was thrown in the
    semantic action code.
    
    The fix is obvious - remove the semantic action code throwing
    errors. The parser rule was copy-pasted for derived tables, i.e. query
    expressions ('select' with or without 'union'.) in the from clause, so
    the fix is [1;31mperf[mormed on both copies.
    
    Fixing this bug also fixes bug no 14731729 WRONG PARSER ERROR WITH
    DOUBLE PARENTHESES. The parser error stopped the parser before it
    reached a point where it was able to give an informative message.
    
    We also remove the redundant st_select_lex_node::set_braces().

[33mcommit 2a2b69229ef83b7fec473fe834e623d0d2477468[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Thu Oct 11 16:50:54 2012 +0200

    Worklog #6073: Remove INSERT DELAYED: Third patch, [1;31mperf[mormance_schema tables.
    
    This is where we remove all knowledge of insert delayed from
    [1;31mperf[mormance_schema.
    
    We remove five columns from
    [1;31mperf[mormance_schema.table_lock_waits_summary_by_table:
    COUNT_WRITE_DELAYED BIGINT, SUM_TIMER_WRITE_DELAYED BIGINT,
    MIN_TIMER_WRITE_DELAYED BIGINT, AVG_TIMER_WRITE_DELAYED BIGINT and
    MAX_TIMER_WRITE_DELAYED BIGINT. Removing them means we have to edit
    several places:
    
    - scripts/mysql_system_tables.sql, containing the DDL
    
    - storage/[1;31mperf[mschema/table_helper.h contained the backing store for
       the columns in a PFS_stat_row struct, which was removed.
    
    - table_tlws_by_table.cc has an array of columns which has to stay in
       sync with the DDL. Fortunately there are consistency checks. One
       such consistency check had a dangling documentation which was
       deleted (table.cc). There was also a function for populating the
       columns which had to be changed since it used column indexes rather
       than logical names.
    
    We remove one possible value for the 'operation' column in the tables
    events_waits_current, events_waits_history and
    events_waits_history_long. The removed value itself is found in
    enum_operation_type as OPERATION_TYPE_TL_WRITE_DELAYED. It is mapped
    onto a char* in operation_names_map[], removal of which requires
    careful orchestration as the values are mapped via indexes only.
    
    We remove the lock type PFS_TL_WRITE_DELAYED from enum
    PFS_TL_LOCK_TYPE. The server's lock TL_WRITE_DELAYED was mapped onto
    it in lock_flags_to_lock_type(uint), we remove this mapping.

[33mcommit 1db0920821d42d2071ac142e6ab389c7fdce05b0[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Thu Oct 11 16:18:20 2012 +0200

    Worklog #6073: Remove INSERT DELAYED: Second patch, mutexes and instruments.
    
    Here we remove the insert delayed mutexes: LOCK_delayed_insert,
    LOCK_delayed_status and LOCK_delayed_create. This in fact causes most
    of the delayed insert code to be removed: delayed_get_table(),
    write_delayed(), end_delayed_insert(), handle_delayed_insert(),
    unlink_blobs(), open_and_lock_for_insert_delayed(), Delayed_insert,
    find_handler(), kill_delayed_threads() and Delayed_prelocking_strategy
    are all gone.
    
    We also remove the dedicated [1;31mperf[mormance_schema instruments:
    
    - Four PSI_mutex_key instances: key_delayed_insert_mutex,
      key_LOCK_delayed_create, key_LOCK_delayed_insert and
      key_LOCK_delayed_status. Corresponding PSI_mutex_info instances are
      also deleted.
    
    - One PSI_thread_key: key_thread_delayed_insert and its corresponding
      PSI_thread_info.
    
    - The user could also see information about insert delayed in the
      [1;31mperf[mormance_schema.setup_instruments table, named
      Delayed_insert::cond and Delayed_insert::cond_client. They were
      backed by two PSI_cond_key's named key_delayed_cond and
      key_delayed_insert_cond_client.
    
    - The instrumented stage "Creating delayed handler" is removed.
    
    References in MTR to these aspects of [1;31mperf[mormance_schema were also
    removed, even though they don't all explicitly fail.

[33mcommit d7c6ec8cbdcec0c9cb191a05be72f52cb567b453[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Thu Oct 11 16:16:25 2012 +0200

    Worklog #6073: Remove INSERT DELAYED: First patch.
    
    This patch causes the DELAYED keyword to be stripped off in the
    parser, thereby making the feature unavailable. All tests of the
    INSERT DELAYED operation have been deleted from MTR, many times entire
    files are deleted. Also, all references to such files are removed,
    such as in disabled-gtid-on.list and disabled-per-push.list.
    
    Nothing is done to the deprecated system variables pertaining to
    INSERT DELAYED, i.e. manipulating them will still give a deprecation
    warning.
    
    The mutexes are not removed either and hence most [1;31mperf[mormance schema
    tests are intact.

[33mcommit 10a3f257410d381515ef22bb025dbe995b20e3a4[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Oct 4 13:06:02 2012 +0530

    - Sub WL#6469:
      - added code to turn-off logging for create/drop of temp-table.
      this will help us gain [1;31mperf[mormance during creation and drop of
      temp-table. this is done by blocking write to SYS_XXXX tables.

[33mcommit 034c40f4baeff1a1d2dd637f6a1f2c2720a4d26f[m
Author: prabakaran thirumalai <prabakaran.thirumalai@oracle.com>
Date:   Mon Oct 1 10:35:24 2012 +0530

    Bug#11751904 : 'SHOW GLOBAL STATUS' CAUSES LOCK CONTENTION ON
    LOCK_THREAD_COUNT
    
    Analysis:
    Lock duration for LOCK_thread_count is high when adding thread
    level status to global status variable in function
    calc_sum_of_all_status(). It [1;31mperf[morms global_thread_list traversal
    in order to calculate the sum. Because of this, new connections
    are blocked till the traversal is completed.
    
    In this scenario, major bottleneck is insertion(accepting new
    incoming connections) during traversal and not removal as it can
    wait till the traversal is completed.
    
    Fix:
    Take copy of THDs from global_thread_list and [1;31mperf[morm calculation
    after releasing LOCK_thread_count. During traversal(on copied
    THDs), removal from global_thread_list is blocked using another
    mutex LOCK_thread_remove such that THD copied are valid during
    computation(otherwise remove destroys THD).

[33mcommit 89d9bab9b40f86da9a5f09c5b0a9fa7c2620c254[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Fri Sep 28 15:02:16 2012 +0200

    ndb - (refactor) removing cached part of hash value for element in hash index - BROKEN!
    
    remove use of storing some hash bits with element
    
    Note, this patch degrades [1;31mperf[mormance significantly
    and should not be pushed alone but together with
    later patch in patch set, reintroducing storing bits
    from hash.
    
    The stored hash bits was used on expand and on lookup
    to quickly discard non matching elements in same bucket.
    
    Now the hash is recalculated if needed or element matching
    always compares element and key by values.

[33mcommit a5aa72f027f313c49220418a5aeafab9f857118f[m
Merge: c5d175cc6d5 1b585e1993a
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Tue Sep 25 16:10:47 2012 +0530

    BUG#13864642: DROP/CREATE USER BEHAVING ODDLY
    
    BACKGROUND:
    In certain situations DROP USER fails to remove all privileges
    belonging to user being dropped from in-memory structures.
    Current workaround is to do DROP USER twice in scenario below
    OR doing FLUSH PRIVILEGES after doing DROP USER.
    
    ANALYSIS:
    In MySQL, When we grant some stored routines privileges to a
    user they are stored in their respective hash.
    When doing DROP USER all the stored routine privilege entries
    associated with that user has to be deleted from its respective
    hash.
    The root cause for this bug is some entries from the hash
    are not getting deleted.
    The problem is that code that deletes entries from the hash tries
    to do so while iterating over it, without taking enough measures
    to address the fact that such deletion can reshuffle elements in
    the hash. If the user/administrator creates the same user again
    he is thrown an  error 'Error 1396 ER_CANNOT_USER' from MySQL.
    This prompts the user to either do FLUSH PRIVILEGES or do DROP USER
    again. This behaviour is not desirable as it is a workaround and
    does not solves the problem mentioned above.
    
    FIX:
    This bug is fixed by introducing a dynamic array to store the
    pointersto all stored routine privilege objects that either have
    to be deleted or updated. This is done in 3 steps.
    Step 1: Fetching the element from the hash and checking whether
    it is to be deleted or updated.
    Step 2: Storing the pointer to that privilege object in dynamic array.
    Step 3: Traversing the dynamic array to [1;31mperf[morm the appropriate action
    either delete or update.
    This is a much cleaner way to delete or update the privilege entries
    associated with some user and solves the problem mentioned above.
    Also the code has been refactored a bit by introducing an enum
    instead of hard coded numbers used for respective dynamic arrays
    and hashes in handle_grant_struct() function.

[33mcommit 223dfa3b7b86bd3dba033d727738fa53ed1e7395[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Tue Sep 18 15:01:40 2012 +0530

    Bug#14338686: MYSQL IS GENERATING DIFFERENT AND SLOWER
                  (IN NEWER VERSIONS) EXECUTION PLAN
    PROBLEM:
    While checking for an index to sort for the order by clause
    in this query
    "SELECT datestamp FROM contractStatusHistory WHERE
    contract_id = contracts.id ORDER BY datestamp asc limit 1;"
    
    we do not calculate the number of rows to be examined correctly.
    As a result we choose index 'idx_contractStatusHistory_datestamp'
    defined on the 'datestamp' field, rather than choosing index
    'contract_id'. And hence the lower [1;31mperf[mormance.
    
    ANALYSIS:
    While checking if an index is present to give the records in
    sorted order(datestamp), we consider the selectivity of the
    'ref_key'(contract_id here) using 'table->quick_condition_rows'.
    'ref_key' here can be an index from 'REF_ACCESS' or from 'RANGE'.
    
    As this is a 'REF_ACCESS', 'table->quick_condition_rows' is not
    set to the actual value which is 2. Instead is set to the number
    of tuples present in the table indicating that every row that
    is selected would be satisfying the condition present in the query.
    
    Hence, the selectivity becomes 1 even when we choose the index
    on the order by column instead of the join_condition.
    
    But, in reality as only 2 rows satisy the condition, we need to
    examine half of the entire data set to get one tuple when we
    choose index on the order by column.
    Had we chosen the 'REF_ACCESS' we would have examined only 2 tuples.
    Hence the delay in executing the query specified.
    
    FIX:
    While calculating the selectivity of the ref_key:
    For REF_ACCESS consider quick_rows[ref_key] if range
    optimizer has an estimate for this key. Else consider
    'rec_per_key' statistic.
    For RANGE ACCESS consider 'table->quick_condition_rows'.

[33mcommit c15723bae3b0278439fcd291465617f8deec1316[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Sep 7 15:36:38 2012 -0500

    Bug#14551372 - INNODB: UNIV_SYNC_DEBUG SYNC ARRAY IS NOT OPTIMIZED
    
    As part of the investigation into Bug 14520559 (LARGE BLOB INSERTS
    ARE SLOW AND TROUBLESOME), I found that the sync array that is used
    under UNIV_SYNC_DEBUG will store the same level-latch pair over and
    over when the latch is recursively locked.  The maximum size of this
    sync array was hit when running innodb_bug13450566.test with a 4k
    page size because seven different latches were being locked
    recursively over and over.  This filled up the available 10,000 slots
    in the array and an assert was hit.  When the assert happened, there
    were actually only 14 unique latches being tracked for that thread.
    
    This patch add a count field to each slot in this sync array. The
    count is > 1 for recursive locking.  This reduces the actual array
    size tremendously and can help [1;31mperf[mormance when using UNIV_SYNC_DEBUG
    as well as prevent this 'array full' assert when a lot of recursive
    locking is happening.  This will require an extra search through the
    array to find the previous latch-level slot, but that is offset when
    it searches only tens of slots instead of thousands of slots.
    
    Approved by Sunny in http://bur03.no.oracle.com/rb/r/1247/

[33mcommit 4d9bcc647e500ae1ba6f7db64506fd8a6c8051f8[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Aug 28 14:26:16 2012 +0300

    Bug#14543059 DUPLICATE STRUCT TYPE DEFINITIONS IN INNODB SOURCE CODE
    
    Declare all structs with a type name that ends in _t instead of _struct.
    
    Replace all instances of
    
    typedef struct foo_struct foo_t;
    
    with forward declarations like this:
    
    struct foo_struct;
    
    Remove unnecessary forward declarations of structs.
    
    The bulk conversion was [1;31mperf[mormed by a Perl snippet:
    perl -i -pe 's/_struct/_t/g;
    s/^typedef struct\s+([a-zA-Z][a-zA-Z0-9_]*_t)\s+\1\;$/struct $1\;/g;' \
    storage/innobase/*/*[ch]
    
    After this, some manual adjustments were made, and the code was
    grepped for 'typedef struct' and 'typedef enum'.
    
    The patch itself was grepped for '^-.*"' to ensure that no strings
    were changed (there is trx_lock_structs in the INFORMATION_SCHEMA
    tables).
    
    rb:1246 approved by Sunny Bains

[33mcommit f14d2bfcaa224f26b9e7313d13cabd1da2256358[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Aug 10 10:52:51 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    Protect access to the per index stats container with a mutex. We choose to
    use a mutex instead of rwlock because the [1;31mperf[mormance critical part is in
    page_zip_compress() and page_zip_decompress() where we need write access.
    So a rwlock would not increase the concurrency of the [1;31mperf[mormance critical
    code but it is slower to acquire than a mutex.
    
    Hide "map<index_id_t, page_zip_stat_t>" behind a typedef.

[33mcommit 7d50a198486681ded647c278ca6cc819766e69ec[m
Author: Praveenkumar Hulakund <praveenkumar.hulakund@oracle.com>
Date:   Tue Jul 17 23:51:43 2012 +0530

    Bug#13819275 - Huge [1;31mperf[mormance bottleneck because views
    definitions aren't cached
    
    Analysis:
    ----------
    Whenever view is opened, the view definition is read from
    the .frm file of view. And then this definition is used
    by the File_parser.
    Reading definition from .frm file every time is one of the
    main reason of [1;31mperf[mormance bottleneck.
    
    Fix:
    ----------
    When view is opened for the first time, TABLE_SHARE is
    created for it (But no open count is mentioned for it).
    The TABLE_SHARE for view is available in the table share
    cache till there is enough space. Till its in the table
    share cache, same TABLE_SHARE of view is used.
    
    To avoid reading view definition from the file every time,
    added new member in TABLE_SHARE to cache File_parser object
    for the view's .FRM file. So that next access to the same
    TABLE_SHARE need not have to read the definition from .FRM,
    but instead can reuse definition stored in the File_parser
    object.
    
    To verify the improvement in the [1;31mperf[mormance, I used the
    same stored procedure provided in this report. Executed
    p1(100000) with view "v1" (say A)  and p1(100000) with
    table "t1" (say B), with and without this fix.
    
    Delay in execution of A compared to B
                      without fix= ~3.99 secs
                      with fix= ~1.66 secs
    
    Improvement in [1;31mperf[mormance = ~58.39%
    
    We also tried to cache results of a later stage of view
    processing, i.e. cache results of File_parser::parse() call,
    but straight forward implementation of this approach didn't
    bring any noticiable improvement.
    praveen@Praveen-MySQL:~/Documents/Bugs/Bug13819275$ cd inter
    bash: cd: inter: No such file or directory

[33mcommit 78f10f9208b56129abf103300d4c66bd76ac9839[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Jul 12 15:58:36 2012 +0530

    BUG#14221840: KILLING HELP COMMAND: ASSERTION FAILED: ! IS_SET(),
                  FILE SQL_ERROR.CC, LINE 465
    
    Analysis:
    --------
    
    MySqld crashes at times on a debug build, when the HELP command
    is killed using KILL QUERY statement.
    
    The crash is observed under a timing window where:
    
    connection1: Run the HELP statement.
    
    connection2: KILL the HELP query running on connection1. This sets
                 the connection1 THD 'killed' status as 'KILL_QUERY'.
    
    connection1: Since the THD is killed, a kill message is sent while
                 attempting to read the help records. This sets the
                 status of the diagnostic area to DA_ERROR.
    
    At the end of execution of the HELP statement, the status in the
    diagnostic area is set to DA_EOF only if no errors are reported
    (DA_EMPTY). On a debug build, MySqld asserts if the status of the
    diagnostic area is not DA_EMPTY. Since the status of diagnostic
    area is set to DA_ERROR in the case mentioned above, MySqld crashes.
    [On a release build, an appropriate error message ER_QUERY_INTERRUPTED:
    Query execution interrupted is reported].
    
    Fix:
    ---
    
    Check the THD 'killed' status prior to setting the EOF status in the
    diagnostic area. If the THD is killed, skip setting the EOF status
    and [1;31mperf[morm the query cleanup.

[33mcommit 9855c45f42009748bfd18fad2067f8cf4d5b0d24[m
Merge: 67152be331a 158d0a091e8
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 21 14:42:50 2012 +0200

    ndb - increase of hashmap size and supporting 1024 partitions
    
    This increased the hashmap size from 240 to 3840 to ensure that we don't get unbalanced
    access to the partitions in a large cluster setup. Influenced [1;31mperf[mormance at 16 nodes and
    beyond by a fairly significant factor.
    
    Part of Mikael Ronstroms "Patches used in benchmark tree with Intel"

[33mcommit 78dde5478e70f3a51778ec5e3b633829dedc8567[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Jun 18 17:18:06 2012 +0300

    part of WL#6347 InnoDB: Index level compression stats
    
    Introduce a config variable to enable/disable
    INFORMATION_SCHEMA.innodb_cmp_per_index (and
    INFORMATION_SCHEMA.innodb_cmp_per_index_reset). Collecting stats for those
    tables may have negative impact on [1;31mperf[mormance so we switch them OFF by
    default.

[33mcommit d5e5170cc8dd6d084e73feb0be51ac2ef0b18a67[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu May 31 16:03:03 2012 +0200

    Revert MCP patches from [1;31mperf[mschema/disabled.def, hopefully tests are stable
    now

[33mcommit 7ae7283bc49c88926e49eb376cf9bb72179acf13[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Mar 28 16:17:32 2012 +0100

    Bug#54854 / Bug#11762277 CAN'T FIND GOOD POSITION FOR REPLICATION BREAK BETWEEN DDL STATEMENTS
    
    Problem
    -------
    
    Replication channel cutover uses the last applied epoch on the slave to determine where
    to begin replication from on the new Master.
    
    The last applied epoch is obtained from the Slave's mysql.ndb_apply_status table.
    The new Master's mysql.ndb_binlog_index table is queried to find the first epoch *after* the
    Slave's last applied epoch, the binlog file and offset of this epoch are used to start
    replication from the new master.
    
    Issues :
     1) There may be *no* epoch after the last applied epoch
        If log-empty-epochs=0, and even in normal cases, where the slave is up-to-date and
        no new epoch has been finalised on the Master.
     2) As epochs are not continuously numbered, there may be a gap between the last applied
        epoch and the next.  It is not possible to determine what the next epoch number will
        be.  If the new Master is missing some epochs, the current cutover mechanism will
        silently skip over them and jump to the first available epoch
     3) Where there is DDL between the last applied epoch and the next epoch, the cutover mechanism
        will skip the DDL.  If the DDL has been applied then this is ok, if it has not, then it
        is silently skipped.
    
    Solution
    --------
    
    This series implements a more precise mechanism for [1;31mperf[morming replication channel cutover.
    This allows us to ensure that a replication channel cutover begins replication precisely
    after the end of the last committed epoch on the Slave.
    
    This involves :
      - Modifications to the MySQL Server Binlog code to record the next position in the Binlog
        after the COMMIT event at the end of an epoch transaction
      - Modifications to the mysql.ndb_binlog_index table schema to include next_file and next_position
        columns
      - Modifications to the Ndb Binlog injector to set the next_file and next_position columns in
        the mysql.ndb_binlog_index table.
    
    The existing replication channel cutover mechanism continues to work, with the same limitations
    as before.
    A new channel cutover mechanism is defined, making use of the new columns.
    
    Old channel cutover mechanism, given a last applied epoch from the slave.
    
      SELECT File, position from mysql.ndb_binlog_index where epoch > <last_applied_epoch>;
    
    New channel cutover mechanism :
      SELECT next_file, next_position from mysql.ndb_binlog_index where epoch = <last_applied_epoch>;
    
    Note that i) This statement uses the last applied epoch directly - there is no dependency on there
    being a following epoch, ii) There is no risk of silently 'jumping' over an epoch gap during
    replication channel cutover, iii) Any DDL after the last applied epoch will be (re)applied.
    
    Reapplying inter-epoch DDL can result in errors on the Slave.  This is considered better than the
    old channel cutover mechanism which can result in silently skipping DDL.  A separate patch series
    implements 'DDL ignore existance errors' handling.
    
    This series includes a testcase which verifies the correctness of the next_position under
    multithreaded Binlog inserts etc.

[33mcommit c597ecad56b59811b2da3f0ebfc0c021f118e029[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Fri Mar 16 09:40:34 2012 +0100

    ndb - autotest - [1;31mperf[morm early filtering away of illegal index combinations (due to extra bit columns added to T6)

[33mcommit bc41686ab4ed678236dd6bc5e022944a6b0c2b40[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Mar 15 12:50:41 2012 +0100

    Disable 2 more unstable [1;31mperf[mschem tests

[33mcommit c48dedf4f951d91cb70a5f7c64c06dd6d74b196c[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Mar 14 20:21:12 2012 -0700

    Revsion 3634 (Nov 7, 2011; 7.2.2) introduced a major [1;31mperf[mormance regression
    in memcached.  The symptom of this was a decline in throughput as memslap
    connections increase from 4 to 6.  The cause seems to be a change in which
    SimpleRead PK reads are executed NoCommit rather than Commit; this patch
    fixes it.

[33mcommit 6a7df9a359ac02cddde4b2078e9bb2c2178b7409[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Mar 14 12:44:51 2012 +0100

    Disable 2 more [1;31mperf[mschema testcases which are known to be unstable

[33mcommit 27add3a8598d2581b07239c93f189bc054a8494c[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Mar 14 09:25:15 2012 +0100

    Disable 3 [1;31mperf[mschema testcases which are known to be unstable

[33mcommit 0726d11a27b796fbc1bbec6cbb820277b981689e[m
Author: magnus.blaudd@oracle.com <>
Date:   Mon Mar 12 13:18:49 2012 +0100

    [1;31mperf[mschema
     - record new .result file since digest changes when parser changes

[33mcommit 49d06d68c7abb25897a28f733be245612bdc4e16[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Wed Feb 1 18:36:16 2012 +0100

    ndb - fix overlapping memcpy causing error with new glibc (that [1;31mperf[morms memcpy backwards)

[33mcommit 8f038bf8d2a662555fde72fa83d0c3a8e2273947[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Feb 1 11:23:10 2012 +0100

    ndb - change ndb_statistics.inc to [1;31mperf[morm insert into myisam and the alter to ndb, to avoid timeout issue on really slow machines in PB2

[33mcommit 9fb512e8a7fccdcd4fc7ab68f066af9423356ab9[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Jan 20 18:22:20 2012 -0800

    Add test for [1;31mperf[mormance testing of inserts

[33mcommit 17634cbde4ecdc0e0ea47b7d7be651b8a6f6e9ba[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Fri Jan 20 08:41:48 2012 +0100

    ndb - fix typo found with new assert (on platforms wo/ epoll). Typo had been there forever, and caused unneeded syscalls (doing nothing). Probably not noticable [1;31mperf[mormance loss...

[33mcommit e0bf7db54d8df75731d8eb8ca3e5954cbaa24679[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Mon Jan 16 09:25:04 2012 +0100

    ndb - TransporterReceiveData state for pollReceive/[1;31mperf[mormReceive, moved into own class to enable multi receive threads

[33mcommit cafa935b0c2f07ed22ac966f9ed0fd5840adb90b[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Thu Dec 15 21:57:23 2011 +0100

    ndb - fix problem in rpl_insert_ignore, in that transactions [1;31mperf[mormed are not imediatly visisble in binlog, but save_master_pos will make sure that any transactions committed from this mysqld has been recorded in binlog

[33mcommit d4ab985ffcabe6e8b444c2c31eea883baafd5e5c[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Dec 14 14:46:17 2011 -0500

    ndb
     - remove su[1;31mperf[mluous check of ndb_binlog_tables_inited from if statement, since iut's been
       set just above it would be weird(and this is not thread safe anyway)

[33mcommit 45edde82dcbeb3170649c1829793205dfb6a9797[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Nov 9 19:34:25 2011 +0000

    Avoid problems with support for binlog-row-image
    
    The binlog-row-image option allows row events to be generated either
    as FULL, MINIMAL, or NOBLOB.
    
    FULL causes all columns, not just those modified, to be logged in
    each event AND it causes update and delete events to have full
    BEFORE images.
    
    MINIMAL logs only modified columns, and update and delete events do
    not need before images if the table has a primary key.
    
    NOBLOB is similar to FULL, except unmodified BLOB columns are not
    logged.
    
    The default value is FULL.
    
    The FULL option is implemented by setting all columns in the write
    set when [1;31mperf[morming an Insert, Update or Delete operation.
    
    The MINIMAL option avoids this, and records only the modified columns
    in the Binlog, and skips before images if not required.
    
    Ndb already supports similar functionality via the
    --ndb-log-updated-only server option and per-table binlog flags.  This
    is implemented in trigger logic in the data nodes, and affects non-MySQLD
    sourced modifications as well.
    
    The part of the FULL implementation which causes operation's write
    sets to be fully set breaks the ndb implementation of ndb-log-updated-only
    as all columns are always written to.
    
    For this reason, the HTON_NO_BINLOG_ROW_OPT flag is set by Ndb to
    avoid the FULL option having this effect.
    
    The FULL, MINIMAL or NOBLOB option still affects how the Ndb Binlog
    Injector records events, in conjuntion with --ndb-log-updated-only.
    The default value, FULL, behaves as it did prior to mysql-trunk-cluster.
    
    When MINIMAL is passed, Delete and Update events do not need to have
    before images, saving space in the Binlog.
    
    The --binlog-row-image option can be set per-server or per-session.
    The Ndb engine uses the per-server setting and ignores per-session
    settings.
    
    This patch results in the following testcases passing :
    ndb_binlog_variants, ndb_rpl_conflict_max,
    ndb_rpl_conflict_old, ndb_rpl_2innodb, ndb_rpl_basic,
    ndb_rpl_idempotent, ndb_rpl_logging

[33mcommit 49a6bec8ee517f57defdf0946650873c1fac16a7[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 9 14:10:53 2011 +0100

    Fix for bug#13355055: CLUSTER INTERNALS FAILS TO TERMINATE BATCH AT MAX 'BATCHBYTESIZE'
    
      We have observed SCANREQs with a surprisingly small 'BatchSize' argument as part
    of debugging and tuning SPJ. Where we expected 'BatchSize=64' (Default) we
    have observed values around ~10. This directly translated into suboptimal [1;31mperf[mormance.
    
    When debugging this, we found the root cause in NdbRecord::calculate_batch_size(), which
    returns the batchsize (#rows) and  arguments for the SCANREQ signal.
    It contained the following questionable logic:
    
     1) Calculate the worst case record length based on that *all columns* are selected
        from a table, and all varchar() columns being filled to their *max limit*.
    
     2) If that record length is such that 'batchsize * recLength' > ,
        reduce batchsize such that batchbytesize would never be exceeded.
    
    This effectively put ::calculate_batch_size() in control of the batchbytesize
    logic. The negative impact if that logic was that 'batchsize' could be severely
    restricted in cases where we could have delivered a lot more rows in that batch.
    
    However, there are logic in LQH+TUP which are intended to keep the delivered batches
    withing the batchsize limits. This is a much better place to control this as
    LQH & TUP knows the exact size of the TRANSID_AI payload being delivered, taking
    actual varchar length and only the selected columns into acount.
    
    Debugging that logic, it turned out that it contained bugs in how the produced
    batchsize was counted: Actually a mixup between whether the 'length' was in
    specified in number of bytes or Uint32. - So the above questionable
    ::calculate_batch_size() logic seems to have been invented only to
    circumvent this bug......
    
    Fixing that bug allowed us to now leave the entire batch control to
    the LQH block.
    
    - ::calculate_batch_size could then be significantly simplified.
    - The specified BatchSize & BatchByteSize arguments could be used as
      specified directly as args in SCANREQ signals.
    - Will likely give better [1;31mperf[mormance (larger effective batches) when
      scanning a table with 'max record length > BatchByteSize / BatchSize'
      (~500 bytes with default config)
    
    
    Fix number of bytes/Uint32 mixup in how m_curr_batch_size_bytes is counted
    ******
    Fix number of bytes/Uint32 mixup in how the SPJ adaptive parallelism count m_totalBytes
    ******
    Simplify ::calculate_batch_size() as LQH now correctly will stay within the specified batch_size rows/bytes limits
    ******
    Remove NdbRecord::m_max_transid_ai_bytes which is now obsolete
    ******
    Remove unused args from NdbRecord::calculate_batch_size()
    ******
    Fix SPJs adaptive paralellism logic to also handle batchsize termination due to BatchByteSize being exhausted

[33mcommit 71e24844e0362ea39cc1d7f674d30967f94efaca[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Oct 18 15:54:36 2011 -0700

    Reimplement query optimization to improve [1;31mperf[mormance:
    Analyze where clause when it is set on the query.
    Rank possible indexes by the number of terms.
    Highest rank for unique indexes (including PRIMARY)
    At query execution time, decide if a unique index is usable (no null parameters)
    If not, choose the ordered index based on whether the first comparison is usable.

[33mcommit 4264f12bfd110512aa2a183f37648c9c024414b2[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Sun Oct 16 20:36:16 2011 +0200

    ndb - test/conf for daily-[1;31mperf[m...all wishful thinking for now

[33mcommit 30241c9c0366839f7c733977fdac8c9775d06c21[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Sep 29 11:32:00 2011 +0200

    ndb
     - remove su[1;31mperf[mluous space

[33mcommit ac29876ab245a5f5906665b9655295946bc6644f[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Sep 28 12:32:50 2011 +0200

    MCP
     - remove unaccounted for line, exact same assignment is [1;31mperf[mormed
       on the following line so this is safe
For keyword optim:
[33mcommit 4168199ad284736c0304b7aa99fb654d85f18bec[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Sep 11 13:30:22 2015 +0200

    Bug#21613615 GCOLS: ASSERTION FAILED: !TABLE || (!TABLE->READ_SET || BITMAP_IS_SET
    
    Taking this table:
    CREATE TABLE v (
    a INT,
    c INT,
    b CHAR(2) GENERATED ALWAYS AS (a IN (1)) VIRTUAL,
    KEY(c,b(1)));
    and query:
    SELECT (SELECT MAX(c) FROM v);
    
    opt_sum_query() [1;31moptim[mizes the query, decides to read the MAX(c) using
    the index over (c,b(1)). It first does this:
              // Set bits for user-defined parts of key
              table->mark_columns_used_by_index_no_reset(ref.key, table->read_set);
    which puts "c" and "b" in read_set (it's actually tmp_set).
    mark_columns_used_by_index_no_reset() adds "b" but doesn't add
    dependencies of "b", which looks correct as otherwise it would force a
    read of the data row and make any index on "b" be useless.
    Then in the index read, update_generated_read_fields() is called,
    which calls index_contains_this_virtual_gcol() which says that "b" is
    *not* in the index; it's because the index is on a prefix of "b",
    (key_part->field isn't equal to vfield, it's a forged field based on
    "b" but with a field_length of 1). It is also correct, "b" isn't in
    the index, only a prefix is.
    So update_generated_read_fields() wants to calculate "b", thus needs
    "a", which isn't in read_set, and it fails.
    
    Fix: put in this temporary read_set only columns which are necessary
    for this index read. They are the columns involved in
    'WHERE col=const' and the aggregated one.

[33mcommit 4f3b53f5f7bda603298326bb12b0fdc7c65afc02[m
Author: Oystein Grovlen <oystein.grovlen@oracle.com>
Date:   Fri Sep 11 13:27:40 2015 +0200

    Bug#20701585  SEMI-JOIN DUPLICATE ELIMINATION GIVES LOWER COST THAN NO DUPLICATE ELIMINATION
    
    After the introduction of filtering estimation for conditions
    (WL#6635), the expected number of rows selected from a table may be
    between 0 and 1.  Some of our algorithms are not prepared for this.
    One observed effect is that semi-join DuplicateWeedout plans include
    tables in the duplicate-eliminating range that are not relevant to the
    semi-join (so-called nt tables).  This happens because extending the
    range with a table where the estimated number of rows are less than
    one make the cost go down.
    
    This patch solves this issue by ensuring that filtering estimates
    always gives at least one row.  I think there are several reasons to
    do this instead of trying to adapt our code to work with values below
    1:
    
     - WL#6635 already put such a lower limit on the filter effect of a
       single condition.  Hence, values lower than 1 may only happen where
       there are multiple conditions on a table.  However, I think the
       same reasoning could be made in case of multiple conditions.  That
       is, filtering estimates should assume at least 1 row regardless of
       number of conditions.
    
     - The assumption when computing filtering effect of multiple
       conditions is that conditions are non-correlated.  In actual
       databases, this is normally not the case.  Hence, the filtering
       estimates of multiple conditions will often be lower than what is
       actual the case.
    
     - I think it is more important to get an [1;31moptim[mized query plan for the
       cases where the result is non-empty than for the case when no
       records are returned.  With very low numbers, one risk that the
       join order has very little impact on the total cost of the query.
       This means that for the cases when there is actual a match, the
       join order may not be [1;31moptim[mal.
    
     - It is not trivial to adjust the [1;31moptim[mizer code to cope with values
       lower than 1.
    
    Note: The existing limitation on filter, "filter*fanout >= 0.05", in
    calculate_condition_filter() is kept.  I am not convinced this
    limitation makes sense, but dropping it had negative effects on DBT-3
    query 8.
    
    This patch does not contain a new test case.  The "How-to-repeat" part
    of the bug report consists of an existing test case in subquery.inc,
    and the subquery_sj_dupsweed.test shows that an nt table is not longer
    included in the duplicate-producing range.  (See first hunk of
    subquery_sj_dupsweedout.result).
    
    File comments:
    (In addition comes result files where all changes are changes in
    filter and row estimates.  That is, there is no plan changes.)
    
    sql/sql_planner.cc
      Make sure calculate_condition_filter() never returns a number that
      corresponds to less than 1 row in the given table.
    
    mysql-test/include/explain_json.inc
      Added some rows to a table of a test case to get a plan which use
      materialization.  According to comment in the test, this is what was
      intended when test case was added.
    
    mysql-test/include/subquery_sj.inc
      Added some rows to a table of a test case to get same plan as before.
    
    mysql-test/r/derived.result
      Join order has changed, but this should not be relevant for the
      objective of the test.
    
    mysql-test/r/explain_json_all.result
      More records in table leads to different cost estimates.
      Test case now use materialization which was the original intention
    
    mysql-test/r/explain_json_none.result
      More records in table leads to different cost estimates, but no plan change.
    
    mysql-test/r/greedy_[1;31moptim[mizer.result
      Query plans show different join order.  New join order is expected
      since tables with lowest filter estimate comes first.  (Previously,
      filter estimates were equal for the tables in question.)
    
    mysql-test/r/greedy_search.result
      One test case has a slight increase in number of partial query plans
      evaluated.
    
    mysql-test/r/partition_explicit_prune.result
      Changes in number of handler accesses.  All changed numbers are
      lower than previously.  (Neither the values before nor after match
      what comments in test says is exepected.)
    
    mysql-test/r/subquery_sj_all.result
    mysql-test/r/subquery_sj_all_bka.result
    mysql-test/r/subquery_sj_all_bkaunique.result
      @@ -3406,12 +3406,13 @@
        Changes from DupsWeedout => MatScan.  According to comment this
        test case is a regression test case for MatScan.
      @@ -4067,12 +4068,12 @@
        According to comment in test, this test case is to reproduce an
        issue with MatLookup.  MatLookup plan is still used in
        subquery_sj_mat.test.  I doubt that it is possible to get MatLookup by
        default as long as condition filtering is used.
      @@ -4383,9 +4388,9 @@
        Added some rows to one table to get same plan as before.
      @@ -6215,11 +6220,11 @@
        New plan is the same plan as when the test case was pushed
      @@ -9675,11 +9680,11 @@
        Different join order from before, but neither old nor new
        results have plans that are similar to original plan.
    
    mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Changes are similar to subquery_sj_all.result except:
      @@ -4298,11 +4299,12 @@
      @@ -4434,11 +4440,12 @@
      @@ -4542,11 +4549,12 @@
      @@ -4632,11 +4640,12 @@
      @@ -4740,11 +4749,12 @@
        DupsWeedout => MatScan. According to test comment, MatLookup is
        what must be avoided
      @@ -6224,10 +6234,10 @@
        No longer DupsWeedout that covers both semi-join nests, but
        different algorithms for the two. This is a result of DupsWeedout
        no longer including more tables than necessary.  (Note that the
        tests where BNL is allowed has FirstMatch for both.)
    
    mysql-test/r/subquery_sj_dupsweed.result
    mysql-test/r/subquery_sj_dupsweed_bka.result
    mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Differences from subquery_sj_all.result:
      @@ -3378,12 +3378,12 @@
        This is the result change proves the point of this patch.  New
        plan does no longer include nt table in the temporary table range.
      @@ -6190,10 +6194,10 @@
        Different join order, but subquery_sj_all.test covers the original plan
    
    mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes that differ from subquery_sj_dupsweed_bka.result are changes
      to use same plan as subquery_sj_dupsweed_bka.result
    
    mysql-test/r/subquery_sj_firstmatch.result
    mysql-test/r/subquery_sj_firstmatch_bka.result
    mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
    mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      @@ -7066,11 +7070,11 @@
      @@ -7112,32 +7116,6 @@
        New join order, but but subquery_sj_all.test covers the old plan
      @@ -6173,8 +6177,8 @@
        New plan is came as query plan when test case was pushed
      @@ -11311,17 +11324,46 @@
        Table with increased filter estimate comes later in join order
    
    mysql-test/r/subquery_sj_loosescan.result
    mysql-test/r/subquery_sj_loosescan_bka.result
    mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
    mysql-test/r/subquery_sj_loosescan_bkaunique.result
    mysql-test/r/subquery_sj_mat.result
    mysql-test/r/subquery_sj_mat_bka.result
    mysql-test/r/subquery_sj_mat_bka_nixbnl.result
    mysql-test/r/subquery_sj_mat_bkaunique.result
    mysql-test/r/subquery_sj_mat_nosj.result
      Results differences is only in tests that use DupsWeedout because
      LooseScan or Materialization is not applicable.  Plans may be
      different from subquery_sj_dupsweedout.test since these tests have
      turned off DuplicateWeedout and that will effect plan pruning.
    
    mysql-test/r/view.result
    
      Tests get different join order due to different filter estimates for
      one table.  This should be OK since this does not seem to be a
      regression test case.

[33mcommit 4fcdb29c88f24239b2d563403a536054050b755e[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Sep 8 16:53:45 2015 +0200

    Bug#21574933 Extra rows with derived table in subquery + XOR
    
    This problem stems from the transformation of IN subqueries
    in Item_in_subselect::single_value_in_to_exists_transformer().
    
    The source of the regression is the fix for bug no. 14358878, which
    wrapped all selected columns from a derived table that was on the
    inner side of an outer join in Item_direct_view_ref objects.
    The purpose of the wrapping was to make the columns nullable and
    nullability depending on some table from the derived table.
    
    single_value_in_to_exists_transformer() calculated orig_item from one
    such column. Prior to the wrapping, nullability information was present
    in orig_item. But after the bugfix, real_item() would pass the
    Item_direct_view_ref object and return the underlying Item object,
    which is not necessarily nullable.
    
    real_item() was originally used so that we do not build a permanent
    subquery transformation on top of a runtime item. The wrapped
    Item_direct_view_ref objects are runtime objects, meaning that
    they cannot be used in a permanent transformation.
    The solution is a kludge: If the Item_direct_view_ref object is
    nullable, we set the item being pointed to from the
    Item_direct_view_ref as nullable as well. This means that we pick up
    correct nullability, but the item does not correctly reflect the
    nullability of the datum. However, we have not been able to identify
    a situation where this causes erroneous or non-[1;31moptim[mal execution.
    
    Further on, this means that NULL values from outer joined tables
    are included in the query result, meaning that the IN subquery
    can return NULL instead of FALSE in cases like this.

[33mcommit c4ce65ca22ee207be543bd07dcbb8cff30853199[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Sep 7 14:44:59 2015 +0200

    BUG#20483278 LAST_QUERY_COST SHOWS DIFFERENT VALUES FOR SHOW_COMPATIBLITY
    FOR ON VS OFF
    BUG#21788549 LAST_QUERY_PARTIAL_PLANS VALUES NOT SAME WITH
    SHOW_COMPATIBILITY_56
    
    Before this fix,
    
    With SHOW_COMPATIBILITY_56=ON,
    
    The statement
      SHOW STATUS like 'Last_query_cost';
    was executed with dedicated code in the information schema
    (sql/sql_show.cc), which is -- not -- implemented as a storage engine.
    As a result, this statement did not execute code in the [1;31moptim[mizer,
    which did not overwrite the session status variable 'Last_query_cost'
    for the current select statement.
    
    As a result, the value of 'Last_query_cost' reported was
    for the previous SELECT statement executed in the session.
    
    With SHOW_COMPATIBILITY_56=OFF,
      SHOW STATUS like 'Last_query_cost';
    is executed as a SELECT ... FROM performance_schema.session_status.
    This table is implemented with a storage engine,
    and the [1;31moptim[mizer code is used.
    By the time the value is returned, the value of 'Last_query_cost'
    reported is the value just overwritten by the [1;31moptim[mizer for the current
    statement, which is not the desired result.
    
    The fix is to separate clearly in the session attributes:
    - the value of 'Last_query_cost' for the current query,
    - the value of 'Last_query_cost' for the previous query,
    and only save the former in the later when query execution is complete.
    
    The same issue exists for Last_query_partial_plan,
    which is fixed the same way.

[33mcommit 95a156f6eb78bf59be0d05c7215cc977aa05d03e[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Feb 19 16:53:51 2015 +0100

    BUG#19579811: SET GTID_PURGED DOES NOT STOP WAIT_FOR_EXECUTED_GTID_SET
    
    Background:
    
    WAIT_FOR_EXECUTED_GTID_SET waits until a specified set of GTIDs is
    included in GTID_EXECUTED. SET GTID_PURGED adds GTIDs to
    GTID_EXECUTED. RESET MASTER clears GTID_EXECUTED.
    
    There were multiple issues:
    
     1. Problem:
    
        The change in GTID_EXECUTED implied by SET GTID_PURGED did
        not cause WAIT_FOR_EXECUTED_GTID_SET to stop waiting.
    
        Analysis:
    
        WAIT_FOR_EXECUTED_GTID_SET waits for a signal to be sent.
        But SET GTID_PURGED never sent the signal.
    
        Fix:
    
        Make GTID_PURGED send the signal.
    
        Changes:
        - sql/rpl_gtid_state.cc:Gtid_state::add_lost_gtids
        - sql/rpl_gtid_state.cc: removal of #ifdef HAVE_GTID_NEXT_LIST
        - sql/rpl_gtid.h: removal of #ifdef HAVE_GTID_NEXT_LIST
    
     2. Problem:
    
        There was a race condition where WAIT_FOR_EXECUTED_GTID_SET
        could miss the signal from a commit and go into an infinite
        wait even if GTID_EXECUTED contains all the waited-for GTIDs.
    
        Analysis:
    
        In the bug, WAIT_FOR_EXECUTED_GTID_SET took a lock while
        taking a copy of the global state. Then it released the lock,
        analyzed the copy of the global state, and decided whether it
        should wait.  But if the GTID to wait for was committed after
        the lock was released, WAIT_FOR_EXECUTED_GTID_SET would miss
        the signal and go to an infinite wait even if GTID_EXECUTED
        contains all the waited-for GTIDs.
    
        Fix:
    
        Refactor the code so that it holds the lock all the way from
        before it reads the global state until it goes to the wait.
    
        Changes:
    
        - sql/rpl_gtid_state.cc:Gtid_state::wait_for_gtid_set:
          Most of the changes in this function are to fix this bug.
    
        Note:
    
        When the bug existed, it was possible to create a test case
        for this by placing a debug sync point in the section where
        it does not hold the lock.  However, after the bug has been
        fixed this section does not exist, so there is no way to test
        it deterministically.  The bug would also cause the test to
        fail rarely, so a way to test this is to run the test case
        1000 times.
    
     3. Problem:
    
        The function would take global_sid_lock.wrlock every time it has
        to wait, and while holding it takes a copy of the entire
        gtid_executed (which implies allocating memory).  This is not very
        [1;31moptim[mal: it may process the entire set each time it waits, and it
        may wait once for each member of the set, so in the worst case it
        is O(N^2) where N is the size of the set.
    
        Fix:
    
        This is fixed by the same refactoring that fixes problem #2.  In
        particular, it does not re-process the entire Gtid_set for each
        committed transaction. It only removes all intervals of
        gtid_executed for the current sidno from the remainder of the
        wait-for-set.
    
        Changes:
        - sql/rpl_gtid_set.cc: Add function remove_intervals_for_sidno.
        - sql/rpl_gtid_state.cc: Use remove_intervals_for_sidno and remove
          only intervals for the current sidno. Remove intervals
          incrementally in the innermost while loop, rather than recompute
          the entire set each iteration.
    
     4. Problem:
    
        If the client that executes WAIT_FOR_EXECUTED_GTID_SET owns a
        GTID that is included in the set, then there is no chance for
        another thread to commit it, so it will wait forever.  In
        effect, it deadlocks with itself.
    
        Fix:
    
        Detect the situation and generate an error.
    
        Changes:
        - sql/share/errmsg-utf8.txt: new error code
          ER_CANT_WAIT_FOR_EXECUTED_GTID_SET_WHILE_OWNING_A_GTID
        - sql/item_func.cc: check the condition and generate the new error
    
     5. Various simplfications.
    
        - sql/item_func.cc:Item_wait_for_executed_gtid_set::val_int:
          - Pointless to set null_value when generating an error.
          - add DBUG_ENTER
          - Improve the prototype for Gtid_state::wait_for_gtid_set so
            that it takes a Gtid_set instead of a string, and also so that
            it requires global_sid_lock.
        - sql/rpl_gtid.h:Mutex_cond_array
          - combine wait functions into one and make it return bool
          - improve some comments
        - sql/rpl_gtid_set.cc:Gtid_set::remove_gno_intervals:
          - Optimize so that it returns early if this set becomes empty
    
    @mysql-test/extra/rpl_tests/rpl_wait_for_executed_gtid_set.inc
    - Move all wait_for_executed_gtid_set tests into
      mysql-test/suite/rpl/t/rpl_wait_for_executed_gtid_set.test
    
    @mysql-test/include/kill_wait_for_executed_gtid_set.inc
    @mysql-test/include/wait_for_wait_for_executed_gtid_set.inc
    - New auxiliary scripts.
    
    @mysql-test/include/rpl_init.inc
    - Document undocumented side effect.
    
    @mysql-test/suite/rpl/r/rpl_wait_for_executed_gtid_set.result
    - Update result file.
    
    @mysql-test/suite/rpl/t/rpl_wait_for_executed_gtid_set.test
    - Rewrote the test to improve coverage and cover all parts of this bug.
    
    @sql/item_func.cc
    - Add DBUG_ENTER
    - No point in setting null_value when generating an error.
    - Do the decoding from text to Gtid_set here rather than in Gtid_state.
    - Check for the new error
      ER_CANT_WAIT_FOR_EXECUTED_GTID_SET_WHILE_OWNING_A_GTID
    
    @sql/rpl_gtid.h
    - Simplify the Mutex_cond_array::wait functions in the following ways:
      - Make them one function since they share most code. This also allows
        calling the three-argument function with NULL as the last
        parameter, which simplifies the caller.
      - Make it return bool rather than 0/ETIME/ETIMEOUT, to make it more
        easy to use.
    - Make is_thd_killed private.
    - Add prototype for new Gtid_set::remove_intervals_for_sidno.
    - Add prototype for Gtid_state::wait_for_sidno.
    - Un-ifdef-out lock_sidnos/unlock_sidnos/broadcast_sidnos since we now
      need them.
    - Make wait_for_gtid_set return bool.
    
    @sql/rpl_gtid_mutex_cond_array.cc
    - Remove the now unused check_thd_killed.
    
    @sql/rpl_gtid_set.cc
    - Optimize Gtid_set::remove_gno_intervals, so that it returns early
      if the Interval list becomes empty.
    - Add Gtid_set::remove_intervals_for_sidno. This is just a wrapper
      around the already existing private member function
      Gtid_set::remove_gno_intervals.
    
    @sql/rpl_gtid_state.cc
    - Rewrite wait_for_gtid_set to fix problems 2 and 3. See code
      comment for details.
    - Factor out wait_for_sidno from wait_for_gtid.
    - Enable broadcast_sidnos/lock_sidnos/unlock_sidnos, which were ifdef'ed out.
    - Call broadcast_sidnos after updating the state, to fix issue #1.
    
    @sql/share/errmsg-utf8.txt
    - Add error message used to fix issue #4.

[33mcommit 5249f4f0790de54eb872d4d4af5874b72add41e9[m
Author: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
Date:   Fri Sep 4 17:12:02 2015 +0530

            Bug#21775221 ADD NUMACTL-DEVEL AS BUILDREQ TO RPM SPEC FILES
    
            Description:
    
            Set Numa Mempolicy for [1;31moptim[mum mysqld performance added
            libnuma.so as build prerequisite on Linux.
    
            To build with support for this feature numactl devel files are required.
    
            Fix:
    
             Added numactl-devel as build pre-requisite for OL/RHEL/Fedora/docker
             and libnuma-devel for SLES in the spec file.

[33mcommit fe46dd23b9f4ec4719bccd014b06c9d35ae6a5b9[m
Author: Sergey Glukhov <sergey.glukhov@oracle.com>
Date:   Fri Sep 4 08:19:04 2015 +0300

    Bug#21621313: ASSERTION FAILED: JOIN == 0 IN SELECT_LEX::OPTIMIZE() WITH MAX_EXECUTION_TIME
    
    Subquery fails during [1;31moptim[mization in make_join_plan() function.
    We exit from JOIN::[1;31moptim[mize with return value TRUE but no error code
    is set in this case. Subquery [1;31moptim[mization and execution is called from
    [1;31moptim[mize_cond() function. This function ignores the fact that thread is killed.
    After [1;31moptim[mize_cond() we check if error happens and since no error code is set
    we continue [1;31moptim[mization. In this case SELECT_LEX_UNIT::is_[1;31moptim[mized is not
    TRUE and we attempt to [1;31moptim[mize already processed SELECT_LEX in
    SELECT_LEX::[1;31moptim[mize which leads to assert failure.

[33mcommit 346519dc8007f71b45f7521d50174180ae7fcf38[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Sep 2 10:12:34 2015 +0200

    Bug#21697002 ASSERTION `KEYS >= 0.0' FAILED
    
    The assert was hit when calling Unique::get_use_cost() to calculate
    the cost of using Unique to process 0 keys. Unique::get_use_cost()
    uses an implementation of Stirling's approximation to calculate the
    number of compare operations this would take. The correct number of
    compare operations for comparing 0 keys should be 0 but due to using
    Stirling's approximation, the estimate became -0.12. This caused the
    assert in Cost_model_table::key_compare_cost() to hit.
    
    In this case, Unique is used by the index merge code to calculate the
    cost of index merge of two indexes. The index merge code gets the rows
    estimates from the storage engine by calling
    handler::records_in_range(). Most storage engines will never return 0
    as the rows estimate even when the table is empty since there is an
    assumption that the [1;31moptim[mizer could interpret this as the range scan
    would return 0 rows (ie. interpret it as an exact number, not an
    estimate). The new native partition engine in InnoDB can in some cases
    return 0 as the result from handler::records_in_range(). This is the
    cause for the index merge code to call Unique::get_use_cost() with 0
    in the key argument.
    
    The fix for this is to make Unique::get_use_cost() handle that it is
    called with 0 as the number of keys argument by adding a special case
    to the implementation of Stirling's approximation so that it does not
    return a negative number.

[33mcommit 227ba53c167ee2bfc4493ba18c7f0b9badaa56cc[m
Author: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
Date:   Thu Sep 3 17:52:13 2015 +0530

        Bug#21775221 ADD NUMACTL-DEVEL AS BUILDREQ TO RPM SPEC FILES
    
        Description:
    
        Set Numa Mempolicy for [1;31moptim[mum mysqld performance added
        libnuma.so as build prerequisite on Linux.
    
        To build with support for this feature numactl devel files are required.
    
        Fix:
    
         Added numactl-devel as build pre-requisite for OL/RHEL/Fedora/docker
         and libnuma-devel for SLES in the spec file.

[33mcommit a7571080513ed2107919ebb1f5633566d3605189[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Thu Sep 3 15:46:18 2015 +0530

    Bug#17769777 : EXCESSIVE MEMORY CONSUMPTION WITH MANY AND / BETWEEN ITEMS
    Bug#17413040 : USING MANY WHERE CONDITIONS MAKES RANGE SCAN DISABLED
    
    Problem: Currently range [1;31moptim[mizer checks for excessive memory consumption
    in a minimalistic way by checking on the MAX_SEL_ARGS created by clone().
    This leads to two problems,
    1. As there are other ways to create SEL_ARG objects and also other ways of
    memory consumption during range analysis, current check will not suffice and
    as a result range analysis can take excessive memory
    2. If a where condition is written in such a way that only clone() is invoked
    to create SEL_ARG objects, it becomes very restrictive as in the Bug#17413040.
    In this case, range [1;31moptim[mizer might consume memory, but the plan
    generated is much more efficient than the current plan.
    
    Solution:
    In case of Bug#17769777 range [1;31moptim[mizer requires a lot of memory and CPU
    resources because of the conditions present in the query. Currently there is
    no way to limit the range [1;31moptim[mizer memory usage in such cases. But given that
    range [1;31moptim[mizer uses its own memroot to allocate memory, we are putting a
    cap on this memroot's memory consumption.
    
    To keep a check on the memory consumption, a user configurable session variable
    named "range_[1;31moptim[mizer_max_mem_size" is introduced. A value of "0" means there is
    no limit on the memory consumption.
    
    A new member 'max_capacity' is introduced in mem_root structure to limit
    the memory consumption for a mem_root.
    A new member 'allocated_size' keeps track of the memory being allocated in mem_root
    Helper functions like
    is_mem_avaiable() - to check if the requested memory can be allocated,
    set_memroot_max_capacity() - Max capacity for this memroot,
    set_memroot_error_handling() - to report error when capacity is exceeded,
    are also added.
    
    Before allocation in memroot we check if memory is available. If yes proceed
    else report error is error_for_capacity_exceeded is set. Otherwise return NULL.
    
    All the range [1;31moptim[mizer's allocations are now made only in range_[1;31moptim[mizer's
    memroot. Earlier SEL_ARG and SEL_TREE allocations were not done consistently
    in memroot.
    
    Error Handling for Out of memory caused due to exceeding max_capacity:
    
    To handle such cases a new error code is added. For the memroot that has
    max_capacity set, error_for_capacity_exceeded needs to be set for alloc_root
    to report error when capacity is exceeded. Else it will return NULL.
    
    For range [1;31moptim[mizer, as the query should not be aborted when this
    error is thrown, instead a warning should be given to user about it, a
    Internal_error_handler is introduced. This handler reduces the error to a
    warning and is given out after the query execution is complete.

[33mcommit c9234a66cf16c9e465b01c244c31067aad4a117d[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 18:23:13 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Post-push cleanup.
    
    Test changes recorded in the following files:
    
    innodb.[1;31moptim[mizer_temporary_table
    innodb.innodb_mysql
    innodb.innodb_bug30423
    main.wl6711_heap_to_disk
    main.innodb_explain_json_non_select_all
    main.innodb_explain_json_non_select_none
    main.innodb_explain_non_select_all
    main.innodb_explain_non_select_none

[33mcommit c44f992c575398e96b69e3430744c914464bf83c[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 15:16:02 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Patch # 5 of 5.
    
    Various cleanup actions
    
    - resolve_const_item() needed error return due to calling val_xxx()
      functions and constructors.
    
    - Item_equal::compare_const() needed THD argument and error return due
      to calling constructors, set_cmp_func() and val_int().
    
    - Item_equal::add(), Item_equal::merge() and Item_equal::update_const() are
      calling Item_equal::compare_const() and needed the same.
    
    - update_const_equal_items() needed a THD argument and error return due to
      the above.
    
    - check_simple_equality() needed a THD argument since it calls
      Item_equal::merge() and ::add().
    
    - Return value after an unsuccessful call to ::set_cmp_func() was wrong.
    
    - Several more error returns were added after call to ::set_cmp_func().
    
    - Safety check (thd->is_error()) before returning from [1;31moptim[mize_cond().
    
    - Table_trigger_dispatcher::mark_fields() could cause errors, but did not
      signal it. This might cause us to call [1;31moptim[mize_cond() with an error
      condition in mysql_update().

[33mcommit 8101f1dc4d1b73960ab30bd61c8a9c5dcd05a0a6[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 14:53:17 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Patch # 4 of 5
    
    Currently, [1;31moptim[mize_cond() builds multiple equalities for all WHERE
    conditions and all join conditions. But there is one omission: If
    WHERE condition is NULL, then no multiple equalities are built for
    any join conditions.
    
    This patch fixes that problem by the following logic:
    - Invoke [1;31moptim[mize_cond() when we have a WHERE condition or an outer join
      (an outer join usually implies at least one join condition).
    - Inside [1;31moptim[mize_cond(), call build_equal_items() whenever there is a
      join list (ie this is not called for HAVING).
    - build_equal_items() then inspects the WHERE clause and the various
      join conditions, and analyzes any conditions that are not NULL.
    
    There are several test changes due to this patch. Most changes just
    reverse the order that operands are tested, this is due to the nature
    of multiple equality processing. The more interesting test changes are
    documented below.
    
    Test change in main.subquery_sj_all:
    
     select * from t1 left join t2 on (t2.A= t1.A And t2.A in (select pk from t10));
    
    Due to multiple equality analysis, a table with REF access is shifted up
    so that it can be evaluated before a table where table scan is applied.
    
     SELECT *
     FROM ot1 LEFT JOIN ot2 ON ot1.a=ot2.a AND
     (ot1.a, ot2.a) IN (SELECT it1.a, it2.a
    
    Due to multiple equality analysis, a join condition can be evaluated earlier
    than it used to be.
    
    Test change in main.derived:
    
      SELECT * FROM (SELECT v1.a
                     FROM v1 LEFT OUTER JOIN v2 ON v1.a = v2.b
                     WHERE v1.a = 3
                     GROUP BY v1.a) p, t q
      WHERE q.id BETWEEN 1 AND 10;
    
    Due to the multiple equality analysis, a "func" ref is changed to a "const",
    which is slightly simpler to evaluate.
    
    Test change in main.join_nested:
    
    Two queries have a WHERE clause moved from third to second table in
    join order.
    
    Test change in main.join_outer:
    
    Comparison against column is replaced with comparison against literal due
    to multiple equality analysis.

[33mcommit 869aeec53cb3420ab8cac3a77526ab5682141079[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 14:36:22 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Patch # 3 of 5.
    
    Clean up a minor issue in JOIN::[1;31moptim[mize(): Print "Zero limit"
    intead of "Impossible WHERE" when [1;31moptim[mizing a query with LIMIT 0.
    Also check LIMIT 0 before [1;31moptim[mizing conditions.

[33mcommit 6ce2a38e36fba5bdca5adf3bace685aaa4430c23[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 14:22:15 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Patch # 2 of 5.
    
    This patch refactors the interfaces for [1;31moptim[mize_cond() and
    related functions. All functions get a THD argument and a bool return
    value that indicates success or error. These functions are refactored:
    
    - [1;31moptim[mize_cond()
    - check_simple_equality()
    - check_row_equality()
    - check_equality()
    - build_equal_items()
    - build_equal_items_for_cond()
    - change_cond_ref_to_const()
    - propagate_cond_constants()
    
    In addition, these changes are done:
    
    - All errors are processed correctly
    - C casts are replaced with down_cast<>
    
    In addition, the logic of [1;31moptim[mize_cond() is slightly changed:
    
    - For HAVING clause, it is only called when HAVING != NULL.
    - For WHERE clause, it is only called when WHERE != NULL or
      there is a least one outer join (this is called after simplify_joins()
      so if we have join nests, we know at least one join condition exists).
    
    This is a preparation for a small [1;31moptim[mization, which will come in patch#4.
    
    Item_hex_string::clone_item() was implemented.

[33mcommit 57ee0d9a9d72ed465999bf7e43d9c83a9f000ce1[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 14:07:30 2015 +0200

    Bug#21067109: Assert 'join == __NULL' failed in ::[1;31moptim[mize()
    
    Patch # 1 of 5.
    
    This is an issue with missing propagation of error values.
    A const subquery is evaluated when removing const items in
    remove_eq_conds(). The subquery fails because it returns more than
    one row. This error code is poorly handled and causes the [1;31moptim[mizer
    to attempt to [1;31moptim[mize a subquery a second time, which causes
    this assert.
    
    This patch changes interfaces for eval_const_cond(), remove_eq_conds()
    and internal_remove_eq_conds(). The interfaces encourage better error
    propagation but requires slightly more code to be written.

[33mcommit c07d43aa988f179c95c258087936ded40cc37d24[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 11:19:14 2015 +0200

    Bug#21624851: Assertion failed: is_prepared && !is_[1;31moptim[mized()
    
    Followup patch, realizing that we do not need two interfaces for
    replacement of rollback place. Only replace_rollback_place_for_value()
    is needed, and we also see that the "value" is always the contents of
    the "place".
    
    - THD::replace_rollback_place_for_ref() is removed.
    
    - THD::replace_rollback_place_for_value() is renamed to
      replace_rollback_place(), since the specialization is no longer needed.
    
    - First argument is removed, value is instead taken from the contents
      of "new_place".

[33mcommit 487e71b01091554030ffb2db669ec24c0728257e[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Sep 2 11:18:10 2015 +0200

    Bug#21624851: Assertion failed: is_prepared && !is_[1;31moptim[mized()
    
    This problem occurs on execution of a prepared statement.
    The root cause is that a call to THD::replace_rollback_place_for_ref()
    in SELECT_LEX::simplify_joins() records an invalid rollback "place"
    for an item representing a predicate: An AND node is built from
    an old AND node and a join condition. A rollback place is recorded
    for one of the conditions of the old AND node. But when the rollback
    is carried out, the "place" is pointing to the old AND node, which is
    then replaced by an old Item_field pointer. What should happen is that
    when the predicate is moved to the new AND node, we should track the
    "place" inside the new AND node instead of the old one.
    And this is what THD::replace_rollback_place_for_value() does.
    
    The solution is thus to call THD::replace_rollback_place_for_ref() for
    all arguments of the AND node.
    
    It was also detected a wrong call to THD::change_item_tree() within
    check option processing, which could be replaced with simple
    assignment.
    
    Test cases for several duplicate bug reports have been added:
    21624851, 21626495, 21624331, 21626722, 21626177.

[33mcommit 9b9ead8dd22275a387ef20c98e7b4d5f6efa1858[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Mon Aug 31 16:26:44 2015 +0800

    Bug#21663612 AUTO INCREMENT COLUMN VALUES ARE NOT VISIBLE IN A TABLE CONTAINING SPATIAL INDEX
    
    Issue:
    When the primary key columns are the only target columns to return in a select
    statement, and the storage engine storing the table has the needed capability,
    MySQL [1;31moptim[mizer will do a full index scan even if the index is a spatial
    index. However, Innodb spatial index doesn't support full index scan, and it returns
    HA_ERR_KEY_NOT_FOUND in this case so no result was returned.
    
    Fix:
    Don't consider spatial indexes when choosing a target to do full index scan.

[33mcommit 3f63a9b2e03f8d41f95510b798a0f2c1aabe9ec6[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Wed Sep 2 08:50:23 2015 +0530

    Bug #19893908   : HANDLE_FATAL_SIGNAL (SIG=6) IN FILESORT | SQL/FILESORT.CC:415
    
    Problem:
    While doing filesort for the update query, [1;31moptim[mizer tries to access a
    variable having a stale pointer and exits.
    
    Solution:
    In table::init() call, reginfo->join_tab should be initialized to NULL. Else
    there is a chance that it will have a stale pointer stored as part of the
    previous query execution.
    
    Fixed in 5.7 as part of WL#6042 "split JOIN_TAB into [1;31moptim[mizer and executor part"
    
    The problem does not exist in 5.5 as the plans and the code path taken has
    changed in 5.6. The table->reginfo.join_tab gets set to NULL in
    JOIN_TAB::cleanup. But this is done only when JOIN_TAB is not NULL. While in
    5.5, this does not get set to NULL before JOIN_TAB::cleanup, in 5.6 it does
    happen. Hence the crash later. The call to
    subselect_single_select_engine::prepare() before or after JOIN::cleanup makes
    the difference.

[33mcommit e7c773a6492a78c16c8a2e8b2664336d4c1fce60[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Sun Aug 30 11:48:00 2015 +0530

    Bug #18698556: UPDATE ORDER BY DOES A FILESORT IF UPDATING IN THE INDEX
    
    Problem:
    Filesort is being done for an update query having order by,
    when the key being used for scanning is also updated.
    
    Analysis:
    Whenever a key that could be used for ordering is identified,
    [1;31moptim[mizer also checks if this key is getting updated.
    Like in the following query:
    update t1 set d = 72 where b = 2 order by c asc limit 1;
    
    the choice of using index for order by is a key having
    (b,c,d) as key_parts.
    As d is getting updated, [1;31moptim[mizer rejects the index.
    
    This rejection is expected if scanning and updation of
    the records are not done in two different passes. But the
    current [1;31moptim[mizer does take care of this.
    Whenever key used for scanning is getting updated,
    it first saves the pointers to the required data and
    then updates in the second pass. As a result, this limitation
    can be lifted.
    
    Solution:
    Do not check if used key is being modified when [1;31moptim[mizer
    chooses index of ordering. Order by always results in two
    passes for an update query, whether filesort is used or
    not used.

[33mcommit c4e3acb332b7494e2df994b2f92a3683b57be548[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Fri Aug 28 22:47:48 2015 +0530

    BUG#20367116: ALTER TABLE BREAKS ON DELETE CASCADE FOREIGN KEY
                  CONSTRAINT
    
    Analysis
    ========
    ALTER TABLE operation on a table with FOREIGN KEY CASCADE
    constraint while a concurrent DML operation is being
    performed on the table referenced may break the referential
    integrity.
    
    In the case of ALTER TABLE operation using COPY algorithm:
    a) The ALTER TABLE operation on the table with FOREIGN KEY
       CASCADE constraint, the data is copied from the original
       file to temp file and transaction is committed releasing
       InnoDB locks (also, as an [1;31moptim[mization, InnoDB commits
       transaction and releases locks every time ALTER TABLE
       copies 10000 rows).
    b) Concurrently if DELETE operation is executed on the
       referenced table, it will trigger delete of records
       on the table being altered. This will end up deleting
       records from the original file since the rename of
       the temp file has not yet completed and locks on the
       original file are not held.
    c) The ALTER TABLE operation continues, renaming the temp
       file to the original file leaving behind orphaned rows.
    
    In the case of ALTER TABLE operation using INPLACE algorithm:
    a) The DELETE operation on the referenced table acquires an
       LOCK_IS on the table being altered due to the CASCADE
       FOREIGN KEY constraint.
    b) The ALTER TABLE operation tries to acquire LOCK_X during
       the commit phase of INPLACE alter and is added to the
       waiting queue since LOCK_IS is acquired by DELETE.
    c) DELETE operation upon finding the record to delete, tries
       to acquire LOCK_IX on the table and is added to the wait
       queue. This results in a deadlock with ALTER operation
       being rolled back.
    Fix:
    ====
    a) INNODB has introduced a new handler API
       'get_cascade_foreign_key_table_list()' which returns a full
       closure of all tables ordered by the dependency on FOREIGN
       KEY CASCADE constraint.
    b) A function called 'lock_fk_dependent_tables()' is added
       which locks the list of tables ordered by the FOREIGN
       KEY CASCADE constraint for the table being altered.
       It is invoked in two places.
       1) Before the copy of data is invoked for the ALTER TABLE,
          COPY algorithm.
       2) Before ALTER TABLE, INPLACE commit.
    This avoids the orphaned rows and deadlock.
    
    Please note that this is a temporary workaround which is
    necessary until WL#6049 "Meta-data locking for FOREIGN KEY tables"
    is implemented
    
    NOTE: Some of the test cases have been removed, since the condition
    of concurrent DDL/DML operation on a parent table while a DDL
    operation is performed on the the child table having a CASCADE
    foreign key constraint is restricted with this patch

[33mcommit 189340df14f9b79bd7c106f5a6f655595167ac5e[m
Author: Sreeharsha Ramanavarapu <sreeharsha.ramanavarapu@oracle.com>
Date:   Fri Aug 28 08:18:53 2015 +0530

    Bug #19333852: RESULT DIFF IN QUERY HAVING DISTINCT WITH
                   GROUP BY
    
    Issue:
    -----
    This problem happens under the following conditions:
    1) Range [1;31moptim[mization is attempted on an index with a
       string column as its first part.
    2) The value used for comparision is fully truncated.
    
    This makes comparisions with index incorrect. The range
    [1;31moptim[mizer incorrectly concludes that there are no values
    in the index that satisfy the relevant comparision.
    
    SOLUTION:
    ---------
    We keep track of whether a string has been fully truncated
    during range [1;31moptim[mization. If yes, that string is not used
    as part of the range [1;31moptim[mization to avoid incorrect
    filtering.

[33mcommit c285e6583e7d612ba01a1853c5612dcbe052039a[m
Author: Sreeharsha Ramanavarapu <sreeharsha.ramanavarapu@oracle.com>
Date:   Sun Aug 23 08:32:58 2015 +0530

    Bug #20229614 : OR CONDITIONS ON MULTI-COLUMN INDEX MAY NOT
                    USE ALL INDEX COLUMNS TO FILTER ROWS
    
    ISSUE:
    ------
    While [1;31moptim[mizing a range scan for the OR-operator, key_or
    incorrectly assumes an out-of-memory situation. This in turn
    results in an incomplete condition being considered for the
    query plan and gives an incorrect row estimate. Hence the
    over-estimation of the number of qualifying rows.
    
    
    SOLUTION:
    ---------
    In key_or(), a key condition is cloned if it's already in
    use in some other condition. In this case NULL should be
    returned only if allocation fails.
    
    The check of key_count > 0 was originally added as part of
    Bug #4157. This was done to check the key_count after the
    swap of keys. Here changing the "||" to "&&" ensures that
    when the new key_count is greater than 0, there will be
    an attempt to clone the tree.
    
    This also exposes a different issue. When two SEL_ARG
    predicates are "Or-ed" together, i.e. combined into a
    common OR expression, the case when either predicate was of
    the type ALWAYS, i.e. unable to reject any rows, was not
    handled. When key_or tries to clone a SEL_ARG object of the
    type ALWAYS, it results in a failed assertion.
    
    This is fixed by treating ALWAYS SEL_ARG's similar to NULL
    SEL_ARG's. If either SEL_ARG type is ALWAYS, return it and
    release the reference to other one. The logic is similar to
    the inference TRUE or P => P.

[33mcommit f4ff086abea975222572fcfd232bf296018f5d85[m
Author: Arun Kuruvila <arun.kuruvila@oracle.com>
Date:   Fri Aug 21 08:35:42 2015 +0530

    Bug#20198490 : LOWER_CASE_TABLE_NAMES=0 ON WINDOWS LEADS TO
                   PROBLEMS
    
    Description:- Server variable "--lower_case_tables_names"
    when set to "0" on windows platform which does not support
    case sensitive file operations leads to problems. A warning
    message is printed in the error log while starting the
    server with "--lower_case_tables_names=0". Also according to
    the documentation, seting "lower_case_tables_names" to "0"
    on a case-insensitive filesystem might lead to index
    corruption.
    
    Analysis:- The problem reported in the bug is:-
    Creating an INNODB table 'a' and executing a query, "INSERT
    INTO a SELECT a FROM A;" on a server started with
    "--lower_case_tables_names=0" and running on a
    case-insensitive filesystem leads innodb to flat spin.
    Optimizer thinks that "a" and "A" are two different tables
    as the variable "lower_case_table_names" is set to "0". As a
    result, [1;31moptim[mizer comes up with a plan which does not need a
    temporary table. If the same table is used in select and
    insert, a temporary table is needed. This incorrect
    [1;31moptim[mizer plan leads to infinite insertions.
    
    Fix:- If the server is started with
    "--lower_case_tables_names" set to 0 on a case-insensitive
    filesystem, an error, "The server option
    'lower_case_table_names'is configured to use case sensitive
    table names but the data directory is on a case-insensitive
    file system which is an unsupported combination. Please
    consider either using a case sensitive file system for your
    data directory or switching to a case-insensitive table name
    mode.", is printed in the server error log and the server
    exits.

[33mcommit 6dd54179d9cd6453e7b87cea76d81209c4979f07[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Thu Aug 20 10:53:31 2015 +0200

    Bug#21299665 INORDERNATE MEMORY USAGE QUERYING I_S.COLUMNS
    
    Some queries using the INFORMATION_SCHEMA can consume excessive
    amount of memory because of a sub[1;31moptim[mal query plan and lack of
    proper materialization. This patch applies a temporary mem_root
    on during the JOIN-execution phase when the I_S tables are
    materialized as temporary tables.

[33mcommit fe3b4b5f314ee4fdffc0e9b9d133b6766ccbd353[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Aug 10 11:31:19 2015 +0200

    Bug#21491442 variant::forced_return() [with t = json_scalar*]: assertion `false' failed.
    
    Post-push fix: broken build with -Werror in [1;31moptim[mized mode.

[33mcommit 6676741723c62031a485f666b7525a247ef7a02e[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jul 15 14:16:20 2015 +0200

    Bug#21447969 FILTER USING TRIM RESULTS IN CORRECT
    
    Problem: a where predicate containing both trim(leading) and trim(trailing)
    was incorrectly [1;31moptim[mized away.
    
    Solution: Change Item_func_trim::func_name() to return different results
    for leading and trailing. Introduce trim_func_name() for printing
    the correct function name for Item_func_trim::print()

[33mcommit 486b4dc42c7fe664458d1a9b04d6eb1f609aa0da[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Wed Jul 15 11:20:19 2015 +0800

    Followup:BUG#21245805   HA_INNOBASE::RECORDS_IN_RANGE()
    RETURNS CONSTANT FOR SPATIAL INDEXES
    
    Fix innodb_gis.rtree_estimate failure with embedded runs.
    
    The root cause is gcc [1;31moptim[mization in linux 32-bit platforms,
    so the fix is enabling the test only for debug build.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com> over IM

[33mcommit 0fd8555617e8e8d309acf41808610e5d5cb97ef1[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 10 10:08:14 2015 +0200

    Bug#21271736 UNITTEST UT0CRC32-T HANGS ON SOLARIS 10 SPARC
    
    Reduce execution time for unit test.
    Change the constant n_megabytes_to_checksum and build [1;31moptim[mized when doing
    performance analysis.

[33mcommit 755b35cd1df3034789fd5cce3041819be9c02d48[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 3 16:21:37 2015 +0200

    BUG#21245805 HA_INNOBASE::RECORDS_IN_RANGE() RETURNS CONSTANT FOR SPATIAL INDEXES
    
    Post-push fix: broken build with -Werror in [1;31moptim[mized mode.

[33mcommit 07cad65513349795322da022b4f6ab17c2c71e70[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Sun Jun 28 11:38:26 2015 +0200

    WL#7340 IO aware cost estimate function for data access
    
    This worklog extends the [1;31moptim[mizer cost model to take into account
    whether data and indexes must be read from disk or are likely already
    present in a memory buffer.
    
    -The cost model will use estimates from the handler/storage engines
     about how much of tables and indexes are present in memory (see
     WL#7168) in its cost calculations.
    
    -A new cost constant named "memory_block_read_cost" representing the
     cost of accessing data that is in a memory buffer inside the storage
     engine. This constant corresponds to the existing cost constant
     "io_block_read_cost" that is used when a block needs to be read from
     disk.
    
    In this initial version the default value for memory_block_read_cost
    is the same as the default value for io_block_read_cost.
    
    Per-file comments for some of the test changes:
    
    main.index_merge_delete:
      One query now merges three indexes instead of two due to changes in the
      model for disk sweeps used by index merge.
    main.index_merge_myisam:
      Minor change in cost estimates in JSON output due to changes to cost
      model for disk sweeps used by index merge.
    main.innodb_explain_json_non_select_all
    main.innodb_explain_json_non_select_none
    main.innodb_explain_non_select_all
    main.innodb_explain_non_select_none main.myisam_explain_json_non_select_all:
    main.myisam_explain_json_non_select_none:
    main.myisam_explain_non_select_all.result
    myisam_explain_non_select_none.result
      Minor changes in cost estimates in JSON output due to changes to cost
      model for disk sweeps used by index merge and DS-MRR.
      Added more data to two queries to preserve original query plan as index
      merge.
    main.partition_index_innodb:
      Increase number of records in table in order to preserve original query
      plan. This was needed due to changes in cost model for disk sweeps used
      by index merge.
    main.subquery_sj_all:
    main.subquery_sj_all_bka:
    main.subquery_sj_all_bka_nixbnl:
    main.subquery_sj_all_bkaunique:
      Two queries changes from using FirstMatch to use Materialization due
      to a small change in the cost estimate for DS-MRR.
    opt_costmodel.result:
      Re-recorded to reflect that the engine_cost table now has a new entry
      for memory_block_read_cost.
    opt_costmodel_flush.test:
    opt_costmodel_flush.result:
      Replaced use of io_block_read_cost with memory_block_read_cost since
      the test tables are so small that they are in the InnoDB buffer.
      Changing the value for io_block_read_cost do not longer have any
      impact on the test queries.
    opt_costmodel_restart.test:
    opt_costmodel_restart.result:
      Replaced use of io_block_read_cost with memory_block_read_cost since
      the test tables are so small that they are in the InnoDB buffer.
      Changing the value for io_block_read_cost do not longer have any
      impact on the test queries.
    opt_costmodel_warnings.result:
      Re-recorded to reflect that the engine_cost table now has a new entry
      for memory_block_read_cost.
    suite/opt_trace/r/bugs_no_prot_all.result:
    suite/opt_trace/r/bugs_ps_prot_all.result:
    suite/opt_trace/r/range_no_prot.result:
    suite/opt_trace/r/range_ps_prot.result:
      Minor changes in cost estimates [1;31moptim[mizer trace due to changes to cost
      model for disk sweeps used by index merge and DS-MRR.
    unittest/gunit/faketable.h:
      Initialize the handler object's table pointer to point to the table.
    unittest/gunit/opt_costconstants-t.cc:
      Added unit tests for the new SE_cost_constants::memory_block_read_cost()
      function.
    unittest/gunit/opt_costmodel-t.cc:
      Added unit tests for the new functions added to the Cost_model_table class.

[33mcommit 8ae45ffe336b3fd2c612af47eedb738d379e904e[m
Author: Allen Lai <zheng.lai@oracle.com>
Date:   Fri Jun 26 10:06:01 2015 +0800

    Bug#21076238 ASSERT !CURSOR->INDEX->IS_COMMITTED(), GIS UPDATE,
    ROW_INS_SEC_INDEX_ENTRY_BY_MO
    
    This bug has 2 issues:
    Issue 1: Same as bug#20734998, partial update cause the problem. The
    different is there're more that 1 update
    in a transaction. For example, in trx1, we execute update table t1, then
    update table t2. Update t1 is ok, but update t2 hit a dup key error, so
    we need to rollback. Since the patch for bug#20734998 will set last
    updated spatial index to NULL before every update, so, update on t1 will
    be skipped. The solution is we need to search spatial index twice, one
    for finding an undel-marked rec, if can't find it, we do the second
    round search for finding a del-marked rec. And we will do some
    [1;31moptim[mization on this later.
    Issue 2: Purge found a undel-marked rec to purge, and this shouldn't
    happen. It's caused by the spatial index is not sync with cluster index.
    And the root cause is, we generate wrong mbr for externally stored
    geometry field in function row_upd_changes_ord_field_binary_func. And
    this will cause we skipped the spatial index on this field in update.
    The solution is read the whole field data from cluster row to generate
    mbr. This issue is a flaw of bug#20313067 fix.
    
    Reviewed-by: Jimmy Yang<jimmy.yang@oracle.com>
    RB: 9223

[33mcommit 244ab46f7dd2bfbdf6327ad7aa91197992afd4da[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Fri Jun 26 09:03:34 2015 +0800

    Bug#20954452  GTID IS NOT RELEASED PROPERLY WHEN PS_PROTOCOL + GTID + BINLOG OFF COMBINATION - post fix 2
    
    In the fixed patch, we [1;31moptim[mized mysqltest to use the PS protocol
    for most DDLs. This [1;31moptim[mization is causing that some main tests
    failed with SQL syntax error, inconsonant result and so on.
    
    To fix these failures on pb2 immediately, we revert the [1;31moptim[mization
    to problematic DDls.

[33mcommit 6be7bc4440489a3bc85f2b6d8809fd8a420f1e45[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Thu Jun 25 15:15:52 2015 +0200

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - changes in [1;31moptim[mizer causes query execution plan changes, fix
       by updating .result files
     - re: Bug #18194196: OPTIMIZER EXECUTES STATEMENT INPERFORMANT

[33mcommit 82a8a407cfddf82e7acc230d7813b8c31a3b274c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jun 18 17:18:32 2015 +0200

    Bug#17312761 MYSQLD WAITS FOR DUMMY NODES (NODEGROUP=65536) TILL NDB-WAIT-CONNECTED PERIOD
    
    On startup mysqld waste time waiting on ndb data nodes that have
    not joined the cluster.
    
    Any ndbapi node including mysqld can only connect to a started
    cluster.
    
    Prior this patch mysqld (or a ndbapi application calling
    wait_until_ready/2) waited until all configured data nodes were
    started.
    
    The ndbapi actually only need one connection to a data node with
    DBTC to work, but this is not [1;31moptim[mal since all data will be
    routed internally in kernel via that data node.
    
    Data nodes that not yet have joined the cluster will of course
    never send any data until it have joined so no need to wait for
    that node.
    
    If a new data node eventually joins the cluster the ndbapi will
    try to connect to the new node with an API-DB heartbeat interval.
    
    With this patch mysqld will wait only for data nodes that have
    joined the cluster.

[33mcommit 2c44be7bcc32836c4e809dc51ac056726cbbcabb[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Jun 15 13:55:43 2015 +0200

    Bug#21251627 ASSERT `ORDER == __NULL && !SELECT_DISTINCT' AT JOIN::REPLACE_INDEX_SUBQUERY
    
    We have IN(subquery on single table with group by and having).
    remove_redundant_subquery_clauses() does not change this, because
    of the presence of HAVING. If there had been ORDER BY it would
    have removed it.
    [1;31moptim[mize_distinct_group_order() sees GROUP BY on primary key,
    concludes that grouping is pointless, except that order still
    has to be guaranteed (as GROUP BY is documented to produce ordered
    groups), so it moves GROUP BY to ORDER BY. Now there is ORDER BY.
    replace_index_subquery() sees IN(subquery) and dies here:
      // Guaranteed by remove_redundant_subquery_clauses():
      DBUG_ASSERT(order == NULL ...);
    The flaw is that [1;31moptim[mize_distinct_etc() adds a clause
    after remove_etc().
    Before the fix for Bug 21038929, [1;31moptim[mize_distinct_etc() didn't add
    the clause because it was a subquery; in general this is wrong reasoning,
    but for IN(subquery) it's correct.
    Fix: don't add the clause if IN(subquery).
    Note that if we, in the future, stop guaranteeing that GROUP BY implies
    ordering, then the modified code can be removed.

[33mcommit 8f37f8f6ae2f00e64253fb8c689587db4e7b250f[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Tue Jun 9 20:18:32 2015 +0530

    BUG#21211524 CRASH IN ACTUAL_KEY_PARTS WITH PARTITIONED TABLES
    
    Problem :
    ---------
    In some rare cases [1;31moptim[mizer would prune all partitions for a table
    but won't remove the table and call storage apis to initialize and
    fetch records. In this case we are not setting the active index during
    initialization[ha_innopart::index_init].
    
    For index scan with a begin key, we attempt to calculate key length
    [Partition_helper::common_index_read] using active index which results
    in the access violation. We return EOF error just after this in case
    there is no partition to fetch data from[partition_scan_set_up].
    
    Solution :
    ----------
    Set active index id in ha_innopart::index_init even if there is
    no partition to select from. This is also the way we handle in 5.6.
    
    Reviewed-by: Mattias Jonsson <mattias.jonsson@oracle.com>
    
    RB: 9231

[33mcommit 649b29e524426d3db7796c14439e3ca61b673743[m
Author: Norvald H. Ryeng <norvald.ryeng@oracle.com>
Date:   Mon Jun 8 15:36:30 2015 +0200

    Bug#21067378 GEOMETRIC OPERATION RETURNS ERROR INSTEAD OF RESULT
    
    Problem: A query on a geometry column will return an error instead of
    a result if there exists a UNIQUE index on the column.
    
    The range [1;31moptim[mizer temporarily transforms geometry columns of
    specific subtypes to generic geometry columns while using the field
    for type transformation. This only happens if the column is of type
    MYSQL_TYPE_GEOMETRY and key_part->image_type is Field::itMBR. However,
    when there's a spatial index on the column, key_part->image_type is
    Field::itRAW, and the column type is not changed to the geometry
    supertype. When a value is inserted that doesn't match the subtype, an
    error is flagged and the query is aborted.
    
    When the error is avoided, the result is still wrong since the
    predicate is not recognized as a spatial operator.
    
    Fix: Temporarily change all geometry columns to the geometry
    supertype, regardless of image_type. Add COVERS and COVEREDBY to the
    list in is_spatial_operator().

[33mcommit 055c521e60fa64398bcd32840d6193413712ac87[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Thu May 28 14:59:55 2015 +0200

    Bug#20949117: Remove obsolete code from UNION processing
    
    After the last refactoring work in preparation and [1;31moptim[mization,
    there are some unused code blocks in sql_union.cc.
    This bug fix eliminates those code blocks completely and performs
    some simple additional refactoring.
    
    - Added an interface st_select_lex_unit::is_simple() that wraps
      testing for !(is_union() || fake_select_lex)
    
    - Cleaned up global_parameters() a bit: Only ORDER BY/LIMIT/OFFSET
      should be accessed through it, otherwise use fake_select_lex.
    
    - Initialization of JOIN::do_send_rows was moved from [1;31moptim[mization to
      execution, since it is used only in the latter.
    
    - st_select_lex_unit::prepare() has mostly cosmetic changes and improved
      comments. Call to set_current_select() eliminated for error case.
    
    - Deleted unused function Query_result::reset_offset_limit_cnt
    
    - st_select_lex_unit::[1;31moptim[mize() had an unused code block started with
      if (sl == global_parameters() && is_union()).
      It was unused because global_parameters() always return the "fake" object
      for a UNION query. Besides, with the introduction of
      Query_result_union_direct, LIMIT/OFFSET handling is correct without
      adjusting offset_limit_cnt and select_limit_cnt.
    
    - An equivalent code block is removed from st_select_lex_unit::execute().
      Variable rows_at_start was found to be redundant.
      Calls to set_current_select() were removed in error case.
      offset_limit_cnt did not need to be assigned here, since it is done
      in set_limit().
      Call info(HA_STATUS_VARIABLE) was moved to a more logical place
      (used to get row count from temporary table used by UNION).
      add_rows was never assigned so it could be removed:
      sl->join->calc_found_rows is never true for a query block that is
      part of a UNION (either braces=true or m_select_limit=HA_POS_ERROR,
      see JOIN::[1;31moptim[mize()), search for comment "Calculate found rows if".
      join->examined_rows is reset in JOIN::exec() so assignment is deleted.
    
    - Added more extensive tests for LIMIT and OFFSET to limit.test
    
    (cherry picked from commit b50c7dc66c6c894772d5da463cd08b9cd9ebd154)
    
    Conflicts:
            sql/query_result.h
            sql/sql_union.cc

[33mcommit e78f3cb6d098e90531ada6186eb33c192bc94dc5[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue May 26 15:32:25 2015 +0200

    Bug#21140111: Explain ... match against: Assertion failed: ret ...
    
    The problem here is missing error check in Item_func_match::init_search()
    after the handler call ft_init_ext_with_hints(), and missing error
    propagation in the call stack.
    
    The fix also includes a new inlined query block property function
    has_ft_funcs() that is used to avoid an unnecessary and costly
    function call to [1;31moptim[mize full-text searches.

[33mcommit d7cbbe9e4c37c533677f7c13a9acf57239ff9d2d[m
Author: Harin Vadodaria <harin.vadodaria@oracle.com>
Date:   Tue May 26 11:07:34 2015 +0530

    Bug#20693153 : ACCESS DENIED WITH SSL CONNECTION FROM MYSQL CLIENT
    
    Description : cli_establish_ssl() tries to establish SSL connection
                  with server. While establishing SSL it calls my_net_writes
                  which writes data into net->buff buffer.
                  In case of [1;31moptim[mized binary, if --general-log=0 and
                  --slow-query-log=0 is set, write operation would overwrite
                  portion of the scramble data sent by server. This happens
                  because when above mentioned variables are set to OFF,
                  server version string does not contain "--log" suffix
                  which otherwise would have prevented overwrite of scramble.
                  This data is yet to be read by authentication plugin on
                  client side. Hence, once control reaches to authentication
                  plugin, it consumes wrong scramble which leads to
                  authentication error if user account has a password.
    Solution : In case of SSL connection, copy scrambe data into new
               buffer and pass new buffer to run_pluggable_auth.
    
    Reviewed-By: Georgi Kodinov <georgi.kodinov@oracle.com>
    Reviewed-By: Robert Golebiowski <robert.golebiowski@oracle.com>

[33mcommit 3322c0c3ee7a4c72ed6bfacb0fda73085219d9d8[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed May 20 21:03:10 2015 +0200

    Bug#20665051 SQL_SHOW.CC:7764: ASSERTION `QEP_TAB->CONDITION() == QEP_TAB->CONDITION_OPTIM()
    
    The scalar subquery is uncorrelated, so it's evaluated during top
    JOIN's [1;31moptim[mize(). This evaluation (JOIN::exec) calls JOIN::prepare_result()
    which materializes the I_S content into a tmp table then sorts this table.
    Then EXPLAIN explains the subquery: it calls JOIN::prepare_result() a
    second time, causing again materialization and sorting.
    This is inefficient, and causes an assertion failure because the
    I_S materialization expects to run before sorting (so it crashes because
    of the previous sorting).
    
    Fix: don't prepare result if already done.

[33mcommit 580179a31d2feb7eb3ee8dfaf0e3cfec49bfe9ea[m
Author: Debarun Banerjee <debarun.banerjee@oracle.com>
Date:   Thu May 21 14:22:39 2015 +0530

    BUG#21075892 SHUTDOWN HANG, WAITING FOR ASYNC ROLLBACK TO FINISH
    
    Problem :
    ---------
    In case of lock conflict, a high priority transaction(certified)
    attempts to rollback other transactions([1;31moptim[mistic). We have two
    different paths here and two distinct symptoms causing shutdown/rollback
    hang.
    
    1. The ASYNC rollback happens while handling DB_LOCK_WAIT error and
    for semi-consistent read we change the wait state to success and return
    the last committed version to SQL. If sql condition skips the row then
    we never rollback the transaction.
    
    2.If the blocking transaction is wating for some other lock, we mark the
    transaction for abort/ASYNC rollback and deadlock victim and wake it up.
    We are overwriting the DB_DEADLOCK state in following two cases
        A. When the transaction is interrupted by shutdown [DB_INTERRUPTED]
        B. When the lock wait timeout is over [DB_LOCK_WAIT_TIMEOUT]
    In both cases we exit innodb without doing the rollback.
    
    3. Another independent issue was observed during debugging where the
    server crashed while printing some info for the victim thread after
    doing ASYNC rollback.
    
    Solution :
    ----------
    1.Check and kill blocking transaction for high priority transaction
    in success path.
    
    2.After wake up from a wait state check if the transaction is already
    marked as deadlock victim and if so don't overwrite the state.
    
    3.Get the victim transaction information before doing ASYNC rollback
    trx_kill_blocking.
    
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    
    RB: 8994

[33mcommit 96fcfcbd7b5120e8f64fd45985001eca8d36fbfb[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed May 20 09:26:17 2015 +0200

    Bug#20819199 ASSERTION FAILED IN TEST_IF_SKIP_SORT_ORDER
    
    When test_if_skip_sort_order("GROUP BY") runs:
    get_best_group_min_max() ignores the "keys_to_use" mandate, so it
    returns a QUICK with an unwanted index, hence the assertion failure:
            DBUG_ASSERT(tab->quick()->index==(uint)best_key);
    Fix: scan the param->keys list instead of scanning the list of all
    keys.
    By doing this, one function's argument becomes unneeded.
    
    There was no apparent problem in 5.6 (i.e. before WL#6016) because
    there was a call to prepare_for_position() before
    test_if_skip_sort_order(), which WL#6016 moved to make_join_readinfo()
    i.e. to after test_if_skip_sort_order(); this led to different bits in
    read_set, and thus different index usage.
    
    Test result changes:
    
    1) in opt_trace, some indexes are not considered anymore which is
    normal
    
    2) in order_by*:
    I added more coverage of IGNORE INDEX and simplified the logic which
    counts tmp tables; I also fixed a problem: the primary key index,
    is always named "PRIMARY", so listing "a" in the IGNORE INDEX
    clause was pointless, it didn't make the primary key index be ignored
    ("a" was interpreted as "ab", because of prefix matching in the hint code,
    filed as Bug 21104060 ).
    Fixed by using name PRIMARY in hints (also filed bug 21105266).
    I compared the output of this modified test, before and after my code
    change: there is no difference.
    
    If we analyze this output (again: identical pre- and post-patch):
    EXPLAIN SELECT a FROM t1 IGNORE INDEX FOR GROUP BY (PRIMARY, ab) GROUP BY a;
    1       SIMPLE  t1      NULL    range   PRIMARY,ab      ab      4       NULL    13      100.00  Using index for group-by; Using temporary; Using filesort
    EXPLAIN SELECT a FROM t1 IGNORE INDEX FOR ORDER BY (PRIMARY, ab) ORDER BY a;
    1       SIMPLE  t1      NULL    index   NULL    PRIMARY 4       NULL    128     100.00  Using index; Using filesort
    EXPLAIN SELECT a FROM t1 IGNORE INDEX (PRIMARY, ab) GROUP BY a;
    1       SIMPLE  t1      NULL    ALL     PRIMARY,ab      NULL    NULL    NULL    128     100.00  Using filesort
    EXPLAIN SELECT a FROM t1 IGNORE INDEX (PRIMARY, ab) ORDER BY a;
    1       SIMPLE  t1      NULL    ALL     NULL    NULL    NULL    NULL    128     100.00  Using filesort
    This supports this interpretation:
    - for IGNORE INDEX FOR GROUP BY/ORDER BY: user wants indexes to not be
    used for GROUP BY/ORDER BY, but he still allows to use indexes for
    reading. So we have range access on "ab", or covering index scan on
    "primary", for reading, and a filesort for grouping/ordering.
    - for IGNORE INDEX, index cannot be used for anything, so we have
    table scan + filesort.
    
    Thus, the following exception, documented in the manual, seems to not
    exist in fact:
    https://dev.mysql.com/doc/refman/5.7/en/index-hints.html
    "However, if there is a covering index for the table and it is used
    to access the table, the [1;31moptim[mizer ignores IGNORE INDEX FOR
    {ORDER BY|GROUP BY} hints that disable that index."; also
    mentioned in process_index_hints() (though I could not find the
    code which implements it).
    So I remove it from the comments in table.cc and will suggest removing
    it from the manual.

[33mcommit 5fef9d4214e43afa4e5fdf395cc1771070bfb546[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Tue May 19 11:23:32 2015 +0800

    Bug#18422162 THE FIND_FLAG PASSED TO MYISAM AND INNODB ARE DIFFERENT
    
    Issue:
    
    The enum items in ha_rkey_function and key_range_flags are bitwise OR'ed
    to store into one integer field during range [1;31moptim[mization when the
    key range flag is GEOM_FLAG, i.e. when doing spatial range [1;31moptim[mization.
    
    Fix:
    
    We should not bitwise-or ha_rkey_function enum items with key_range_flags
    items.
    
    So we now add a 'gis_rkey_func' field in SEL_ARG, QUICK_RANGE and
    RANGE_SEQ_ENTRY structures or classes to pass around the
    ha_rkey_function flag for GIS ranges during range [1;31moptim[mization,
    and pass it to the handler interface for target rtree index scans.

[33mcommit 06c55c59e3f9b35d919679adf189c5db2fdabb2e[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Sat May 9 14:02:53 2015 +0530

    Bug #19138298 RECORD IN INDEX WAS NOT FOUND ON ROLLBACK, TRYING TO INSERT
    
    Scenario:
    
    1. The purge thread takes an undo log record and parses it and forms
       the record to be purged. We have the primary and secondary keys
       to locate the actual records.
    2. Using the secondary index key, we search in the secondary index.
       One record is found.
    3. Then it is checked if this record can be purged.  The answer is we
       can purge this record.  To determine this we look up the clustered
       index record.  Either there is no corresponding clustered index
       record, or the matching clustered index record is delete marked.
    4. Then we check whether the secondary index record is delete marked.
       We find that it is not delete marked.  We report warning in [1;31moptim[mized
       build and assert in debug build.
    
    Problem:
    
    In step 3, we report that the record is purgeable even though it is
    not delete marked.  This is because of inconsistency between the
    following members of purge_node_t structure - found_clust, ref and pcur.
    
    Solution:
    
    In the row_purge_reposition_pcur(), if the persistent cursor restore
    fails, then reset the purge_node_t->found_clust member.  This will
    keep the members of purge_node_t structure in a consistent state.
    
    rb#8813 approved by Marko.

[33mcommit 39f83c586ad3662039006cfbb14fa4bdfd0c4133[m
Author: Haixiang Li <haixiang.li@oracle.com>
Date:   Sat May 9 03:55:10 2015 +0200

    Bug#20835095 CRASH AT CREATE_REF_FOR_KEY IN SQL/SQL_SELECT.CC
    
    Description:
    ------------
    After bug#19695490 was fixed, a special case missed, that leads to this bug.
    
    For a VARCHAR type, if MySQL uses it as a key of temp table, MySQL will add
    HA_KEY_BLOB_LENGTH to the store length in order to calculate the key length
    in init_from_field(), so we add HA_KEY_BLOB_LENGTH to the length of a
    possible index to prevent the [1;31moptim[mizer to use to materialized lookup.
    If we don't do this, MySQL would triggers an assertion, which was added in
    bug#19695490.
    
    Fix:
    ----
    Add HA_KEY_BLOB_LENGTH to 'lookup_index_length' in order to get the right
    index length in semijoin_types_allow_materialization() when the field is
    VARCHAR, BLOB or GEOMETRY.
    
    Test case added.

[33mcommit 80094d93ace2aec30e25920e4c52c289e20594e9[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed May 6 10:26:53 2015 +0200

    Post-push fix for BUG#20470724: SET GTID_PURGED SHOULD NOT ROTATE BINLOG
    
    In Sys_var_gtid_purged::global_update, the two local variables
    current_gtid_executed and current_gtid_lost would be passed to my_free
    without having been initialized in case the Gtid_set constructor
    failed when initializing the local variable gtid_set. This caused
    compilation errors with -Werror, and crash in sys_vars.gtid_purged in
    [1;31moptim[mized build. Fixed by initializing the two variables to NULL.

[33mcommit 5a6e30144a08b6f2d200387dbe28fca47966b774[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Mon Apr 27 12:04:47 2015 +0200

    WL#8165 Use new records per key interface in NDB
    
    - change the ha_ndbcluster to provide index statistics using the
      new KEY::set_records_per_key() interface.
    
    - "the new records per key interface should improve the accuracy
      of the index statistics and make the [1;31moptim[mizer create better
      query plans for NDB."
    
    (cherry picked from commit cc49493aebeae6d4db65a27088f039b0372ce4aa)

[33mcommit a631781db17a4906caa394a1e3233d671e154eda[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Mon Apr 27 12:04:47 2015 +0200

    WL#8165 Use new records per key interface in NDB
    
    - change the ha_ndbcluster to provide index statistics using the
      new KEY::set_records_per_key() interface.
    
    - "the new records per key interface should improve the accuracy
      of the index statistics and make the [1;31moptim[mizer create better
      query plans for NDB."

[33mcommit 09f9f7dc3ba870c9d9b060a571dafbec7e9499f1[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 24 14:27:50 2015 +0200

    Fix for Bug#20954804
    
      'WATCHDOG DETECTS NO PROGRESS IN SEND THREAD AND KILLS DATANODE'
    
    Patch is a backport of 'part 3 of 3' of WL#7654:
    
      'Reduce lock contention in NDB mt-scheduler'
    
    That WL was originaly pushe dto Cluster-7.4 as a performance
    [1;31moptim[mization, but it has later turned out that it also
    fixed a potential deadlock between send- and worker-threads
    in the multithreaded scheduler.
    
    Pasted commit comment from original WL:
    
    ---------------------------------------
          Reduce lock contention on send_lock when using send threads.
    
          Previously multiple send threads could be invoked for
          handling sending to the same node. These would then
          compete for the same send_lock. Furthermore, while
          being blocked on the send lock, there could be available
          sending work to other nodes.
    
          This patch ensures that new send threads are not activated
          while there already is an active send thread assigned to
          a node. Neither would a node being handled by an active send
          thread be visible to other, already active, send threads.
          This is achived by not putting the node into the 'node-list'
          while it is assigned to a send thread.
    
          See comment in code how different states are now defined for
          handling this.

[33mcommit 39dc143d968242bb9f8271f6f8b82b4c9ac2fe56[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Apr 24 14:27:50 2015 +0200

    Fix for Bug#20954804
    
      'WATCHDOG DETECTS NO PROGRESS IN SEND THREAD AND KILLS DATANODE'
    
    Patch is a backport of 'part 3 of 3' of WL#7654:
    
      'Reduce lock contention in NDB mt-scheduler'
    
    That WL was originaly pushe dto Cluster-7.4 as a performance
    [1;31moptim[mization, but it has later turned out that it also
    fixed a potential deadlock between send- and worker-threads
    in the multithreaded scheduler.
    
    Pasted commit comment from original WL:
    
    ---------------------------------------
          Reduce lock contention on send_lock when using send threads.
    
          Previously multiple send threads could be invoked for
          handling sending to the same node. These would then
          compete for the same send_lock. Furthermore, while
          being blocked on the send lock, there could be available
          sending work to other nodes.
    
          This patch ensures that new send threads are not activated
          while there already is an active send thread assigned to
          a node. Neither would a node being handled by an active send
          thread be visible to other, already active, send threads.
          This is achived by not putting the node into the 'node-list'
          while it is assigned to a send thread.
    
          See comment in code how different states are now defined for
          handling this.

[33mcommit e114f952a65d69f6ddbf0579472033a50ee95f23[m
Author: David.Zhao <david.zhao@oracle.com>
Date:   Tue Apr 21 20:05:44 2015 +0800

    Bug#20911624 THE SERVER CRASH WHEN TEST ST_INTERSECTS WITH ST_BUFFER
    
    The issue is caused by unaligned memory access in [1;31moptim[mized (-O3) code,
    the fix is to do memcpy/memset in class Geometry::Flags_t's
    constructors and assignment operator.

[33mcommit 5efaade10dbf178e6e6629beef7d2d323494b96d[m
Author: Norvald H. Ryeng <norvald.ryeng@oracle.com>
Date:   Tue Apr 21 15:02:00 2015 +0200

    Bug#19593342 CREATE SPATIAL INDEX FAILS TO FLAG AN ERROR FOR INVALID
    GEOMETRY DATA
    
    Post push fix to remove compiler warning in [1;31moptim[mized mode.

[33mcommit c3950fe9e1bab3b998944601b2a50d82abca480b[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Apr 14 19:08:47 2015 +0200

    Bug#20615023 SIGNAL 11 IN ITEM_FIELD::RESULT_TYPE DURING 1ST EXECUTION OF PREPARED STMT
    
    There is a correct [1;31moptim[mization which replaces
    "x IN (SELECT y FROM DUAL)"
    with "x=y". But it ignores WHERE, so it also replaces
    "x IN (SELECT y FROM DUAL WHERE condition)" with "x=y".
    This is wrong, "condition" might be FALSE or UNKNOWN, so the subquery
    should then produce a NULL result, and the predicate should be UNKNOWN.
    In other words: presence of WHERE should turn the [1;31moptim[mization off.
    As all in-to-exists subquery code, this [1;31moptim[mization is from 2003-2004;
    a similar problem was found and fixed in 2006 (bug 24670) but this code
    block was not handled.

[33mcommit f48f0626a9954a7fc7973e781c618aee254afb20[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Wed Apr 15 15:25:57 2015 +0800

    BUG 20459905 - DEADLOCK OF THREADS DETECTED! 5.7.5, 1 THREAD SQL TESTCASE,
    SPORADIC, IN IB_LOGF
    
    If we set btr_cur_limit_[1;31moptim[mistic_insert_debug, we have to make sure that
    we do the same judgement in btr_cur_will_modify_tree as in [1;31moptim[mistic insert
    function, to prevent making an incorrect decision the tree aren't about to
    be modified.
    
    RB: 8607
    Reviewed-by: Jimmy Yang <jimmg.yang@oracle.com>

[33mcommit 1362730394a29b79d896551f77917a7ae4aa7ef3[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Thu Apr 9 14:19:09 2015 +0200

    Bug#20788811 MAX_STATEMENT_TIME: ASSERTION `EXPLAIN_OTHER || UNIT->IS_OPTIMIZED()' FAILED.
    
    We open many connections sending repeatedly:
    explain select 1 from t1 where a in (select 1 union select 1)
            union select 1;
    And we have max_statement_time=1.
    Occasionally, one such query takes more than one second, and
    the KILL_TIMEOUT is given while it's [1;31moptim[mizing:
    (select 1 union select 1) (the inside of IN());
    during that [1;31moptim[mization, we [1;31moptim[mize the two "select 1"
    then proceed to [1;31moptim[mizing the fake select lex, which is when the
    timeout occurs:
    Optimize_table_order::best_extension_by_limited_search()
    gets the timeout:
    (gdb) p thd->killed
    $39 = THD::KILL_TIMEOUT
    so returns here:
      if (thd->killed)  // Abort
        DBUG_RETURN(true);
    That makes select_lex::[1;31moptim[mize() fail (select_lex is fake_select_lex).
    This makes select_lex::[1;31moptim[mize() fail (this time select_lex is the first
    query of the top union, i.e. the owner of IN()).
    So we break from the loop of unit->[1;31moptim[mize() (where 'unit' is the top
    UNION), we still move on to [1;31moptim[mize unit->fake_select_lex (the mistake)
    which resets 'status' to 'false'. Then it goes wrong, as
    status==false => We set unit->is_[1;31moptim[mized() to true and return "ok".
    Later we try to explain that and crash on the non-properly-[1;31moptim[mized
    part UNION-in-subquery.
    
    Fix: don't return "ok" if a query block had a failure; then statement
    will terminate before reaching EXPLAIN code.
    No testcase: it requires concurrency, and it's just a plain stupid
    coding mistake. I tested manually - without patch, crash in 5 minutes;
    with patch, no crash in two runs of 3 hours each.

[33mcommit b3c8fc90223a2a2b685125630d5f94936f09458b[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Thu Apr 9 09:59:26 2015 +0200

    Bug#20443863 USE OF WORST_SEEKS IN FIND_BEST_REF() CAN LEAD TO WRONG QUERY PLAN
    
    This patch fixes a bug in how the [1;31moptim[mizer uses the
    JOIN_TAB::worst_seeks variable when limiting the cost of a key
    lookup. The variable JOIN_TAB::worst_seeks is initialized to be a cost
    estimate for the maximum cost for a key lookup but it is used as an
    estimate for the maximum number of blocks to read when calculating the
    cost of key lookup in Optimize_table_order::find_best_ref(). If we
    change the cost constant for IO block reads, this bug can impact the
    cost of doing ref access versus other access methods and may produce
    worse query plans.
    
    Example: For a query like this:
    
      SELECT i2 FROM t1 WHERE i1 = 1 AND i2 = 1;
    
    if we have an index on i1, this query can be executed using ref
    access. But if we double the values for all cost constants, due to the
    error in use of JOIN_TAB::worst_seeks, the cost for ref access is more
    than doubled and can cause this query to be executed as a table scan.
    
    The fix for this problem is to change the code in
    Optimize_table_order::find_best_ref() to use JOIN_TAB::worst_seeks as
    a cost estimate instead of a block estimate.

[33mcommit 6bdccb7fada44161d1ed7df2824877c82627cf61[m
Author: Oystein Grovlen <oystein.grovlen@oracle.com>
Date:   Wed Mar 18 13:45:47 2015 +0100

    WL#8244 Hints for subquery strategies.
    Part#2: Add support for SEMIJOIN/_SEMIJOIN and SUBQUERY hints
    
    (Below are commit comments for several commits that have been squashed into one)
    
    Part#2: Add support for SEMIJOIN/NO_SEMIJOIN hints
            (Parsing of SUBQUERY hint added, but not the functionality)
    
    sql/lex.h
      Added symbols for new [1;31moptim[mizer hint keywords
      Fixed some typos in comments
    
    sql/sql_hints.yy
      Added parser rules for query block level hints in general, and specific
      rules for SEMIJOIN, NO_SEMIJOIN, and SUBQUERY hints
    
    sql/parse_tree_hints.h
    sql/parse_tree_hints.cc
      Added parse tree class for query block hints and enums for the strategies
      that can be used as arguments to hints
    
    sql/opt_hints.h
    sql/opt_hints.cc
      Added pointers to Opt_hints_qb for parse tree classes for
      SEMIJOIN/NO_SEMIJOIN and SUBQUERY hints.
      Added functions semijoin_enabled() and sj_enabled_strategies() to be used to
      compute effects of semijoin hints a query block.
    
    sql/sql_lex.h
    sql/sql_lex.cc
      Added private functions semijoin_enabled() and sj_enabled_strategies() to
      st_select_lex class.  If query block level hints are defined for this
      query block, calls are forwarded to corresponding Opt_hints_qb functions.
      Otherwise, return value is based on [1;31moptim[mizer_switch setting.
    
    sql/sql_resolver.cc
      In resolve_subquery() call semijoin_enabled() instead of
      checking [1;31moptim[mizer_switch directly.
      In convert_subquery_to_semijoin() call sj_enabled_strategies() to determine
      which semi-join strategies may be used.  Stored this information in the
      NESTED_JOIN object of corresponding semijoin nest.
    
    sql/table.h
      Added member sj_enabled_strategies to NESTED_JOIN to be used to store which
      semijoin strategies may be used.
    
    sql/sql_planner.cc
      Change code that checks which semijoin strategies may be used to use
      NESTED_JOIN::sj_enabled_strategies of the corresponding semijoin nest instead
      of [1;31moptim[mizer_switch.
      Added [1;31moptim[mizer trace for the case where new plan is cheaper but contains
      disabled strategy.  Changed wording of [1;31moptim[mizer trace for case when
      a less cheap plan is selected.
    
    sql/sql_[1;31moptim[mizer.cc
      Change code that checks which semijoin strategies may be used to use
      NESTED_JOIN::sj_enabled_strategies of the corresponding semijoin nest instead
      of [1;31moptim[mizer_switch.
    
    mysql-test/r/opt_hints_subquery.result
    mysql-test/t/opt_hints_subquery.test
      Tests for subquery hints
    
    mysql-test/suite/opt_trace/r/subquery_no_prot.result
    mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Changes to [1;31moptim[mizer trace (see sql_planner.cc)
    
    WL#8244 Hints for subquery strategies.
    Part#2 followup: Fix friend declaration
    
    Prior to c++11, friend declaration of a class needs to contain the "class" word.
    
    WL#8244 Hints for subquery strategies.
    Part#3: Add support for SUBQUERY hint
    
    sql/sql_hints.yy
      Changed parser rules for SUBQUERY hint according to WL specifications.
      (It is not possible to list multiple strategies for this hint.)
    
    sql/parse_tree_hints.cc
      PT_qb_level_hint::contextualize()
        Added checks that makes sure that only one SEMIJOIN, NO_SEMIJOIN, and
        SUBQUERY hint is registered per query block.
      PT_qb_level_hint::append_args()
        Added handling of SUBQUERY argument (removed previous dummy implementation)
    
    sql/opt_hints.h
    sql/opt_hints.cc
      Added function Opt_hints_qb::subq_strategy() that returns which subquery
      execution strategy to use if SUBQUERY hint is specified.
      Modified Opt_hints_qb::semijoin_enabled() to take SUBQUERY hints into account.
    
    sql/sql_lex.h
    sql/sql_lex.cc
      Added function st_select_lex::subq_strategy() that return which subquery
      strategies may be used for the query block
      Added some missing documentation to functions added in Part#2.
    
    sql/sql_[1;31moptim[mizer.cc
      Use st_select_lex::subq_strategy() instead of checking [1;31moptim[mizer_switch
      settings directly when determining which subquery execution strategy to use.
    
    mysql-test/r/opt_hints_subquery.result
    mysql-test/t/opt_hints_subquery.test
      Tests for SUBQUERY hint plus a few additional tests for SEMIJOIN/NO_SEMIJOIN
    
    WL#8244 Hints for subquery strategies.
    Fixed a warning
    
    WL#8244 Hints for subquery strategies.
    Addendum#1: Fix review comments from Guilhem
    
    sql/sql_hints.yy
      Use same grammar rule for SEMIJOIN/NO_SEMIJOIN hints with and without list
        of strategies.
      Removed Pt_hint_sj::Strategy.  Use OPTIMIZER_SWITCH_* values instead.
      subq => subquery in names
    
    sql/parse_tree_hints.h
      Removed Pt_hint_sj::Strategy.  Use OPTIMIZER_SWITCH_* values instead.
      All enum types shouled have prefix enum_ according to code standard.
      subq => subquery in names
      Removed trailing white space
    
    sql/parse_tree_hints.cc
      Restructured PT_qb_level_hint::contextualize() and
        PT_qb_level_hint::append_args()
    
    sql/opt_hints.h
      subq => subquery in names
      Fixed comments
      Removed trailing white-space
    
    sql/opt_hints.cc
      Restructured Opt_hints_qb::semijoin_enabled() and
        Opt_hints_qb::subquery_strategy()
      Removed trailing white-space
    
    sql/sql_lex.h
      subq => subquery in names
      Removed trailing white-space
    
    sql/sql_lex.cc
      Removed Pt_hint_sj::Strategy.  Use OPTIMIZER_SWITCH_* values instead.
      Restructured st_select_lex::semijoin_enabled()
      subq => subquery in names
      Removed trailing white-space
    
    sql/sql_planner.h
      Changed doc for found_plan_with_allowed_sj
    
    sql/sql_planner.cc
      consider_plan(): check semijoin nest of all tables in DW range for
        allowed strategies, not just the first table.  (In case tables of multiple
        sj nests are interleaved.)
      best_extension_by_limited_search():  No need to check has_sj when deciding
        on pruning.  If there is no semijoin, found_plan_with_allowed_sj will be
        set to true when first plan is found.
      Fix in [1;31moptim[mizer_trace:
        plan_use_disabled_strategy => plan_uses_disabled_strategy
      Removed redundant comments
      Removed trailing white-space
    
    sql/sql_[1;31moptim[mizer.cc
      subq => subquery in names
      Removed trailing white-space
    
    sql/table.h
      Changed doc for sj_enabled_strategies
    
    mysql-test/t/opt_hints_subquery.test
      Fixed typos in comment statements
    
    mysql-test/r/opt_hints_subquery.result
      Plan changes due to change in consider_plan()
      Fixed typos in comment statements
    
    mysql-test/suite/opt_trace/r/subquery_no_prot.result
    mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Fixed a grammatical error in Optimizer Trace output
    
    WL#8244 Hints for subquery strategies.
    Addendum#2: More review comments from Guilhem
    
    sql/sql_hints.yy
      Use Item_exists_subselect::enum_exec_method instead of
        PT_hint_subquery::enum_strategy.
    
    sql/parse_tree_hints.h
      Use Item_exists_subselect::enum_exec_method instead of
        PT_hint_subquery::enum_strategy.
      Renamed flag/flags() to args/get_args() in PT_qb_level_hint
    
    sql/parse_tree_hints.cc
      Renamed flag/flags() to args/get_args() in PT_qb_level_hint
      Updated some comments
    
    sql/opt_hints.h
      Renamed flag/flags() to args/get_args() in PT_qb_level_hint
      Changed get_complex_hints argument from uint to opt_hints_enum
      Some corrections to function docs
    
    sql/opt_hints.cc
      Renamed flag/flags() to args/get_args() in PT_qb_level_hint
      Changed get_complex_hints argument from uint to opt_hints_enum
      Restructured if-tests of Opt_hints_qb::sj_enabled_strategies() and
        Opt_hints_qb::subquery_strategy()
    
    sql/sql_lex.h
      Added doc for opt_hints_qb
    
    sql/sql_lex.cc
      Move initialization of opt_hints_qb to constructor
      Restructured if-tests of st_select_lex::subquery_strategy() and
        st_select_lex::sj_enabled_strategies()
    
    sql/sql_planner.cc
      Updated a comment
    
    sql/sql_resolver.cc
      In setup_tables(): No need to access hints through context.select_lex within
        st_select_lex.
    
    mysql-test/t/opt_hints_subquery.test
    mysql-test/r/opt_hints_subquery.result
      Change comment for query to explain why LooseScan can not be used
    
    WL#8244 Hints for subquery strategies.
    Cleanup
    
    Reformatted opt_hints_subquery to use uppercase of SQL reserved words.
    Removed trailing whitespace
    
    WL#8244 Hints for subquery strategies. Addendum#3
    
    Changes in [1;31moptim[mizer_switch after prepare should be reflected in available
    semijoin strategies for execution.  (This WL should not change how things
    work without hints.)  Hence, enable subquery strategies needs to be recomputed
    on every execution instead of during semijoin transformation.
    
    sql/sql_lex.h
    sql/sql_lex.cc
      Replaced sj_enabled_strategies() with update_semijoin_strategies().
      The new function iterates over all semijoin nests and recomputes the
      enabled strategies and store result in the corresponding nested_join object.
      Query block level hints are located from first table of semijoin nest since
      original select_lex was lost during semijoin transformation.
    
    sql/sql_[1;31moptim[mizer.cc
      Call update_semijoin_strategies() from JOIN::[1;31moptim[mize() so that it is
      recomputed on every execution.
    
    sql/sql_resolver.cc
      No longer compute available strategies during semijoin transformation since
      it will be recomputed later anyways.
      setup_tables() should only call adjust_table_hints() on first execution.
      Otherwise, one may adjust hints after semijoin transformation.  This would
      be wrong since tables need to refer to the hints of the original query block.
    
    sql/sql_parse.cc
      Initialize TABLE_LIST hint pointers in st_select_lex::add_table_to_list()
    
    mysql-test/t/opt_hints_subquery.test
    mysql-test/r/opt_hints_subquery.result
      Update tests to reflect changed behavior
    
    WL#8244 Hints for subquery strategies. Addendum#4
    
    Addressing review comments from Guilhem on Addendum#3.
    
    sql/sql_lex.cc
      No need to check table pointer.  There is always at least one table per nest
      Fixed comments
    
    sql/sql_lex.h
      Fixed typo
    
    sql/sql_parse.cc
      Removed the added initialization of TABLE_LIST members.  They are
      automatically zeroed by calloc.
    
     WL#8244 Hints for subquery strategies. Addendum#5.7
    
    Result changes when porting to 5.7 branch.

[33mcommit 1cde3fe3e7d06947544595effbebe9a96fafc9d3[m
Author: Oystein Grovlen <oystein.grovlen@oracle.com>
Date:   Mon Mar 16 12:25:55 2015 +0100

    WL#8244 Hints for subquery strategies.
    Part#1: Add [1;31moptim[mizer_switch for DuplicateWeedout strategy.
    
    Change the join order [1;31moptim[mization so that it also possible to turn
    off DuplicateWeedout semi-join strategy, and add an [1;31moptim[mizer_switch
    to use to turn it on/off.  Since for some join orders,
    DuplicateWeedout may be the only possible strategy, we allow for it to
    be selected even if strategy is turned off.  However, if we later find
    a join order that support an enabled strategy, we switch to this even
    if it is more expensive than the join order with DuplicateWeedout.
    For this to work, we will have to do some changes to how plans are
    pruned during greedy search.  See below for details.
    
    sql/sql_const.h
    sql/sys_vars.cc
      Add [1;31moptim[mizer_switch duplicateweedout
    
    sql/sql_planner.h
      Add Optimize_table_order::found_plan_with_allowed_sj.
      If DuplicateWeedout is disabled, this will be set to false until a
      plan without DuplicateWeedout has been found.
    
    sql/sql_planner.cc
      Optimize_table_order::greedy_search
        Initialize found_plan_without_dupsweedout.  It is not sufficient to set
        it in constructor since if greedy search is done in several iterations,
        due to setting of [1;31moptim[mizer_search_depth, it needs to be reset for each
        iteration.
      Optimize_table_order::consider_plan
        If DuplicateWeedout is disabled, check if current join order
        requires this strategy.
        If it does, only select it if it is cheaper than current best plan AND
        no plan without DuplicateWeedout has been found.
        If it does not, select it if is cheaper than current best plan OR
        no plan without DuplicateWeedout has been found.
        Added a line to Optimizer Trace to explain why a plan that is not
        cheaper is still selected.
      Optimize_table_order::best_extension_by_limited_search
        If DuplicateWeedout strategy is disabled, and current selected plan
        use DuplicateWeedout, do not prune other plans.
      Optimize_table_order::advance_sj_state
        If DuplicateWeedout strategy is disabled, do not select it when
        there are other applicable strategies for this join order.
    
    mysql-test/t/subquery_sj_firstmatch.test
    mysql-test/t/subquery_sj_loosescan.test
    mysql-test/t/subquery_sj_mat.test
        Use new [1;31moptim[mizer_switch to turn off DuplicateWeedout strategy for
        these tests.
    
    mysql-test/r/subquery_sj_firstmatch.result
    mysql-test/r/subquery_sj_firstmatch_bka.result
    mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
    mysql-test/r/subquery_sj_firstmatch_bkaunique.result
        No plans with DuplicateWeedout anymore
    
    mysql-test/r/subquery_sj_loosescan.result
    mysql-test/r/subquery_sj_loosescan_bka.result
    mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
    mysql-test/r/subquery_sj_loosescan_bkaunique.result
    mysql-test/r/subquery_sj_mat.result
    mysql-test/r/subquery_sj_mat_bka.result
    mysql-test/r/subquery_sj_mat_bka_nixbnl.result
    mysql-test/r/subquery_sj_mat_bkaunique.result
        Less plans with DuplicateWeedout, but some plans still use
        DuplicateWeedout since LooseScan and Materialization is not
        applicable for all queries.  Note that plans that still use
        DuplicateWeedout may have changed to a different plan using
        DuplicateWeedout since pruning is no longer done in such cases and
        a cheaper plan may now be found.
    
    mysql-test/suite/opt_trace/include/subquery.inc
    mysql-test/suite/opt_trace/r/subquery_no_prot.result
    mysql-test/suite/opt_trace/r/subquery_ps_prot.result
       Also turns off DuplicateWeedout for a test case.  Test now trace
       LooseScan which seems to be the original intention of the test.  It
       also shows the line added to [1;31moptim[mizer trace when one switch to a
       more expensive plan because the previous plan used a disabled semijoin
       strategy.
    
    mysql-test/r/index_merge_myisam.result
    mysql-test/r/mysqld--help-notwin.result
    mysql-test/r/mysqld--help-win.result
    mysql-test/r/[1;31moptim[mizer_switch.result
    mysql-test/suite/sys_vars/r/[1;31moptim[mizer_switch_basic.result
        Changes in result since output of [1;31moptim[mizer_switch now contains an
        additional switch.

[33mcommit f06b514cce9f3b0062265fbeaeeb69d404f902c4[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed Mar 25 14:56:43 2015 +0100

    Bug#19585938 Crash in get_full_func_mm_tree with null item_field->table_ref
    
    The problem may occur if we have a grouped query with a non-grouped
    subquery that contains a reference to an aggregate function, e.g. in
    the WHERE clause. The aggregate function must not reference any columns.
    
    Example query:
    
    SELECT (SELECT 1
            FROM t1
            WHERE SUM(1) < id
           )
    FROM t1
    GROUP BY col1+col2;
    
    The range [1;31moptim[mizer is attempting to [1;31moptim[mize the predicate SUM(1) < id.
    SUM(1) is represented by an Item_aggregate_ref object in get_mm_tree().
    Since aggregation is performed in the outer query, this item is an outer
    reference. get_mm_tree() will then call get_full_func_mm_tree() with
    the real_item() of the Item_aggregate_ref object as predicand, which is
    an Item_field. This Item_field represents a field in the temporary table
    allocated for grouping the outer table. But since this table has no
    assigned TABLE_LIST object, trying to calculate the map() for this table
    fails.
    
    However, this seems to be a legacy issue. Before WL#7540 was pushed,
    table map was 1. But this is actually an outer reference, so it was
    wrongly seen as a local table. Actually, being an outer reference, this
    item is const during evaluation of the inner query, so this is not a
    candidate for range [1;31moptim[mizer analysis at all.
    
    The fix is just to skip the call to get_full_func_mm_tree() if the
    argument representing the Item_aggregate_ref is an outer reference.
    
    The problem was also identified for other predicate types like BETWEEN.
    Fix has been provided for this too, together with test cases.
    
    (cherry picked from commit 6a71c3bf2c2f8f3e771173c38157fd42a6806cd3)

[33mcommit 1dcad1c1ea9b19ce58a15776e0628b4337f0c7e2[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Fri Mar 27 08:17:16 2015 +0530

      Related to bug#20390670 - Following adjustment for WL#7868
    
      - Small adjustments for more [1;31moptim[mial behavior
      - Revive IO burst at checkpoint around max_modified_age_sync
      - added bool option innodb_flush_sync = [ON|OFF] to choose the behavior.
        ON is the default to keep previous behavior for upgraded users
    
        static MYSQL_SYSVAR_BOOL(flush_sync, srv_flush_sync,
        PLUGIN_VAR_NOCMDARG,
        "Allow IO bursts at the checkpoints ignoring io_capacity setting.",
        NULL, NULL, TRUE);
    
      Reviewed by: Sunny Bains (sunny.bains@oracle.com)
      RB: 7849
      Created by: Yasufumi

[33mcommit eddee99225ffbfc8f6cbc33b720e1eb803e8e9d4[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Mar 24 16:44:58 2015 +0100

    Bug#20430526 ADDRESSSANITIZER: HEAP-BUFFER-OVERFLOW IN FIELD_BLOB::GET_KEY_IMAGE FLOAT8STORE
    
    The range [1;31moptim[mizer interpreted a hidden key part as an MBR index.
    (Hidden key parts are innodb primary keys, cannot be used as MBR)

[33mcommit 0b5894ce581d3a5c82847bd6e73448821b80de8c[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Tue Mar 10 12:36:30 2015 +0400

    Bug #20087571: 5.7 EASILY HITS ERROR 1436 (HY000): THREAD STACK OVERRUN ERRORS ON MANY EXPR'S
    
    This bug is a regression of WL7200: that WL introduced an unconditional
    recursive processing of AND and OR parse tree nodes during the
    contextualization, when the original parser had some conditional
    flattening of nested AND/OR expression.
    That affected very long recursive AND/OR expression: instead of the normal
    execution the parser failed with a parse error: ER_STACK_OVERRUN_NEED_MORE.
    
    This bugfix re-introduces an [1;31moptim[mization that flattens recursive AND/OR
    expressions at parse time (before the itemization/contextualization):
    
     (X1 AND X2) AND (Y1 AND Y2) ==> AND (X1, X2, Y1, Y2)
     (X1 AND X2) AND Y ==> AND (X1, X2, Y)
     X AND (Y1 AND Y2) ==> AND (X, Y1, Y2)
    
    and the same for OR.

[33mcommit 1598cc4889c584fa08c8092f447e41e34e8bea03[m
Author: Dmitry Lenev <dmitry.lenev@oracle.com>
Date:   Wed Feb 11 15:56:11 2015 +0300

    WL#8356 "Improve scalability by not acquiring unnecessary locks for internal temp tables".
    
    Do not acquire LOCK_plugin when doing ha_lock_engine() for builtin
    engines in production builds. This is possible thanks to the fact
    that in such builds for builtin plugins we don't do reference counting.
    To implement this [1;31moptim[mization introduced new builtin_htons[] array
    to be able easily identify builtin handlertons.
    
    This patch solves the problem with LOCK_plugin mutex showing up as
    a scalability bottleneck in workloads which often create internal
    temporary tables, such as 1-table Sysbench SELECT_DISTINCT/InnoDB
    test.

[33mcommit 9ea54a7358082ac8cc8160f5d04893563f643dde[m
Author: Benny Wang <benny.wang@oracle.com>
Date:   Thu Mar 5 01:43:32 2015 +0100

     Fixed bug#20590162: incorrect assumption about innodb
                            record length in [1;31moptim[mizer temporary tables
    
    For ha_innobase::max_supported_key_part_length(), the returned value relies on
    innodb_large_prefix. However, in innodb itself, the limitation on key_part
    length is up to the ROW_FORMAT. In current trunk, internal temp table's
    ROW_FORMAT is COMPACT. In order to keep the consistence between server and
    innodb, here we hard-coded 767 as the maximum of key_part length supported by
    innodb until bug#20629014 is fixed.
    
    (cherry picked from commit 04f46cef6b5d05d6a2e16fc44439608364774699)

[33mcommit db9d035e6431e1cc9e492394c2cf24cc5cdac7f4[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed Mar 4 14:44:27 2015 +0100

    Bug#17766653 CRASH IN ADD_KEY_FIELD Bug#20558891 HANDLE_FATAL_SIGNAL (SIG=11) IN QEP_SHARED_OWNER::KEYS
    
    Problem:
    SELECT a.a FROM `t1` `a`, t1 b
    HAVING 1 NOT IN (SELECT a.a FROM `t1`);
    - We resolve the clauses of the subquery: an Item_field is created
    for 'a.a', with depended_from=select#1.
    - the subquery is inside HAVING, and as usual all columns used by
    HAVING must be Item_ref, so an Item_ref is created, with
    depended_from=select#1; this Item_ref then goes through fix_fields()
    which finds 'a.a' is also present in the SELECT list of select#1 and
    thus makes Item_ref wrap the Item_field of the SELECT list of
    select#1, which has depended_from=NULL. Note that the first Item_field
    is thus dropped.
    - So far so good.
    - in-to-exists transformation then wants to build an equality of the
    form: left_expr==right_expr; when it does so, it uses
    right_expr->real_item() for the right side of the equality; right_expr
    is the Item_ref but its real_item() is the SELECT list element of
    select#1, which has depended_from=0.
    - thus, with this real_item(), we end up with 'a.a' on the right side,
    in the WHERE clause of the subquery, with depended_from=0
    i.e. considered as a non-outer, local column, which is wrong.
    - top query is completely [1;31moptim[mized
    - In [1;31moptim[mization of the subquery, we then look for Keyuse-s for this
    column, which is absurd; add_key_field() reaches to reginfo.join_tab
    which is NULL (because the outer query has already been [1;31moptim[mized, and
    WL#6042 zeroes the join_tab pointers at the end of [1;31moptim[mization), and
    we get a problem.
    The root cause is: we use real_item().
    
    Fix:
    - I first tried to use the solution of Bug 18014565
    (substitutional_item() at line 1996 of item_subselect.cc);
    but it does not solve the problem in ps-protocol mode; indeed in that
    mode, the Item_ref being a rollbackable one (runtime-created),
    substitutional_item() is equal to real_item()
    - removing real_item() isn't a solution either, for reasons explained
    in new comments in item_subselect.cc
    - So, until wl#6570 is implemented, two if()s are added, which are
    sufficient to fix all testcases of the two bug reports.
    
    (cherry picked from commit 9567b5cd4b69fae241ec16436a3c40748646fbea)

[33mcommit 1a4e8b44bac037173b21babe3d570a6ade8037cd[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Mar 3 09:53:56 2015 +0100

    Bug#20454833 PREFIX INDEX FIELD, ON AN INAPPROPRIATE DATA TYPE Bug#20506527 FAILING ASSERTION: !MBMAXLEN || !(PREFIX_LEN % MBMAXLEN)
    
    Both bugs have the same cause; I'll describe the scenario of the first
    one. Query is:
    select `t2`.`b`,`t1`.`b`,`t1`.`a`
    from `t2` inner join `t1` on 1 cross join `t2` `a`  on 1
    group by `t1`.`b`;
    
    In trunk as in 5.6, [1;31moptim[mizer decides to create a tmp table
    for the grouping. It will have 3 columns: t2.b (blob), t1.b (int),
    t1.a (blob). Notice the same name (but different source table) for
    the two first.
    An index is wanted, on t1.b (for grouping on it).
    In 5.6, it works.
    In trunk (if internal_tmp_disk_storage_engine=innodb), we go into
    ha_innodb.cc's create_index(), which has this loop to find which Field
    is related to the key_part (key_part describes the index which the
    [1;31moptim[mizer asks to create):
    
         for (ulint i = 0; i < key->user_defined_key_parts; i++) {
             KEY_PART_INFO*    key_part = key->key_part + i;
    ...
             Field*    field = NULL;
             for (ulint j = 0; j < form->s->fields; j++) {
    
                 field = form->field[j];
    
                 if (0 == innobase_strcasecmp(
                         field->field_name,
                         key_part->field->field_name)) {
                     /* Found the corresponding column */
    
                     goto found;
    
    See how it compares field names. key_part->field->field_name is "b" (t1.b);
    in our case, t2.b is the first column of form->s->fields, its field_name
    is "b", so we have a (wrong) match; then things go wrong as expected
    (field t2.b is a blob, though the index was supposed to be on an int (t1.b)),
    and we have:
    [ERROR] MySQL is trying to create a column prefix index field, on an
    inappropriate data type. Table name tmp/#sql_4d6e_0, column name b.
    
    The loop above was surely relying on the fact that in a user-created
    table, field names are unique. But this is not (yet) true for internal
    tmp tables.
    Fix: replace the loop with:
    field=form->field[key_part->field->field_index];
    I verified with dbug_assert + running mtr, that the loop above effectively
    finds "field" equal to the proposed form->field[etc], except in the bugs'
    testcases.
    
    (cherry picked from commit 742a75f5a487fa8c2d4aadd8d4c1fa19f098ecc2)

[33mcommit 7f166d9f0cb91fedadcb72fdb1a5d2cf3cc46bf4[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Mon Feb 16 11:33:12 2015 +0000

    WL#8313: Set ROW based binary log format by default
    
    Make the following compiled-in default change:
    binlog-format=ROW
    
    All test cases outside binlog or rpl test suites that required binlog
    and statement based binlog format (previous default format) were changed
    to force the required binlog_format, or else these test cases would never
    run.
    
    It was found and fixed an issue in sql_delete.cc were it was avoiding the
    use of the [1;31moptim[mized "delete_all_rows()" for binlog_format = ROW even when
    the binary log was disabled or the user disabled the binlog in the session.

[33mcommit a8b622b9eee3380ce66a2be45cfaee72dfb38165[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Tue Nov 11 19:40:34 2014 +0400

    WL#8016: Parser for [1;31moptim[mizer hints
    
    See WL page for details.

[33mcommit d9b81fb51387b8aee5b7bf6a343a4cbdd381257d[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Feb 27 12:32:06 2015 +0100

    Bug#20460208: !table || (!table->read_set || bitmap_is_set)
    
    The [1;31moptim[mizer relies on thd->mark_used_columns to have the value
    MARK_COLUMNS_READ for columns in WHERE conditions, etc.
    If this is not the case, fields are sometimes erroneously marked for
    write or not marked at all. In particular, if a failed statement of
    some "wrong" type preceded a correct statement, this problem might hit.
    In the bug report, a failed INSERT statement is followed by a DELETE
    statement.
    
    Followup patch: Make sure that we start all executions of prepared
    statements too with thd->mark_used_columns = MARK_COLUMNS_READ.

[33mcommit 952ccc32a33ac764d9cffb684e4447aea37de1e0[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Wed Feb 25 16:27:27 2015 +0100

    Bug#20593065 MULTI UPDATE RETURNS DIFFERENT DATA WITH ENGINE NDB
    
     - the UPDATE query is now [1;31moptim[mized differently and the ORDER of the
       records from t is undefined(the ORDER BY is ignored)
     - disable the [1;31moptim[mization in test like in so many other places
       and update the result files.
    
    (cherry picked from commit 58def93a271e6ecd267c5362cd0ec934a83ab33f)

[33mcommit 47fa02d3cbc5eae3e6959715255fe070c47a8649[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Wed Feb 25 16:27:27 2015 +0100

    Bug#20593065 MULTI UPDATE RETURNS DIFFERENT DATA WITH ENGINE NDB
    
     - the UPDATE query is now [1;31moptim[mized differently and the ORDER of the
       records from t is undefined(the ORDER BY is ignored)
     - disable the [1;31moptim[mization in test like in so many other places
       and update the result files.

[33mcommit 86ce784cfa959dbc9511abc42f9556cec66a7646[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Wed Feb 25 22:47:49 2015 +0530

    Bug #11747810: EXPLAIN EXTENDED SHOWS BOGUS VALUE FOR 'FILTERED'
                   COLUMN FOR LIMIT QUERY
    
    Problems:
    a- in EXPLAIN, "filtered" column could show values >100%.
    b- QEP_shared::m_rowcount is often a copy of
    POSITION::rows_fetched, but in rare cases it's something else; it's
    not well-defined
    c- it's only used by EXPLAIN, which itself has complex logic to decide
    what to show in "rows"/"filtered", depending on the access method.
    d- post-greedy-search [1;31moptim[mizations (like test_if_skip_sort_order,
    adjust_access_methods and make_join_select)
    change the access method without changing rows_fetched, or without
    changing POSITION::filter_effect. Leads to displaying an obsolete
    value in EXPLAIN.
    e- test_if_skip_sort_order, when it switches to index scan, and sees
    there's a small limit, decides to show this limit as "rows" in
    EXPLAIN; it is the root cause of the values >100%.
    This assignment to QEP_shared::m_rowcount is also the only use
    of this variable in the phase before we have QEP_TABs, so it is
    forcing us to share this variable between JOIN_TAB and QEP_TAB.
    f- because for an index, table or range scan,
    rows_fetched is usually set by calculate_scan_cost (called by
    best_access_path), thus
    includes filtering by constant conditions and filter_effect
    doesn't include this filtering, make_join_readinfo() had to do
    compensation calculations so that in EXPLAIN the user sees constant
    conditions in "filtered" and not in "rows"; however, sometimes
    rows_fetched was modified by (d), which didn't include constant
    condition filtering, then the compensation calculation did wrong.
    
    Fix:
    - for (b) QEP_shared::m_rowcount is removed and rows_fetched is always
    used, instead; as a consequence, EXPLAIN logic is simplified and (c)
    solved. A side-effect of this simplification is that
    semijoin-materialized tmp tables show "filtered=100" instead of
    "filtered=0", which is an improvement.
    - test_if_skip_sort_order() retains its logic, but we use rows_fetched
    instead of row_count. We also set the filter_effect to magic value
    of -2 stating that filter effect need not be calcuated for
     constant-condtion-filter. make_join_readinfo(), with more sane
     decision logic, solving (e) and (a).
    - post-greedy-search logic now sets filter_effect to a magic value
    (=-1), if it changes the access method; in make_join_readinfo() this
    triggers a complete recalculation of filter_effect, if we're
    explaining the statement. Solves (d).
    - post-greedy-search logic doesn't anymore change rows_fetched, so
    that constant-condition-filter compensation in make_join_readinfo()
    always works, as it is now sure that rows_fetched is always from
    best_access_path: the new rows_fetched is determined depending on the
    latest access method (chosen by post-greedy-search logic), and
    filter_effect is compensated. Solving (f). IF this fix is a problem
    for some reason, if post-greedy-search logic wants to set
    rows_fetched, then it is probably possible to do the filter_effect
    compensation right after greedy-search. Assuming post-greedy-search
    logic prefers to have, as input, rows_fetched without constant
    condition.
    
    Result files changes are described below; IMHO they are all correct.
    
    distinct.result
    explain SELECT distinct a from t3 order by a desc limit 2;
    rows=3 becomes 204, as test_if_cheaper_ordering calcultes the
    select limit wrongly. IMHO it is the problem that needs a fix
    in test_if_cheaper_ordering as it calculates select limit w.r.t
    the estimate from range [1;31moptim[mizer. For this case range [1;31moptim[mizer
    gives an estimate of 3 which is thought to be the number of qualifying
    rows after range is satisfied. But in this case it should have been
    204 as there is no range in this query.
    
    explain_for_connection*
    changes look reasonable, however some of them are difficult to
    inspect; the goal of these tests are to check equality between EXPLAIN
    and EXPLAIN CONNECTION, so if both plans change it's ok.
    Some changes are an effect that filter recalculation is done only by
    normal EXPLAIN.
    
    explain_other.result
    using LIMIT was wrong. For the last change:
    1       SIMPLE  t1      NULL    range   a       a       11      NULL    3       100.00
    ("rows" was 1), it's because we simplified the calculation of rows in
    opt_explain.cc for dynamic range; there's no best value to show, as
    during execution we switch back and forth between index scan (fanout
    3) and range scan (fanout 1).
    
    filter_single_*
    rows=LIMIT(=1) is correct.
    
    innodb_mrr*
    1       SIMPLE  t1      NULL    index   ind_parent_id   PRIMARY 4       NULL    7       57.14   Using where
    yes 57.14 is the filtering effect of IS NOT NULL (4 rows out of 7
    match this, if you look at the table's data).
    Line 732: another effect of EXPLAIN simplification, for
    ref-using-quick-with-filesort (see comment in opt_explain.cc)
    
    myisam_mrr*
    see innodb_mrr
    
    subquery_all*:
    again ref-using-quick-with-filesort
    
    subquery_sj*
    0->100 is good; for 26.53 to 19 it's again the change for "range
    checked for each record".

[33mcommit 7e963b54ab8fcd2f84d686f71b6b2bc86e23f729[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Feb 18 12:20:19 2015 +0100

    Bug#20458574 FIX ALL THE ARRAY BOUNDS WARNINGS
    
    Patch for 5.7
    Remove compiler warnings when building in [1;31moptim[mized mode:
    
    In strings/decimal.c fix numerous warnings of the type:
    array subscript is below array bounds [-Werror=array-bounds]
    This part of the patch is based on a patch
    from https://github.com/webscalesql/webscalesql-5.6
    That patch added some bounds-checks for loops, and worked for
    gcc4.8.3 but not for gcc 4.9.1 so it was re-written to unroll the loops instead.
    
    In other source files, fix numerous warnings of the type:
    variable XX set but not used [-Werror=unused-but-set-variable]
    variable XX may be used uninitialized in this function [-Werror=maybe-uninitialized]
    These were variables used only for debugging purposes,
    or variables not proven to be set by all execution paths.
    
    Compiled with gcc 4.7.2 4.8.3 4.9.1 in [1;31moptim[mized mode, with -Wall -Werror

[33mcommit ca02877576b4c8a434cc93fb3af9dbc9e52154f1[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Feb 13 16:00:24 2015 +0100

    Bug#20455184: Assertion failed: join_cond in [1;31moptim[mizer.cc
    
    The problem query performs a NATURAL LEFT JOIN between a derived table
    and a regular table, before applying a NATURAL LEFT JOIN with a
    second regular table. The first natural join has no common columns in
    the two tables, so there is no generated join condition. In such case,
    store_top_level_join_columns() (line 8541) will add TRUE as join
    condition to the inner table of the outer join, since an outer join
    should, by convention, contain a non-empty join condition.
    
    But the criterion for adding a join condition is:
    
        if (table_ref_2->outer_join &&
            !table_ref_1->join_cond() && !table_ref_2->join_cond())
          table_ref_2->set_join_cond(new Item_int((longlong) 1,1)); // Always true.
    
    As we see, it also checks whether the join condition of the outer table
    is non-empty, which is not the case here, since a WHERE clause of
    the derived table has been promoted to become the "join condition"
    of this table. Hence, the TRUE condition is not appended to the join,
    and subsequent code fails to recognize this as a proper outer join,
    which eventually causes an assertion.
    
    The fix is to modify the test to exclude the reference to
    table_ref_1->join_cond(), since this is not related to the outer join.

[33mcommit 30ad56aee52bf9b72a9c3d9eaa38977f3cfcd645[m
Author: Magnus Blåudd <magnus.blaudd@oracle.com>
Date:   Fri Feb 13 14:58:10 2015 +0100

    WL#6815 Adapt MySQL Cluster to 5.7
    
     - update the .result file for ndb_statistics0, where the
       "rowsfiltered" value for one EXPLAIN has changed
       from 5.00 to 10.00.
     - Differences like this are normally caused by [1;31moptim[mizer changes,
       the suspect for this one is "Bug19505175 REGRESSION IN Q21 OF DBT3
       TEST FOR WL7339" which touches lots of similar .result files.

[33mcommit 78158e80dbc7aac496076f141d2f9a8ca6990c16[m
Author: haixli <haixiang.li@oracle.com>
Date:   Thu Feb 12 01:13:07 2015 +0100

    Bug#20119743 ASSERTIONQEP_TAB->USE_ORDER() IN ENUM_NESTED_LOOP_STATE
                 EVALUATE_JOIN_RECORD
    
    Description:
    ------------
     - LooseScan algorithm requires 'sorted' retrieval of keys, so it can't
       use EQ_REF access type(calling DBUG_ASSERT(qep_tab->use_order()) in
       evaluate_join_record() function).
     - For a first inner table of semi join(IN-Subquery), MySQL believes
       LooseScan is the best by calculating its cost
       with semijoin_firstmatch_loosescan_access_paths().
     - If this first inner table only use a primary key, its access type is
       EQ_REF, which violates the constraint of LooseScan algorithm, so
       it triggers a ASSERT in execute phase.
    
     - 5.6.x believes Materialize algorithm is the best for this case.
     - Why does this test case use Loosescan algorithm in 5.7.x?
       It is because WL#6635 fixed semijoin_firstmatch_loosescan_access_paths(),
       it reduced the cost of LooseScan algorithm, so MySQL believes LooseScan
       is a best way.
    
    Fix:
    ----
    Allow a table to use EQ_REF and be [1;31moptim[mized by LooseScan, because EQ_REF can keep a unique order.
    
    Test case added.

[33mcommit 7363f343917f5213b840184594844511cc589afb[m
Author: Mattias Jonsson <mattias.jonsson@oracle.com>
Date:   Fri Feb 6 15:51:23 2015 +0100

    WL#6035: Native InnoDB Partitioning.
    
    Implementing partitioning natively in InnoDB, so it does not need
    ha_partition generic partition engine for supporting partitioning.
    
    ha_partition uses one ha_innobase handler for each partition which
    does not share common data with each other resulting in high resource
    usage which this WL fix.
    
    It also makes it easier to support other InnoDB features that partitioning
    currently does not support.
    
    A new handler ha_innopart is added, inheriting both ha_innobase (for InnoDB
    access) and Partition_helper (for partitioning support, see wl#4807).
    
    And to avoid a proxy object for Partition_handler (see wl#4807) it is also
    inherited.
    
    Also ha_partition is changed to inherit Partition_handler directly.
    
    As a result of this there is no longer any need for .par files
    for partitioned InnoDB tables, since InnoDB can use its internal
    data dictionary for finding partitions during rename and delete.
    
    The [1;31moptim[mizer estimate is also changed, especially for
    records_in_range, where all used partitions are checked instead
    of only the biggest ones.
    
    rb#7636
    Approved by Krunal and Annamalai

[33mcommit 1d05b689c09656529ac76f0bca546b0bc8c4ad8f[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Fri Feb 6 19:42:12 2015 +0530

    WL#6986: Make switching of index due to small limit cost based
    
    This worklog makes the [1;31moptim[mization of queries of the form
    "SELECT .. ORDER BY LIMIT"  cost based.
    
    Problem:
    The decision to use an index that gives ordered results is not
    cost based in all cases when [1;31moptim[mizing "order by limit" queries.
    This can result in a bad query plan for some queries.
    Analysis:
    Using the following example query from Bug#16522053
    SELECT patientId, time
    FROM t1
    WHERE patientId > 41 AND patientId < 43 AND time > 0 AND
    illness="Headache" ORDER BY time LIMIT 1;
    
    1. During the initial access method estimations 5.6 selects
    the same index and access strategy as 5.5 as the best:
    Index merge on (patientId, primary key)(note that this
    strategy will require file sort of the final result)
    
    2. During join [1;31moptim[mization this access method is kept as
    the best access method.
    
    3. After join [1;31moptim[mization, the [1;31moptim[mizer sees if there
    are further opportunities for [1;31moptim[mizing, and in this case
    there is: The query contains the following:
    order by time limit 1;
    
    Since there is a "limit 1", we check if there is an opportunity
    to avoid having to do both the complete join execution and
    the file sort. And in this case there is one such possibility:
    by using the KEY time (time).
    
    we "only" need to read entries from this index until the join
    has produced the first result record. So the [1;31moptim[mizer decides
    to use this index for the first table of the join.
    
    In most cases this is a very good [1;31moptim[mization since we both
    avoid the complete join and the file sort of the result.
    Unfortunately, in this case it seems like the query will produce
    zero results. As a consequence we will read the entire time index
    to its end - and for each index entry we need to look
    up the record in the clustered index.
    The difference between 5.5 and 5.6 is that in 5.5 many/most
    cases failed to select an index that could be used to avoid
    the sorting. This bug has been fixed in 5.6. And in a case
    like this one, this fix made the bad decision.
    
    Solution:
    test_if_cheaper_ordering() uses a cost based model to choose
    the index for ordering when limit is specified.
    Use the same in make_join_select() instead of the heuristic
    based decision which is present right now.
    
    Test file changes for myisam_explain_non_select_none, etc
    is because, the test requires to see that the same plan is chosen
    for DELETE and SELECT. With the previous queries, this is what is
    happening w.r.t select queries.
    Optimizer thinks that doing table scan is much cheaper
    than doing the range scan initially. But, later
    when we re-consider the plan because of small limit, even with the
    changes made to test_if_cheaper_ordering, cost is more than
    the cost for table scan.
    As a result, we do not create a range scan.
    But earlier, because cost based decision was not in picture,
    [1;31moptim[mizer used to create a range scan because the key gives the required
    sort order.
    
    As a result, limit value had to be changed to get range scan picked
    over table scan.
    Same for single_delete_update.test.
    
    This worklog only implements the cost based handling for select queries.
    Most single table insert/delete/update statements takes a simpler path
    through the [1;31moptim[mizer. For these queries the choice of using an
    "order by limit" supporting index is still not cost based in some cases.
    
    Existing issues that are not solved by this worklog:
    1. Cost calculation in test_if_cheaper_ordering() does not take into
    consideration the rows_evaluate_cost after the table scan/index scan
    is done. This will make a difference when the key also has a where
    condition that can filter many rows.
    2. Along with this, we also need to consider the cost for file sort, which
    is currently not taken into consideration.
    3. The current cost model in test_if_cheaper_ordering does not
    take into consideration that, when the key that gives order
    also has a range condition, then the total number of records
    that need to be scanned will be the number of rows that
    satisfy the range condition, not the number of records
    present in the table.
    This can be considered during cost calculation as, we are eventually
    going to create range scan if this is the case.
    4. select_limit is not reset after checking for an index. This can
    lead to bugs when there are multiple indexes for order by
    5. Check for a covering index is not complete. In case of innodb
    a secondary index always has access to primary key fields.

[33mcommit aa4c751d090332834e2baaad64dfa924f00f6ec0[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Feb 6 12:28:07 2015 +0100

    Bug#20086791 ASSERT `! IS_SET()` IN DIAGNOSTICS_AREA::SET_OK_STATUS ON DELETE (ER_SUBQUERY..)
    
    Don't continue execution in JOIN::[1;31moptim[mize()
    if make_join_select() returns an error.

[33mcommit e624bf43b93afac225fd2ad411dbae170f7e73f4[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Sat Nov 15 20:36:17 2014 +0100

    WL#7083 step 1.1. Preparation steps: Small simplifications.
    
    @extra/comp_err.c
    - This is the script that generates error messages from sql/share/errmsg-utf8.txt. With a malformed errmsg-utf8.txt it produced a not very helpful error message. Added a printout of the bad line to help debugging.
    
    @mysql-test/include/assert.inc, @mysql-test/include/master-slave.inc
    - Document existing, undocumented parameters.
    
    @mysql-test/include/show_rpl_debug_info.inc
    - Select from mysql.gtid_executed table.
    
    @sql/binlog.cc
    - Print reason for switching to row format to the debug trace.
    
    @sql/log_event.cc
    - Use encapsulated function instead of accessing owned_gtid directly.
    
    @sql/rpl_gtid.h
    - Add Checkable_rwlock::dbug_trace when DBUG mode is enabled. This flag controls whether all lock/unlock operations are written to the DBUG trace or not.
    
    @sql/rpl_gtid_execution.cc
    - Remove redundant statement 'thd->owned_gtid= gtid_next'. This was done already in gtid_state->acquire_ownership, called above this line.
    
    @sql/rpl_gtid_persist.cc
    - Add comment to suggest future [1;31moptim[mization.
    
    @sql/rpl_slave.h
    - Document undocumented function.
    
    @sql/sql_parse.cc
    - Add DBUG_ENTER/DBUG_RETURN.

[33mcommit 789a5cc659a2a0b22e78cfada053b3b8069b89d9[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Feb 4 16:48:04 2015 +0100

    Bug#20261601 Bug#20442572 ASSERTION FAILED: !FIRST_QEP_TAB->TABLE()->NO_KEYREAD
    
    Bug#75688 Assertion `!first_qep_tab->table()->no_keyread' failed.
    
    Regression from:
    WL#7123 Additional query [1;31moptim[mization for Fulltext Search
    
    Item_func_match::fix_fields() may set table->no_keyread= true
    to avoid using non doc_id indexes. This also affected subsequent
    queries against the same table.
    
    Solution: reset the no_keyread flag during query cleanup.

[33mcommit 91b65d99c4099979e6e39580829aff3948088bc2[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Mon Jan 12 10:41:38 2015 +0100

    WL#7168 API for estimates for how much of table and index data that is in memory buffer
    
    To improve the current cost estimation functions for reading data from
    a table or index, information about whether table or index data is in
    memory or need to be read from disk is needed. The worklog extends the
    handler with:
    
    a) Data structures where storage engines can provide estimates for
       how much of data from tables and indexes that are stored in a main
       memory buffer.
    b) An API that the [1;31moptim[mizer cost model can use to access this information.
    c) A default implementation/heuristic that can be used if the storage
       engine has not provided this information.
    
    In addition, the ha_heap::info() function for the Memory engine is
    extended to provide these estimates.
    
    Per file comments:
    
    sql/handler.cc:
      Default implementations for the functions that provide estimates
      about how much of tables and indexes are stored in a memory buffer.
    sql/handler.h:
      Extend the handler class with:
      a) data structure for the storage engines to provide estimates for
         how much of a table that is currently stored in a memory buffer.
         This is done by extending the ha_statistics class with a new
         variable.
      b) an API that the cost calculation functions can use for getting
         information about how much of tables and indexes are in a
         memory buffer.
    sql/key.h:
      Extend st_key with a new variable and functions that can be used by
      storage engines for providing an estimate of how much of the index data
      that is currently stored in a main memory buffer.
    sql/table.cc:
      Initialize the st_key::in_memory_estimate to a default value.
    sql/sql_tmp_table.cc:
      Initialize the st_key::in_memory_estimate to a default value.
    storage/heap/ha_heap.cc:
      Extend ha_heap::info() to provide estimates for how much of
      tables and indexes that are currently stored in a main memory buffer.
    unittest/gunit/key-t.cc:
      Added unit test for interface for setting and getting in-memory estimates.
    unittest/gunit/handler-t.cc:
      Added unit test for handler::table_in_memory_estimate() and
      handler::index_in_memory_estimate().

[33mcommit b8362f8f51a90ceadc3e0e6f8063db090108f8ae[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Feb 3 10:46:07 2015 +0100

    Bug#20460208: !table || (!table->read_set || bitmap_is_set)
    
    The [1;31moptim[mizer relies on thd->mark_used_columns to have the value
    MARK_COLUMNS_READ for columns in WHERE conditions, etc.
    If this is not the case, fields are sometimes erroneously marked for
    write or not marked at all. In particular, if a failed statement of
    some "wrong" type preceded a correct statement, this problem might hit.
    In the bug report, a failed INSERT statement is followed by a DELETE
    statement.
    
    This is a quick fix that should provide safeguards for DELETE and
    UPDATE statements. It sets mark_used_columns to MARK_COLUMNS_READ
    when needed and restores the old value after use.
    
    A more thorough fix with complete safeguard will follow later.

[33mcommit 85a06abb469497aa88086251b1fcf9b31748d5a1[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Feb 3 08:40:05 2015 +0100

    Bug#20456259 gcc runs out of memory, split item_geofunc_relchecks
    
    The build fails in [1;31moptim[mized mode on intel solaris.
    The reason is that gcc runs out memory.
    
    Split complex templates in two:
    declaration in one file, implementation with explicit instantiation in another.
    This avoids inlining, and out-of-memory problems.

[33mcommit 8f11af7734509821b654f2c330dff5374c89073c[m
Author: binsu <bin.x.su@oracle.com>
Date:   Thu Jan 29 18:30:11 2015 +0800

    Bug #20415831 - REMOVE INNODB_OPTIMIZE_POINT_STORAGE AWAY AND DISABLE
    THE FIXED LENGTH POINT
    
    We should remove the innodb_[1;31moptim[mize_point_storage away form the
    configuration list, and make POINT always mapped to DATA_GEOMETRY
    internally. Although mapping POINT to DATA_GEOMETRY would already
    eliminate the validation in InnoDB, we still remove the checking
    parts introduced by WL#6942 explicitly.
    
    rb: 7825
    approved by: Marko

[33mcommit c27561965d472aaa51397fffc5182fad608951a9[m
Author: BennyWang <benny.wang@oracle.com>
Date:   Mon Jan 26 14:57:28 2015 +0100

    Fixed bug#20406510: WL411:VALGRIND WARNINGS WITH COUNT DISTINCT QUERY
     ON VIRTUAL GC VARCHAR COLUMN
    
    This bug is because when calling handler::ha_index_first, mysql tries
    to update all of virtual generated columns in read_set. However, such
    an index-scan is used for [1;31moptim[mized MIN function(get_index_min_value)
    and it's a covering-index scan. It doesn't get all of the base columns'
    value virtual generated columns depend on. Therefore, if trying to update
    the virtual generated columns, valgrind will report the server uses
    uninitialized value.
    
    Solution:
    Before updated virtual generated columns, we need check whether the
    index-scan gets all the columns' value the virtual generated columns
    depend on. If it does, we update the values. Otherwise, we skip to update.

[33mcommit 5c04f373d94b552aa6bb8507fa625d93776d6da2[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Jan 21 16:13:11 2015 +0100

    Bug#20219846 EXPLAIN FOR LIMIT QUERY SHOWS FILESORT BUT EXECUTION IS DONE WITH INDEX ONLY
    
    The explain output for the following query:
    
      SELECT *
      FROM t1 JOIN t2 ON t1.i1=t2.i1
      WHERE t2.i2 > 3
      ORDER BY t1.i1 LIMIT 20;
    
    says the query will be done by doing file sort but when it is executed
    it is done by reading from the "order by" friendly index on t1.i1.
    
    The switch to use the "order by" friendly index on t1.i1 is done in
    test_if_skip_sort_order() by calling test_if_cheaper_ordering(). The
    same test is done in both the explain case and in the execution of the
    query. The choice of whether to switch index in
    test_if_cheaper_ordering() takes the filtering effect of the tables
    into account when computing the expected number of rows that is
    needed to read before having produced enough rows to fulfill the
    limit criterium. The cause for doing a different choice is that the
    filtering effect on the last table is different in these two
    cases. When doing explain of a query we always calculate the condition
    filtering effect for all tables. When executing the query, we only
    calculate the filtering effect of the last table when it is actually
    needed. In most cases it is never used but in the case of "order by
    limit" [1;31moptim[mization it is used (but was not computed).
    
    The fix for this problem is to always calculate the condition
    filtering effect the last table if the query has an order by or a group
    by clause and a limit clause.
    
    Changes to existing test cases:
    
    myisam_explain_*_non_select_*:
      The printout of status variable from execution now corresponds to the
      explain of the same query.
    single_delete_update:
      Adjusted the limit for some queries in order to produce the same
      query plan.
    filesort_pq:
      Re-recorded due to changes in rows and filter estimates.

[33mcommit 3b3f88f336df03b16e4b3a4cf5f1f3d044e13288[m
Author: BennyWang <benny.wang@oracle.com>
Date:   Mon Jan 26 08:34:23 2015 +0100

    Fixed bug#20406510: WL411:VALGRIND WARNINGS WITH COUNT DISTINCT QUERY
     ON VIRTUAL GC VARCHAR COLUMN
    
    This bug is because when calling handler::ha_index_first, mysql tries
    to update all of virtual generated columns in read_set. However, such
    an index-scan is used for [1;31moptim[mized MIN function(get_index_min_value)
    and it's a covering-index scan. It doesn't get all of the base columns'
    value virtual generated columns depend on. Therefore, if trying to update
    the virtual generated columns, valgrind will report the server uses
    uninitialized value.
    
    Solution:
    Before updated virtual generated columns, we need check whether the
    index-scan gets all the columns' value the virtual generated columns
    depend on. If it does, we update the values. Otherwise, we skip to update.

[33mcommit 67e977f9ebdeed57276c1314b35a952081725459[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Fri Jan 9 13:33:17 2015 +0800

    Bug#20125560-SEGV IN FTS_OPTIMIZE_THREAD, FTS_IS_SYNC_NEEDED
    
    The access violation happens in case that table is freed but it's
    not removed from fts [1;31moptim[mize queue. So the solution is below:
    1. Guarantee table is removed from fts [1;31moptim[mize queue before freed;
    2. Add/delete slot in fts [1;31moptim[mize queue by checking table other
    than table->id(because of BUG#17373659).
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 7594

[33mcommit 3b07407b51219c412ba795fcc6c568dbb1e21db4[m
Merge: f3e7d27ba61 ab845f24aba
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jan 6 18:08:25 2015 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      Bug#20031761 ASSERTION `SCALE >= 0 && PRECISION > 0 && SCALE <= PRECISION' FAILED
      Bug#20304224: FIX #INCLUDE DEPENDENCIES IN ITEM.H
      Fix mysql_config_editor test after new copyright year
      Bug#20304224: FIX #INCLUDE DEPENDENCIES IN ITEM.H
      WL#6737: InnoDB: Enabling InnoDB temp-tables as          default internal SE for MySQL Optimizer
      Disable three RPL tests in Valgrind since they are too unstable.
      Bug#20307193 - INNODB_FTS.NGRAM_1 UNSTABLE ON PB2
      Correcting copyright text.
      Followup patch of wl7740 for fixing asan test and valgrind test failure.
      Update grammar to include ngram parser changes (wl6607).
      Bug#20302351: MOVE SELECT_RESULT_INTERCEPTOR SUBCLASSES FROM SQL_CLASS.H
      Bug#20303205 INNODB FAILS TO UPDATE UPDATE_TIME AFTER XA COMMIT
      Bug #19670915 GROUP_CONCAT_MAX_LEN=18446744073709547520 NOT ACCEPTED IN MY.CNF
      - Raise version number after cloning 5.6.23 - Updated the copyright year in the welcome message for MySQL
      Raise version number after cloning 5.5.42
      WL#7420 Geometry Collection Support
      Bug#17238670 MAIN.FUNC_GCONCAT SPORADICALLY FAILING ON PB2 WITH RESULT MISMATCH
      Bug#20294158 INCORRECT EXPRESSION
      Followup for wl7740, disable test case check_rtree for release build.
      WL#7740: InnoDB GIS: Enhance Check Table for InnoDB Spatial index
      Follow-up to WL#6607 : InnoDB FULLTEXT SEARCH: CJK support
      Fixed some comments
      Fixed commandline to run with 64K. 16K is the default and does not need special run
      Bug#20075406 TEMPLATE-BASED QUEUE/HEAP IN BOUNDED_QUEUE
      Bug #19788198         MUTEX CONTENTION CAUSED BY DUMMY TABLE/INDEX CREATION/FREE
      Bug #20043707 RENAME FTS COMMON TABLE DURING ALTER RENAME FAILURE.
      WL#7800 : PERFORMANCE SCHEMA, SETUP_ACTORS ENABLED COLUMN.
      Bug#19820550 : DISABLE SSL 3.0 SUPPORT IN OPENSSL
      Bug#19601484: REMOVE MYSYS THREAD LOCAL USAGE FROM THR_LOCK.C
      Bug#19729545: REMOVE UNNEEDED CMAKE CHECKS AND #IFDEFS IN 5.7.6
      Bug#20285744 MISSING VA_END'S The return was added as part of WL#6205 refactoring.
      Correct a stale comment that was missed in
      Added keep-my-cnf to mysql_install_db
      Follow-up to Bug#19514950 CLEAN UP TESTS THAT KILL OR RESTART THE SERVER
      WL#6607 : InnoDB FULLTEXT SEARCH: CJK support
      Bug#17326406 INSTABILITY IN A TEST FOR RANGE SCAN
      Bug#20201864 : UPGRADE TO YASSL 2.3.7
      WL#7315 Optimizer cost model: main memory management of cost constants
      Bug#19928622: ASSERTION `! IS_SET()' FAILED. | ABORT IN DIAGNOSTICS_AREA::SET_OK_STATUS
      Bug #19363615 INNODB.LOG_FILE FAILS SPORADICALLY ON PB2
      Bug#19941492 - ASSERTION IN TRX0ROLL.CC LINE 275 | (SIG 6)                TRX_ROLLBACK_LAST_SQL_STAT_FOR_MYSQL
      Bug#20266847 : PURGE THREAD SHOULD CHECK PURGE_SYS->STATE AFTER EVERY BATCH
      Bug#20264865   WITH --IDB ALWAYS,   N TEST DOES NOT RUN RECOVERY VERIFCATION STEP AFTER A FAILURE   Relax relevant conditions.
      Bug#19505175 REGRESSION IN Q21 OF DBT3 TEST FOR WL7339
      Bug#18661573 : MIN VALUE FOR STORED_PROGRAM_CACHE TOO HIGH,                DEFAULT MAX PACKET = HUGE SP MEMORY
      Bug#20041860: SLAVE ERROR WHEN DROP DATABASE
      This is follow-up for bug#19792203 - FAILING ASSERTION: TRX->IS_DD_TRX == FALSE WHEN KILLING QUERY WITH CONVERT_TZ
      enhancement Bug#20257520 -   N TESTS: IMPLEMENT SUPPORT FOR TABLE COMPRESS RELATED OPERATIONS n/n_ct/ibtest_ctd_w.pl: add new options:   --utpc: Boolean, enable transparent page compression, disabled by default.   --atpcpm N: 1000-based probability of COMPRESS changing ALTER TABLE   --optpm N: 1000-based probability of executing OPTIMIZE TABLE   Defaults for the numeric options are 1.
      Fix code formatting.
      Bug#19792203 - FAILING ASSERTION: TRX->IS_DD_TRX == FALSE WHEN KILLING QUERY WITH CONVERT_TZ
      This is a fix for Bug#11745851 - MYSQL_TZINFO_TO_SQL DOES NOT WORK WITH STRICT_ALL_TABLES
      WL#7160 - Move plugin and servers tables from MyISAM to transactional storage.
      Bug #20144839 AFTER UPDATING TO MYSQL 5.6.22 SERVER           CRASHES ON EVERY START ATTEMPT
      WL#7440: Moving binlog event decoding into separate package.
      Bug #19386426 SHUTDOWN HANGS: CLIENT THREAD IN A LOOP                         AROUND SRV_CONC_ENTER_INNODB()
      Fix testcase failure due to extra backticks around column name
      Bug#20180331: HA_NDBCLUSTER::CAN_SWITCH_ENGINES() NEEDS CREATE_INFO WHEN CALLED FROM MYSQLD.
      Bug #19363615 INNODB.LOG_FILE FAILS SPORADICALLY ON PB2
      Fix test failures on pb2: innodb_zip.4k and innodb_zip.8k
      Bug#20059644: WL6737:CRASH AT OPERATOR<< IN DICT/DICT0MEM.CC
      - WL#6737:  InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737:  InnoDB: Enabling InnoDB temp-tables as default internal SE for MySQL Optimizer   - Re-recording TCs to cover out for switch from MyISAM to InnoDB SE.
      - WL#6737:  InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737:  InnoDB: Enabling InnoDB temp-tables as default internal SE for MySQL Optimizer   - Removed reduntant setting of innodb as storage engine.   - Explicitly set big_tables in all cases that ensure use of InnoDB as SE for Optimizer.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Merge caused build failure (stale function left over). Corrected it.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Re-recorded test-cases after modifying the query to use order-by clause.     (ORDER_BY Clause was added in previous checking but TC was not re-recorded      then)
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Rectified error message to use proper variable to display table name.
      Bug#19191377: WL#6737: NOT NULL COLUMN IS BEING PROJECTED AS               NULLABLE IN TEMPORARY TABLES Introduced more details on such a bug fix:
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Address review comment.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Post merge (from mysql-trunk) changes in test-cases.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Avoid releasing lock for intrinsic table as we don't take lock on such tables.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Addressed review comments.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Fixed valgrind warning. For temporary tablespace too we don't initialize     complete page as we do for system tablespace. Expanded the exception     clause to include temporary tablespace too.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Enforced explicit ordering for query that try to select result from     information_schema.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Corrected comment/documentation for function.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Re-recording of TC after making InnoDB default SE.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Missed removing sys_var test-case for innodb_create_intrinsic.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Removed innodb_create_intrinsic as this WL will enable use of InnoDB     as default SE for Optimizer.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      Bug#19191377: WL#6737: NOT NULL COLUMN IS BEING PROJECTED                        AS NULLABLE IN TEMPORARY TABLES Althought there is an error on Nullability when tranforming outer-join into inner-join, after analysis, the root cause for this bug is the logic to compute the keyinfo->key_length is not completed in wl6711.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Removed query producing in-consistent result because of case sensitive     file-system issue.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Remove Little Endian [1;31moptim[mization as it is not compatible with Sparc     architecture.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Corrected length reading for varchar column.   (Length encoded by MySQL native format is always in little endian    format ir-respective of architecture.)
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Re-recording of test-case to keep innodb as default SE.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer     - Fixed incompatibility in forward declaration of dict_table_t to avoid       Windows compilation failure.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Use MyISAM as SE for some selected test-case (to investigate in due-course)
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - For intrinsic table trx are not started. Avoid checking for trx abort     state if trx is not started.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Improved error handling.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Fixed incompatibility in forward declaration of dict_table_t to avoid     Windows compilation failure.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - After recent addition of new data-type POINT and VAR_POINT expanded     assert in function for intrinsic table to take-care of these data-types.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Fixed the merge conflict issues.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Missed handling of DATA_GEOMETRY case. Added it now.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Optimized write_row by early bypassing the checks not needed for intrinsic     table.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Corrected condition to set column length while creating node->row for     intrinsic tables.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Avoid memset for new page created from temp-tablespace.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Ported to retain Litter Endian format for all data-types of intrinsic     tables.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Disable WL TC for 16K as it tries to print [1;31moptim[mizer plan which differ     in terms of stats if run for 4K causing TC to fail.   - Re-recording Windows TC post enabling InnoDB as default SE.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer   - Re-recording failing test-cases due to use of innodb as default SE     for [1;31moptim[mizer.
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer
      - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for   MySQL Optimizer

[33mcommit 9eae0edb7a8e4004328e61157f5f3b39cebe1b2b[m
Author: Allen.Lai <zheng.lai@oracle.com>
Date:   Mon Jan 5 12:15:22 2015 +0800

    WL#7740: InnoDB GIS: Enhance Check Table for InnoDB Spatial index
    
    We enhanced check table for spatial index.
    1. Check R-tree is valid or not;
    2. Check R-tree row count which should match with cluster index.
    
    We also [1;31moptim[mized the non-leaf level node store of R-tree, by
    removing pk field from them.
    
    Reviewed-by: Annamalai <annamalai.gurusami@oracle.com>
    Reviewed-by: Bin <bin.x.su@oracle.com>
    
    RB: 7366

[33mcommit 5a473a621d4149b4163c396998557ee759f8313f[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Nov 28 14:57:54 2014 +0100

    Bug#19505175 REGRESSION IN Q21 OF DBT3 TEST FOR WL7339
    
    WL#7339 started to use more correct records per key estimates from
    InnoDB. This caused the cost estimate for the original query plan
    to become four times higher than previous, which caused another
    query plan to be selected.
    
    The new selected query plan does a table scan on the orders table.
    This table has 1.5 million records. On this table we have the
    following condition:
    
      orders.o_orderstatus = 'F'
    
    This is used for calculating the condition filter effect for this
    table. With the current guestimate for equality conditions of 0.005,
    this caused the estimated number of partial rows to be produced from
    this table to be only 7500. In reality, 730.000 rows were produced.
    
    The cause for this very wrong estimate is that the o_orderstatus
    column only contains three distinct values and almost half of the
    records have the value 'F'.
    
    To make the condition filter produce a better (more conservative)
    estimate for cases like this and to reduce the likelihood of
    similar regressions, the fix for this problem is to increase the
    condition filter constant for equality estimates to 0.1. With this
    we estimate that 1 of 10 records will pass the filter (instead of
    only 1 out of 200).
    
    Changes in tests:
    
    @ mysql-test/r/compress.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/derived.result
       Mostly changes to filtered estimates. One change in query plan.
       This query now gets the same plan as it had before WL#6635.
    @ mysql-test/r/ds_mrr-big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/eq_range_idx_stat.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/explain_for_connection_rqg_json.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_for_connection_rqg_trad.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_other.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_small.result
       Mostly changes to filtered estimates. One query has
       changes to its query plan: changes from using FirstMatch
       to do materialization.
    @ mysql-test/t/filter_single_col_idx_small.test
       Added test to verify that the filter estimate for basic
       filter constants is not less than one row
    @ mysql-test/r/greedy_[1;31moptim[mizer.result
       Mostly changes to filtered and cost estimates. Eight queries
       has changes in query plans. Four of these changes back to
       the query plan they had before WL#6335 was pushed. The four
       last had similar changes in query plans as for when WL#6635 was
       pushed.
    @ mysql-test/r/greedy_search.result
       Mostly changes to filtered estimates. Two queries has
       changes to its query plan. One of these are returned to
       what it was before WL#6635 was pushed. The other has
       similar changes to as what was introduced by WL#6335.
       The number of partial query plans for some queries are
       increased. With the exception of one of these, all
       new numbers are below what it was before WL#6335.
       The one the has a very high increas in the number of
       partial query plans, changes from 22201 to 5799004.
       Before WL#6635 this query considered 735518 partial
       query plans.
    @ mysql-test/r/group_by.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap_hash.result
       One change in query plan. The new plan is more similar
       to what the query plan was before WL#6635 was pushed. It has
       the same join order but uses table scan on the second
       table instead of ref access as it did before WL#6635.
    @ mysql-test/r/index_merge_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_intersect_dml.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_myisam.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_none.result
       Changes in filtered, rows and estimates in explain output.
    @ mysql-test/r/innodb_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bkaunique.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_nojb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_none.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/named_pipe.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/negation_elimination.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/order_by_all.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_icp_mrr.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_none.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/partition_locking.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/partition_pruning.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr_cost.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/shm.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl_compress.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_sj_all.result
      Mostly changes to filtered, rows and cost estimates. Three queries
      has plan changes. Two of these returns to what the plan was before
      WL#6635. The last has changes that makes the plan look more like
      it was before WL#6635 but not identical.
    @ mysql-test/r/subquery_sj_all_bka.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Five of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_mat.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Six of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_nosj.result
      Changes to filtered and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka_nixbnl.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bkaunique.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subselect_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/type_temporal_fractional.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/wl6711_heap_to_disk.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/innodb/r/innodb_lock_wait_timeout_1.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/general2_no_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/general2_ps_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_no_prot.result
      Changes in filtered and rows estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_ps_prot.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_no_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_myisam.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/perfschema/r/batch_table_io_func.result
      Changes in filtered estimate in explain output.
    @ sql/item.h
      Change the value for the condition filter constant COND_FILTER_EQUALITY
      from 0.005 to 0.1.
    @ unittest/gunit/item_filter-t.cc
      Change in unit test for condition filter for IN lists:
      Reduced from having six values in the IN list to four
      values. The reason is that with six values in the
      IN list the calculated condition filter will be larger
      than 0.5 and then rounded down to 0.5.

[33mcommit 57b4da607c1b829fb9ce721f4b3589b41ce5c95b[m
Merge: 6e6d8c1423a d2f276138e8
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Dec 22 11:10:53 2014 +0200

    Merge remote-tracking branch 'origin/mysql-trunk' into mysql-trunk-wl5889
    
    * origin/mysql-trunk:
      Bug #19815702 TIS620: CRASH WITH MULTI TABLE DELETE
      Bug#20246211 - IMPLEMENT --CREATE_OPTION IN IBTEST_CTD_W.PL It allows passing arbitrary clauses for CREATE TABLE.
      Bug#20221262: MEMORY LEAK IN MYSQLTEST AFTER UNCLEAN SHUTDOWN OF THE SERVER
      Bug#20221262: MEMORY LEAK IN MYSQLTEST AFTER UNCLEAN SHUTDOWN OF THE SERVER
      Bug#19940297 UPGRADE TO BOOST 1.57
      Revert "WL#7706 : SSL cert and key generation for MySQL Community"
      bug1984591 (post push) valgrind failure fixing.
      WL#7706 : SSL cert and key generation for MySQL Community
      Bug#20111105 CRASH IN BTR_CUR_LATCH_LEAVES
      WL#5757 :  InnoDB: Support Page Sizes 32k and 64k
      WL#7440: Moving binlog event decoding into separate package.
      Follow-up fix to Bug#19694618 DEFINE A DATA TYPE WRAPPER FOR PRETTY-PRINTING TABLE NAMES
      Bug #20065517 MEMORY LEAK OF 8160 BYTES IN MYSQL_STMT_PREPARE() API
      WL#7440: Moving binlog event decoding into separate package
      Bug #19845913 MTS: WITH MSR, SLAVE CRASHES ON SHUTDOWN COMMAND AT RPL_MTS_SUBMODE.H:35
      BUG#19704710: TWO DIFFERENT REPLICATION CHANNELS WITH SAME (HOST, PORT) ARE POSSIBLE
      Bug#20225524 TESTS SHOULD USE AT LEAST 60 SEC TIMEOUT FOR SHUTDOWN_SERVER COMMAND
      BUG#20029625 - HANDLE_FATAL_SIGNAL (SIG=11) IN DICT_MEM_TABLE_COL_RENAME_LOW
      Bug #20065461 MEMORY LEAK IN MYSQL_LIST_FIELDS() API
      WL#8216: Deprecate and remove the sync_frm sysvar
      Fix result content mismatch after merge from mysql-5.6
      BUG#20080942 - ADAPTIVE_HASH_SEARCHES_BTREE NOT UPDATED
      BUG#19665003: RPL.RPL_MULTI_SOURCE_BASIC UNSTABLE ON PB2
      BUG#20080942 - ADAPTIVE_HASH_SEARCHES_BTREE NOT UPDATED
      WL#7440: Moving binlog event decoding into separate package.
      Fix test failures like blob_redo on pb2
      wl5757: Skipping the testcases running in parallel
      Empty commit for gcov
      renamed:  libbinlogevents/include/binary_log_funcs.h ->  libbinlogevents/export/binary_log_funcs.h
      Simply max record size check when [1;31moptim[mistic update
      wl5757: Reolved an issue of build and install locations
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      wl5757# Test: adding a new inc file, reolved windows problem and experiment with parallel run on PB2
      wl5757# Test:Windows platform issue was resolved ,experiment with parallel run on PB2.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding
      WL#7440
      Fix innodb.innodb_update_time failure on pb2
      Force pb2 rebuild
      Fix pb2 failures with 64k page size: innodb.blob_redo innodb.innodb-wl5522-1 i_innodb.innodb_bug14529666 innodb.innodb-import-partition innodb.innodb_update_time
      WL#7440: Moving binlog event decoding into separate package
      perl code added for platform independent for removeal of files
      Modify default.push to run mtr test on 32k and 64k page sizes
      Fix innodb_zip.16k failure on pb2
      Fix pb2 failure of innodb.innodb, innodb_zip.16k and innodb.innodb_trx_weight
      WL#7440: Moving binlog event decoding into separate package.
      wl5757:2014-11-11:Fix fb compatible problem with UNIV_EXTERN_STORAGE_FIELD
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440:Moving binlog event decoding into a separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package.
      WL7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Moving binlog event decoding into separate package.
      Wl#7440: Moving binlog event decoding into separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7440: Moving binlog event decoding to a separate package.
      WL#7440: Moving binlog event decoding into a separate package.
      WL#7546: Moving binlog event decoding into a separate package
      WL#7546: Moving binlog event encoding into a separate package
      WL#7440: Moving binlog decoding to a separate package.
      WL#7440: Moving binlog decoding into a separate package.
      WL#7440: Moving binlog decoding into a separate package.
      WL#7440: Moving binlog decoding to a separate package
      WL#7440: Moving binlog decoding into separate package
      WL#7440: Moving binlog events decoding to a separate package.
      Wl#7440: Moving binlog decoding into separate package.
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Fixing Windows compilation error
      WL#7440: Fix for Windows build failures
      Fixing compilation error on Solaris:
      Error: The function "strndup" must have a prototype. Error: The function "memcpy" must have a prototype.
      Add dependency on GenError, to avoid race conditions.
      When building with -DWITHOUT_SERVER=ON and SunStudio, we need <stdlib.h> rather than <cstdlib> to find the definition of size_t.
      Add -m64 to link command when building libbinlogstandalone.so
      WL#7440: Moving binlog event decoding into separate package
      WL#7440: Addressing Comments from reviewers
      WL#7440: Post-push fix
      Wl#7440: Fixing valgrind error:        Cause:          The variable server_version in class Format_description_event is initialized          either by a global variable or through the parameter server_ver passed          to the Format_description_log_event constructor.          In the cases when it is initialized by server_ver, some part of the          variable server_version remains uinitialized.
      Wl#7440: Moving decoding into a separate package (post-push fix)
      -Adding comments for all the binlog event class constructors. -Removed the check for finding the definition of macros min and max in the library  libmysqlclient and undefining them, as they are not required. -Added files decoder.cpp & decoder.h
      WL#7440: Addressing comments from Reviewers.
      Addressing review comments from Mats and Shubhangi on the patch for Removing the use of global variable server_version from libbinlogevents
      WL#7440: Addressing comments from reviewers
      Addressing review comments on the patch for Moving the code for advancing the event buffer to point to the beginning of post-header from all event constructors.
      Addressing review comments on the patch for removing the is_valid virtual method.
      WL#7466: Post push fix, resolving build issues on windows-32 bit OS
      WL#7440 : Fixing Linker errors on Windows-32bit OS
      - The bapi_memdup method shouldn't have the dest pointer as const - And the dest pointer should be passed to memcpy and not its address.
      WL#7440: Updating Query_event and Query_log_event
      WL7466: Renaming the directory libbinlogapi to libbinglogevents
      WL#7466: Renaming the directory libbinlogapi to libbinlogevents
      Addressing review comments from Mats about changing the comment.
      Fixed failing test case, i_binlog.binlog_large_row
      WL#7440: Post push fix: Addressing comments from reviewers
      - Enabling compilation of free standing version of the library libbinlogevents   - The library name is updated from libbinlogevent to libbinlogevents   - Added comments and fixed indentation   - Added getters for Rows_event, Table_map_event and Incident_event members
      Fixing failing test rpl_known_bugs_detection
      ISSUE: Earlier is_valid was a pure virtual method declared inside the class Log_event and implemented by all the event classes.
      Issue: Fixing valgrind errors   ------
      ISSUE ===== The previous implementation for fetching the type_code for an event was: get_type_code() was a pure virtual function in the class Log_event, which was implemented by all the event subclasses, and returned the enum value, Log_event_type. To fetch the type_code, a call to the method was made using a Log_event pointer.
      Addressed review comments: -Addition of one more structure Uuid_parent in binlog_event.h which is the parent structure for Uuid struct in rpl_gtid.h -Two elements are moved from struct gtid_info to the struct Uuid_parent, namely   -bytes_to_copy   -uuid_buf
      Summary: --------    Changing the header guards to the format XXX_INCLUDED for libbinlogevent
      Fixing failing test cases on Solaris platform
      Fixing the Solaris build failure and test failures:
      - Removed the following:  Binary_log.cpp : Contained the error messages which not generated from                   libbinlogevent anymore. It is, therefore, removed.
      Addressed review comments.
      1. Fixing build errors on Solaris
      - Changed the name of the library generated from libbinlogapi to libbinlogevent  - Removed dependency on dbug library, when the library is compiled independently  - Removed the files protocol.h and protocol.cpp, for they are not a part of the    decoder  - Removed the files resultset_iterator.h and resultset_iterator.cpp, for they    are a part of the bindings and not the Decoder
      Resolving build issues on Windows-64bit
      Description: ------------
      Fixing Valgrind Errors
      Removing the content handlers from the Decoder code.
      - Adding header file for LOAD_DATA_INFILE events - Fixing failing test cases
      The patch addresses the following:
      - Adding documentation for User_var_event and Rand_event - Changed the name of method get_version_product to get_product_version as it make more sense that way. - Fixing the Solaris compiliation failure
      Fixing the tests failures arised after pushing Ignorable_event and Rows_query_event
      Address comments from reviewers
      Addressing review comments
      Addressing review comments.
      7763d4167ec47828853cf2f36e48cd2e0f032d23
      Description:  ============  - Fixing build errors on Solaris-x86, 64 bit machine  - Adding zlib include paths
      Post Push Fix: Resolving Valgrind errors on WL branch
      Post Push Fix: Fixing failing tests on Windows
      Post push fix
      BAPI 82: Linking binlog event processing library to the MySQL server
      BAPI 85: Moving Decoding of binary log events in a separate package
      Fixing the valgrind error on pb2:
      Fixed failing tests on solaris
      Fixed failing test cases, and valgrind errors also
      Fixed failing tests
      -Fixing test failures -Removed the file access_method_factory.h as it is not needed
      -Fixed the failing test cases on pb2 -Missed to add the call for ctor of Binary_log_event through Gtid_event ctor,  added now
      Post Push Fix: Resolving linker errors on Windows in WL branch
      Replaced uint*korr with le*toh
      Fixing compilation error when compiled with GCC 4.7+
      BAPI 85: Moving Incident_log_event
      BAPI 85: Moving Gtid_log_event to libbinlogapi
      BAPI 85 : Moving Previous_gtids_log_event
      BAPI 85: Refactoring Heartbeat_log_event
      Fixing the tests failures arised after pushing Ignorable_event and Rows_query_event
      - Fixing valgrind errors on WL branch    Error:      conditional jump or move depends on uninitialized value
      - Fixing valgrind errors on WL branch for Rows_log_event::do_add_row_data   - Adding Documentation   - Addressed Review comments   - Addressing a few TODO's
      BAPI 85  :Moving Ignorable_log_event and Rows_query_low_event
      BAPI 85: Moving Decoding of Rows event into libbinlogapi
      Removing the usuage of global variable server_version from libbinlogapi
      - Reverting changing uint*korr methods with le16toh, le32toh and le64toh   as they are not working as expected. - Uint*korr methods will be removed in subsequent patches once the cause of   the above failure is detected
      Fixing compilation error on solaris,
      - Added documentation for Format_description_event and Start_event_v3 - Removed use of uint*korr methods. - Fixing the Solaris compiliation failure
      BAPI 85:     Refactoring Start_log_event_v3 and Format_description_log_event
      BAPI-100 : Adding the HAVE_MYSYS flag
      Post-push Fix
      BAPI 85:  Refactoring Rand_log_event and User_var_log_event
      BAPI 85: Refactoring Intvar_log_event
      Post push Fix
      Post Push Fix
      BAPI 85: Moving Decoding logic of QUERY_EVENT and EXECUTE_LOAD_QUERY_EVENT            into binlogapi library
      BAPI 85: Moving the decoding logic of Xid_log_event into binlogapi library          Xid_log_event has been made the subclass of Xid_event
      Fixing failing test cases:
      -Added one more ctor for Binary_log_event   - Binary_log_event(const char **buf, const Format_description_event->binlog_version)     This will create an object of Log_event_header and initialize m_header - And added one more ctor for Log_event also   - Log_event(const char *buf, const Format_description_event)   - this will be called by the events ctor which are not yet implemented -Removed the creation of Log_event_header object from log_event.cc, rpl_slave.cc,  rpl_binlog_sender.cc as it will be done in the above ctor of Binary_log_event
      Post push fix: Resolving build errors on Windows
      BAPI 85: Moving decoding logic of the events representing LOAD_DAT_INFILE            SQL query into binlogapi library
      BAPI 85: Moving the decoding logic of Rotate_event and Stop_event into binlogapi library
      post push fix
      pot push fix for solaris
      post push fix
      post push fix
      post push fix
      post push fix
      Post push fix
      Fixing issues on solari and rhelx86_64
      reverting last 5 pushes
      Post push checks To be reverted
      post push fix to be reverted
      display dbug messages
      post push fix
      post push fix
      post push fix.  Note: we need to revert this patch later
      Fixing undefined reference errors to functions defined in endian.h
      Post push fix
      post push fix
      Post push fix for windows+solaris
      post push fix
      post push fix
      Post-push fix
      BAPI 85: rb2997: Moving event header from class Log_event into a separate class
      BAPI 84: rb2984: Moving methods and variables used for event checksum tests            into libbinlogapi
      BAPI 84: Linking libbinlogapi to mysql-trunk
      Initial import of binlog-api

[33mcommit 7960bd7050efe0c4f28791dd02895f62a2dff7a6[m
Author: BennyWang <benny.wang@oracle.com>
Date:   Mon Dec 22 08:01:29 2014 +0100

    Bug#20059644: WL6737:CRASH AT OPERATOR<< IN DICT/DICT0MEM.CC
    
    In latest wl6737, the bug isn't crashed any more. It reports
    ERROR 1034 (HY000): Incorrect key file for table '/tmp/#sql_4266_0'; try to
    repair it
    
    The error is because MySQL doesn't remove the duplicated order-by(group-by) list
    completely. In current trunk, if the item->type() of order-by(group-by) is not
    Item_field, [1;31moptim[mizer skips to remove duplicated items. This doesn't make sense.
    When internal tmp table is used, Innodb doesn't support to create a key on
    duplicated columns.
    
    So this bug fix removes the duplicated order-by(group-by) items completely
    during query [1;31moptim[mization.
    
    BTW, because there is an error when compare two weight_string functions to be
    equal or not, this patch fixes this problem too.

[33mcommit 58415bdd7731cbb3ef52ae8a78696138a636609c[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Fri Dec 12 15:18:46 2014 +0800

    Simply max record size check when [1;31moptim[mistic update

[33mcommit 1e8fbb57cb49eb325c28f43f71ae2bbc5a0a2cc6[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Wed Dec 10 18:03:13 2014 +0530

    Bug #19992856 MYSQL CLUSTER TRIGGER ON DELETE FALSE POSITIVE
    
    For a delete query, mysqld does a range read to read all the PKs and
    ranges given in the query. If the range read does not return an error,
    the delete triggers are executed.
    
    The issue occurs because read before write removal is attempted with
    a delete query, on a table which has a delete trigger. The read is
    [1;31moptim[mised away, so the range read cannot identify the empty range
    and return an end-of-file error. Since no error is returned, the
    delete trigger is executed even though no rows have been deleted.
    
    While running a delete or update query, there is a check to determine
    whether read before write removal should be used. Modified this check
    to disable read before write removal 1) for delete if the table has a
    delete trigger and 2) for update if the table has a update trigger.

[33mcommit 7804b76e2f3f3f668f3c0a7a354976fc310bd258[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Nov 26 11:11:16 2014 +0100

    Bug#16403708 SUBOPTIMAL CODE IN MY_STRNXFRM_SIMPLE()
    
    Based on a contribution from Alexey Kopytov
    
    Add benchmarking tests showing that my_strnxfrm_simple is not sub[1;31moptim[mal
    compared to the proposed patch.
    
    However: if we take the proposed patch, and do some loop unrolling,
    we get improved results for both (src == dst) and (src != dst) transformations.

[33mcommit b741f803a99d45a3c9d54964f501c4e3cb1db903[m
Author: mcy <mithun.c.y@oracle.com>
Date:   Thu Nov 20 12:37:03 2014 +0530

    Bug #18607971 : 5.5 TO 5.6 REGRESSION WITH A SUBQUERY IN THE FROM CLAUSE.
    
     ISSUE:
     ------ Materialization of derived
     tables has been delayed until it is actually required, because of this
     estimation of row count of derived tables can no longer be as exact as it
     used to be. In some cases where GROUP BY clause of Query is [1;31moptim[mized away
     because all group expressions can be reduced to some constant, we can say
     query can produce at most one row. But we are not using this info to
     calculate estimated row count. This lead to inaccurate estimates and hence
     sub[1;31moptim[mal plan.
    
    Fix:
    ----
    For cases where GROUP BY CLAUSE is [1;31moptim[mized away
    because all group expressions are constants set
    estimated_rowcount of the query to 1.

[33mcommit e03d01533cce55839d217e8d4043462b80adf6b9[m
Author: haixli <haixiang.li@oracle.com>
Date:   Wed Nov 12 03:19:30 2014 +0100

    Bug#19822146 EXPLAIN FOR CONNECTION CRASHES IN EXPLAIN_JOIN::EXPLAIN_QEP_TAB
    
    Description:
    ------------
     - If there are two clients, the first is C-1, the second is C-2,
       MySQL will create two threads, one is T-1(C-1), another is T-2(C-2).
     - If MySQL will call 'join_init_quick_read_record()' to do range scan when
       T-1 is doing query statement.
       The process of 'join_init_quick_read_record()' below:
        - delete tab->quick();
        - create a new quick by calling 'test_quick_select()'
        - tab->set_quick(qck);
        - mysql_mutex_lock(&thd->LOCK_query_plan);
        - tab->set_type(qck ? calc_join_type(qck->get_type()) : JT_ALL);
        - tab->set_quick_[1;31moptim[m();
        - mysql_mutex_unlock(&thd->LOCK_query_plan);
     - If T-2 is doing 'explain for connection 1;' , it would call
       'explain_qep_tab()' to get the T-1's QEP.
       The process of 'explain_qep_tab()' below:
        - if (tab->type() == JT_RANGE || tab->type() == JT_INDEX_MERGE)
        - {
        -   DBUG_ASSERT(tab->quick_[1;31moptim[m());
        -    quick_type = tab->quick_[1;31moptim[m()->get_type();
        - }
     - If T-1 is suspended and T-2 is executing as following order, MySQL will crash:
       -------+------------------------+-------------------------------
        Order |        T-1             |  T-2
       -------+------------------------+-------------------------------
        1     | delete tab->quick();   |
       -------+------------------------+-------------------------------
        2     | T-1 is suspended       | tab->quick_[1;31moptim[m()->get_type()
       -------+------------------------+-------------------------------
       MySQL crash is because T-1 delete quick object so that T-2 gets a NULL
       pointer.
    
    Fix:
    ----
    For a range scan of this bug's testcase, MySQL call
    'join_init_quick_read_record()' to replace old quick() with a new quick
    object, but old quick() and quick_[1;31moptim[m() point to the same object,
    so yes deletion of quick() has to be protected.
    
    Added test case.

[33mcommit c111689bc2bc0fa23f75fafaff593a4211e9f9bf[m
Merge: b2555aa6172 3f8b4ad8120
Author: Chaithra Reddy <chaithra.gopalareddy@oracle.com>
Date:   Sat Nov 8 16:33:47 2014 +0530

    Bug#18036143 - CRASH (SEGFAULT) IN ST_JOIN_TABLE::GET_SJ_STRATEGY ON
                   UPDATE STATEMENT
    
    Problem:
    JOIN_TAB pointer is not checked before de-referencing it.
    
    Analysis:
    The fact that JOIN_TAB objects are not used for single-table
    UPDATE and DELETE, is ignored when [1;31moptim[mizer is trying to
    generate simple equality condition from multiple
    equalities in eliminate_item_equal().
    As a result server fails accessing the uninitialized pointer.
    
    Solution:
    Check that JOIN_TAB pointer is set before dereferencing it.

[33mcommit fc3c271adf2cac75fd3cc545047cf70b42c71788[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Wed Nov 5 12:34:42 2014 +0100

    Bug#19931126: VALGRIND REPORTS USE OF UNINITIALIZED VALUE IN
                  MY_WILDCMP_BIN_IMPL
    
    The LIKE operator accepts ESCAPE clauses that contain expressions that
    are constant at execution time. However, it only evaluates the
    expression in the ESCAPE clause if its value is known at resolve time.
    If the ESCAPE clause contains an expression that is constant at
    execution time, but unknown at resolve time, the escape character will
    be uninitialized, and the LIKE operator will produce unreliable
    results. This is a regression in 5.6.
    
    The fix factors out the code that initializes the escape character
    from Item_func_like::fix_fields() into a new function. If the escape
    expression is constant and known at resolve time, it is initialized by
    fix_fields() as before. If its value is not known at resolve time,
    Item_func_like::val_int() will now initialize the escape character the
    first time it is called.
    
    The fix also makes the range [1;31moptim[mizer skip the [1;31moptim[mization with LIKE
    if the escape character is unknown at resolve time. Otherwise, it
    would use the uninitialized value for the escape character and produce
    unreliable results.

[33mcommit 813a0266f00df02634a5f20abbcd82ca500bb8a8[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Oct 30 17:34:44 2014 +0100

    Bug#19844782 SWITCH TO A TEMPLATE-BASED QUEUE/HEAP
    
    Post-push fix:
    One unit test failed to compile with -O3 -Werror -Wstrict-overflow
    Make unit test slightly more complicated, so 'x + 10 > x' could not be [1;31moptim[mized away.
    That uncovered a bug in update(size_type i, value_type const &x)

[33mcommit 5e5c5f4e0c97f6d7cd046abdbb66ef5ea07c158d[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Fri Oct 24 13:47:59 2014 +0530

    Bug #19593613 PROVIDE A MECHANISM TO PRINT THE FILE SEGMENT HEADER INTO A
    C++ STREAM
    
    My previous push for this bug doesn't compile properly for [1;31moptim[mized build.
    Fixing the compilation issue.
    
    approved by Marko over IM

[33mcommit 75d7315f7e73ca65ddb684b1933406bc1b8e97b1[m
Merge: 1b9480c0248 c11767e88b2
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Fri Oct 24 10:07:41 2014 +0800

    Merge from mysql-5.6 to mysql-trunk.
    
    BUG#19314480 SEGV IN FTS_OPTIMIZE_THREAD, DICT_TABLE_OPEN_ON_NAME()
    OR FTS_IS_SYNC_NEEDED()
    
    Pass table name to fts_[1;31moptim[mize_thread other than table object.

[33mcommit 25cc59a1fedd8f9f73aa0fd898454427c4aff59e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Oct 16 13:23:24 2014 +0200

    Fix for bug#18352514
    
       STRUCT MEMBERS IN MT-SCHEDULER ARE NOT CACHELINE ALIGNED AS ASSUMED
    
    A lot of performance tuning has been performed inside the
    multi threaded scheduler to [1;31moptim[mize the cache behavior of its
    internal data structures.
    
    Sub-members in these structures has been placed such that thread local
    members doesn't 'overflow' onto a cacheline possible being accessed by
    another thread. Where required, extra padding bytes has been inserted
    to isolate cachelines owned, or shared, by different threads.
    Thus avoiding entire cache line to be invalidated if another thread
    write into a cacheline not entirely owned by itself.
    
    Micro benchmarking has previously proved that such
    [1;31moptim[mization improved the performance several percent.
    
    This entire [1;31moptim[mization depends on that the global
    'struct thr_repository' instance itself starts at
    a cache line aligned base address *and* that the compiler
    doesn't rearrange or add extra padding to the scheduler
    struct.
    
    It turns out that the above prerequisits are not guaranteed
    or checked by the code. Thus these cacheline [1;31moptim[mization
    has only worked when the global thr_repository instance by coincidence
    ended up being cacheline aligned. Furthermore, on 64bit build the compiler
    also added extra padding words in the 'struct thr_safe_pool' such that
    it broke our own attempt to pad it to a cacheline aligned size.
    
    This fix ensures that 'g_thr_repository' is constructed '(via placement new)
    on a cacheline aligned address. Furthermore, asserts are added in the
    constructors to verify cacheline aligned adresses where such are assumed
    by design.

[33mcommit 7b899ac00c5d6a18b5b8eda661885f6f1afc502a[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Oct 8 10:31:13 2014 +0300

    Bug#19632776 Code gets incorrectly [1;31moptim[mized away by gcc
    
    As reported in https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=764220
    it is possible that gcc incorrectly [1;31moptim[mizes away some code
    when a function that takes a pointer or reference as a parameter
    is declared as attribute((const)) or attribute((pure)).
    
    The function affected was page_zip_rec_needs_ext().
    
    As a preventive measure, we will remove the potentially problematic
    attributes from all functions that take pointers or references that
    they are dereferencing. Functions that perform pointer arithmetics
    without dereferencing the pointers should be safe: page_offset() is
    an example.
    
    While we are at it, remove also some attribute((nonnull)).
    This attribute is dangerous, because it does not always generate
    a warning when NULL may be passed, but it may [1;31moptim[mize away code
    for handling the NULL case. We wanted this attribute for the sake
    of the warnings, not for the [1;31moptim[mizations.
    
    rb#6940 approved by Vasil Dimov and Jimmy Yang

[33mcommit 4b58afca7e96410cc037c724393a8b7cca124b22[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Sep 29 15:29:06 2014 +0530

    - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for
      MySQL Optimizer
      - Remove Little Endian [1;31moptim[mization as it is not compatible with Sparc
        architecture.

[33mcommit 16d498f22f33201d0c3603f31a96255ea50db60f[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 24 15:31:51 2014 +0200

    Patch 1 of 3 for WL#7654: "Reduce lock contention in NDB mt-scheduler"
    
    This part reduce lock contentions on the 'jba_write_lock'
    
    When a signal has been sent on the JBA queue, a wakeup is
    signaled to any waiters. However, this wakeup was signaled
    when still holding the 'jba_write_lock'. The following
    scenario was then quite likely:
    
     - As wakeup signal is sent by thread t1, the waiter, t2 is
       prepared for reschedulling.
    
     - The OS may conclude that t1 has consumed its CPU quota.
       As t2 is now waiting for the CPU, the control is
       given to this thread (t2).
    
     - One if the first thing the t2 will do, is to check its
       'delayed-queues'. Any expired signals will
       then be sent on the 'JBA queue', which require grabbing
       the jba_write_lock.
    
     - As t1 holds this lock, t2 will be blocked, and has to wait
       for t1 to be rescheduled such that the lock can be released.
    
    This fix defers sending of the wakeup signal to after jba_write_lock
    has been released by using the 'non signaling' flush_write_state().
    The previous flush_write_state_wakeup has become obsolete, and
    is removed.
    
    The flush_write_state-codepath itself has become slightly
    less [1;31moptim[mized as a result of this fix. However, avoiding two
    extra thread switches should more than make up for this.

[33mcommit c0c72692b25606d8ac367208cc9e89260a960e14[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Sep 24 10:02:24 2014 +0300

    Bug#18645050 WL#7142 CAUSED PERFORMANCE REGRESSION
    
    Reduce the use of space_id lookups. Remove the fil_system->mutex
    acquisitions from mtr_commit(), which is the likely primary cause of
    regression.
    
    fil_space_get(): New function, to look up a tablespace by ID.
    This function can replace the use of existing functions that combine
    a lookup with a member access, such as fil_space_get_flags(),
    fil_space_get_latch(), fil_space-get_page_size().
    
    fil_system_t::named_spaces: Note that this is only protected by
    log_sys->mutex, not fil_system->mutex any more.
    
    fil_node_close_to_free(): Refactored from fil_node_free(). Prepares
    to free a file node object. The actual freeing will be done in
    fil_space_free_low().
    
    fil_space_detach(): Refactored from fil_space_free_low().
    Detaches a tablespace object from the common cache.
    After detaching, fil_space_free_low() may be called.
    
    fil_space_free(), fil_delete_tablespace(): First detach the tablespace
    and release the fil_system->mutex. If the tablespace was in the
    fil_system_t::named_spaces list, acquire log_sys->mutex for removing
    it from the list. Finally, invoke fil_space_free_low() to free the
    memory.
    
    fil_space_free_low(): Assume that fil_space_detach() has been invoked
    and that the tablespace is no longer in the fil_system_t::named_spaces
    list.
    
    fil_names_dirty(): Change the return type to void. This function is
    now only called by redo log scan, not during normal processing.
    
    fil_names_write_if_was_clean(), fil_names_dirty_and_write(): Replaces
    fil_names_dirty() and fil_names_write().
    
    fil_names_write_low(): Renamed to fil_names_write().
    
    mtr_t::m_user_space_id, mtr_t::m_user_space: Replaces mtr_t::m_named_space.
    
    mtr_t::m_undo_space, mtr_t::m_sys_space: New fields, for caching tablespace
    handles.
    
    mtr_t::set_sys_modified(): Looks up the system tablespace and caches it.
    In WL#7806 this would also flag the tablespace as modified.
    
    mtr_t::set_named_space(): Look up, cache and return the tablespace object.
    
    mtr_t::set_spaces(const mtr_t&): Copy the associated tablespaces from
    an existing mini-transaction.
    
    mtr_t::lookup_sys_space(), mtr_t::lookup_user_space(): Look up and cache
    a tablespace when it was not already cached.
    
    mtr_t::x_lock_space(), mtr_x_lock_space(space_id): New method, to X-latch a
    tablespace and to cache the tablespace pointer in the mini-transaction.
    
    mtr_t::Command::prepare_write(): Instead of [1;31moptim[mistically calling
    fil_names_write() and then removing the records if they were not
    needed, invoke fil_names_write_if_was_clean() while holding
    log_sys->mutex. We no longer acquire fil_system->mutex here.
    
    mtr_write_log_t::m_len: Remove. We no longer need to omit some redo
    log records from the tail of the buffer.
    
    btr_root_adjust_on_import(), btr_validate_level(): Use fil_space_get()
    instead of fil_space_get_latch().
    
    btr_estimate_n_rows_in_range_on_level(), buf_dblwr_process(): Use
    fil_space_get() instead of fil_space_get_page_size().
    
    btr_free_externally_stored_field(): Use mtr_t::set_spaces() instead of
    mtr_t::set_named_space().
    
    rb#5918 approved by Yasufumi Kinoshita

[33mcommit a1ce6acf951c56e5b1a6efc9847fd61f8ec22bcc[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Wed Sep 17 18:27:31 2014 +0100

    WL#6128, WL#6972
    
    Cleanup a few [1;31moptim[mizations and fixed a bug
    where a read lock was being taken instead of
    a write lock.

[33mcommit c8fc9bc951b4330dba652609906e38479ab15f52[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Sep 16 14:33:30 2014 +0800

    BUG#19611367 INNODB: CLEAN UP PAGE I/O IN BTR0BULK.CC
    
    Store modify_clock to make buf_page_[1;31moptim[mistic_get succeed in most
    cases, and remove extra PAGE_HEAP_TOP and FIL_PAGE_TYPE set.
    
    rb#6663 approved by Marko.

[33mcommit 4f81455952a67b8fc35c9481a0e4ae7db5b49fd6[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Sep 15 14:37:31 2014 +0200

    Repush of WL#7544 after 7.4.1 clone tag.
    
    ------------------------------------------------------------
    revno: 4458
    revision-id: mikael.ronstrom@oracle.com-20140915090314-4feki0sr3fcmsflr
    parent: mikael.ronstrom@oracle.com-20140912185859-6417bvvey4ih4blu
    committer: Mikael Ronstrom <mikael.ronstrom@oracle.com>
    branch nick: push_wl7544
    timestamp: Mon 2014-09-15 11:03:14 +0200
    message:
      WL#7544: Optimisations of the NDB API receiver thread in various manners. 1) Optimising method to unpack field data from TRANSID_AI signal, 2) [1;31moptim[mised handling of trp_client locking check, 3) reorganised code in NDB API receiver thread for better performance and more readability, 4) Optimisation signal receive handler

[33mcommit c81405fd95aa83d89d20abc7195d76caab7f3df2[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Mon Sep 15 11:03:14 2014 +0200

    WL#7544: Optimisations of the NDB API receiver thread in various manners. 1) Optimising method to unpack field data from TRANSID_AI signal, 2) [1;31moptim[mised handling of trp_client locking check, 3) reorganised code in NDB API receiver thread for better performance and more readability, 4) Optimisation signal receive handler

[33mcommit d1609b64b20255daead6d870ec51c9d43d4ea60a[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue Sep 9 11:43:37 2014 +0530

    Bug #19495721 REMOVE C-STYLE IB_LOGF FUNCTION, REPLACE USAGE WITH IB::INFO(),
    IB::WARN(), ETC
    
    Fixing a compilation error (on [1;31moptim[mized build) introduced by the fix for
    this bug.
    
    approved by Marko over IM.

[33mcommit bc8c1f49752246504f8bd8d2bd1edf27e2b181f2[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Sep 4 15:59:17 2014 +0200

    Bug#19047527 SQRT() : UNINITIALISED VALUE IN MY_STRTOD
    
    We were doing an out-of-bounds read when parsing "0E+"
    Visible only in valgrind run of an [1;31moptim[mized build.
    Note that the compiler is free to re-arrange the evaluation order of
    (s < end && c >= '0' && c <= '9')
    as long as program semantics are kept.
    That's why we got a valgrind warning when testing (c >= '0')

[33mcommit d57524950878e092e5c151ffe1b0685101234597[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Aug 29 13:13:33 2014 +0200

    WL#7316 Optimizer cost model: Command for online updating of
            cost model constants
    
    Implements the FLUSH OPTIMIZER_COSTS command. This command will
    reload the [1;31moptim[mizer "cost constants" from the configuration
    tables into the "cost constant" cache. New sessions will use
    the updated "cost constants".

[33mcommit b333dcfb3571df2ce38b3049c801c31c123015d3[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Thu Aug 28 13:56:14 2014 +0200

    WL#7339 Use improved records per key estimate interface in [1;31moptim[mizer
    
    Follow-up patch: The group_min_max test failed when running on 32 bit
    platforms in due to off by one "rows" estimates. This is caused by
    different results from doing floating point operations where we
    previously used integer values for rec_per_key variables in the
    cost model for loose index scan. Adjusted the test to accept
    both values as the expected "rows" estimate.

[33mcommit c92102a6ef0f280bfb56e5585fca0d0cdcc34890[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Aug 28 13:43:15 2014 +0900

    Some POWER specific [1;31moptim[mizations
    
    Bug#18842925 : SET THREAD PRIORITY IN INNODB MUTEX SPINLOOP
    Like "pause" instruction for hyper-threading at Intel CPUs,
    POWER has special instructions only for hinting priority of hardware-threads.
    
    Bug#18814859 : "CACHE_LINE_SIZE IN INNODB SHOULD BE 128 ON POWER")
    Data arrangement is be better to be conscious about data-cache unit size of CPU, for more CPU cache consistency.
    Currently it is fixed to 64 bytes in InnoDB, but 128 bytes is better for POWER.
    
    Approved by Sunny in rb#6256

[33mcommit a78a3ad8d0039e94d67e0028621e8fee55e150c2[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Aug 27 16:07:29 2014 +0200

    WL#7276 Configuration data base for Optimizer Cost Model
    
    This worklogs adds two new tables to the "mysql" database for storing
    "cost constants" that will be used by the [1;31moptim[mizer cost model. The
    two new tables are:
    
    - server_cost: This stores cost constants for server cost model.
    - engine_cost: For storing cost constants for the cost model
                   for storage engines.
    
    The tables are created by the mysql_system_tables.sql script that
    is run when bootstrapping and upgrading the MySQL server.

[33mcommit 04e51dd32c9517db8140d4926e87bae9e68f5864[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Aug 27 19:14:06 2014 +0530

    - Bug #16479309: FUNCTION LOCK_NUMBER_OF_ROWS_LOCKED MAY BE INEFFICIENT
    
      Locks held by a transaction are tracked using a bit-vector.
      For reporting, bit-vector is traversed to calculate number of locks held.
      This could be [1;31moptim[mized by using a simple counter for tracking
    
      Approved: Sunny (rb#6478)

[33mcommit ffa96335a9412713e11f8b36e5fbadb5d82f7a35[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Tue Aug 26 13:44:54 2014 +0200

    Follow-up WL#5869: Optimizer cost: use Cost_estimate in range [1;31moptim[mizer
    
    Fixed one compiling error without opt.trace.

[33mcommit 58264f60c82deae05376d0200af118cd9a6bc609[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Aug 25 15:32:57 2014 +0200

    Bug#19459193 GET RID OF DYNAMIC_ARRAY IN CIRCULAR_BUFFER_QUEUE
    
    Post-push fix: -Werror build broken in [1;31moptim[mized mode
    
    gcc 4.4 was warning: error: 'group.st_slave_job_group::ts'
    solution: use value-initialization of two stack-allocated objects of type Slave_job_group
    
    gcc 4.7 gives warning warning: variable ‘ret’ set but not used [-Wunused-but-set-variable]
    solution: annotate it with __attribute__((unused))

[33mcommit 617ab590fae6b709cdce91dad427c9d23888c57d[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Sun Aug 24 12:59:53 2014 +0200

    WL#2489: Better ONLY_FULL_GROUP_BY mode
    
    Complete patch. See sql/aggregate_check.h or the WL for design.
    
    Moreover, this contains changes (not written in the WL page):
    
    1) Item_ident::alias_name_used used to convey the two pieces of
       information below; it's removed and replaced with:
    
       * m_alias_of_expr tells if this is a reference, through an alias, to a
         SELECTed expression which has "AS alias"
       * to know if this is a reference to a column of an aliased table, we
         consult table->alias_name_used
    
    2) only_full_group_by checks are now done after simplify_joins();
       the latter function may convert outer joins to inner joins, and
       inner joins allow more functional dependency detection;
       so with this change we can allow more queries.
    
    3) moved JOIN::outer_join to SELECT_LEX::outer_join; indeed, this is
       constant through executions, and we now need it for FD detection so
       computing this variable at [1;31moptim[mization time was too late.
    
    4) in trunk we eliminate DISTINCT if all group expressions are in the
       select list, but we do it at [1;31moptim[mization stage; in this WL we do
       it at preparation stage. This makes only_full_group_by
       distinct-related checks to allow more queries, like
    
         SELECT distinct a, min(b) FROM t1 group by a order by count(*)-count(*);
    
       (which otherwise would yield the new error
       ER_AGGREGATE_IN_ORDER_NOT_SELECT): and it allows the [1;31moptim[mizer in
       general to choose better plans (see the change in group_min_max.result).
    
    5) a new class Bool3 for holding true/false/unknown values.

[33mcommit c9787e5c4a2e86dbaa68632b24832e6842619db2[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Aug 27 09:25:41 2014 +0200

    WL#7339 Use improved records per key estimate interface in [1;31moptim[mizer
    
    This worklog implements the needed changes in the [1;31moptim[mizer
    and server to start using the new "records per key" interface that
    was implmeneted in WL#7338. This interface allows storage engines
    to provide "records per key" values using float values instead of
    integer values. Using this new interface instead of using the
    currently used rec_per_key values gives more correct record estimates
    for ref access against an indexed column.

[33mcommit 0a27b72171c4449f506ad4e93df8c29fead1ed72[m
Author: bin.x.su@oracle.com <>
Date:   Fri Aug 15 09:51:12 2014 +0800

    WL#6942 InnoDB GIS: Store GIS POINT datatype as fixed length column than BLOB
    
    We can store POINT as fixed length data(DATA_POINT) or var length data
    (DATA_VAR_POINT). A new session variable called innodb_[1;31moptim[mize_point_storage
    is introduced to control the format of POINT we want.
    
    rb#3839, approved by Jimmy and Allen

[33mcommit 85fdf717b6b25f1e709b52cf561bab56d7cd0670[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Wed Aug 6 15:55:55 2014 +0200

    WL5869: Optimizer cost: use Cost_estimate in range [1;31moptim[mizer
      Introduced some changes for other bug fix by mistake. Rollbacked them.

[33mcommit 516a1386c0b27b977af40b8130a3a87205302a88[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Wed Aug 6 13:44:25 2014 +0200

    WL#5869: Optimizer cost: use Cost_estimate in range [1;31moptim[mizer

[33mcommit c19f5d8858f4762ad1b9ed268ac64e02f442632e[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed Aug 6 16:29:11 2014 +0200

    Bug#19336348 DEBUG CRASH AT SETUP_SEMIJOIN_DUPS_ELIMINATION IN SQL/SQL_SELECT.CC
    the assertion failure was introduced by wl#6042; but even before the
    WL the chosen plan was buggy - semijoin loose scan was using table
    scan which gives unsorted rows.
    The cause is: pre-WL, when we notice that a QUICK cannot be used for
    sj-loosescan, we drop this QUICK, accidentally reverting to table
    scan; pre-WL it was in an "else" branch which was not covered:
            if (tab->select && tab->select->quick)
            {
              if (tab->select->quick->index == pos->loosescan_key)
                tab->select->quick->need_sorted_output();
              else
                tab->select->set_quick(NULL); <<<<<<<<<<
    In the WL, the inner if() was changed to an assertion.
    Dropping the QUICK in setup_semijoin_dup_etc was wrong, because at
    this stage the QUICK is already considered the final access method.
    We must indeed drop QUICK, but must do it earlier, at a stage where
    QUICK is still "the best QUICK _if we want to use a QUICK_ "; then
    other options are still open, and the [1;31moptim[mizer will consider ref or
    index scan for sj-loosescan.

[33mcommit febe1236e040784dde0263e16d3481f2fb1724fa[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sun Aug 3 11:08:23 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    Avoids to generate mysqld-debug.dmp during the test.
    IO thread can exit always when SRV_SHUTDOWN_EXIT_THREADS, if page cleaner already exited.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for performance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      [1;31moptim[mal performance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit ff659570dd24733c162fd260d52dc23eee56cbdf[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jul 31 11:18:11 2014 +0200

    Followup for Bug#18805275 fix.
    
    Fixed problem for innodb.log_file test for Windows.
    There are race between io threads and page cleaners to exit only when abort initialize InnoDB.
    Especially, Windows native AIO is weak for the race.
    IO threads should not exit during page cleaners active, in accurate.
    
    ------------------------------------------------------------
    revno: 8489
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Sat 2014-07-26 03:22:05 +0900
    message:
      Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
      From 5.7, page cleaner is multi-threaded for performance scalability.
      But it is not used during shutdown and recovery phases.
    
      It should be multi-threaded during shutdown and recovery phases for their
      [1;31moptim[mal performance.
    
      * The previous problem was fixed. (releasing the event objects was too early)
    
      Approved by Sunny in rb#5465
    ------------------------------------------------------------

[33mcommit fca850aef9724ac200fc2e75a7f6ae130904cbcf[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Jul 29 11:55:16 2014 +0200

    Fix for Bug#19307777 ASSERTION `QEP_TAB->CONDITION() == QEP_TAB->CONDITION_OPTIM()' FAILED:
    qep_tab's condition_[1;31moptim[m() is set in make_tmp_tables_info().
    In this testcase, JOIN::[1;31moptim[mize leaves early with "impossible HAVING", no call
    to make_tmp_tables_info; so qep_tab's condition_[1;31moptim[m() is left unset (NULL pointer).
    Then in JOIN::exec(), prepare_result() calls do_fill_table()
    which has this new assertion:
    
      /*
        We pass a condition, which can be used to do less file manipulations (for
        example, WHERE TABLE_SCHEMA='test' allows to open only directory 'test',
        not other database directories). Filling schema tables is done before
        QEP_TAB::sort_table() (=filesort, for ORDER BY), so we can trust
        that condition() is complete, has not been zeroed by filesort:
      */
      DBUG_ASSERT(qep_tab->condition() == qep_tab->condition_[1;31moptim[m());
      ... code which uses condition() ...
    (Before the WL, this same code was using:
    "filesort && filesort->select ? filesort->select->cond : condition()" ).
    
    So it fails, because condition() is Item_equal while condition_[1;31moptim[m() is still
    unset (NULL).
    Fix: no matter how we leave JOIN::[1;31moptim[mize() we must set condition_[1;31moptim[m()
    (except in error paths), this is here achieved by extending set_plan_state()
    (which is logical, as condition_[1;31moptim[m() is also used by EXPLAIN).
    With this patch, there are less places where *[1;31moptim[m variables are updated,
    which is better for maintenance.
    A comment is added for having_for_explain.

[33mcommit 87c3ffbd40979610544578d8d1ad43b4e7e66e51[m
Merge: e1d9beffbe5 e0a309bd92f
Author: Marc Alff <marc.alff@oracle.com>
Date:   Mon Jul 28 21:51:36 2014 +0200

    Merge mysql-trunk --> mysql-trunk-wl7802
    
    Removed all changes in the [1;31moptim[mizer done for WL 7802
    due to the refactoring introduced in mysql-trunk,
    which caused too many merge conflicts.
    
    The patch in the [1;31moptim[mizer for WL 7802 needs to be revised,
    and applied again on the refactored code.

[33mcommit e0a309bd92f12b344c9995511d47fd7aee07fd70[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Jul 28 15:29:25 2014 +0200

    wl#6042 "split JOIN_TAB into [1;31moptim[mizer and executor part".
    The "executor part" is a new class QEP_TAB. JOIN_TABs are dropped at end
    of JOIN::[1;31moptim[mize. EXPLAIN and JOIN::exec use only QEP_TABs. So does the
    construction of the AQP.
    QEP_TAB and JOIN_TAB share certain members, put in a class QEP_shared.
    SQL_SELECT is removed, most of it goes into QEP_shared or QEP_TAB.
    The memcpy of all JOIN_TABs at end of get_best_combination() is gone;
    instead, we keep the old JOIN_TABs for a bit longer, but access them
    in the final plan's order thanks to best_ref[] pointers.
    This refactoring fixes
    Bug#18921626 DEBUG CRASH IN PLAN_CHANGE_WATCHDOG::~PLAN_CHANGE_WATCHDOG AT SQL_OPTIMIZER.CC
    and Bug#18535226 DEBUG CRASH ON QUICK_RANGE_SELECT::RESET.

[33mcommit ca25011abb142c193c8a58de1222f2e971e016bc[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sat Jul 26 03:22:05 2014 +0900

    Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
    From 5.7, page cleaner is multi-threaded for performance scalability.
    But it is not used during shutdown and recovery phases.
    
    It should be multi-threaded during shutdown and recovery phases for their
    [1;31moptim[mal performance.
    
    * The previous problem was fixed. (releasing the event objects was too early)
    
    Approved by Sunny in rb#5465

[33mcommit c4597498d6d9d424dba5bde1422ec45581222e1a[m
Author: David Zhao <david.zhao@oracle.com>
Date:   Wed Jul 23 12:17:37 2014 +0800

    st_distance performance [1;31moptim[mization: given a geometry collection,
    compact its points/multipoints into one multipoint object and linestrings/multilinestrings into one multilinestring object
    before calling BG::distance for better performance, because BG::distance has O(NlogN) complexity for calculating distance of such objects.

[33mcommit 32ae62756fbe7f30ef005c23f11c8403c4a5f1fc[m
Author: Sergey Glukhov <sergey.glukhov@oracle.com>
Date:   Fri Jul 18 12:53:52 2014 +0400

    Fix for Bug#18924341 CRASH IN TEST_IF_SKIP_SORT_ORDER, GROUP BY MATCH AGAINST DESC
    Regression from WL#7123 Additional query [1;31moptim[mization for Fulltext Search.
    Tetsing of tab->join->simple_order is insufficient to check
    the presence of ORDER BY clause. simple_order can be true even
    if there is no ORDER BY.
    Additonal check for presence of ORDER BY is added.

[33mcommit 187f66c843aa3854b27e6d1f0ce97e4b11dcb540[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Jul 14 10:12:12 2014 +0530

    - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for
      MySQL Optimizer
      - Disable WL TC for 16K as it tries to print [1;31moptim[mizer plan which differ
        in terms of stats if run for 4K causing TC to fail.
      - Re-recording Windows TC post enabling InnoDB as default SE.

[33mcommit aca9e464fcc80a3cd211a9e2ff71c2366aa3255c[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Jul 11 10:05:55 2014 +0530

    - WL#6737: InnoDB: Enabling InnoDB temp-tables as default internal SE for
      MySQL Optimizer
      - Re-recording failing test-cases due to use of innodb as default SE
        for [1;31moptim[mizer.

[33mcommit 40ec5373c044547a66d5456b15d61553de8f3401[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Jul 10 10:46:28 2014 +0200

    Bug#19142753 ASSERT: MODE !=LOCK_X || LOCK_TABLE_HAS(THR_GET_TRX(THR), INDEX->TABLE, LOCK_IX)
    
    WL6742-Improve InnoDB SELECT COUNT(*) performance by using handler::records()
    introduced new functionality in InnoDB.
    
    handler::records can now fail in ways never seen by the server before.
    
    This improved things on the InnoDB side:
    kevin.lewis@oracle.com-20130626194505-muocxuem7bd2wiw8
      bug#16802288 - FAILING ASSERTION: PREBUILT->SQL_STAT_START ||
      TRX->STATE == TRX_STATE_ACTIVE
    
      Add error handling for DB_DEADLOCK and DB_LOCK_TABLE_FULL to
      handler::records() for COUNT(*).  Also rollback transaction when
      handler::records receive DB_DEADLOCK, DB_LOCK_TABLE_FULL or
      DB_LOCK_WAIT_TIMEOUT.
    
    However: thd_mark_transaction_to_rollback() isn't enough.
    The [1;31moptim[mizer needs to abort immediately for the cases mentioned above.
    
    Fix: improve error handling for all calls to handler::records()
    Add a new public function ha_records() which does the error checking,
    make handler::records() protected.

[33mcommit 1e66da10a18e15c6675823e5e787c50a604e2bce[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Jul 4 14:07:00 2014 +0200

    Bug#19143857 GET RID OF DYNAMIC_ARRAY IN RPL_FILTER
    
    Post-push fix
    gcc 4.4.6 with -Werror -O3 said:
    prealloced_array.h:226: error: 'job_worker.st_slave_job_group::ts' may be used uninitialized in this function
    
    when we did push_back on an uninitialized object.
    Fix: use value-initialization on the job_worker object.
    
    Also fix some warnings from gcc 4.7.2 in [1;31moptim[mized mode.

[33mcommit 48290cf4ee5afeaabe58915f3b9b39d8f9ccb8c5[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Jun 27 11:51:42 2014 +0200

    Fix for Bug#19063289 CRASH IN PRINT_TABLE_ARRAY ON 2ND EXECUTION OF PS WITH SUBQUERY AND VIEW
    The fix for bug 18945693 missed something: reinit_before_use() does not cover join nests.
    In this testcase, the join condition is on a join nest:
        C AS SQ2_alias1 LEFT  JOIN
           ( B AS SQ2_alias2 INNER JOIN C AS SQ2_alias3
             ON 1)
           ON 1
    So again, we get a dangling [1;31moptim[m_join_cond pointer at start of second execution.
    Fix: set join->[1;31moptim[mized a few lines later, so it can tell if [1;31moptim[mized conditions are readable.
    Testcase is added to opt trace suite because opt trace prints the query in many places
    so gives more coverage.

[33mcommit 00a5f24dde3c032f989983df266ef4424713be08[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Jun 26 16:07:15 2014 +0200

    The AutoTest, testBitfield does not correctly word align
    the length of a default bit values.
    
    The length of the default value of a bit column, need to be word aligned.
    Incorrect alignment seems to have caused several (undetected) coredumps
    from testBitfield, like in:
    
    Thread 1 (Thread 0x7f159a9bf720 (LWP 869)):
    #0  0x0000000000431975 in u_32_value (this=0x7fffa6765988, pRow=0xb5d600) at /data0/ndbdev/autotest/build/clone-mysql-5.5-cluster-7.2-2014-06-23.24869/storage/ndb/test/src/HugoCalculator.cpp:299
    #1  HugoCalculator::verifyRowValues (this=0x7fffa6765988, pRow=0xb5d600) at /data0/ndbdev/autotest/build/clone-mysql-5.5-cluster-7.2-2014-06-23.24869/storage/ndb/test/src/HugoCalculator.cpp:299
    #2  0x000000000041f528 in HugoTransactions::pkReadRecords (this=0x7fffa6765930, pNdb=0xac27c0, records=1000, batch=13, lm=NdbOperation::LM_Read, _rand=0) at /data0/ndbdev/autotest/build/clone-mysql-5.5-cluster-7.2-2014-06-23.24869/storage/ndb/test/src/HugoTransactions.cpp:1078
    #3  0x000000000040c23a in transactions (pNdb=0xac27c0, tab=<value [1;31moptim[mized out>) at /data0/ndbdev/autotest/build/clone-mysql-5.5-cluster-7.2-2014-06-23.24869/storage/ndb/test/ndbapi/testBitfield.cpp:187
    #4  0x000000000040dc68 in main (argc=0, argv=0xab6808) at /data0/ndbdev/autotest/build/clone-mysql-5.5-cluster-7.2-2014-06-23.24869/storage/ndb/test/ndbapi/testBitfield.cpp:99
    
    
    Failures resulting in core dumps seems to be incorrectly trapped på AutoTest:
    The coredumping test seems to be incorrectly recorded as PASSED, and the coredump
    is then related to the next failing tests - Like in:
    
    http://ndbmaster.se.oracle.com/~autotest/report.pl?id=23419
    
    Where we find:
    
    - testBitfield  PASSED
      .... ~50 tests later:
    - testSRBank -n SR -l 300 -r 15 T1      FAILED(TEST)    Y
    
    The failed testSRBank has attached a coredump from testBitfield!
    
    This fix doesn't fix the incorrect reporting from AutoTest.
    However, it likely fix the coredump itself by correctly alligning
    the bit-type columns.
    
    NOTE: We also find in ha_ndbcluster.cc, which has
          similar code for handling bit columns:
    
    static int create_ndb_column(THD *thd,
                                 NDBCOL &col,
                                 Field *field,
                                 HA_CREATE_INFO *create_info
    
    .......
    
                /* For bit columns, default length is rounded up to
                   **nearest word**, ensuring all data sent    <<<< HERE!
                */.
                Uint32 defaultLen = field_used_length(field);
                if(field->type() == MYSQL_TYPE_BIT)
                  defaultLen = ((defaultLen + 3) /4) * 4;      <<<<<<<< and HERE !
                col.setDefaultValue(buf, defaultLen);
    
    Which is the correct way to handle length of default bit values

[33mcommit 10423d72193fcdf6ad17ab599344b72e1ebb00e4[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Thu Jun 26 18:07:30 2014 +0530

    Bug#18044717: INCORRECT LCP STATE CAUSES AN INTERNAL ERROR
    
    An autotest failure was caused by the [1;31moptim[misation to skip metadata
    updates while removing a node if the LCP status is LCP_STATUS_IDLE.
    The failing test was 'testNodeRestart -n RestartNodeDuringLCP T6'.
    Removed the [1;31moptim[misation to fix the autotest failure.

[33mcommit 988944182dc07a8cb12b8b7cf0f45d3103530984[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Jun 25 16:26:16 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - Revert back the [1;31moptim[mization of delayed truncate.
      - This has to be atomic operation and we start delaying truncate
        moment we mark tablespace for truncate but if server is shutdown
        before that then metadata can be in-consistent.
        Why inconsistent ?
        - Because we delay only free of segment but besides that there is
          freeing of the pages too. Tracking freeing of pages will add extra
          overhead than helping in general case.

[33mcommit a26cd1a88d6aba91afe0df774ef7870a8771280f[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Jun 20 11:02:36 2014 +0200

    Bug #18805275 : PAGE CLEANER SHOULD BE MULTI-THREADED ALSO FOR RECOVERY AND SHUTDOWN
    
    From 5.7, page cleaner is multi-threaded for performance scalability.
    But it is not used during shutdown and recovery phases.
    
    It should be multi-threaded during shutdown and recovery phases for their
    [1;31moptim[mal performance.
    
    Approved by Sunny in rb#5465

[33mcommit a68d8466e21bc71e3adb6d1351f17de7c1395aa3[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Jun 18 09:29:19 2014 +0530

    - WL#6965: Truncate UNDO logs.
      - Fixed decrement of history size as we now used [1;31moptim[mized way
        to free the purge segment if the segment is due for truncate.

[33mcommit fd7a0790cd5941b19235241342c6b1d909060f9c[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Sun Jun 15 14:24:02 2014 +0300

    bug#18183143 aliasing.diff
    avoid pointer aliasing which was [1;31moptim[mized away
    version: gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-3)

[33mcommit 3e0cc937cc2de93f8830cdef7e167238f0046697[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Thu Jun 12 10:09:42 2014 +0200

    fixed: Added INSTALL_MYSQLSHAREDIR to default search paths
    fixed: missing 'else'-statements in Path class allowed for sub[1;31moptim[mal execution
    fixed: use stat() instead of my_stat() to simplify dependencies.

[33mcommit 8a43bc7e158972941673cb9e734bf8262c403e83[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Jun 10 14:46:49 2014 +0200

    Bug #18945693 CRASH IN PRINT_TABLE_ARRAY AT SQL/SQL_LEX.CC ON 2ND EXEC OF PREPARED STATEMENT
    Introduced by the fix for Bug 18791851 .
    SQ4_alias1 has a join condition.
    In first execution, some [1;31moptim[m_join_cond is created for this table,
    by get_[1;31moptim[mizable_join_conditions().
    At end of this execution, the [1;31moptim[m_join_cond pointer remains dangling
    (points to freed memory).
    In second execution, we prepare/[1;31moptim[mize the top SELECT; this wants to
    print HAVING, and thus print the subquery containing SQ4_alias1. Note
    that this subquery has _not_ been [1;31moptim[mized yet, in this second
    execution; so get_[1;31moptim[mizable_join_conditions() has _not_ been called,
    so SQ4_alias1->[1;31moptim[m_join_cond() is still the dangling pointer.
    We try printing the join condition of that table; because the pointer
    is dangling, it is not (Item*)1, so we assume it is usable, and crash:
        // Print join condition
        Item *const cond=
          (curr->select_lex->join && curr->[1;31moptim[m_join_cond() != (Item*)1) ?
          curr->[1;31moptim[m_join_cond() : curr->join_cond();
    To sum up: pointer is not reset to (Item*)1 at end of first execution,
    so is not trustable at second execution.
    Fix: reset dangling pointer at start of second execution.
    Moreover, I remove a redundant setting of having_for_explain
    which is already done at the goto label.

[33mcommit f45c834dffb420110a0eab33d4690e1b9ba85010[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Jun 5 11:41:50 2014 +0200

    Adapt pushed joins to WL#6016
    
     1) The call to ha_make_pushed_join() was moved to a place where
       "semijoin dups elimination" had not yet been performed and
       thus the AQP code failed to detect that such part of the
       query should not be pushed. Fix by moving the call to ha_make_pushed_join()
       even later in the [1;31moptim[mizer(although that code is in sql_select.cc).
       This is more or less the exact same place as in 5.6 based code.
    
     2) The join type is now determined earlier and serves in the current
        design as the "primary source of the type". Thus the AQP code
        will see join types of JT_RANGE and JT_INDEX_MERGE which can be treated
        the same way as JT_ALL. In long run the two new join types
        can each have separate implementations to avoid the checks to see
        if "quick" is used or not since that is now determined by the join type.
        - JT_RANGE/INDEX_MERGE have to have "quick" set
        - JT_ALL can have "quick" set

[33mcommit daea9a4fe12ef152b54b0b1668cbbaafa674f02f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Jun 4 13:27:08 2014 +0200

    Bug#18759597 MISSING ROWS ON WHERE .. IN QUERY WITH VARIABLES AND CONCAT
    
    cmp_item_sort_string_in_static::store_value() assumed that its input
    string could not change. It turns out it can.
    
    In this case we stored 'm' 'n' 'a' 'l' 'xm' in column one of each row.
    We sort the data to get 'a' 'l' 'm' 'n' 'xm'
    
    Then the expression is analyzed again, and Item_func_concat::tmp_value
    changes value. So when we do lookup, we search in 'a' 'l' 'm' 'n' 'm'
    
    The binary-search failed, since the array of rows is no longer
    properly sorted.
    
    Fix: Save string results locally in
    cmp_item_sort_string_in_static::store_value()
    This is done only once during the prepare phase, so it should not have
    any negative performance impact.
    
    After doing that change, we see that the classes
    cmp_item_sort_string and cmp_item_sort_string_in_static are nearly identical.
    We can remove them both, and move all member functions into cmp_item_string instead.
    
    Also: remove some C-style casts, introduce new template function down_cast instead.
    
    Where/why is it analyzed again?
    The range [1;31moptim[mizer does some analysis and ends up calling Item::save_in_field_no_warnings
    which ends up calling Item_func_concat::val_str() again.
    Looking at the source code of that function, we see there are many
    ways that tmp_value can change.

[33mcommit 1007d851b8e2a0e1ff05e0f63f1927623ecae54a[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Wed Jun 4 08:16:09 2014 +0200

    1. Added big-test flag to tests that take more than a minute on the fastest platform OEL6 on PB2. This will help reduce time taken for per push runs
    2. Disabled rpl.rpl_stm_mixed_mts_rec_crash_safe_small. This is supposed to be shorter version of .rpl_stm_mixed_mts_rec_crash_safe suitable for PerPush runs. This is taking about 2 minutes and hence needs to be tuned down
    3. Moved rpl.rpl_[1;31moptim[mize from experimental to disabled list. It is timing out on windows leading to very long running time for windows in daily.

[33mcommit 7710a138a90ae83152ebaa0793428874ddb3c3a1[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Jun 3 12:53:05 2014 +0200

    Merge of the fix for Bug#18791851 CRASH IN ST_SELECT_LEX::PRINT WITH OPTIMIZER_TRACE ON SUBQUERY.
    Because of wl7082 the same bug cannot happen (we still add the testcase), but this variant can crash:
          - query is
          SET @a:=(SELECT ROW(1, 2)=
          ROW((SELECT 1 FROM t1 LEFT JOIN t1 t2 ON 1
                         HAVING 3 IN (SELECT 2 FROM DUAL)),
              1));
          (i.e. the rare conditions of fixed bug18345786 CRASH AROUND ST_JOIN_TABLE::AND_WITH_CONDITION).
          - simplify_joins() runs in JOIN::[1;31moptim[mize(), when JOIN::where_cond/having_cond and TABLE_LIST::[1;31moptim[m_join_cond()
          are still unset (==0x1).
          - it prints the transformed query; select_lex::print() sees JOIN::[1;31moptim[mized==true,
          so uses unset members above. Crash.
    
    Fix: break the link between JOIN::[1;31moptim[mized and conditions: in select_lex::print(), do
          cur_where = (join && join->where_cond != 0x1) ?
          // it is ready for reading, and the most up-to-date condition
                                   join->where_cond :
                                  select_where->where;

[33mcommit fe8c4339de0ed67321e96793adcf89f652aa4dbf[m
Author: Priyanka Sangam <priyanka.sangam@oracle.com>
Date:   Thu May 29 16:37:59 2014 +0530

    Bug#18044717: INCORRECT LCP STATE CAUSES AN INTERNAL ERROR
    
    An LCP is initialized by setting the global LCP state to
    LCP_INIT_TABLES, then looping over all the ndb tables and
    setting the table-specific LCP status to TLS_ACTIVE. If any
    table is not ready for LCP, the process continues using a
    CONTINUEB signal. This is done to break up the work across
    multiple signals to maintain the responsiveness of the
    signal processing thread doing the work.
    
    After all the tables are updated, the global LCP status
    is set to LCP_STATUS_ACTIVE. Therefore, during the init
    procedure, some tables may have a status of TLS_ACTIVE while
    the global LCP status is LCP_INIT_TABLES.
    
    While handling node failure of the master during an LCP, the
    LCP takeover code checks the global LCP status and, if the
    status is LCP_INIT_TABLES, changes it to LCP_STATUS_IDLE. This
    results in a case where the global LCP status is LCP_STATUS_IDLE
    while some tables may have a state of TLS_ACTIVE.
    
    However, there is an ndbrequire in the node-failure handler
    which states that the lcp status cannot be LCP_STATUS_IDLE while
    any table has a status of TLS_ACTIVE. This ndbrequire fails and
    results in the core.
    
    Removed the ndbrequire to fix this bug. Also added an [1;31moptim[misation
    to skip reverting metadata updates while removing a node from a
    table if the LCP status is LCP_INIT_TABLES. The revert is not
    required in this case since no metadata updates have taken
    place yet.

[33mcommit 4b0fdde5a6f6fc705200ac32c642a230eea4677e[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Tue May 27 13:24:16 2014 +0200

    WL#6635 Make use of condition filtering in the [1;31moptim[mizer
    
    Stabilizing test results: Added FORCE INDEX and STRAIGHT_JOIN
    to one multi-table update statement in innodb_multi_update.test
    to increase likelihood that the same query plan is produced on
    every run.

[33mcommit fad2fbb2e84552f34496fbbde5cd72543ebef071[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed May 21 12:05:28 2014 +0200

    WL#6635 Make use of condition filtering in the [1;31moptim[mizer
    
    Stabilizing test results: Added ANALYZE TABLE to two tables
    in innodb_multi_update.test to increase likelihood that the
    same query plan is produced on every run.

[33mcommit fe2886e22e0e4aa5d69d4182f768556be0b0c4ca[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Wed May 21 09:54:23 2014 +0200

    Bug#14743180: Refactor pushdown_on_conditions()
    
    The current implementation of pushdown_on_conditions() is rather badly
    structured and it is possible to make it simpler and slightly more efficient.
    
     - Comments are added all over the place and use of local variables cleaned up.
    
     - Function pushdown_on_conditions() is renamed to
       JOIN::attach_join_conditions().
       The name is based on the fact that conditions are isolated and "attached"
       to the table where they can be evaluated in an [1;31moptim[mal way.
    
     - pushdown_on_conditions() has a separate section that analyzes const
       conditions for outer join. This code is called for every table that
       is added to the plan and when called, it visits every table present
       in the plan. However, it is more efficient to call it only when a
       complete outer join operation has been formed and then attach the
       condition to the first inner table of the outer join.
    
     - pushdown_on_conditions() loops over all tables added to the plan,
       but it is only needed to loop over all inner tables of the plan.
       This is handled by setting the "prefix_tables" field for the first
       inner table to cover all tables up to and including this table.
    
     - The effect of these changes is also a simplification of some redundant
       table conditions.
    
     - make_join_select() has a first stage that looks at const join conditions.
       However, this is easily handled exclusively within
       JOIN::attach_join_conditions().
    
     - Function add_found_match_trig_cond() can be slightly rewritten and
       comments can be improved. Recursion is replaced with iteration (which
       causes slight change to [1;31moptim[mizer trace).
       add_found_match_trig_cond() is called by pushdown_on_conditions().
    
     - The test file join_nested.test contains a few test cases that check
       nested outer join operations. However, these test cases use MySQL-
       specific syntax making it difficult to compare test results with
       other SQL databases. The patch adds duplicate test cases with
       standard compliant syntax.
    
    mysql-test/r/join_nested.result
    mysql-test/r/join_nested_bka.result
    mysql-test/r/join_nested_bka_nixbnl.result
      Test results for duplicate tests with standard compliant SQL syntax.
    
    mysql-test/r/subquery_sj_dupsweed.result
    mysql-test/r/subquery_sj_dupsweed_bka.result
    mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
    mysql-test/r/subquery_sj_dupsweed_bkaunique.result
    mysql-test/r/subquery_sj_firstmatch.result
    mysql-test/r/subquery_sj_firstmatch_bka.result
    mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
    mysql-test/r/subquery_sj_firstmatch_bkaunique.result
    mysql-test/r/subquery_sj_loosescan.result
    mysql-test/r/subquery_sj_loosescan_bka.result
    mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
    mysql-test/r/subquery_sj_loosescan_bkaunique.result
      Some simplified conditions.
    
    mysql-test/suite/opt_trace/r/general2_no_prot.result
    mysql-test/suite/opt_trace/r/general2_ps_prot.result
      Optimizer trace: Reversed wrapping order of triggered conditions.
    
    mysql-test/t/join_nested.test
      Added some duplicate nested join tests with standard SQL syntax.
    
    sql/sql_[1;31moptim[mizer.cc
      Altered make_join_select(), attach_join_conditions() and
      add_found_match_trig_cond() according to above specification.
      quick_fix_fields() replaced with fix_fields() because hitting a problem
      where data is not correctly propagated using update_used_tables().
    
    sql/sql_[1;31moptim[mizer.h
      Added attach_join_conditions() as member function in class JOIN.
    
    sql/sql_select.h
      Added comments.
    
    sql/table.h
      Added comments.

[33mcommit fdd2e495d24da1507d250cf44c2a25ba26745cd5[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Tue May 20 15:25:22 2014 +0200

    WL#6635 Make use of condition filtering in the [1;31moptim[mizer
    
    Stabilizing test results: remove an EXPLAIN that temporarily
    was added to one query in innodb_multi_update.test in order
    to understand what triggered the test failure.

[33mcommit e4f0cd8bce87dfc1f1e3f8c2bf1e54e741320722[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon May 19 10:52:40 2014 +0200

    Fix for autotest failure in: 'testNdbApi -n MaxOperations'
    
    This test has been failing for *every day* since 2008 with the following
    problems:
    
    1)
      Got error '1217 Out of operation records in local data manager ..'
      instead of the expected error:
      '233 Out of operation records in transaction coordinator'
    
      This was caused by only generating pkReadRecord-ops for
      row #1 in the test, thus all LQHKEYREQs ended up on the same
      datanode and thus exhausted the 'MaxNoOfLocalOperations'
      in that node.
    
      Fixed by pseudo randomly creating rowNo in the range 1..256
    
    2)
      As all pkReadRecords were executed in a giant execute batch,
      the Transporter got overloaded for some of the larger tables.
      (ERROR: 1218 Send Buffers overloaded in NDB kernel)
    
      Fixed by sending an 'execute_NoCommit()' after every 1000'th row.
      Note: The no-commit keeps the operation records in TC which the
      test try to exhaust.
    
      As previously, a execute_Commit() is still done after the last
      row.
    
    An [1;31moptim[mization is also introduced in HugoOperations.cpp, where we
    use the getValue() variant taking a 'Column*' arg. instead of the
    'char* name' argument of the same column. Debugging with this testcase
    revealed that the test client spent significant time doing strcmp
    in getValue.

[33mcommit f7bdd34913513272f631fce57b3d521ea973338b[m
Author: Sergey Glukhov <sergey.glukhov@oracle.com>
Date:   Mon May 19 12:50:55 2014 +0400

    WL#7123 Additional query [1;31moptim[mization for Fulltext Search

[33mcommit 60f88d2dd6e0bf631025f4b560b96ba1bf0bcd97[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri May 16 11:02:00 2014 +0200

    WL#6635 Make use of condition filtering in the [1;31moptim[mizer
    
    Stabilizing test results: adding EXPLAIN to one query in
    innodb_multi_update.test in order to understand what triggers
    the test failure.

[33mcommit 810de5cecbebaf2ea0d8784d2decbbc184c9553d[m
Merge: 64a39949165 d9943287566
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed May 14 10:44:57 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
    
      There are specific use-cases where-in modules needs a temporary table for
      stagging data for a while and doesn't really care about transaction semantics.
    
      Given the use-case we have designed a special type of temporary table that
      are light weight (no-undo, no-transactional semantics, no-locking,
      no-doublewrite-buffer, operational even in read-only, etc....) and are
      [1;31moptim[mized for performance.
    
      These tables will come into existence for a while
      (for example: during query exeuction) for stagging a data of subquery
      and so shortlived. Being temporary table they would continue to reside
      in shared temporary tablespace.
    
      Approved by: rb#4870 (Sunny + BinSu)

[33mcommit 0857f5dfc4aed510b6c064b47feaaafbf7fd44a7[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu May 8 16:38:56 2014 +0200

    Bug#18715670 CRASH IN DECIMAL_ACTUAL_FRACTION COMPARING DECIMAL TO NULLS
    
    Another bug in the range [1;31moptim[mizer, uncovered by the patch for
    Bug#18556403 USE STD LIBRARY FOR SORTING AND MATCHING IN SUBCLASSES OF
    
    The range [1;31moptim[mizer would build predicates for empty in-lists
    (since NULL values are removed from NOT IN (in-list))

[33mcommit a89b175a2030985a569c63d2551b216b1f88c62e[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu May 8 12:27:07 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Further [1;31moptim[mized to avoid passing extra arguments.

[33mcommit da60614e443f9be6201ea182e9d0f67da92aae3d[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Apr 30 11:11:59 2014 -0700

    In the common case where an async method call returns int zero,
    try to [1;31moptim[mize away creating a new JavaScript value.

[33mcommit 8c4c372ae1d0de3b752a8d078010d0b2c0418ce1[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Apr 30 13:27:21 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Realigned dict_index_t to use [1;31moptim[mized packing.

[33mcommit d4b980895110acf0f95f0a9ca73b045c33eae6a5[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Apr 25 14:54:21 2014 +0200

    Fix for Bug #18486607 ASSERTION FAILED:
    IN_SUBQ_PRED->LEFT_EXPR->FIXED IN CONVERT_SUBQUERY_TO_SEMIJOIN .
    Query:
     select # S1
         (
          select 1 # S2
          from t2
          where a in (
                                select 1 # S3
                                from t2
                                )
         ) from a group by a ;
    
    we prepare S1 (the "first SELECT" from the top). Do setup_fields() on it.
    This does fix_fields() on subquery containing S2. Which does prepare() on S2,
    thus setup_conds(), thus fix_fields() which does two things:
    1) fixes left arg "a" of "a in (select S3)"; this arg being an outer reference (to grouping column of S1)
    it's not quite fixed but rather added to S1::inner_refs list.
    2) fix_fields() on subquery containing S3, which calls resolve_subquery() on S3
       => S3 is candidate for sj-merging in S2.
    Back to preparation of S2: flatten_subqueries() runs, to merge S3, asserts that
    left arg is fixed and it's not.
    It would have been fixed only after fix_inner_refs() for S1.
    This didn't crash before WL 7082, because back then, sj was merged in JOIN::[1;31moptim[mize() i.e. after all JOIN::prepare() are done, which included fix_inner_refs().
    Now sj is merged at end of preparation of the sj's owner (=parent of the subuery).
    The fix: relax the assertion, as in the fixes for:
    Bug 14601664 ASSERTION FAILED: *REF && (*REF)->FIXED, FILE ITEM.CC
    and
    Bug 13735050: CRASH IN END_WRITE WHEN SUBQUERY
    GROUPS BY OUTER FIELDS.

[33mcommit 2b26839d3b78caf973650f665b3ec0b886e2dae7[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Thu Apr 24 15:12:24 2014 +0200

    Bug#18556403 USE STD LIBRARY FOR SORTING AND MATCHING IN SUBCLASSES OF IN_VECTOR, PART 2
    
    As demonstrated by the patch for Bug#18486249
    in_vector and its derived classes can benefit from some code modernization.
    
    standard library functions are
     - type safe
     - easier to read/maintain
     - faster
    
    Introduce typed vectors of objects, rather than the "blob vector" in the base class in_vector.
    The re-write uncovered a bug in the range [1;31moptim[mizer:
    it was building range predicates for unused parts of the IN-vector.

[33mcommit 2cfe8b28322329d5f32c54398a39c11a67fa910a[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Apr 16 14:50:23 2014 +0300

    WL#7806 InnoDB: Log-based discovery of built-in tablespaces
    
    This is follow-up to
    WL#7142 InnoDB: Simplify tablespace discovery during crash recovery
    
    We will write MLOG_FILE_NAME for all persistent tablespaces, not only
    for *.ibd files. Currently, this includes the following:
    
    * The InnoDB system tablespace (ibdata*)
    * The InnoDB undo log tablespaces (undo*)
    
    On startup, the InnoDB system tablespace and the InnoDB redo log will be opened.
    If there are redo log records to be applied since the latest checkpoint,
    any tablespaces requiring cleanup will be opened and recovered based on
    MLOG_FILE_NAME records in the redo log.
    
    If the MLOG_FILE_NAME records for the system tablespace disagree with
    the server configuration affecting the data file names for the system
    tablespace, recovery will be aborted with an error message, before
    applying any redo log.
    
    After recovery, any undo log tablespaces for which no redo log records were
    applied will be opened based on existing mechanism. The system tablespace will
    remain open at all times.
    
    is_predefined_tablespace(): Remove. All redo-logged tablespaces will
    be treated in the same way.
    
    mtr_t::m_undo_space, mtr_t::set_undo_space(): New field and method, to
    associate an undo tablespace associate with the mini-transaction.
    
    mtr_t::m_modifies_sys_space, mtr_t::set_sys_modified(): New field and
    method, to note that the mini-transaction is modifying the system
    tablespace.
    
    mtr_t::set_spaces(): A kind of copy constructor that copies the
    information on modified tablespaces from another mini-transaction.
    
    mtr_t::is_undo_space(): A debug method to ensure that set_undo_space()
    has been called.
    
    mtr_t::Command::prepare_write(): Invoke fil_spaces_lookup() before
    log_mutex_enter(), to look up all tablespaces that were flagged as
    modified by the mini-transaction, and [1;31moptim[mistically invoke
    fil_names_write() for the system tablespace. (This is equivalent to
    old behaviour introduced in WL#7142, with the exception that we may
    look up an undo tablespace and the system tablespace.) After
    log_mutex_enter(), truncate the log if the fil_names_write() was not
    needed. If an undo tablespace or the system tablespace were flagged
    and fil_names_dirty() holds for them, invoke fil_names_write() for
    them.
    
    dyn_buf_t::set_size(): New method, used for truncating unneeded
    MLOG_FILE_NAME records from the tail of mtr_t::log.
    
    mtr_write_log_t::m_len: Remove. We will write the entire log.
    The log can be truncated by dyn_buf_t::set_size().
    
    srv_undo_tablespaces, srv_undo_tablespaces_open: Set the initial value
    to 0 on server startup, so that fil_space_belongs_in_lru() will behave
    in a predictable way during redo log apply.
    
    trx_rseg_t: Note that space,page_no are constant and need not be
    protected by mutex.
    
    fil_load_single_file_tablespace(): Renamed from
    fil_load_single_table_tablespace().
    
    fil_space_system_check(): New function, to check that MLOG_FILE_NAME
    records match the system tablespace data files.
    
    fil_space_undo_check(): New function, to reopen possibly existing undo
    log files after redo log apply has completed.
    
    fil_spaces_lookup(): New function, to replace previous usage of
    fil_names_write(). At mini-transaction commit, this looks up all
    modified redo logged tablespaces.
    
    fil_names_write(): Renamed from fil_names_write_low(). We will get the
    fil_space_t* looked up by fil_spaces_lookup().
    
    fil_name_parse(): Support undo tablespaces and multi-file system
    tablespace.

[33mcommit 7d5be17136361d222e3a69d3a868c7c29f8df2a3[m
Author: Erlend Dahl <erlend.dahl@oracle.com>
Date:   Tue Apr 15 11:59:15 2014 +0200

    Stop running rpl.rpl_[1;31moptim[mize in valgrind as it creates havoc in the weekly testing.
    
    The test is in any case experimental for the time being.
    
    Cf. Bug#18128323
    
    Approved by Anitha over IM.

[33mcommit 32e5550ba55415f658353477756d7fa538d8da08[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Apr 4 14:00:52 2014 +0200

    Follow-up patch for Bug#13106350: MRR initialization on a derived
                                      table caused crash
    
    Updated test case by removing out-commented setting of
    [1;31moptim[mizer_switch materialization to off.
    This was commented out due to at the time this test was
    back-ported to mysql-trunk, the materalization switch was
    not part of [1;31moptim[mizer_switch on mysql-trunk.

[33mcommit 4a2b168ffc95ebcd95f711087093e51476dab81a[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Mon Mar 31 09:03:38 2014 +0200

    Bug#18364815: OPTIMIZER PREFERS TABLE SCAN WHEN USING "IN"
                  WITH VALUE OF DIFFERENT TYPE
    
    Item_func_in checked whether there were items of different type
    in the IN clause and would automatically reject the use of
    range access in that case. However, this is too restrictive
    since a great deal of type conversions are automatically done
    in MySQL, e.g. conversion of quoted numbers to integer.
    "col IN (1, "2")" is therefore perfectly legal but used to be
    rejected.
    
    The fix is to remove the automatic rejection and let the range
    [1;31moptim[mizer make the decision on its own since it already
    performs this check on its own (see comparable_in_index())
    anyway.

[33mcommit 8b31cf4cd8b943a115f60148e4c8448401d773e0[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Mar 27 21:53:50 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - While traversing to previous record for intrinsic table
        we don't do [1;31moptim[msitic restore and so there is no left
        block that is pinned as done in normal cases always.
        Skip the section that after restore frees the pinned
        left block.

[33mcommit 4e74a3e08e7644cf45dde1e502c84d395a7a642e[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Mar 26 12:11:13 2014 -0700

    Attempt to [1;31moptim[mize encodeKeyBuffer() for single-column indexes

[33mcommit 6396f023f2fb856a12eca6cf057a27b86ff6e154[m
Author: Thayumanavar <thayumanavar.x.sachithanantha@oracle.com>
Date:   Wed Mar 26 15:22:41 2014 +0530

    WL#7582 - LEX_USER String transitional refactoring.
    The patch does transitional refactoring which is required to
    migrate to the new strings framework. A number of places in
    the server uses char* instead of const char* which includes
    the LEX_STRING which has a char* member. A gradual
    transformation of the server code is required to use const
    char* and use const references to pass objects around functions.
    This will enable smooth transition of ther server code to use
    the new string framework classes.
    This commit transforms the LEX_USER members user,host,
    password plugin and auth to use LEX_CSTRING. Thus the
    following classes/data members are modified to acommodate
    this change:
    1. LEX_USER (sql/structs.h)
             LEX_CSTRING user
             LEX_CSTRING host
             LEX_CSTRING password
             LEX_CSTRING plugin
             LEX_CSTRING auth
    2. ACL_USER (sql/auth/sql_auth_cache.h)
           LEX_CSTRING plugin
    3. MPVIO_EXT (sql/auth/sql_authentication.h)
           LEX_CSTRING acl_user_plugin;
    4. change of following variables in
         sql/auth/sql_authentication.h to be of LEX_CSTRING
         (1) native_password_plugin_name
         (2) old_password_plugin_name
         (3) sha256_password_plugin_name
         (4) validate_password_plugin_name
         (5) default_auth_plugin_name
    5. Event_time (sql/event_data_objects.h)
          LEX_CSTRING m_definer_user
          LEX_CSTRING m_definer_host
    6. Event_job_data (sql/event_data_objects.h)
          LEX_CSTRING m_definer_user
          LEX_CSTRING m_definer_host
    7. THD (sql/sql_class.h)
          LEX_CSTRING m_invoker_user
          LEX_CSTRING m_invoker_host
    
    The function names whose prototypes or other changes that are modified as part
    of the refactoring include:
    1. [1;31moptim[mize_plugin_compare_by_pointer
    2. acl_insert_user
    3. ACL_PROXY_USER::store_pk
    4. ACL_PROXY_USER::store_data_record
    5. acl_update_user
    6. do_auth_once
    7. check_change_password
    8. plugin_is_ready
    9. create_string (sp.cc)
    10. sp_head::set_definer
    11. make_lex_string_root
    12. make_lex_string
    13. change_security_context
    14. check_string_byte_length
    15. check_host_name
    16. check_string_byte_length
    17. check_string_char_length
    18. plugin_find_internal
    19. plugin_status
    20. plugin_lock_by_name
    21. plugin_find_by_type
    22. append_definer
    23. reconstruct_definer_clause
    The refactoring has been done to some cohesive set of functions that relates to
    the authentication subsystem with some changes in plugin & stored procedure
    layer. Refactoring has been stopped and char* string is passed around as changes
    need to be done at storage engine handler layer or certain
    low-level platform API(memroot,string related functionality) that will affect
    every part of the code.

[33mcommit 063734feff5d54b323e3360fcebd983556cf2e3f[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Fri Mar 21 08:11:41 2014 +0100

    WL#6635: Make use of condition filtering in the [1;31moptim[mizer
    
    When the join [1;31moptim[mizer calculates the order tables should be
    joined, fanout is of great importance. The fanout for a table
    't_y' in a join plan consistig of (t_1,...,t_x,t_y,..) is the
    average number of rows from 't_y' that will match each partial
    row combination from (t_1,...,t_x) and all predicates applicable
    to 't_y'. It is important to keep the fanout as low as possible
    because it directly influences how many row combinations will be
    joined with the next table in the join order.
    
    Before this worklog, MySQL only considered:
     * the fanout that is directly determined by the chosen access
       method.
     * a very basic heuristic for table/index scans:
       - If 'ref' access is applicable, the fanout for scans is 75%
         of the number of rows in the table.
       - If 'range' access is applicable, the fanout for scans
         equals the number of rows estimated by 'range' access
         relative to number of rows in the table
         ("rows_estimate_range_access/rows_in_table").
    
    This worklog changes this by making the join [1;31moptim[mizer consider
    fanout estimates for any access method and for all conditions
    (except subqueries and outer join ON predicates), including
    non-indexed columns, when calculating partial plans. By taking
    this into account, query conditions can be evaluated earlier
    during query execution which in turn can reduce the number of
    inspected rows.
    
    The entry point for the new calculation of fanout estimates
    beyond that of the access method is the function
    calculate_condition_filter()

[33mcommit e79cadbb429ba2164cb1c6f3fe33af7c501887c5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Mar 19 15:07:55 2014 +0200

    Test push of WL#7142 InnoDB: Simplify tablespace discovery during crash recovery
    
    When the setting innodb_file_per_table=ON was introduced in MySQL 4.1,
    InnoDB crash recovery was changed so that the directories will be
    searched for *.ibd files if any redo needs to be applied.
    
    The scanning and opening of all *.ibd files (including ones for which
    no redo log needs to be applied) can be very slow, especially on
    deployments that contain a large number of *.ibd files. Furthermore,
    if we allow a more liberal placement of tablespace files in the file
    system, we might have to extend the search to an even broader range of
    directories.
    
    This worklog eliminates the *.ibd file scan by guaranteeing the
    following:
    
    If there are redo log records for any non-predefined tablespace, there
    will also be an MLOG_FILE_NAME record.
    
    The InnoDB redo log format will be changed as follows:
    
    MLOG_FILE_NAME(space_id, filename): A new redo log record.
    Replaces MLOG_FILE_CREATE, MLOG_FILE_CREATE2.
    
    MLOG_FILE_RENAME2(space_id, old, new): The names will be file names
    (directory/databasename/tablename.ibd). Replaces MLOG_FILE_RENAME,
    which used table names (databasename/tablename).
    
    NOTE: We will write MLOG_FILE_NAME once since the latest redo log
    checkpoint. Immediately after a checkpoint, the log may contain some
    MLOG_FILE_NAME records that were "copied across the checkpoint" and a
    MLOG_CHECKPOINT marker to signal the end of a checkpoint.
    
    On redo log apply during crash recovery, we will scan the log up to
    three times:
    
    Recovery scan 1: Look for the first MLOG_FILE_CHECKPOINT marker since
    the latest checkpoint.
    
    If there is no MLOG_FILE_CHECKPOINT, we will skip the entire log. The
    data files will correspond to the system state as of the checkpoint.
    
    Recovery scan 2: Read the redo log since the latest checkpoint. Copy
    scanned records to recv_sys->addr_hash, and construct a map of
    recv_spaces, based on MLOG_FILE_NAME and MLOG_FILE_DELETE records.
    
    Before applying the records from recv_sys->addr_hash, we will check if
    any tablespace files are missing. If there are missing tablespaces, we
    will refuse to start up, so that the DBA can intervene, for example to
    manually rename files. This new safeguard of WL#7142 can be disabled
    by setting innodb_force_recovery.
    
    If not all redo log records in recv_sys->addr_hash, we will need a
    third log scan:
    
    Recovery scan 3: Read the redo log since the latest checkpoint. If
    recv_sys->addr_hash fills up, apply the batch of log records and read
    a new one.
    
    mlog_id_t: Remove MLOG_FILE_CREATE, MLOG_FILE_CREATE2, MLOG_FILE_RENAME.
    Add MLOG_FILE_NAME, MLOG_FILE_RENAME2, MLOG_CHECKPOINT.
    
    MLOG_FILE_FLAG_TEMP: Remove. This was a flag for MLOG_FILE_CREATE*.
    
    enum dict_check_t: Remove DICT_CHECK_ALL_LOADED. Crash recovery no
    longer loads all tablespaces.
    
    mtr_t::m_named_space: Associates a tablespace with a
    mini-transaction. A mini-transaction may be associated with up to one
    non-predefined tablespace. It may also modify predefined tablespaces
    for change buffering and undo logging.
    
    mtr_t::set_named_space(ulint space): Sets m_named_space.
    
    mtr_t::is_named_space(ulint space): Checks if the mini-transaction is
    associated with a given tablespace.
    
    mtr_t::Command::prepare_write(): Write an MLOG_FILE_NAME record if
    needed. This is executed as part of mtr_commit().
    
    mtr_t::commit_checkpoint(): A special method to emit redo log records
    to the redo log buffer when the caller already invoked
    log_mutex_enter(). This is only used by fil_names_clear().
    
    fil_space_t::max_lsn: LSN of the most recent fil_names_write() call,
    or 0 if the tablespace has not been dirtied since fil_names_clear().
    
    fil_space_t::named_spaces, fil_system_t::named_spaces: List of
    tablespaces for which MLOG_FILE_NAME has been written since the latest
    checkpoint.
    
    recv_sys_t: mlog_checkpoint_lsn: The LSN of the first scanned
    MLOG_CHECKPOINT record, or 0 if none was read yet.
    
    fil_space_get(): Look up a tablespace. This is invoked during
    mtr_t::Command::prepare_write() while not holding the log mutex, to
    prepare for a fil_names_write() call. The idea is to minimize the
    log_mutex hold time.
    
    fil_space_create(): Add an output parameter for returning a duplicate
    tablespace (same space_id).
    
    fil_space_free(): Make this an externally callable function, to free a
    tablespace from the cache when applying MLOG_FILE_DELETE.
    
    fil_space_free_low(): Renamed from fil_space_free(). The new wrapper
    fil_space_free() will acquire fil_system->mutex.
    
    fil_op_log_parse_or_replay(): Change the order of parameters. Remove
    log_flags, and rename parse_only to replay. We no longer attempt to
    replay log records of a multi-item mini-transaction, unless the
    MLOG_MULTI_REC_END was seen.
    
    fil_rename_tablespace(): Change the function signature. Take old_path,
    new_name, new_path_in. MLOG_FILE_RENAME2 is logging file names, not
    table names like MLOG_FILE_RENAME was. Also invoke fil_name_write().
    
    enum fil_load_status: Outcomes of fil_load_single_table_tablespace().
    
    fil_load_single_table_tablespace(): Do not exit on failure. Instead,
    return a status value to the caller.
    
    fil_load_single_table_tablespaces(): Remove. We no longer try to load
    all *.ibd files.
    
    fil_create_new_single_table_tablespace(): Do not write any
    MLOG_FILE_CREATE or MLOG_FILE_CREATE2. Instead, invoke
    fil_name_write() to write MLOG_FILE_NAME.
    
    fil_mtr_rename_log(): Change the signature. Take dict_table_t instead
    of names. Take a tmp_name.
    
    fil_names_write_low(): Write MLOG_FILE_NAME record(s) for a
    tablespace.
    
    fil_names_write(): Write MLOG_FILE_NAME record(s) for a tablespace if
    not already written since the latest checkpoint.
    
    fil_names_clear(): Write MLOG_FILE_NAME records and MLOG_CHECKPOINT on
    a log checkpoint or at system startup. If do_write=true, writes
    MLOG_CHECKPOINT even if no MLOG_FILE_NAME was written.
    Reset those fil_space_t::max_lsn for which fil_names_write() has not
    been invoked after the checkpoint LSN. Return true to the caller if
    any redo log was written.
    
    fil_op_write_log(): Replace log_flags with first_page_no, and replace
    table names with file paths. The parameter first_page_no is currently
    being passed as 0, because we do not have non-predefined multi-file
    tablespaces yet.
    
    fil_name_write(): Write an MLOG_FILE_NAME record for a file.
    
    Datafile::open_read_only(): Add the parameter bool strict.
    
    fsp_names_write(): Wrapper for mtr->set_named_space(). This must be
    called when a mini-transaction is going to modify a non-predefined
    tablespace.
    
    is_predefined_tablespace(): Check if a tablespace is a predefined one
    (system tablespace, undo tablespace or shared temporary tablespace).
    
    enum recv_addr_state: Add RECV_DISCARDED, so that buffered redo log
    records can be retroactively deleted if an MLOG_FILE_DELETE was
    later recovered for a tablespace.
    
    btr_free_but_not_root(), btr_free_root(): Call fsp_names_write().
    
    btr_cur_ins_lock_and_undo(), btr_cur_[1;31moptim[mistic_insert(),
    btr_cur_pessimistic_insert(), btr_cur_update_in_place(),
    btr_cur_[1;31moptim[mistic_update(), btr_cur_pessimistic_update(),
    btr_cur_del_mark_set_clust_rec_log(),
    btr_cur_del_mark_set_clust_rec(), btr_cur_[1;31moptim[mistic_delete_func(),
    btr_cur_pessimistic_delete(): Call fsp_names_write() after successful
    locking and undo logging.
    
    btr_store_big_rec_extern_fields(), btr_free_externally_stored_field(),
    row_ins_index_entry_big_rec_func(): Call fsp_names_write().
    
    dict_build_tablespace(), dict_create_index_tree_step(),
    dict_recreate_index_tree(), fil_reinit_space_header(): Call
    fsp_names_write().
    
    page_cur_insert_rec_write_log(),
    page_copy_rec_list_to_created_page_write(),
    page_cur_delete_rec_write_log(), page_cur_delete_rec(), page_create():
    Assert that fsp_names_write() has been called.
    
    dict_table_rename_in_cache(): Pass old_path to
    fil_rename_tablespace().
    
    dict_check_tablespaces_and_store_max_id(): Remove the logic for
    DICT_CHECK_ALL_LOADED. We could probably remove this entire function,
    given that the maximum is also stored in the DICT_HDR page.
    
    mlog_write_initial_log_record_for_file_op(): Replaced by
    mlog_write_initial_log_record_low().
    
    log_checkpoint(): Before invoking log_write_up_to(), invoke
    fil_names_clear() to copy any MLOG_FILE_NAME records across the
    checkpoint. Flush the log up to the MLOG_CHECKPOINT marker, instead of
    only up to the checkpoint LSN. Without this step, the log between
    oldest_lsn and log_sys->lsn would be essentially corrupted (missing
    MLOG_FILE_NAME records on redo log apply). When the redo log scanner
    sees the first MLOG_CHECKPOINT since the latest checkpoint, it knows
    that there must be no missing MLOG_FILE_NAME record for any page
    operation on a non-predefined tablespace. If the MLOG_CHECKPOINT
    marker is missing, no redo log will be applied, and the system would
    be at the state of the checkpoint.
    
    fil_name_parse(): New function, to update the recv_spaces map based on
    MLOG_FILE_NAME and MLOG_FILE_DELETE records during recovery.
    
    recv_parse_or_apply_log_rec_body(), recv_parse_log_rec(): Add the
    parameter "apply". Do not apply file-level redo log records unless the
    entire mini-transaction has been recovered. Fail if an MLOG_FILE_NAME
    record is missing for a page-level operation.
    
    recv_recover_page_func(): Assert that no LSN is after the latest
    scanned redo log LSN.
    
    recv_parse_log_rec(): Check for some more log corruption.
    
    recv_parse_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    Add a parameter "apply" to specify whether log records should be applied
    (apply=false during the first scan for MLOG_CHECKPOINT). Return true
    if an MLOG_CHECKPOINT record was seen for the first time.
    Improve DBUG_PRINT output, and detect some more log corruption.
    
    recv_scan_log_recs(): Add a parameter "store_to_hash" to control
    whether the records should be stored into recv_sys->addr_hash.
    
    recv_group_scan_log_recs(): Initialize the variables and data
    structures to begin reading redo log records. Add a parameter
    "last_phase" that is set when a multi-pass recovery is needed and we
    are scanning the redo log for a third time. In last_phase, we will
    invoke recv_apply_hashed_log_recs() to empty recv_sys->addr_hash
    between passes. If last_phase=false, we would stop filling
    recv_sys->addr_hash, only processing file-level redo log records.
    
    recv_init_crash_recovery(): Split some code into
    recv_init_crash_recovery_spaces(), to be invoked after the first call
    to recv_group_scan_log_recs().
    
    recv_recovery_from_checkpoint_start(): Invoke
    recv_group_scan_log_recs() up to 3 times if needed.
    After processing all redo log, write an MLOG_CHECKPOINT marker
    so that in case we will crash before making a checkpoint, the log
    will be replayed by subsequent crash recovery.
    
    checkpoint_now_set(): Avoid an infinite loop in case an MLOG_CHECKPOINT
    marker is the only thing that was written since the latest checkpoint.
    
    rb#4700r6

[33mcommit 24b962c7bd0e6bcd9acffab8a8a01a9da4c1e06e[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Mar 14 16:49:26 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - If during [1;31moptim[misitic insert page doesn't have enough space
        option-1 is to try re-organizing the page.
      - For intrinsic table we select a consistent option to use
        pessmisitic insert.

[33mcommit 4a6f9f69863e30c49d65f7b792c79c187be5c2eb[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Tue Mar 11 13:07:11 2014 +0100

    Bug#18220153: RANGE OPTIMIZER INCORRECTLY SETS
                  SEL_TREE::KEYS_MAP FOR NOT USABLE INDEX
    
    The tree_and() function in the range [1;31moptim[mizer sets
    SEL_TREE::keys_map bits for applicable indexes. In this bug,
    two geometry ranges were attempted ANDed by key_and(). That's
    not currently possible, so key_and() decided that this index
    was not usable. Despite this, tree_and() set the bit for the
    index.
    
    Fix: only set SEL_TREE::keys_map bits for usable indexes in
    tree_and()

[33mcommit d620c25aef9254d47031ebf63c7a2a88e2ef46f1[m
Author: Libing Song <libing.song@oracle.com>
Date:   Sun Mar 9 19:33:46 2014 +0800

    Bug#17932935 CALLING IS_SEMI_SYNC_SLAVE() IN EACH FUNCTION CALL
                 HAS BAD PERFORMANCE
    
    Semisync master Binlog_transmit_observer needs to know if the
    dump thread is from a semisync slave. Because the logic is
    different between semisync slave and normal slave. So
    is_semi_sync_slave() is called in each Binlog_transmit_observer
    function. is_semi_sync_slave() reads the user variable
    'rpl_semi_sync_slave' through checking the dump thread's user
    variable hash table. That is very slow and has remarkable impact
    on semisync replication performance.
    
    the user variable 'rpl_semi_sync_slave' is never changed after
    it is initialized. So we [1;31moptim[mized the code as below:
    * is_semi_sync_slave() is only called in
      repl_semi_binlog_dump_start() when starting the dump thread.
      And then its value is stored in a pthread_key.
    
    * Other Binlog_transmit_observer functions just get it value
      through reading the pthread_key.

[33mcommit 859683084bd1e46c3cee12fc8bca098ff7116dcb[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Mar 7 14:08:50 2014 +0100

    Fix for Bug#18346750 OPTIMIZER_TRACE & DBUG_PRINT CRASH IN ST_SELECT_LEX::PRINT
    Regression of WL#7082.
    JOIN::[1;31moptim[mize of select#1 wants to evalute select#2 during partition pruning. This triggers JOIN::[1;31moptim[mize() on select#2.
    During [1;31moptim[mization of GROUP BY of select#2 we evaluate select#3 ; this evaluation gives an error (too many result rows), so [1;31moptim[mization of select#2 aborts.
    So evaluation of select#2 simply returns 0, because Item::val*() functions never signal an error:
    longlong Item_singlerow_subselect::val_int()
    {
      DBUG_ASSERT(fixed == 1);
      if (!no_rows && !exec() && !value->null_value)
      {
        null_value= FALSE;
        return value->val_int();
      }
      else //we take this branch!
      {
        reset();
        return 0;
      }
    }
    then [1;31moptim[mization of select#1 goes on, at some point wants to prints the complete query to [1;31moptim[mizer trace, which prints select#2 which wants to print having_for_explain but this member has not been set properly as [1;31moptim[mization of select#2 aborted. Fix: don't print a select if there was any error. After this fix, [1;31moptim[mization of select#1 goes on, finally checks thd->is_error() and aborts.
    Pre-WL, we also tried to print the broken select#2, but because having_for_explain was initialized to NULL (not 0x1 like post-WL) there was no crash.

[33mcommit f0ac35fe72dba1fee7261558417ddb4233076a0b[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Mar 7 14:03:56 2014 +0100

    Fix for Bug#18345786 CRASH AROUND ST_JOIN_TABLE::AND_WITH_CONDITION.
    Regression introduced by WL#7082.
    The necessary ingredients to crash are: SET, ROW (), outer join.
    
    When we are [1;31moptim[mizing the most inner subquery, we crash in pushdown_on_conditions (), because outer join information is incorrect, because simplify_joins () was not called. Now let's explain why it was not called.
    The statement is SET, this is important, because the first thing it does is locking tables (plain SELECT doesn't, rather does post-prepare locking in mysql_prepare_and_[1;31moptim[mize_select()).
    Then, we go into fix_fields of this Item_row:
    row((select 1 from `t1` left join `t1` `t2` on 1),1)
    first it does fix_fields on the subquery argument, which does select_Lex:::prepare () on this subquery (the most inner one, with the left join); during this prepare (), we SKIP apply_local_transforms () (and thus we skip simplify_joins ()); we skip it because we say "I belong to an outer subquery, let it transform me at the end of its own prepare (); transformations are to be done top-down".
    Then Item_row:: fix_fields wants to know if the subquery argument is constant, so it calls Item_subselect::const_item (), which says "yes" because tables ARE locked.
    Then Item_row:: fix_fields, seeing that the subquery argument is constant, wants to know if it is NULL, thus evaluates it; so we go into JOIN:: [1;31moptim[mize for the most inner subquery, hence the crash as simplify_joins () has not been called yet for this subquery.
    
    Before the worklog, there was no problem, because JOIN:: [1;31moptim[mize called simplify_joins().
    Fix: add some robustness: in JOIN::[1;31moptim[mize, if simplify_joins() has not been called, call it. This
    is expected to be an uncommon case; so far we know only of "set x=(subq);".

[33mcommit 3205f547510b9daf6f7d7c0b182bf5d7b5e5d7d8[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Thu Mar 6 09:57:35 2014 +0100

    BUG#18195373: OPTIMIZER TRACE SHOULD PRINT NAME OF INDEX FOR
                  RANGE ACCESS
    
    Improve [1;31moptim[mizer trace by
     - Printing more details about range access in the
       "considered_access_path" section
     - Instead of always printing "access_type": "ref" for index
       lookup types, "eq_ref", "ref" or "fulltext" is now printed.

[33mcommit 54472396cada8faec88b1e840f345a76f035258c[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Mar 5 15:25:36 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Corrected [1;31moptim[mized condition that was used to update secondary
        index only if needed.
      - Added test-sceanrio to cover big-data-types (blob).

[33mcommit 779f8110278421dc1dd7e19fe676d5c1fa9f3a54[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Mar 5 11:29:56 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Introduced special type of temporary tables named as
         intrinsic tables. These tables will inherit properties
          of temporary tables with added semantics:
    
          - intrinsic tables can reside only in shared temporary
            tablespace.
          - DML on intrinsic tables is not redo logged so no
            rollback action is possible.
          - intrinsic table uses [1;31moptim[mized row-structure.
            (roll-ptr hidden column is not appened.)
          - most of the intrinsic table operation are [1;31moptim[mized
            to take advantage of non-shared architecture
            and so locking is [1;31moptim[mized + table and index ids
            are locally generated, no entry to dictionary (instead
            a reference is maintained in user thread), etc.

[33mcommit 2a1975757944f5d17e5ffcc719d52d051e38720d[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Mon Mar 3 13:53:59 2014 +0100

    WL#7082 -  Move permanent transformations from JOIN::[1;31moptim[mize () to JOIN::prepare ().
    As one single patch.
    
    semijoin, outer-join-to-inner, parenthesis-removal,
    join-condition-to-where-clause are moved from JOIN::[1;31moptim[mize() to
    JOIN::prepare(), which is renamed to select_lex::prepare().
    
    Old approach for WHERE/HAVING conditions:
    at end of preparation, save copies of conditions in
    sl->prep_where/having, then allow oneself to trash sl->where/having in
    [1;31moptim[mization; in [1;31moptim[mization make sure to keep in sync
    sl->where/having with join->conds/having. At start of next execution,
    recreate sl->where/having from the "prep_" copies.
    New approach:
    At end of preparation, sl->where/having is considered frozen. In
    [1;31moptim[mization, make trashable copies of it, and use only those
    copies. Ditch them at end of execution.
    
    Some functions like mysql_select() are made to use select_lex->where;
    in the pre-patch situation select_lex->where was passed as argument
    AND the function assumed that this argument was
    ==select_lex->where... this change makes code simpler.
    
    Made some functions which use JOIN to rather use, and belong to, SELECT_LEX:
    record_join_nest_info
    simplify_joins
    convert_subquery_to_semijoin
    resolve_subquery
    flatten_subqueries.
    They try to use JOIN as little as reasonably possible.
    
    Moved JOIN::prepare() to select_lex, and simplified its signature
    (arguments can be found in select_lex).
    Made setup_conds() member of select_lex, with less arguments.
    Removed arguments from setup_ref_array().
    
    Simplified setup_wild(), more JOIN members are made private,
    JOIN::join_list removed, reset_nj_counters goes to select_lex.
    
    JOIN::table_list is now used only in [1;31moptim[mization/execution.
    
    JOIN_TAB::on_expr_ref, JOIN/select_lex::where/conds/having are
    renamed, some getters/setters are added.
    
    Simplified select_lex::first_cond_[1;31moptim[mization: rename member to be
    more specific, and removed argument in make_join_statistics().
    
    I remove some hacks which came in previous PS fixes (see the WL for
    bug numbers), because they become superfluous.
    
    Changes in opt trace tests: trace blocks for
    semijoin/outer-join-to-inner move from the join_[1;31moptim[mization block to
    the join_preparation block. In ps-specific tests, where the shown
    trace is of EXECUTE, it implies that now the trace starts with the
    transformation already done (by PREPARE).
    
    See the WL text for overview of goal and code changes.
    
    Additional details are in these commit comments.
    
    @  mysql-test/suite/opt_trace/include/bugs.inc
    
    bug was fixed long ago
    
    @  mysql-test/suite/opt_trace/r/bugs_no_prot_all.result
    
    Also fixed a bad number for the query block where the view is merged
    ("in_select#").
    
    @  mysql-test/suite/opt_trace/r/general2_no_prot.result
    
    fixed trace of view merging/materialization
    
    @  mysql-test/suite/opt_trace/r/general_ps_prot_all.result
    
    Around line 7866, we see:
    "original_condition": "(1 and (`t2`.`s` = 'c') and (`t6`.`d` = `f1`()))",
    "steps": [
      {
        "transformation": "equality_propagation",
    -    "resulting_condition": "(1 and multiple equal('c', `t2`.`s`) and multiple equal(`f1`(), `t6`.`d`))"
    +    "resulting_condition": "(1 and (`t6`.`d` = `f1`()) and multiple equal('c', `t2`.`s`))"
    A multiple equality is not created anymore for t6.d=f1(). I think this
    is ok. Creation of such item requires f1() to be a constant (see
    check_simple_equality() which looks for field=constant), and f1()
    should not be considered constant, it is a stored function which may
    return a different value for each row of t2.
    
    @  mysql-test/t/sp.test
    
    Tests added along the way, when fixing bugs in the prototype
    
    @  mysql-test/r/subquery_all.result
    Result changes in EXPLAIN: they come from the can_skip_order change in sql_union.cc:
    we used to pass a NULL pointer to prepare(), now we instead empty the list in select_lex;
    this has the advantage of showing the [1;31moptim[mization in the query printed by EXPLAIN;
    and this alternative technique looks ok because remove_redundant_subquery_clauses
    does it too.
    
    @  sql/item.h
    
    removed one hack ("real_items" argument), added chop_ref
    
    @  sql/item_cmpfunc.cc
    
    removed one hack ("real_items" argument)
    
    @  sql/item_cmpfunc.h
    
    removed one hack ("real_items" argument)
    
    @  sql/item_subselect.cc
    
    About the removal of "// did we changed top item of WHERE condition":
    - in-to-exists is, as before, a permanent transformation
    - pre-patch, Item_in_subselect::fix_fields() would be passed &JOIN::conds as "ref"
    argument, so when it changes the condition (injects outer=inner
    equality in subquery's WHERE), it changes *ref, which changes
    JOIN::conds, but because it is a permanent transformation, it also
    needs to manually "keep in sync" select_lex->where.
    - post-patch, fix_fields() operates on &select_lex->where_cond, and
    JOIN::where_cond is not "alive" yet (it starts its life in
    JOIN::[1;31moptim[mize()), so no manual syncing is needed.
    
    Likewise, no manual syncing of having_for_explain is needed.
    
    @  sql/sql_base.cc
    
    No manual syncing needed (see comment of item_subselect.cc).
    
    @  sql/sql_delete.cc
    
    Don't pass "conds", just use select_lex->where_cond as input. When we
    want to [1;31moptim[mize the condition, we make a copy of it.
    
    @  sql/sql_lex.cc
    
    Part of the end-of-prepare job of
    st_select_lex::fix_prepare_information() has moved to
    the start-of-[1;31moptim[mize get_[1;31moptim[mizable_conditions().
    One real_item() is removed, in this move.
    
    @  sql/sql_[1;31moptim[mizer.cc
    
    Get trashable copies at start of JOIN::[1;31moptim[mize() and use only them.
    Removed dead code in #ifdef.
    Transformations move to JOIN::prepare(), and the horror of "let's
    update prep_where because we did permanent transformations in
    JOIN::[1;31moptim[mize()", is gone - this saves some copying and memory allocations.
    In simplify_joins(), the part:
              /* If join condition has a pending rollback in THD::change_list */
              join->thd->change_item_tree_place(table->join_cond_ref(), &conds);
    was useless: "conds" is a local variable, &conds could never be found
    when change_item_tree_place() searches. So I replace "conds" by an
    Item** passed in argument. In a test file I added some queries which
    used to break (lack a rollback) due to this useless code.
    In record_join_nest_info(), prep_join_cond; the corresponding job is
    now in get_[1;31moptim[mizable_join_conditions().
    In replace_subcondition(), removed useless manual syncing.
    At the end of flatten_subqueries(), same.
    
    @  sql/sql_prepare.cc
    
    use setup_fields_with_no_wrap, makes less code lines.
    In reinit_stmt_before_use(), don't recreate select_lex->where_cond, it is
    already good (== made of permanent items); only need to clean up those
    items, to make them ready for reusal.
    
    @  sql/sql_resolver.cc
    
    JOIN::prepare now operates only on select_lex->where, not JOIN::conds
    which is now reserved for JOIN::[1;31moptim[mize.
    JOIN::prepare does transformations at its end.
    Some functions now done at "prepare" time are moved to this file.
    
    @  sql/sql_union.cc
    
    We work around an oddity of IN->EXISTS (whose effects were only a
    strange WHERE in the trace, when I started trusting
    select_lex->where_cond instead of always passing conds=NULL
    to JOIN::prepare() of the fake select lex).
    
    @  sql/sql_update.cc
    
    When we start [1;31moptim[mizing UPDATE, we get trashable conditions.
    
    @  sql/sql_view.cc
    
    Fixed a bad number for the query block where the view is merged
    ("in_select#"), this is visible in bugs_no_prot_all.result file.
    
    @  sql/table.h
    
    In TABLE_LIST, m_join_cond becomes the permanent condition,
    m_[1;31moptim[m_join_cond a trashable copy created at start of [1;31moptim[mization.
    Moved all [1;31moptim[mization-only members to one place.

[33mcommit 2a419862337ad32333e894b69eca2396e982a077[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Fri Feb 28 14:32:30 2014 +0530

    Bug#18014565: WRONG RESULT COMPUTATION USING ALL() AND GROUP BY
    
    Problem:
    Aggregation does not happen correctly because substitution for
    subquery is created with wrong item.
    
    Analysis:
    Currently while creating a substitution for ALL/ANY subqueries,
    [1;31moptim[mizer uses "real_item" instead of the ref_item.
    If this happens to be a reference to an aggregate function
    then it would be creating the substitution with SUM_FUNC_ITEM
    rather than the REF_ITEM.
    
    If an aggregate function is present in having clause, [1;31moptim[mizer
    calls split_sum_func2 to add the aggregate functions
    to the list of items in the select list.
    
    Ex: In the following query
    select f1, sum(f2) as sum from t1 group by f1 having sum >
                                                  all (select 1);
    
    "sum" in having clause is a reference to sum(f2). So while
    creating the substitution we create using the real_item of
    "sum" which is sum(f2). In split_sum_func2 we add this
    "sum(f2)" to the item list. As a result the item list now
    becomes "sum(f2), f1, sum(f2)". This results in creation
    of three fields in tmp_table. But both the sum(f2)'s would
    be pointing to the same result_field. So, while aggregating
    same result is added twice.
    Before the fix for Bug#16095534, the type would remain
    REF_ITEM and thereby [1;31moptim[mizer would not be adding "sum"
    to the select item list.
    
    Solution:
    The problem addressed in Bug#16095534 exists only for Item_ref
    objects created while resolving not for the ref objects created
    during parsing. So use real_item only for such items.

[33mcommit dc1c0eb8b6e03d390ffd9f90fe662f23e6e88278[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Feb 28 09:54:21 2014 +0100

    WL#7182 Optimizer Cost Model API
    
    This worklog implements the initial version of a new Cost model API
    for the [1;31moptim[mizer. The existing hard coded cost constants are
    removed from the [1;31moptim[mizer and handler code and moved into
    the new cost model module.
    
    Two new classes are implemented:
    
    * Cost_model_server: This provides cost estimates for operations the
      server does that are not dependent the storage engine or storage
      device type.
    
    * Cost_model_table: This provides cost estimates for basic operations
      on tables that can depend on the storage engine.

[33mcommit 4993303a08424a75e6d2121825bdd7ac7263362d[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Feb 28 01:50:48 2014 +0100

    Bug#18324285 PERFORMANCE OVERHEAD IN CONNECT DISCONNECT WITH PERFORMANCE
    SCHEMA
    
    This fix is a performance improvement.
    
    1)
    
    When a thread connects, reset of all per thread statistics
    are now delayed until a statistic is actually collected.
    
    This lazy initialization benefits workloads with very short lived sessions,
    for which instrumentation is disabled.
    
    2)
    
    When a thread disconnect, the per thread statistics are aggregated
    to a parent only for statistics that actually collected data.
    
    This [1;31moptim[mization benefits workloads with very short lived sessions,
    for which instrumentation is disabled.
    
    3)
    
    For the statement instrumentation,
    reset of an individual event_name statistic is also now delayed
    until a statistic is actually collected.
    
    This benefit all workloads, because all workloads only contain a few
    types of statements (SELECT, INSERT, UPDATE, DELETE, ...),
    from the very long list of statements supported in MySQL.
    
    Only statements for event names actually executed are aggregated on
    disconnect.
    
    4)
    
    The memory footprint of internal memory buffers is reduced,
    by removing some attributes reserved for future use,
    that were in fact not used.
    
    In particular, statistics for mutexes, rwlocks and conditions
    now need less memory.

[33mcommit c1e9c27ad77b30c323f210bfe158c52efea62641[m
Author: Anitha Gopi <anitha.gopi@oracle.com>
Date:   Wed Feb 26 07:10:00 2014 +0100

    rpl.rpl_[1;31moptim[mize fails frequently on PB2. Moving to experiumental group

[33mcommit a92903844c1fed3c82a413c51a68bee10059553f[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Mon Feb 24 14:47:28 2014 +0100

    WL#7338 Interface for improved records per key estimates
    
    This worklog adds an interface that storage engines can use for providing
    records per key estimates using float values. By using float, the
    records per key estimates used by the [1;31moptim[mizer will be more accurate
    and the rounding issues the current integer based rec_per_key values have
    can be avoided. This should result in more accurate records per key estimates
    and improved query plans.
    
    The interface consists of the following:
    
    -the KEY struct is extended with a new array for storing records per key
     values using float. This corresponds to the existing rec_per_key array.
    -new member functions to the KEY struct for setting and getting
     records per key values.
    
    Note that with this worklog, only the interface is introduced. It is not
    yet used by the [1;31moptim[mizer.

[33mcommit 2b0a9b1193ff07a0ca9302a92df710e9fd3cf383[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Mon Feb 24 13:38:40 2014 +0100

    Bug#18136628: RANGE OPTIMIZER: IMERGE_LIST_OR_TREE() ERROR DUE
                  TO SHALLOW COPY
    
    imerge_list_or_tree() performs the following OR operation in
    the range [1;31moptim[mizer: "<list_of_index_merges> OR <sel_tree>",
    e.g
    
    Index merge list:
       (pred1_idx1 OR pred1_idx2)    <- an index merge
       AND
       (pred2_idx1 OR pred2_idx2)    <- another index merge
    OR
    sel_tree:
       pred3_idx1
    
    as follows: 'sel_tree' is first ORed with
    "(pred1_idx1 OR pred1_idx2)" and then with
    "(pred2_idx1 OR pred2_idx2)" by calls to tree_or(). However,
    tree_or() is allowed to modify its parameters, and because both
    index merges should be ORed with the same 'sel_tree' predicate,
    imerge_list_or_tree() makes a copy of it for each call to
    tree_or(). This is done to avoid the following situation:
    
    Index merge list:
       (col1 < 10 OR <something>)
       AND
       (col1 = 5 OR <something>)
    OR
    sel_tree:
       col1 > 9
    
    imerge_list_or_tree() does:
    1) Perform "col1 < 10 OR col1 > 9" => always true, 'sel_tree'
       is marked accordingly
    2) Perform "col1 = 5 OR true" => always true since 'sel_tree'
       was marked as true. This is obviously incorrect and was
       caused by step 1) modifying 'sel_tree'. The remedy is to
       copy 'sel_tree'.
    
    Without copying 'sel_tree', the range would therefore be:
    
       (TRUE OR <something>) AND (TRUE OR <something)
    => TRUE
    
    While the correct range is:
    
       (TRUE OR <something>)
       AND ((col1 = 5 OR col1 > 9) OR <something>)
    => ((col1 = 5 OR col1 > 9) OR <something>)
    
    The problem in this bug was that while imerge_list_or_tree()
    correctly made a copy of 'sel_tree' before each OR, the copy
    it made was shallow. Results from step 1) therefore still
    affected the copy of 'sel_tree' used in step 2.
    
    The fix is to change the SEL_TREE ctor to make a deep
    copy. The ctor is not used anywhere else, so this only
    affects imerge_list_or_tree()

[33mcommit 9ca915bfa70260cd29806a7cf3ad47a1f04f4f42[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Mon Feb 24 10:27:00 2014 +0100

    Fix for Bug#18172819 CRASH ON DSMRR_IMPL::CHOOSE_MRR_IMPL IN SQL/HANDLER.CC
    
    This patch fixes two issues when an internally created temporary table
    attempts to use MRR/DS-MRR for reading data from the table:
    
    1. In some cases when the [1;31moptim[mizer has created a temporary table for
       a derived table, it calls ha_myisam::multi_range_read_info() to get
       the cost for doing a range scan (this is called from
       get_quick_select_for_ref()). This call will eventually end up in
       DsMrr_impl::choose_mrr_impl() which needs to check whether the
       index is the primary key or not:
    
         keyno == table->s->primary_key
    
       but in this case, the table pointer is NULL which causes the crash.
    
       The reason for this to fail is that for internally created
       temporary tables we do not set the handler::table pointer to point
       to the corresponding TABLE object. To fix this, ensure that when
       creating the TABLE and handler objects for internally created
       tables, the handler object is informed about which TABLE object it
       is representing.
    
    2. When forcing DS-MRR to be used for internally created temporary
       tables (by using the mrr_cost_based [1;31moptim[mizer switch), we hit the
       following assert:
    
         DBUG_ASSERT(h2 == NULL);
    
       in the DsMrr_impl() destructor. The reason for hitting this is that
       the h2 dialog has not been closed and deleted. The cause for this
       to not happen is that the code for derived tables does not call
       handler::ha_reset() when it has finished the statement and just
       closes the handler. The fix for this problem is to extend the
       DsMrr_impl() destructor to close and delete the h2 dialog if this
       has not been done.

[33mcommit 678ea5c5c362ebed2a07ca60eb19be715a239edd[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Tue Feb 18 17:26:14 2014 +0100

    WL#7532: Scan [1;31moptim[misations, asynch signals -> synch signals, use of local variables to assist compiler, improved checksum algorithm, prefetch of fixed part of rows, many new extensive comments, execute up to 3-4 rows before sending asycnh signal in LQH scanning, removal of lots of dead code, minor jam improvements, turned bit variables into 32-bit variables for higher speed

[33mcommit 01fc240b0fcf2b4ee00d113e2a5131a7639c1f73[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue Feb 11 11:15:46 2014 +0100

    Bug#18158812 ENV CFLAGS OVERRIDES DEFAULT COMPILER FLAGS
    
    The default compiler flags for debug/[1;31moptim[mized builds were changed by the patch for
    Bug#13595996 CREATE A CONSISTENT DEVELOPMENT CMAKE OPTION SET AND MAKE IT A DEFAULT
    We pick CFLAGS and CXXFLAGS values using cmake build-type
    Debug or RelWithDebInfo respectively.
    
    However: this was all silently overridden if CFLAGS was set in the environment.
    
    Fix: Remove the special handling of ENV{CFLAGS}.
    If you don't want the builtin defaults: use -DWITH_DEFAULT_COMPILER_OPTIONS=0
    (or set CMAKE_BUILD_TYPE to something other than Debug/RelWithDebInfo)
    
    Also: output relevant C/CXX flags at the end of the cmake run.

[33mcommit 19965922770b2f43107296d9ae9f8460e42de79d[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Tue Feb 4 10:09:12 2014 +0100

    Fix for Bug#18013520 DEBUG CRASH IN OPTIMIZE_TABLE_ORDER::SEMIJOIN_FIRSTMATCH_LOOSESCAN_ACCESS_PATHS.
    Bug introduced in 5.7 in Dec 2013 by the fix for Bug 11762236
    "[1;31moptim[mizer use ref joins where it should use eq_ref"
    
    As pre-condition, semijoin loosescan cost-based logic
    semijoin_loosescan_fill_driving_table_position() needs
    Key_use::bound_keyparts/read_cost/fanout to be filled for each key
    (exactly: for the first Key_use of each key).
    This was, so far, guaranteed by a previous call to best_access_path()
    which calls find_best_ref() which updates members of Key_use-s.
    
    But the guilty bugfix has added, in find_best_ref()
      if some heuristic applies
        then choose this Key_use (of clustered pk) _and_leave_the_function_.
    
    Thus, when we leave find_best_ref(), all Key_use-s located after the
    clustered pk in the JOIN_TAB::key_use array, are not updated. Their
    bound_keyparts, for example, is left to what it was in a previous call
    to find_best_ref(), which was possibly in a completely different
    partial plan.
    
    Thus, semijoin_loosescan_fill_driving_table_position() gets wrong
    input data, and in our bug's case, bound_kepyarts has the stale value
    1, so the function selects semijoin LS, whereas, if
    it had good input data (bound_keyparts==2), it would realize that this
    strategy is impossible.
    
    Later, when more plans are explored, the heuristic does not always
    apply, so bound_keyparts sometimes gets properly reset to 0.
    Finally, the chosen plan is the one with impossible LS.
    fix_semijoin_strategies () is called, wants to setup LS as commanded,
    and crashes because it's impossible (now it sees that it's impossible
    because bound_keyparts is now 0).
    
    It is important to note that the clustered pk chosen by the heuristic
    is not always what LS wants. Look at the crashing query:
    SELECT    table1 . `pk` AS field1
    FROM
    ( C AS table1 INNER JOIN ( ( CC AS table2 STRAIGHT_JOIN D AS
    table3 ON (table3 . `col_varchar_key` = table2 . `col_varchar_nokey`
    ) ) ) ON (table3 . `pk` = table2 . `pk`  ) )
    WHERE (  ( table1 . `col_int_key` , table2 . `col_int_nokey` )  IN
    ( SELECT DISTINCT  SUBQUERY1_t1 . `col_int_key` AS SUBQUERY1_field1 ,
                       SUBQUERY1_t1 . `pk` AS SUBQUERY1_field2
      FROM
      ( CC AS SUBQUERY1_t1 LEFT JOIN CC AS SUBQUERY1_t2 ON
      (SUBQUERY1_t2 . `col_int_key` = SUBQUERY1_t1 . `col_int_nokey`  ) )
      ) ) AND ( table2 . `col_varchar_nokey` = 'd' OR table1
      . `col_int_nokey` IS  NULL )
    GROUP BY field1  ORDER BY table1 . `col_time_key` ASC
    
    In our case, the partial plan was:
     "`CC` `table2`", "`CC` `SUBQUERY1_t1`", "`CC` `SUBQUERY1_t2`",
    "`D` `table3`".
    
    The clustered pk SUBQUERY1_t1.pk, though suitable for ordinary
    (non-sj) ref access, is not suitable for LS because it does not
    handle the equality of col_int_key in IN(). LS needs to examine other
    indexes. For example, if there had been an index on (pk, col_int_key)
    it would have been suitable. To examine all indexes, Key_use data must
    be correct for all of them.
    
    Fix: let LS logic force find_best_ref() to not skip any key.
    
    No testcase; it is not a repeatable crash (varying InnoDB
    statistics?); with RQG it is repeatable after few minutes;
    after applying the fix, I ran RQG for 4 hours without any problem.

[33mcommit bad4ac6af0e10c68f0d9de9333961991f0d87f30[m
Merge: 63941373ffb 1cda5b587a5
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Feb 3 11:26:45 2014 +0200

    Fix Bug#18073495 REPLACE SPACE,OFFSET WITH PAGE_ID_T AND ZIP_SIZE
    WITH PAGE_SIZE_T
    
    A page in InnoDB is identified by a tablespace number (called space or
    space_id in the code) and a page number within that tablespace (called
    page_no, offset or page in the code). Often we also use zip_size along
    with space,page_no to designate the page size and whether the page is
    compressed or not within that tablespace although it can be retrieved,
    knowing the space id. Another parameter that is also used often with
    space,page_no,zip_size is the fold value, derived from space,page_no
    used in some hashing functions.
    
    In this patch we introduce two new classes: page_id that represents a
    page identifier - it contains space,page_no and page_size that holds info
    about the page size and whether it is compressed or not. The main purpose
    of this patch is to simplify the code - e.g. instead of passing 3 or 4
    parameters to functions space,page_no,zip_size,fold pass just one or two
    and to [1;31moptim[mize the fold/zip_size value recalculations by caching the result.
    
    This patch is also a step in the direction of adding support for
    per-tablespace page size.
    
    Approved by:    Kevin (rb:3493)

[33mcommit afc8243dd23624c29012f9606b8d579ab227aeda[m
Merge: 662835ae3a9 de3115aa2f1
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Wed Jan 29 14:24:41 2014 +0530

    Merge from 5.6 to 5.7
    
    
    Bug#17814492 - INVALID RESULTS FROM SUBQUERY WITH IN CLAUSE
    
    Problem:
    Optimizer decided to push down a condition, when the index
    does not have the keypart present in the condition.
    
    Analysis:
    While checking for possible keys to be considered for a query,
    [1;31moptim[mizer comes up with a list of possible keys on derived table.
    In the query given, [1;31moptim[mizer thinks it can generate two keys,
    one having a,b and one having just a.
    Once the possible keys are updated, [1;31moptim[mizer also updates the
    corresponding flags for the field on which index could be used.
    
    While creating the index on derived table, [1;31moptim[mizer sees that
    the length of the key for key a,b will be more than the length
    of MI_MAX_KEY_LENGTH, so it doesn't create the key. But it
    doesn't update the flags in field.
    
    Later this lets [1;31moptim[mizer to think that there is an index which
    has both a and b keyparts and thereby pushing the condition
    a1.b == 'xxx' while reading from the materialized table.
    This eventually results in evaluation against a field in
    record0 that has not been read yet, resulting in lost results.
    
    Solution:
    When [1;31moptim[mizer doesn't create the key on derived table, flags that
    were enabled while adding the derived key to the derived key list
    should be disabled.
    
    This is currently done partially in TABLE::use_index. Do it correctly
    now.

[33mcommit b3002c9c00c492d2255f0563aa24d9f1fea6e14d[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Tue Jan 28 22:42:55 2014 +0100

    Bug#18024455 NUMBER OF ROWS IN PERFORMANCE SCHEMA TABLES
    
    Before this fix, the estimated number of rows for a performance schema
    table was a hard coded value 1000 for most tables.
    
    This value was intended only as a way to convey to the [1;31moptim[mizer
    the information that a table has "many" rows,
    as opposed to none (0) or just one (1),
    which can be [1;31moptim[mizer differently.
    
    This innacurate row count has no negative effects,
    but is still visible in:
    - the output of EXPLAIN
    - the information_schema
    
    In particular, it causes confusion, as the number of rows claimed
    in EXPLAIN is inconsistent with the number or rows really used.
    
    This fix implements a ::get_row_count() method for every performance_schema
    table, which computes a more accurate number of rows,
    based on sizing parameters used when allocating memory for each table.

[33mcommit 4954d467288ea96f2d5387c98bdf75035dca16cd[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Mon Jan 27 13:27:55 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Rename function name and [1;31moptim[mize comment.

[33mcommit 7ca22be9bbbdea846fcf0be8f2008ede0a10a554[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Sun Jan 26 18:54:11 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Define compression thread related functions in rpl_gtid_persist.cc and [1;31moptim[mize code.

[33mcommit 8e5300f7a5fb2cd140be5ee358fc0fb5326e4dba[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 23 14:28:57 2014 +0100

    ndb - cherrypick: disable [1;31moptim[mization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
    revision-id: mauritz.sundell@oracle.com-20140123120237-l5p6o1m3kg8apvu3
    parent: mauritz.sundell@oracle.com-20140115081453-6cglx3zmar7ovdin
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-6.3
    timestamp: Thu 2014-01-23 13:02:37 +0100
    message:
      ndb - disable [1;31moptim[mization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
      Bug #18122881  REVERT FIX FOR BUG #18055285 FOR LOWER VERSIONS (<4.8) OF GCC
    
      GCC_VERSION was not always defined before include of Ndbfs.cpp as in VoidFs.cpp
      so also change to test __GNUC__ and __GNUC_MINOR__ that are predefined for gnu
      compiler.
    
      note that for non gnu compiler __GNUC__ and __GNUC_MINOR__ will be treated as 0 in arithmetics
      and so [1;31moptim[mization not disabled.

[33mcommit 0f460bd66c81c74c167595bedf9531585dc38684[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 23 14:10:57 2014 +0100

    ndb - cherrypick: disable [1;31moptim[mization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
    revision-id: mauritz.sundell@oracle.com-20140123120237-l5p6o1m3kg8apvu3
    parent: mauritz.sundell@oracle.com-20140115081453-6cglx3zmar7ovdin
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-6.3
    timestamp: Thu 2014-01-23 13:02:37 +0100
    message:
      ndb - disable [1;31moptim[mization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
      Bug #18122881  REVERT FIX FOR BUG #18055285 FOR LOWER VERSIONS (<4.8) OF GCC
    
      GCC_VERSION was not always defined before include of Ndbfs.cpp as in VoidFs.cpp
      so also change to test __GNUC__ and __GNUC_MINOR__ that are predefined for gnu
      compiler.
    
      note that for non gnu compiler __GNUC__ and __GNUC_MINOR__ will be treated as 0 in arithmetics
      and so [1;31moptim[mization not disabled.

[33mcommit 812b0e7852f923e5f4d520c3b3b677b4421e992a[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 23 13:02:37 2014 +0100

    ndb - disable [1;31moptim[mization of Ndbfs::readWriteRequest only for gcc 4.8 and up
    
    Bug #18122881    REVERT FIX FOR BUG #18055285 FOR LOWER VERSIONS (<4.8) OF GCC
    
    GCC_VERSION was not always defined before include of Ndbfs.cpp as in VoidFs.cpp
    so also change to test __GNUC__ and __GNUC_MINOR__ that are predefined for gnu
    compiler.
    
    note that for non gnu compiler __GNUC__ and __GNUC_MINOR__ will be treated as 0 in arithmetics
    and so [1;31moptim[mization not disabled.

[33mcommit 4745b919a125d0da3959214f36a9bc5b49f113c7[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Sat Jan 18 17:59:51 2014 +0900

    Bug#17666170 : BTR_PCUR_RESTORE_POSITION CAN TRY OPTIMISTIC RESTORATION FOR BACKWARD CURSOR
    
    btr_pcur_restore_position_func() can try [1;31moptim[mistic restoration also for backward cursor.
    (can skip tree search from root block)
    This is [1;31moptim[mization for btr_pcur_move_backward_from_page() performance.
    
    Approved by Marko Mäkelä, Mattias Jonsson in rb#3841

[33mcommit b7b4e6631910206a24b529bf916fff23b4627527[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 9 17:18:15 2014 +0100

    ndb - disabling [1;31moptim[mization for Ndbfs::readWriteRequest() for gcc 4.4 and up
    
    Temporary workaround for
    Bug #18055285    LOTS OF TESTS FAILS IN CLUB MADNESS WITH NEW GCC 4.8.2 -O3

[33mcommit ad605779e3e40d2a21e8f4889e304fac4272a24a[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Mon Jan 6 09:16:20 2014 +0100

    BUG#18023222 OPTIMIZER TRACE ERROR ON RANGE ANALYSIS OF
                 INDEX ON A BINARY COLUMN
    
    The opt trace code in the the range [1;31moptim[mizer outputs raw binary
    data to the [1;31moptim[mizer trace. This may create unprintable
    characters. This patch makes sure binary data is printed in hex
    format.

[33mcommit 72fdfb69d20076868997a003922561f916ac7f26[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Dec 31 18:42:13 2013 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Resolve the problem between 'RESET MASTER' thread and 'TRANSACTION' theads,
    which will cause inconsistency between GLOBAL.GTID_EXECUTED and gtid table
    after resetting the master. And [1;31moptim[mize code.

[33mcommit fa8adbe6fc4fc323f2adf8aaec421eae919d1e78[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Fri Dec 20 10:34:00 2013 +0100

    Bug#13814468: DYNAMIC RANGE IS SET UP TO BE USED WHEN TYPE
                  CONVERSION PROHIBITS RANGE ACCESS
    
    Consider a predicate of the form
    
       WHERE t1.a OP t2.b
    
    where OP is a comparison operator usable by range access. Assume
    that the join order is (t1,t2). For table t2, the [1;31moptim[mizer may
    choose to use access method DYNAMIC RANGE which means that
    the range [1;31moptim[mizer (and either range access or table/index scan)
    is run for each row in t1. However, if t1.a is not comparable to
    t2.b, the range [1;31moptim[mizer will not be able to create a range
    access plan. In other words, the execution of the range
    [1;31moptim[mizer for each row in t1 will be utterly pointless.
    
    The fix is to detect common cases of incompatibility in the
    range [1;31moptim[mizer when fields from two tables are compared.

[33mcommit 962331ac80c7aa93de189076e140d10126624854[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed Dec 11 11:22:36 2013 +0100

    Addendum to fix for Bug#17875885 "ONE MYSQL ACCESS SERVER IS HUNG".
    
    THD::~THD() will in some cases call Thd_ndb::~Thd_ndb() which used 'curret_thd', leading to the
    crash below (triggered by the ndb_dist_priv MTR test). This commit fixes that, by using Thd_ndb::m_thd
    instead of 'current_thd'.
    
    Stack from core dump:
    #0  0x00007fd46ddb7f8c in pthread_kill () from /lib/x86_64-linux-gnu/libpthread.so.0
    #1  0x0000000000950360 in my_write_core (sig=11) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/mysys/stacktrace.c:433
    #2  0x00000000007dd31c in handle_fatal_signal (sig=11) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/signal_handler.cc:247
    #3  <signal handler called>
    #4  Thd_ndb::~Thd_ndb (this=0x29f2b30, __in_chrg=<[1;31moptim[mized out>) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/ha_ndbcluster.cc:1231
    #5  0x0000000000a1d423 in Thd_ndb::release (thd_ndb=0x29f2b30) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/ndb_thd_ndb.cc:58
    #6  0x00000000009db5e0 in ndbcluster_close_connection (hton=0x245bac0, thd=0x2a9c610) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/ha_ndbcluster.cc:11138
    #7  0x00000000007decac in closecon_handlerton (thd=0x2a9c610, plugin=0x7fffed70a898, unused=0x0) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/handler.cc:725
    #8  0x000000000068875a in plugin_foreach_with_mask (thd=0x2a9c610, func=0x7dec32 <closecon_handlerton(THD*, plugin_ref, void*)>, type=1, state_mask=4294967287, arg=0x0) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/sql_plugin.cc:2024
    #9  0x00000000007decf8 in ha_close_connection (thd=0x2a9c610) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/handler.cc:739
    #10 0x00000000006439b9 in THD::~THD (this=0x2a9c610, __in_chrg=<[1;31moptim[mized out>) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/sql_class.cc:1396
    #11 0x0000000000643c62 in THD::~THD (this=0x2a9c610, __in_chrg=<[1;31moptim[mized out>) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/sql_class.cc:1427
    #12 0x00000000009ddb3c in ndb_wait_setup_func_impl (max_wait=119) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/ha_ndbcluster.cc:11822
    #13 0x00000000005dd3d9 in mysqld_main (argc=53, argv=0x248d908) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/mysqld.cc:4701
    #14 0x00000000005d6674 in main (argc=9, argv=0x7fffed70ab08) at /export/home/tmp/jw159207/mysql/repo/mysql-5.5-cluster-7.2/sql/main.cc:25

[33mcommit acdb426910bfc0adcc0cb393526244b731f49018[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Dec 11 10:39:47 2013 +0100

    Bug#17826757 CMAKE SUPPORT FOR GCC ON SOLARIS
    
    We got segfault with -03 on intel/solaris/gcc/32bit
    
    $gcc -c -Q -O3 --help=[1;31moptim[mizers > foo
    $gcc -c -Q -O2 --help=[1;31moptim[mizers > bar
    
    $diff foo bar | grep enabled
    <   -fgcse-after-reload                         [enabled]
    <   -finline-functions                          [enabled]
    <   -fipa-cp-clone                              [enabled]
    <   -fpredictive-commoning                      [enabled]
    <   -ftree-loop-distribute-patterns     [enabled]
    <   -ftree-partial-pre                          [enabled]
    <   -ftree-vectorize                            [enabled]
    <   -funswitch-loops                            [enabled]
    <   -fvect-cost-model                           [enabled]
    
    this made mysqld crash, so disable it:
     -ftree-vectorize                            [enabled]

[33mcommit 6c137d1c73b1e42e3c5317556b7190f4df1f06de[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Dec 10 16:43:27 2013 +0100

    Bug#17855328: Refactor make_join_statistics()
    
    make_join_statistics() has been refactored based on these principles:
    
    - Make small functions with a single purpose, which are simpler to maintain.
    
    - Add simple tests to control calls to small functions.
      It is often possible to eliminate calls, and skipping a simple
      function call might cause less cache misses than skipping
      a large code block.
    
    - Small functions are simpler to document.
    
    - When possible, the functions are made private member functions of
      class JOIN.
    
    - Passing arguments when data are available in JOIN or SELECT_LEX is
      unnecessary.
    
    - Local variables are given local scope.
    
    - Labels and gotos are eliminated.
    
    - Local variable "s" is renamed to "tab", as in best_access_path().
    
    - Local variable of type TABLE_LIST * is named "tl".
    
    - JOIN_TAB array is built incrementally, so there is no need for
      special error handling. (The old code required an error label for
      the cleanup - this is no longer needed as initialization is done
      so that JOIN::cleanup() will correctly cleanup the array).
    
    - Function set_position() is renamed to JOIN::mark_const_table() and
      argument list is simplified.
    
    Functions added in this refactoring:
    
    JOIN::make_join_plan() (renamed from make_join_statistics())
    JOIN::init_planner_arrays() (initializes arrays for make_join_plan).
    JOIN::propagate_dependencies() (completes dependency analysis)
    JOIN::extract_const_tables() (extracts const tables with <= 1 rows)
    JOIN::extract_func_dependent_tables() (extracts const tables from FD)
    JOIN::update_sargable_from_const() (creates more sargables)
    JOIN::estimate_rowcount() (estimates rowcounts, makes range scan)
    
    Some more functions are made members of class JOIN:
    
    JOIN::make_outerjoin_info()
    JOIN::[1;31moptim[mize_keyuse()
    JOIN::update_depend_map()
    JOIN::remove_const()

[33mcommit 63c946fc304903b072e4dc8a1b965899afd35d48[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Dec 3 18:34:59 2013 +0200

    Fix Bug#70768 Persistent [1;31moptim[mizer statistics often causes LOCK_open stalls
    
    Protect each table's dict_table_t::stat* members with a latch dedicated
    for this table instead of using a global pool of 64 shared latches.
    With 6 tables, the chances of at least two ending up with the same latch
    is 23.9%. With a lots of tables, there are tons of collisions.
    
    Reviewed by:    Kevin (rb:3805)

[33mcommit 1496cc312e678e0c9da3d7f8c2a9ae575a6b184c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Tue Dec 3 14:07:19 2013 +0400

    WL#7304 "Improve MDL performance and scalability by implementing 'fast-path'
    for DML locks".
    
    Since typical user workload consists mostly of DML statements it makes sense
    to improve performance/scalability by [1;31moptim[mizing MDL subsystem for such type
    of statements.
    
    This patch implements "fast-path" for metadata locks acquired by DML
    statements. Acquisition/release of DML lock (S, SH, SW, SR locks) are
    converted into counter increment/decrement (under protection of
    MDL_lock::m_rwlock) instead of more complex code involving list
    manipulation (at expense of acquisition/release of locks typical for DDL
    statements).
    
    Such a step reduces size of critical section associated with
    MDL_lock::m_rwlock and increases scalability/performance in benchmarks
    involving DML workload. Particularly, benchmarking of draft patch
    implementing this idea shown that it provides at least 10% performance
    improvement in single-table OLTP_RO/POINT_SELECT SysBench tests.
    
    Details
    =======
    
    We split all lock types for each of MDL namespaces in two sets:
    
    A) "Unobtrusive" lock types
       1) Each type from this set should be compatible with all other
          types from the set (including itself).
       2) These types should be common for DML operations.
    
    We [1;31moptim[mize acquisition and release of locks of this type by avoiding
    complex checks and manipulations on m_waiting/m_granted sets/lists and
    replacing it with a check of and increment/decrement of integer counters.
    We will call the latter type of acquisition/release "fast path".
    
    2) "Obtrusive" lock types
       1) Granted or pending lock of those type is incompatible with some
          other lock (including itself).
       2) Not common for DML operations
    
    These locks have to be always acquired in the old fashion - involving
    manipulations with m_waiting/m_granted sets/lists, i.e. using "slow path".
    Moreover in the presence of active/pending locks from "obtrusive" set we
    have to acquire even locks of "unobtrusive" type using "slow path".
    
    -------
    
    For GLOBAL/COMMIT and SCHEMA namespaces (i.e. namespaces with lock
    represented by MDL_scoped_lock class):
    
    "Unobtrusive" locks set consists of IX lock type.
    "Obtrusive" locks set consists of S and X lock types.
    
    For all other namespaces (i.e. represented by MDL_object_lock class):
    
    "Unobtrusive" locks set consists of S, SH, SR and SW locks.
    "Obtrusive" locks set consists of SU, SNW, SNRW and X.
    
    -------
    
    To implement the above MDL_lock object got two new members:
    
    - MDL_lock::m_obtrusive_locks_granted_pending_count - number of granted
      or pending locks of "obtrusive" types. Necessary to quickly verify that
      we can grant "unobtrusive" locks without further checking.
    
    - MDL_lock::m_fast_path_granted_count - packed counter of number of
      granted locks of specific "unobtrusive" type which were granted using
      fast-path algorithm and not using "slow path" (e.g. for MDL_object_lock
      we use 20-bit chunks of this counter to represent number of S/SH, SR and
      SW locks acquired).
    
    The above two members are still protected by MDL_lock::m_rwlock lock.
    
    Essentially this patch replaces:
    
    1) addition/removal of "unobtrusive" lock to MDL_lock::m_granted list during
       lock acquisition/release with and incrementing/decrementing of corresponding
       part of m_fast_path_granted_count.
    2) check for granted or pending locks which conflict with "unobtrusive" lock
       is replaced with check on m_obtrusive_locks_granted_pending_count counter.
    
    We still allocate MDL_ticket objects for requests which are satisfied using
    fast path algoritm, but we mark them using MDL_ticket::m_is_fast_path member,
    so we know that such ticket can be released in simplified fashion.
    
    In order for deadlock detection algorithm to work properly we need to do so
    called "materialization" of fast path tickets for thread which is about to
    start waiting. This process clears MDL_ticket::m_is_fast_path flag, add ticket
    to appropriate m_granted list and decrement corresponding
    m_fast_path_granted_packed_count counter under protection of MDL_lock::m_rwlock.
    We also have to do this when acquiring "obtrusive" locks and locks for the
    threads with open HANDLERs.
    
    -------
    
    Unit tests for MDL subsystem were extended to cover scenarios important for
    the changes described above.
    Also this patch changes perfschema.mdl_func test to make it robust against
    line number changes in MDL code.

[33mcommit d8965c5b3b409ba372b00fde7d440249813c4046[m
Merge: 6c737c878bc e3697ba8c33
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Tue Dec 3 09:49:57 2013 +0530

    Bug#11762236 - [1;31moptim[mizer use ref joins where it should use eq_ref
    
    Problem:
    Optimize_table_order::find_best_ref() tries to pick the best index
    to access a table. If multiple indexes can be used, the choice of
    which index to pick is cost based. However, if a non-unique index
    has a rec_per_key estimate of one, the cost of using that index
    would be calculated to be the same as using EQ_REF access on a unique
    index even though EQ_REF guarantees "at most one match" semantics.
    This means that if the non-unique index is evaluated before the unique
    index, the unique index will not be chosen.
    
    Note:
    We should choose EQ_REF over REF, as we know that there will be
    no more than one matching row with EQ_REF while with REF the estimate
    may be completely wrong for this particular value.
    
    
    Solution:
    The best index is choosen using the following priority list
    1) A clustered primary key is always chosen if all key parts have
       equality predicates.
    2) A non nullable unique index with equality predicates on
       all keyparts is preferred over a non-unique index,
       nullable unique index or unique index where there are some
       keyparts without equality predicates.
    3) Otherwise, the index with lowest cost estimate is chosen.

[33mcommit ed3c4c96bfb286f6b40f44d32f8dc829fa495cb9[m
Author: Norvald H. Ryeng <norvald.ryeng@oracle.com>
Date:   Thu Nov 14 14:54:05 2013 +0100

    Bug#17727506 REGRESSION: CRASH IN JOIN::RECOUNT_FIELD_TYPES
    
    This is a regression introduced by the fix for bug #16967281.
    
    Problem: Server may crash when executing an INSERT SELECT with UNION,
    ROLLUP and ON DUPLICATE KEY UPDATE with a subquery.
    
    When an INSERT SELECT statement has an ON DUPLICATE KEY UPDATE clause
    that contains a subquery, the SELECT_LEX_UNIT for that subquery is
    added as the slave of the last SELECT_LEX in the INSERT SELECT, even
    though it is not part of the query itself. This subquery in the update
    values list is not prepared before JOIN::[1;31moptim[mize() on the INSERT
    SELECT.
    
    When JOIN::recount_field_types() tries to recursively call itself on
    subqueries, it also traverses the SELECT_LEX for the update subquery,
    which isn't prepared and doesn't have a JOIN object.
    
    Fix: Don't try to recount field types in unprepared subqueries.

[33mcommit 4d46b7560a4d91c85d10ef68ee349e4b1b4a7e17[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Nov 8 20:58:48 2013 +0100

    Bug#17766582 PERFORMANCE SCHEMA OVERHEAD IN PFS_LOCK
    
    This fix is a general cleanup for code involving atomic operations in the
    performance schema, to reduce overhead and improve code clarity.
    
    Changes implemented:
    
    1)
    
    Removed 'volatile' in the code.
    The C 'volatile' keyword has nothing to do with atomic operations,
    and is confusing.
    
    This is a code cleanup.
    
    2)
    
    Added missing PFS_cacheline_uint32 to atomic counters,
    to enforce no false sharing happens.
    
    This is a performance improvement.
    
    3)
    
    Modified [1;31moptim[mistic locks, for clarity.
    
    Pattern before:
      pfs_lock lock;
      m_lock.begin_[1;31moptim[mistic_lock(&lock);
      m_lock.end_[1;31moptim[mistic_lock(&lock);
    
    Pattern after:
      pfs_[1;31moptim[mistic_state lock;
      m_lock.begin_[1;31moptim[mistic_lock(&lock);
      m_lock.end_[1;31moptim[mistic_lock(&lock);
    
    The new type, pfs_[1;31moptim[mistic_state, better reflects that a state information
    is used in a begin / end section.
    This provides better typing, for type safety.
    
    Adjusted all the code accordingly.
    
    4)
    
    Modified pfs_lock allocation, for performances.
    
    Pattern before:
      m_lock.is_free();
      m_lock.free_to_dirty();
      m_lock.dirty_to_allocated();
      total: 0+1+2 = 3 atomic operations
    
    Note that free_to_dirty() could fail even for free locks,
    because the CAS can use an old version number,
    making the code attempt again another record.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.free_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      total: 2+1 = 3 atomic operations.
    
    Now the code is garanteed to detect free records,
    because free_to_dirty() does an atomic load then a CAS.
    
    Adjusted all the code accordingly.
    
    5)
    
    Modified pfs_lock deallocation, for performances.
    
    Pattern before:
      m_lock.allocated_to_free();
      Total 2 atomic operations.
    
    Pattern after:
      m_lock.allocated_to_free();
      Total 1 atomic operation.
    
    6)
    
    Modified record updates, for performances.
    
    Pattern before:
      m_lock.allocated_to_dirty();
      m_lock.dirty_to_allocated();
      Total 4 atomic operations.
    
    Pattern after:
      pfs_dirty_state dirty_state;
      m_lock.allocated_to_dirty(& dirty_state);
      m_lock.dirty_to_allocated(& dirty_state);
      Total 2 atomic operations.
    
    Adjusted all the code accordingly.
    
    7)
    
    Modified record peek, for reliability.
    
    Pattern before:
      m_lock.is_populated()
      used a dirty read, causing spurious missing records
    
    Pattern after:
      m_lock.is_populated()
      uses an atomic load, for correctness.
    
    This change is expected to resolve spurious test failures,
    where some records in performance schema tables are sometime missing,
    os some statistics (when computed on the fly) are sometime under evaluated.

[33mcommit e3697ba8c33eaf69a744b590053a94358110e457[m
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Wed Nov 6 16:45:49 2013 +0530

    Bug#11762236 - [1;31moptim[mizer use ref joins where it should use eq_ref
    
    Problem:
    Optimize_table_order::find_best_ref() tries to pick the best index
    to access a table. If multiple indexes can be used, the choice of
    which index to pick is cost based. However, if a non-unique index
    has a rec_per_key estimate of one, the cost of using that index
    would be calculated to be the same as using EQ_REF access on a unique
    index even though EQ_REF guarantees "at most one match" semantics.
    This means that if the non-unique index is evaluated before the unique
    index, the unique index will not be chosen.
    
    Note:
    We should choose EQ_REF over REF, as we know that there will be
    no more than one matching row with EQ_REF while with REF the estimate
    may be completely wrong for this particular value.
    
    Solution:
    The best index is choosen using the following priority list
    1) A clustered primary key is always chosen if all key parts have
       equality predicates.
    2) A non nullable unique index with equality predicates on
       all keyparts is preferred over a non-unique index,
       nullable unique index or unique index where there are some
       keyparts without equality predicates.
    3) Otherwise, the index with lowest cost estimate is chosen.

[33mcommit 35b723bcec63c8a8bab361defe79ff7e7a1aee94[m
Author: Norvald H. Ryeng <norvald.ryeng@oracle.com>
Date:   Fri Nov 1 16:02:43 2013 +0100

    WL#1763 Avoid creating temporary table in UNION ALL
    
    Post push fix to remove gcc warning in [1;31moptim[mized build.
    
    The local variable instantiate_tmp_table in
    st_select_lex_unit::prepare() is never set for non-union queries.
    
    Fix: The variable is only used for union queries. Initialize to
    false.

[33mcommit 6958f5f423c89f0c16cca3e34cf12dbb6f4fc4b1[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Wed Oct 30 13:36:43 2013 +0100

    Fix for Bug#17154722 REMOVE COMPILER WARNINGS PRODUCED BY SUN STUDIO COMPILER
    
    Fixes the following compiler warning caused by a local variable having the
    same name as a class variable:
    
       Warning: keyuse hides JOIN::keyuse
    
    The solution for this is to rename the JOIN::keyuse variable to keyuse_array.
    The patch also changes make_join_statistics() and [1;31moptim[mize_keyuse() to use
    JOIN::keyuse_array directly instead of getting it as an argument.

[33mcommit c2be0f9f1e623cc25fc7ba96b9f141b151c5667f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Oct 30 12:55:47 2013 +0100

    WL#7019: Add support for row value constructors in in predicates to range [1;31moptim[mizer
    
    Post-push fix: fix broken build for -DMERGE_UNITTESTS=0

[33mcommit dcf0e0f2e7c9dcd5df08038a3a9823b153c6eaff[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Wed Oct 30 10:56:28 2013 +0100

    WL#7019: Add support for row value constructors in in predicates to range [1;31moptim[mizer

[33mcommit 402fcb0a17406d3aeca052204f1fb75762558726[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Oct 17 19:05:54 2013 +0900

    WL#7050 - InnoDB: Refactor redo log write code for better performance
    
    - This is rewrite of log_write_up_to() to improve its performance in case where innodb_flush_log_at_trx_commit = 2.
    
    In log_write_up_to():
    
    * Remove wait mode. We always wait with one exception. And that is when doing log
    sync from master thread. It makes that synchronous as well because that happens
    only once per second.
    
    * Because we only have one log group therefore we don't need two flush_events.
    * Remove unnecessary fields like written_to_some_lsn, written_to_all_lsn.
    * If only write is requested we don't have to acquire the log_sys::mutex after we
    release it. We currently do that only to do event handling but event handling is
    really only needed in case where flush is requested i.e.: a thread should be
    waiting on the event iff it is interested in flushing. Writes are serialized under
    log_sys::mutex.
    
    This patch was originally written by Inaam Rana.
    
    rb#2389 Approved by Sunny and Yasufumi
    
    ===========
    
    Adjustment for performance was done therough inherited rb#3373
    
    - [1;31moptim[mize log_write_up_to() more
            * remove the second log_sys->mutex obtain also for "innodb_flush_log_at_trx_commit = 1" path
            * remove unnecessary ut_memcpy. (because log_group_write_buf() is protected by log_sys->mutex)
            * remove dirty-read from flush_to_disk=true case. (to avoid regression at some cases)
              (to keep current arbitration for write/fsync contention between log and data file)
            * fix wrong handling of O_DSYNC
    - revive log_buffer_sync_in_background(). (because it needs to be used)

[33mcommit fafdf33d714036f6c59694615dc480478e71fe77[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Oct 16 09:56:18 2013 +0200

    Bug#16924125: LOG_SLOW_STATEMENT COULD BAIL OUT EARLIER IF
                  SLOW LOGGING IS DISABLED
    
    The problem was that checking if the slow query log was enabled,
    was one of the last checks that was done before writing to the
    slow query log. Since the slow query log by default is disabled,
    it makes sense to have this check earlier.
    
    This patch moves the check from Query_logger::slow_log_write() to
    the start of log_slow_applicable().
    
    As this is a minor [1;31moptim[mization with no changes of behavior,
    no test case is added.
    
    The patch also fixes the --log-throttle-queries-not-using-indexes
    startup option so that it requires a value (=# instead of [=#]).

[33mcommit ab1f37e6d5b854410022de2c32290c1304e787ac[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Oct 15 09:48:05 2013 +0300

    A page in InnoDB is identified by a tablespace number (called space or
    space_id in the code) and a page number within that tablespace (called
    page_no, offset or page in the code). Often we also use zip_size along
    with space,page_no to designate the compressed page size within that
    tablespace although it can be retrieved, knowing the space id. Another
    parameter that is also used often with space,page_no,zip_size is the
    fold value, derived from space,page_no used in some hashing functions.
    
    In this patch we introduce a new class that represents a page
    identifier - it contains space,page_no and also the derived values
    zip_size and fold to avoid multiple recalculation or retrieval of the
    latter. The main purpose of this patch is to simplify the code - e.g.
    instead of passing 3 parameters to functions space,page_no,zip_size
    pass just one and to [1;31moptim[mize the fold value recalculation.

[33mcommit 7c3e36efc28d8bd9f9c4ac2086cbd45c2825c4e6[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Thu Oct 10 14:24:07 2013 +0200

    Some more docs of mt.cpp and also some local [1;31moptim[misatons there

[33mcommit 951d0e82aa562f05c4b7859a2906aee7bc2e8639[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Thu Oct 10 13:55:38 2013 +0200

    Bug#17405466: USE_COUNT: WRONG COUNT FOR KEY AT 0X27547278,
                  3 SHOULD BE 4
    
    Consider an index (kp1, kp2, kp3), and a WHERE clause involving
    all three keyparts ('kpx_pred' represents a predicate usable by
    the range access method on keypart 'x'):
    
    "WHERE (kp2_pred OR kp2_pred) AND
            kp3_pred AND
           (kp1_pred OR kp1_pred)"
    
    The range [1;31moptim[mizer first creates a SEL_ARG tree for kp2 with
    two SEL_ARGs, then another SEL_ARG tree for kp3. Both SEL_ARGs
    for kp2 will point to this kp3-SEL_ARG via the next_key_part
    pointer. The use_count of the root of the SEL_ARG tree for
    kp2 is now 1 (it is pointed to by SEL_TREE::keys[]), and
    the use_count of the root of the SEL_ARG tree for kp3 is 2
    (next_key_part pointers from both kp2 SEL_ARGs).
    
    Now the range [1;31moptim[mizer creates a SEL_ARG tree with two
    SEL_ARGs for kp1. Both these need to point to the SEL_ARG tree
    for kp2 via the next_key_part pointer. This should increase the
    use_count of the kp2 SEL_ARG tree by 1 (+2 next_key_part
    pointers for the kp1 SEL_ARG tree, -1 since SEL_TREE::keys[]
    now points to the kp1 SEL_ARG tree and not directly to the
    kp2 SEL_ARG tree).
    
    This increase in use_count also needs to be propagated to the
    kp3 SEL_ARG tree, and this is where the bug was: instead of
    increasing the use count for each next_key_part pointing to
    the kp3 SEL_ARG tree, only the next_key_part of the root of
    kp2 SEL_ARG tree was increased. The fix is to propagate the
    increase to all SEL_ARG trees pointed to by all SEL_ARGs.
    
    Debugging this without printing the full SEL_ARG range trees
    would have been very hard. The patch therefore also adds a
    parameter to print_tree() so that a full printout is done when
    printing to the debug trace but not when printing to [1;31moptim[mizer
    trace.

[33mcommit d0b83b28988c3bdff5a4ac9d4a4112bd574e1d5a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Fri Sep 27 12:02:22 2013 +0200

    Bug#17503130 TABLE PERFORMANCE_SCHEMA.TABLE_HANDLES BECOMES EMPTY AFTER A
    BIG WORKLOAD
    
    Before this fix, the implementation of table performance_schema.table_handle
    would appear empty after the server executed some code,
    causing spurious failures in test cases like mdl_func.
    
    The failure was spurious, because the same test did work after a server
    restart.
    
    The root cause is that the logic to check for locks in
    table_table_handles::make_row() is broken, mismatched locks are used.
    
    It works only by accident after a server restart,
    because the PFS_table::m_lock and the PFS_table_share::m_lock happen to be
    initialized to the same initial value.
    
    The fix is to use the proper lock for end_[1;31moptim[mistic_lock().

[33mcommit ad2521dd3c93416785c3bbcb1dad7b88663c435d[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed Sep 11 14:25:20 2013 +0200

    Traditional update of ndb_join_pushdown*.result files due
    to improved EXPLAIN output
    
    Mainly due to:
     - slightly changed wording
     - improvements to [1;31moptim[mizer

[33mcommit 072f2e154e4cb2f668637399b360af40b829fdc8[m
Author: Evgeny Potemkin <evgeny.potemkin@oracle.com>
Date:   Mon Sep 9 11:10:38 2013 +0400

    Bug#17428655: INCORRECT EXPLAIN JSON PRINTOUT OF MATERIALIZED VIEWS
    After refactoring done for EXPLAIN FOR CONNECTION in order to correctly
    print subqueries they should be appropriately marked during parse time.
    However, this wasn't done for views, as each view is parsed in a separate
    lex. This made EXPLAIN code to think that view's subquery were [1;31moptim[mized
    away.
    Now mysql_make_view correctly marks top SELECT_LEX_UNIT of newly created
    lex.

[33mcommit f36f916124203b36447fef788cc6608909fedf6c[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Wed Sep 4 15:14:09 2013 +0200

    BUG#16982071: ASSERT `TAB->KEYS.IS_SET(KEYNO)' FAILED IN
                  SETUP_SEMIJOIN_DUPS_ELIMINATION
    
    Consider a query where SemiJoin LooseScan is used on a
    subquery. After deciding on join order, the range [1;31moptim[mizer
    is rerun in the "rechecking index usage" code block of
    make_join_select(). After that, tab->keys (which is the bitmap
    of keys that are usable by the query, and is also used to print
    "possible_keys" in EXPLAIN) is overwritten by
    SQL_SELECT::quick_keys under the assumption that the range
    [1;31moptim[mizer sets bits for all usable indexes in
    SQL_SELECT::quick_keys:
    
      if (!sel->quick_keys.is_subset(tab->checked_keys) ||
          !sel->needed_reg.is_subset(tab->checked_keys))
      {
        tab->keys=sel->quick_keys;  // HERE
        ...
      }
    
    Unfortunately, the range [1;31moptim[mizer never sets
    these bits, it sets the bits in the variable with the same
    name in TABLE::quick_keys instead. Thus, tab->keys is
    incorrectly cleared. When setup_semijoin_dups_elimination()
    is called later, it fails on a DBUG_ASSERT() that is meant to
    verify that the index chosen for LooseScan is actually one
    that is applicable (i.e., asserts that the bit is set in
    tab->keys).
    
    Since the SQL_SELECT variable never gets any bits set, it is
    removed by this patch to avoid code duplication and improve
    readability. The few locations that used to check for set
    bits (always 0) in this variable now check TABLE::quick_keys
    instead.
    
    This commit also:
     - removes an unused enum value (SEL_ARGs 'MAYBE' type) from
       opt_range.cc
     - adds info to [1;31moptim[mizer trace: if the range [1;31moptim[mizer cannot
       use an index because the range predicates depend on values
       from another table, the previously printed
       "chosen: false, cause: unknown" is now printed as
       "chosen: false, cause: depends_on_unread_values".

[33mcommit 65b622e2e07c4d278f66a96b3687dd00165f86bd[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Sat Aug 24 12:36:23 2013 +0200

    Fix for Bug#17154722 REMOVE COMPILER WARNINGS PRODUCED BY SUN STUDIO COMPILER
    
    Fixes most of the compiler warnings where a local variable had the same name as
    a class variable in source files related to the [1;31moptim[mizer. These warnings has the
    form of:
    
       Warning: _name_ hides Class::_name_.

[33mcommit 206707fd65bf7239e2d90a2597997b74bff90dc2[m
Merge: d95cabdc738 81d0397fb97
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Aug 9 21:26:03 2013 +0900

    - WL#6326 : InnoDB: fix index->lock contention
    
    The acquisition of node pointer page latches is protected by index->lock latch.
    
    Before WL#6326, index->lock protected all node pointer pages either in S or X
    mode, and no individual block->lock were acquired on node pointer pages.
    
    After WL#6326, node pointer pages are protected by individual block->lock
    S-latch or X-latch. The acquisition of node pointer page latches is covered by
    index->lock, for preventing deadlocks.
    
    This WL# depends on WL#6363
    
    (rb#1099 : Approved by Kevin and Jimmy)
    
    
    - WL#6363 : implement SX-lock for rw_lock
    
    InnoDB internally uses rw-lock implementation to keep consistency of internal
    resources. Basically the rw-lock has 2 types S-lock (shared) and X-lock (exluded).
    The fix adds the new type SX-lock (shared excluded) for room to [1;31moptim[mize
    concurrency and improve scalability more.
    
    At least, S-lock and X-lock behave same, and compatible for current code. So,
    nothing changed by only this fix as it is. (no functional/performance changes
    for users)
    
    The new state SX-lock will be used by the future work. (e.g. WL#6326: InnoDB:
    fix index->lock contention)
    
    new state of rw_lock: SX-lock
          | S|SX| X|
        --+--+--+--+
         S| o| o| x|
        --+--+--+--+
        SX| o| x| x|
        --+--+--+--+
         X| x| x| x|
        --+--+--+--+
    
    (rb#1098 : Approved by Inaam and Kevin)

[33mcommit ff9b73f60b88d29a04aed01dc540565999f9e381[m
Author: Martin Hansson <martin.hansson@oracle.com>
Date:   Wed Aug 7 16:15:12 2013 +0200

    Bug#11748775: OPTIMIZER USES WRONG INDEX FOR THE RIGHT TABLE IN LEFT
    JOIN
    
    The range [1;31moptim[mizer used the wrong prerequisite for concluding that a
    table is the inner table of an outer join. It looked at
    TABLE::maybe_null which, if true, means that a table is the inner
    table of an outer join. However, this flag is not unset if the outer
    join is rewritten to an inner join. This caused the range [1;31moptim[mizer to
    bail out on such indexes. The ref [1;31moptim[mizer, on the other hand, used
    the correct information. The effect is that the [1;31moptim[mizer gets a cost
    estimate from the ref [1;31moptim[mizer only, and it is so far off that the
    wrong index gets chosen.
    
    The fix is to make the range [1;31moptim[mizer correctly recognize what is
    actually an inner table of an outer join. This causes it to produce a
    range access plan, but - more importantly - a cost. Thanks to the
    heuristic that range statistics take precedence if the ref estimate
    seems too [1;31moptim[mistic, the index gets rejected, and the obviously
    better index is chosen.
    
    Fixing the bug has the additional benefit that a lot of impossible
    where conditions are now discovered that previously were not, more
    precisely when there is an outer join with an IS NULL predicate on a
    non-nullable column of the inner table, and the outer join is
    rewritten to inner join.
    
    A lot of test cases change. Some change because of the newly
    discovered impossible where clause mentioned above, while some change
    because we can now do a range scan where previously the [1;31moptim[mizer
    thought a table scan was the only option.

[33mcommit bcd614f35479cd94e3c9f6848817ab7d8cfbe020[m
Author: Tiago Jorge <tiago.jorge@oracle.com>
Date:   Mon Aug 5 11:59:52 2013 +0100

    BUG#15909788-TEMPORARY FILES CREATED BY BINARY LOG CACHE ARE NOT PURGED AFTER
                 TRANSACTION COM
    
    Problem:
    When you modify a large amout of data inside a transaction,
    for example a big LOAD DATA INFILE, and the size overcomes the binary log cache
    it will create temporary files in the tmp partition. Those files don't disappear
    until you close the connection, thus potentially filling up all available disk
    space in MySQL configured temp directory.
    
    Analysis:
    
    The binlog uses an internal cache implementation (IOCACHE) to store operations
    which have not yet been commited, as such, not yet written in the Binlog file.
    As stated in the bug, that cache is created per connected
    client, since all commit operations are intercepted by the binlog if it is
    active. When binlog_cache_size overflows, a file is created in the configured
    temporary directory by IOCACHE and it can grow up until
    max_binlog_cache_size (4GB). It has a "greedy" [1;31moptim[mized approach,
    meaning that the create temp file is reused after a commit,
    having its only file seek cursor back to 0
    (mf_iocache.c::reinit_io_cache(...)), but will take the size of
    the largest transaction ever made by that client. When a client leaves, and
    as stated in the bug, IOCACHE is closed and the temporary file is deleted.
    
    Fix:
    The implementation was refactored in order to be intelligent and regain
    resources by reducing the file size to 0 without deleting
    it in the reset() method of the binlog_cache_data class, which is called
    after COMMIT or ROLLBACK operations.

[33mcommit 9bd4cfd4dbe1cc4262bea57a2e5a4b60fbc7877c[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Sun Aug 4 21:11:57 2013 +1000

    WL#6044 - Remove unused sync levels, [1;31moptim[miser the UNIV_SYNC_DEBUG code. Add
    some self check code.

[33mcommit f36a548695d635114f3c9de5095bb4c6e68f034b[m
Author: Tarique Saleem <tarique.saleem@oracle.com>
Date:   Fri Jul 5 18:38:14 2013 +0530

    updated the test with sync point before_reset_query_plan to after_join_[1;31moptim[mize for Range checked queries. since at the moment plan is ready it set to ALL, but when explain for connection hits the query, the query is being executed and at that moment executor uses range.

[33mcommit 945ddbb5a6d2536bf553ca5f5ab58f25ee81d61a[m
Author: Tarique Saleem <tarique.saleem@oracle.com>
Date:   Thu Jul 4 17:19:43 2013 +0530

    Changed the sync point from before_query_reset_plan to after_join_[1;31moptim[mize. At the moment plan is ready it set to ALL, but when explain for connection hits the query, the query is being executed and at that moment executor uses range.

[33mcommit 4c856a70738ae737cdae4021f6445e9a214b103b[m
Merge: 3b5a46bec61 188beb80e2f
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Wed Jul 3 14:16:31 2013 +0530

    Merge from 5.6 to 5.7
    
    
    Bug #16436567:REGRESSION: CRASH AFTER JOIN::MAKE_TMP_TABLES_INFO()
    
    Problem:
    While creating temporary table, server crashes checking if
    [1;31moptim[mizer is using loose_index_scan.
    
    Analysis:
    In get_best_combination(), join_tab structures are created
    based on the number of primary tables, semi-join nests and
    temp_tables. While calculating the number of temp_tables,
    [1;31moptim[mizer estimates it to be 'one' for the query
    "select ( select distinct q.a+ variance(g.a) from g q ) from g; "
    as there is only distinct in it.  As a result space for only
    two join_tab structures is allocated(one for the primary
    table and one for the tmp table).
    
    In make_tmp_tables_info(), we see that
    "using_indirect_summary_function" is enabled and as a result
    [1;31moptim[mizer has to create the second tmp table. Here, its presumed
    that the join_tab structure for the second tmp table is
    already allocated and hence it tries to write on to this memory
    while creating the intermediate temp table.
    In doing so, it over-writes some of the  memory allocated for
    "select" and later leads to crash.
    
    Solution:
    Calculate the number of outer aggregation functions while
    counting field types and use the same to allocate the second
    join_tab structure in get_best_combination.

[33mcommit 922d257d1e27697180f526f18a4f7e1f2c04c3d5[m
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Tue Jul 2 12:24:08 2013 +0530

    Bug#16361170 - assertion failed: join->best_read < 1.7976931348623158e+308,
    file sql_planner.cc
    
    Problem:-
    In Optimize_table_order::greedy_search, When 'best_read < DBL_MAX'
    means that [1;31moptim[mizer managed to find some join order, but when
    we are not able to choose any order, it assert.
    
    Analisys:
    In Item_sum::update_used_tables(), we are setting used_tables_cache.
    But it is correct as long as aggregation is done within the same
    query block. But in case when it is refering to outer query block.
    We are setting to according to outer query tables.
    That's why we can't able to choose the order in
    Optimize_table_order::best_extension_by_limited_search() and
    had assert.
    
    Solution:-
    In case, when we refering to outer_query block, we need to set it
    to OUTER_REF_TABLE_BIT.

[33mcommit 939c68b906f31de3f7d11de39032080aa1e29102[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Thu Jun 27 17:39:55 2013 -0700

    clusterj: improve test reporting
    
    suite/ndb/include/have_java.inc
      make MTR_JAVA behave like JAVA_HOME
      improve message to describe MTR_JAVA behavior
      add message for each Java tested, including successes and failures
      [1;31moptim[mize code to extract common code from unix 32 and 64 bit cases
      remove extraneous map statement
      add /usr/lib/jvm/default-java/bin to paths tested (Ubuntu location)

[33mcommit 085f27db6ef9b56238e2438c294fdb735d827c34[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Jun 10 12:24:00 2013 +0200

    This commit fixes bug#16853897 'NDB_JOIN_PUSHDOWN_BKA.TEST FAILS ON
    SOL10-SPARC'. This bug caused queries using BKA to give wrong results in
    certain cases. This happens if all of the following are true:
    - mysqld runs on a big-endian machine.
    - the table that is buffered in the BKA join cache use a storage
      engine with little-endian data format (e.g. InnoDB or MyISAM).
    - The next table use a storage engine with native-endian data format
      (i.e. ndb).
    
    The keys used for accessing the native-endian table is then in little-endian
    format, giving wrong results. This is caused by an [1;31moptim[mization in the BKA
    code: when possible, it will use a pointer into the BKA chache as a key,
    instead of running the regular code for extracting fields from a record
    and building a key. There is a method called
    JOIN_CACHE_BKA::check_emb_key_usage() that checks if this [1;31moptim[mization is
    possible. The fix consists in adding an extra test to this method, to check
    if both tables have the same endianness.
    
    Presumably the error would also happen if the two tables were switched, such
    that native-endian records were buffered in the BKA cahce and used for building
    keys for a little endian table. But I was not able to write a query that would
    use BKA this way.
    
    This bug breaks an exiting ndb regression test. Unfortunately there does
    not seem to be any way to reproduce this error without ndb, since this
    is the only native-endian storage engine.

[33mcommit 2c86e2d096a42463b7747d5d0d51d53010592ea3[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Jun 3 11:16:48 2013 +0530

    - bug#16845421 INNODB: CRASH WHILE RECOVERING A TRANSACTION
      Follow-up fix to [1;31moptim[mize trx_init invocation.
      Also, fixing existing issues. Transaction freed during recovery
      should do trx_init.
      Transaction freed during normal operation (user/background) should avoid doing
      it as action is already done as part of trx_commit.
    
      Approved by: Sunny (rb#2567)

[33mcommit 7aeaa66df9e6a2c0b3f3b163925fa82b8d612aa7[m
Merge: ef29bad017c 4aa7c5beb8e
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Tue May 28 10:10:13 2013 +0200

    ndb - merge 72 to 73...remove testcase for size_ident as server now gives error for this...not [1;31moptim[mal

[33mcommit 59cca8d98301c90b72eff0e5dab5081bad5c3ea6[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue May 21 15:14:46 2013 +0200

    Fix for Bug#16749788 :
       ASSERTION: !USES_BLOB_VALUE(TABLE->READ_SET) IN MULTI_RANGE_START_RETRIEVALS
    
    Starting with MySQL-5.6-cluster-7.3.1, (bug 16020945) columns in 'read_set' required due
    to implementation specific details of each handler is not any more calculated
    (read: 'guessed') by the [1;31moptim[mizer.  Instead additional columns may be
    added to the read_set by each handler before the table access is
    init'ed.
    
    ha_ndbcluster::get_read_set() was introduced for this purpose.
    
    However, the native MRR implementation in Cluster called get_read_set()
    too late. Thus we failed to detect Blob colums being part
    of the read set, which should have forced is to use use the
    default MRR impl. instead of the native NDB MRR impl..
    
    This caused us to later hit the assert mentioned in the bug synopsis

[33mcommit fd491b45451a22398a9abcdd2b1bb2e29aaef804[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon May 20 14:16:22 2013 +0300

    WL#6871 [1;31moptim[mization.
    
    PageCur::appendEmptyNoZip(): Do not set the next-rec offset of the
    last user record in ROW_FORMAT=REDUNDANT because it is already set.

[33mcommit 1a1b7473b1a00d186fa67e982342f4f00b03e963[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri May 17 13:36:39 2013 +0300

    WL#6871 page creation [1;31moptim[mization.
    
    page_dir_get_nth_slot(): Add parenthesis to help the compiler perform
    constant folding, when calling the macro with a constant n.
    
    page_create_low(): Use low-level byte access to initialize the page,
    instead of invoking the high-level methods.

[33mcommit 318a58339afebdaf91cbcc452fb5fb80d4656e13[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed May 8 08:49:47 2013 +0530

    - WL#6501: fixed linking issue with [1;31moptim[mize build

[33mcommit 5802e8958759a2a28ca98b078ce5926f949dd405[m
Merge: 875cbd4f154 4f6a90c57d8
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Sat Apr 20 13:02:29 2013 +0530

    Bug#16073689 : CRASH IN ITEM_FUNC_MATCH::INIT_SEARCH
    
    Problem:
    In query like
    select 1 from .. order by match .. against ...;
    causes a debug assert failue.
    
    Analysis:
    In union type query like
    
    (select * from order by a) order by b;
    or
    (select * from order by a) union (select * from order by b);
    
    We skip resolving of order by a for 1st query and order by of a and b in
    2nd query.
    
    
    This means that, in case when our order by have Item_func_match class,
    we skip resolving it.
    But we maintain a ft_func_list and at the time of [1;31moptim[mization, when we
    Perform FULLTEXT search before all regular searches on the bases of the
    list we call Item_func_match::init_search() which will cause debug assert
    as the item is not resolved.
    
    
    Solution:
    We will skip execution if the item is not fixed and we will not
    fix index(Item_func_match::fix_index()) for which
    Item_func_match::fix_field() is not called so that on later changes
    we can check the dependency on fix field.

[33mcommit 3755fb864002d824d1d99a1bc2b1b5f1d2997f9a[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Apr 10 10:18:26 2013 +0530

    - Bug #16575799: A RECORD LOCK WAIT HAPPENS IN A DICTIONARY OPERATION
      As part of WL#6469 [1;31moptim[mization we modified code to avoid starting
      trx if table is temp-table but failed to reset to state back to
      init on completion of operation which is usually done by trx_commit
      if trx is started.
    
      Approved by: Sunny (rb#2296)

[33mcommit 364241835905e8450a5ce6ede0d4ca8e95931e17[m
Author: Aditya A <aditya.a@oracle.com>
Date:   Mon Apr 8 11:58:30 2013 +0530

    Bug#11766777    LIKE WILDCARD MATCH FAILS IN SOME CASES
    
    PROBLEM
    --------
    
    The Item_func_like::turboBM_matches() uses Turbo Boyer-Moore
    algorithm which fails to detect some patterns. In the
    traditional brute force methods the search pattern is
    shifted by 1 after each mismatch with the text. Boyer-Moore
    uses some rules to determine the amount of shift to the
    search patterns .These rules are
    
    1. Bad character shifts
    2. Good character shifts
    
    The maximum shift amount the two is chosen and then used to
    shift the seacrh pattern.The Turbo-BM is a modification of
    Boyer-Moore algorithm which remembers the last macthed
    search pattern segment before the mismatch occured and
    uses this information to do a turbo shift which [1;31moptim[mizes
    the shift by skipping the already matched part. The problem
    lays in this turbo shift part, for some patterns the shift
    is incorrect.The implemantation is according to the
    algorithm but gives wrong results. If we change this logic,
    we are not sure how it will effect other search patterns,
    since there is no standard data for testing this algorithm.
    Therefore we decided to go for much safer Boyer-Moore
    algorithm.
    
    FIX
    ---
    We changed the algorithm to implement the original
    Boyer-Moore algorithm instead of Turbo Boyer-Moore
    algorithm.

[33mcommit 510dd48bf510dc0a3bda9e62cede698325d05fdd[m
Author: kevin.lewis@oracle.com <>
Date:   Fri Apr 5 00:21:47 2013 -0500

    WL6742-Improve InnoDB SELECT COUNT(*) performance by using
           handler::records()
    
    As a first step toward implementing WL6605 (Improve InnoDB SELECT
    COUNT(*) performance), where a known record count is adjusted by
    transaction deltas, this worklog implements the handler::record()
    call in InnoDB to return the record count to the [1;31moptim[mizer instead
    of making it count records itself.
    
    This patch improves the cost of an in-memory table scan by about 20%
    since record data does not need to be returned to the [1;31moptim[mizer via
    the handler interface.  It effectively pushes down the counting effort
    from the server into InnoDB, reducing a lot of memcpy calls and data
    conversions.
    
    Just as before, only records visible to the current transaction are
    counted. There is a new testcase in this patch to prove that the
    correct count(*) is returned even when other transactions of each
    isolation level are in various stages of activity.
    
    Approved by Marko in http://rb.no.oracle.com/rb/r/1693/
    QA approval by Amit in http://wl.no.oracle.com/worklog/?tid=6742

[33mcommit bd8898ad818e7faf4f10fa7e7afb49e99543bfaa[m
Merge: 1272ff5a38d e440e2c6b6a
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Apr 2 10:51:29 2013 +0530

    - WL#6470: InnoDB: [1;31moptim[mize DML operations for temporary table
    
      Worklog aims at [1;31moptim[mzing DML for temp-table.
    
      DML operations viz. Insert/Update/Delete are [1;31moptim[mized by turning-off
      redo logging + locking as temp-table are not restored on recovery
      and not visible accross connections.
    
      Approved by: Sunny + Jimmy (rb#1823)

[33mcommit 39900ad9092767ec9cf59ca5855441d51ea37a56[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Wed Mar 20 08:42:08 2013 -0700

    Implement crund adapter for felix node-js adapter
    Batch is implemented the same as each
      create batch begins a transaction
      each operation is executed immediately
        the callback is not awaited but the felix driver internally will serialize operations
      execute batch commits the transaction
    This will be a testing platform for [1;31moptim[mizations of the batch function
      we can experiment with rewriting the sql to consolidate all statements into one

[33mcommit 5c0ef07d7f183cb83c8636f83b729a611231c02b[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Mon Mar 18 16:53:21 2013 +0100

    Bug#16359402 CRASH WITH AGGREGATES: ASSERTION FAILED: N < M_SIZE
    
    Skip the re-allocation check in [1;31moptim[mized mode.

[33mcommit 253b271343019ed066dee2b983192f268218f357[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Mar 14 17:12:43 2013 -0700

    More work [1;31moptim[mizing DBOperation.prepare()

[33mcommit 82972a5ec2499aee4ba396c58e784f9771aecb5d[m
Author: Ahmad Abdullateef <ahmad.abdullateef@oracle.com>
Date:   Wed Mar 13 16:46:37 2013 +0530

    BUG#16366994 - DBUG_ASSERT HIT WHILE EXECUTING UPDATE OR DELETE STATEMENT
    
    ANALYSIS :
    When an UPDATE or DELETE statement is being executed and another
    connection happens to hold locks being accessed by this thread,
    then multi_range_read_next() called by the range [1;31moptim[mizer could
    fail with HA_ERR_LOCK_DEADLOCK or HA_ERR_LOCK_WAIT_TIMEOUT. However,
    this was not detected by mysql_update() and no error was reported
    to the Diagnostics Area. Hence an assertion would be triggered
    in Protocol::end_statement() when trying to send OK or ERROR to
    the client. On release builds the bug caused OK to be sent to the
    client for a statement that had in fact failed.
    
    FIX :
    If call to select->quick->reset() in mysql_update() or mysql_delete() fails then
    an error is reported using file->print_error() to set DA to a proper state.

[33mcommit 02a93fb1191c360177832cae4fa531d3b5db044e[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Mar 12 12:02:12 2013 +0100

    Fix for Bug#16437431: INCORRECT RESULT FROM PUSHED JOIN IN COMBINATION WITH BKA.
    
    The semantics of AQP::Table_access::uses_join_cache() changed from 7.2 to 7.3
    without the callee code being updated:
    
    - In 7.2 'uses_join_cache' should be interpretted as
      'rows from this table are stored in the join cache'
    
    - in 7.3 the interpretation is:
      'join condition on this table refers to rows in a
       join cache (from a previous table)'
    
    So there is an 'offset of 1' difference in the semantics
    between these versions.
    
    I would have prefered to change the name of
    AQP::Table_access::uses_join_cache() in 7.2 and 7.3
    in order to clearify this difference. However this
    would have involved a review from the [1;31moptim[mizer team as
    this interface is in their area of the code... so rather not...
    
    Instead the usage of ::uses_join_cache is updated (only)
    to correct the usage of it.
    
    Fixing this bug also revealed that severeal of the
    ndb_join_pushdown_....test had recorded an incorrect result.
    
    ndb_join_pushdown_bka.result is completely re-recorded
    as part of this fix and the test is now enabled. It also
    turns out that *NO joins are pushed* when BKA is enabled.
    We have to look into how this can somehow be fixed or
    tuned in the [1;31moptim[mizer - A worklog is likely required.

[33mcommit 74e734347642dd13c18757a4040ec59b7d9fcc63[m
Author: Ahmad Abdullateef <ahmad.abdullateef@oracle.com>
Date:   Tue Mar 12 14:35:23 2013 +0530

    BUG#16366881 - LOCK TIMEOUT DURING FILESORT CAUSES ASSERT
    
    ANALYSIS :
    When a query is being executed and another connection happens to hold
    locks being accessed by this thread, then multi_range_read_next()
    called by the range [1;31moptim[mizer could fail with HA_ERR_LOCK_DEADLOCK or
    HA_ERR_LOCK_WAIT_TIMEOUT. However, this was not detected by
    find_all_keys() and no error was reported to the Diagnostics Area.
    Hence an assertion would be triggered in filesort(). On release
    builds the consequence of this bug was that the ER_FILSORT_ABORT
    error was sent to the client rather than the real error reported
    by the storage engine.
    
    FIX :
    If call to select->quick->reset() in find_all_keys() fails then an error is reported
    using file->print_error() to set DA to a proper state.

[33mcommit 61a465368982e3efd24478d5b45504dac6c2a1aa[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Mar 11 12:53:14 2013 +0100

    Fix for Bug#16020945 HA_NDBCLUSTER ADAPTION TO 5.6 RBWR
    
    RBWR (Read Before Write Removal) previously relied on
    the [1;31moptim[mizer calculating the 'read_set' for which rows
    requiring to be read as part of an update or delete
    operation. The handler::table_flags() returned
    HA_PRIMARY_KEY_REQUIRED_FOR_DELETE which caused the
    [1;31moptim[mizer to add primary key columns to the read_set
    for update and delete operations.
    
    The problem with this approach is that the [1;31moptim[mizer
    has insufficient information about how the handler
    will perform the read & update/delete operation at
    this stage. A conservative strategy was thus used which
    in several cases added the primary key column even when
    not required. This later prevented RBWR to 'remove' the
    read where this would have been possible.
    
    This fix removes returning HA_PRIMARY_KEY_REQUIRED_FOR_DELETE
    from ha_ndbcluster::table_flags(). Thus the RBWR [1;31moptim[mizer
    will only see the read_set required by the runtime to evaluate
    any conditions and the new updated values. Any additional
    columns required by ha_ndbcluster in order to update or delete
    the rows and BLOB columns are now added to read_set inside
    the handler after RBWR has been decided.

[33mcommit df9375aaa1cef4d10eabba5c66f1e88637196ddc[m
Author: Ahmad Abdullateef <ahmad.abdullateef@oracle.com>
Date:   Mon Mar 11 17:20:15 2013 +0530

    BUG#16367039 - LOCK TIMEOUT IN UPDATE SUBQUERY CAUSES ASSERT
    
    ANALYSIS :
    When an UPDATE statement containing a nested SELECT statement is being executed
    and another connection happens to hold locks to the rows being accessed by this thread
    then [1;31moptim[mize_cond() in mysql_update() fails due to a lock wait timeout and the
    Diagnostic Area status is set to DA_ERROR.
    
    However this DA status is not checked and the execution continues, subsequently call to
    info.read_record() returns -1 which is a Success condition stating that all records have been
    processed. This results in a call to my_ok() which causes an ASSERT because the DA has already
    been set to DA_ERROR.
    
    In release build however Diagnostics_area::set_ok_status() has explicit code to which set DA_OK
    only if the DA state has not already been set, therefore the behavior remains consistent with debug
    builds.
    
    FIX :
    The DA status is checked for an error status and if it has been set, the execution of the statement
    aborted.

[33mcommit 2e37846bdd076dbe91930efcb684a3803225323e[m
Merge: 86d1dc5af23 04ecc289338
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Wed Mar 6 19:04:28 2013 +0530

    Bug#16272562 - WRONG RESULTS: SELECT DISTINCT ... XOR ..
    
    Problem:-
    The range [1;31moptim[mizer used to set up incorrect ranges for queries with XOR.
    This resulted in missing rows
    
    Analysis:-
    The range [1;31moptim[mizer set up incorrect range for query like this
    select ... where any_field XOR 'any_constant_value'
    
    Solution:
    The fix is to not use range access for the XOR operation by treating
    'a XOR b' as always true. Thus, for conditions like
    
    WHERE <something> AND (a XOR b)
    
    range access can still be used on <something>.
    
    It is possible to [1;31moptim[mize XORs better by treating them as normal ORs and
    let condition filtering remove rows that are 'true XOR true'.
    This will be fixed by WL#5800.

[33mcommit b25ec4102ea972828a7e8156d48963c604a494a3[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Mar 5 12:55:13 2013 +0100

    Fixed incorrect syntax in ndb_join_pushdown_bka.test
    used to enable BKA [1;31moptim[mizer switch.
    
    Then disabled it from MTR tests being run as the SPJ / BKA
    combination seems to be bugy.

[33mcommit 097b5f132fca393d344cba3835b60183236f2f1e[m
Merge: 7a27650278d 96174ec7295
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Tue Mar 5 08:36:04 2013 +1100

    WL#6047 - Do not allocate trx id for read-only transactions
    
    Read view creation is expensive. Two [1;31moptim[misations have been made over
    the past year that mitigate the problem:
    
     1. Special handling of auto-commit-non-locking-ro transactions
    
     2. START TRANSACTION READ ONLY; explicitly
    
    For users to take advantage of these improvements, they have to use
    autocommit or explicitly make the changes to their code to tag the
    transactions as read-only.
    
    This fix handles the case where users don't have to make any changes to
    their code and by default puts all transactions on the read-only list
    unless the transaction is tagged as read-write or is determined to be
    an ac-nl-ro transaction. The former is put on the read-write transaction
    list and a rollback segment is assigned to it. However, for all other
    transactions, they are put on the read-only list but a rollback segment
    is not assigned to such transactions. They are also not flagged as read-only,
    because their intentions are unknown. If a transaction that is tagged as
    read-only but writes to a temporary table it will be assigned a ROLLBACK
    SEGMENT when it first attempts to write to the temporary table.
    
    A transaction is allocated a new ID iff:
    
    1. Read only transaction writing to a TEMP table
    2. When a transaction acquires an X or IX lock on a table
    3. Explicitly tagged as a read-write transaction
    
    Otherwise all transactions by default are treated as read-only and an ID
    is not assigned to them, note: they are not tagged as read-only. Only
    transactions that are explicitly started with "START TRANSACTION READ ONLY"
    are set as read-only.
    
    Some user visible changes:
    For I_S and SHOW ENGINE INNODB STATUS; we print the trx_t instance (pointer)
    value if the transaction has not been assigned an ID.
    
    rb#1059 Approved by Inaam Rana and Jimmy Yang.

[33mcommit f9917c4eb592cbd94ad14157cbf048b18a08a5d1[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Mar 4 16:39:27 2013 +0100

    Fix compiler warning in [1;31moptim[mized build caused by previous commit:
    
    5094 Jan Wedvik 2013-03-04
          This is a fix for bug#16176006 (TUPLE WITH CHECKSUM ERROR SILENTLY DISCARDED).
          The fix corrects an error in an if-predicate that checks which error codes
          that should be propagated to the application (via TC). As it was, any error
          code less than 6000 would cause the current row to be skipped, even those
          codes that should have caused the query to be aborted.
    
          This commit also fixes a related issue: If the scan aborts due to an error from
          TUP and *no* rows had been sent to the API, the correct behavior is to return
          SCAN_FRAGREF to TC right away. If one or more rows has been sent, and locking
          is used, LQH should send a SCAN_FRAGCONF, wait for the next SCAN_NEXTREQ and
          then release the locks and then send SCAN_FRAGREF. As it was, LQH would send
          SCAN_FRAGCONF even when zero rows had been sent. This led to a situation where
          TC would eventually get a timeout while waiting for the api to close the scan.

[33mcommit 9f668214ac351ea90bf6f9eaf560110824ec5378[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Thu Feb 28 14:53:36 2013 +0100

    Bug#16407965: Item::save_in_field_no_warning() does not return correct conversion
                  status for strings
    
    When using Item::save_in_field_no_warning() on string data types, it always
    returns TYPE_OK even when the string is truncated due to being longer than
    the field it is stored into. Due to this, Item::save_in_field_no_warning()
    can not be used to determine if a string value fits completely into a string
    field or not.
    
    This patch changes Item::save_in_field_no_warning() for string items so that
    TYPE_NOTE_TRUNCATED or TYPE_WARN_TRUNCATED is returned if the string does
    not fit in the field.
    
    This patch also contains a few additional changes (in the range [1;31moptim[mizer
    and in the code for execution of subqueries) that used
    Item::save_in_field()/Item::save_in_field_no_warning() but did not handle
    that TYPE_WARN_TRUNCATED got returned.
    
    A unit test for Item_string::save_in_field() and
    Item_string::save_in_field_no_warning() is added. This shows that without this
    change Item_string::save_in_field_no_warning() returns TYPE_OK even when
    a longer string than the field can store is saved in the field.

[33mcommit 9ec3d37a43e8cbb2c7f0443e4b1a6e563ccb3099[m
Author: Nirbhay Choubey <nirbhay.choubey@oracle.com>
Date:   Wed Feb 27 16:44:51 2013 +0530

    Bug#11761614 MYSQLD SEGFAULTS WHEN BUILT USING
        --WITH-MAX-INDEXES=128
    
    The MySQL server bitmap implementation defines two
    template Bitmap classes. One [1;31moptim[mized for 64-bit
    (default) wide bitmaps while the other one is used for
    all bitmaps with non-default width.
    
    In order to [1;31moptim[mize the computations, Bitmap<64> class
    has defined its own member functions for bitmap operations,
    the other one, however, relies on mysys' implementation of
    bitmap (mysys/my_bitmap.c).
    
    Issue 1:
    In mysys bitmap implementation, while setting the prefix
    bits, an extra byte beyond the bitmap buffer can be set
    to 0. Now, as this piece of code is used only for bitmaps
    with non-default width, the issue wasn't visible in the
    64-bit wide (default) builds.
    
    Fixed the offending mysys bitmap function by properly
    adjusting the number of prefix bytes, which is used to
    bzero the remaining bitmap buffer.
    
    Issue 2:
    In case of non-default Bitmap class, the intersect function
    wrongly resets the received bitmap while initialising a new
    local bitmap structure (bitmap_init() clears the bitmap
    buffer) and hence the received bitmap was getting lost.
    
    Fixed by initializing the local bitmap structure by using a
    temporary buffer and later copying the received bitmap
    to the initialised bitmap structure.
    
    Also added a cmake variable to hold the MAX_INDEXES
    value when supplied from the command prompt.
    (eg. cmake .. -DMAX_INDEXES=128U). Checks has been put
    in place to trigger a build failure if MAX_INDEXES value
    is greater than 255.
    
    Test modifications:
    * Introduced include/have_max_indexes_[64|128].inc to
      facilitate skipping of tests for which the output
      differs with different MAX_INDEXES.
    
    * Introduced include/max_indexes.inc which would get
      modified by cmake to reflect the MAX_INDEXES value
      used to build the server. This file simply sets an
      mtr variable '$max_indexes' to show the MAX_INDEXES
      value, which will then be consumed by the above
      introduced include file.
    
    * Some test's outputs were dependent on MAX_INDEXES
      value, they have been moved to new test files.
    
    * Added gunit tests for the fixed issues.

[33mcommit 5477f00935d10a2a12adc6f2c59d5b0baa369d11[m
Merge: 1e6af3993f3 d2bb5f41115
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Feb 26 14:29:19 2013 +0530

    - WL#6469: Optimizing CREATE/DROP performance for temporary tables
      goal: [1;31moptim[mize temp-table ddl performance.
      how: temp-table lifetime is limited to connection/server lifetime
      and so lot of actions like redo logging (needed for recovery),
      writing metadata to SYSTEM tables (needed for re-loading table
      on re-start) can be avoided.
      Post Optimization create/drop performance of innodb-temp-table is
      comparable with memory-temp-table.
    
      Review: rb#1544
      Approved by: Sunny + Jimmy + Michael

[33mcommit fd0fe2906e3fbb39fb430911e178fae4a31a571f[m
Author: viswanatham gudipati <viswanatham.gudipati@oracle.com>
Date:   Tue Feb 19 16:52:45 2013 +0530

    added --replace_column # in the Explain plan test, where row_count varies
    This was alerady approved by [1;31moptim[mizer team

[33mcommit 1469be6c310dc7311299d9fd5ec4dc0eb489424d[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Feb 8 16:10:45 2013 +0530

    - WL#6470: part-2 of [1;31moptim[mizing locks for temp-table

[33mcommit 8a183852885dce52cb4eb9a599b29992178ac1f0[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Feb 8 14:01:26 2013 +0530

    - WL6470:
      - given lifetime and visibility of temp-table, locking overhead of temp-table
      can be reduced. in light of this we are trying to [1;31moptim[mize locks acquired by
      temp-tables.

[33mcommit c96166ff040dd1bec8e7eaababf6a7620384e77e[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Feb 4 16:13:34 2013 +0530

    - WL6470:
      - [1;31moptim[mized code-path for temp-table by avoiding calculation of
        rec_get_offsets.
      - for temp-table redo logging is disabled and so the function
        page_cur_insert_rec_write_log doesn't make sense.
        Even though function use to return w/o logging it use to calculate
        lot of details needed for logging. Avoided this by doing an
        early return if temp-table and logging is disabled.

[33mcommit 996a8d753b807b70676ffd5093a18dd63a9cd8df[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Jan 25 17:16:34 2013 +0530

    - Bug#16207240: INNODB: PB2 TESTS FAILING WITH UN-EXPECTED ERROR
      (HITTING ASSERT-ABORT)
    
      Trying to start InnoDB with invalid params leads to a race condition with
      main thrd exiting exist (detecting invalid param) but InnoDB spanwned
      IO threads continue to initialize leading to an assert (abort).
      Added cleanup action in innobase_start_or_create_for_mysql() to ensure
      on error all IO thrds are notified for STOP with some [1;31moptim[mal wait.
      (notifying IO thrds is existing logic.)
    
      Approved by: Sunny (rb#1890).

[33mcommit b74d92d327a9a3de3e08d3d366f8aa58639f43a5[m
Merge: a4bbe14617a 29aae21d500
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Jan 21 10:57:01 2013 +0530

    - WL#6560: InnoDB: separate tablespace for innodb-temp-tables.
    
      - enable separate tablespace for temp-tables.
      - as part of overal temp-table [1;31moptim[mization we are limiting
        logging and so during crash-recovery for complete cleanup we
        need this separate tablespace.
      - semantics or behavioral changes enabled:
        - temp-tablespace is configurable using cnf using
          innodb_temp_data_file_path. all semantics are same as
          innodb_data_file_path except no use of raw device.
        - temp-tablespace get dynamically generate space-id.
        - temp-tablespace is always re-created on server start.
        - non-compressed temp-tables resides in temp-tablespace.
          compressed temp-tables continue to operate as is.
    
    Test-Case: innodb_wl6560, innodb_wl6560_debug, innodb_wl6560_1.
    
    Testing: pb2 on mysql-trunk-stage found passing.
    
    Approved by: Sunny/Jimmy (rb#1472).

[33mcommit 91b07bb55fc70b8b1aeb21a4a1e1d0032f685aca[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Dec 18 13:53:03 2012 +0530

    - WL#6560: rolling back changes related to select count(*) [1;31moptim[mization as the counter is not updated on rollback

[33mcommit ba4b8b78cb9ff8d1e49f410e4f9d577b7684dc0c[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 28 12:53:15 2012 +0100

    Accepted new result files for ndb_join_pushdown_*.test.
    
    The [1;31moptim[mizer has become better in removing redundant
    where-predicate terms which are also covered by a (eq-)ref.
    This cause some 'Using where' and '... pushed condition...' to
    be removed from the EXPLAIN output
    
    NOTE: There is also a single change in query result of one
    of the test due to this change. It is questionable whether
    the new or old result is correct
    (due to MySQL infamous implicit type conversion)
    
    Olav Sandstaa is invest this, anyway this diff is not
    Cluster related.

[33mcommit 09a0735834ef2bbc00e7eeea9eeefd844a001bed[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 28 09:18:29 2012 +0100

    Accept new ndb_bushy_joins.result with changed EXPLAIN output due to 5.6 [1;31moptim[mizer changes.
    (Obsolete 'Using where' is now eliminated)

[33mcommit 28a3e44921e5ca28b2a47a9cd516b7940908e434[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 27 10:59:20 2012 +0100

    ndb
     - fix warnings from VS10 about unused tablePtr, it's used by sizeof() but only at compile time
      so we expect TablerecPtr tablePtr to be [1;31moptim[mized away(and if not, it's not much extra code)

[33mcommit 7707866a4e107a581b5468bd4fd0845725a1045f[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Nov 12 11:21:03 2012 +0530

    - [1;31moptim[mize count(*) from temp-tables.
      temp-tables are not shared across trx and so [1;31moptim[mizing
      this is simply return currently maintain stats.num_of_rows
      counter.
      [1;31moptim[mizing this is needed as we plan to push innodb for
      [1;31moptim[mizer usage wherein [1;31moptim[mizer can query a table using
      count(*) to findout table cardinality.

[33mcommit 1a7e537d632718e7a404f24657e76fa5ae6ce5a1[m
Merge: eca4cbef5a5 3f13555e300
Author: Aditya A <aditya.a@oracle.com>
Date:   Tue Nov 6 19:09:52 2012 +0530

    Bug#11751825 - OPTIMIZE PARTITION RECREATES FULL TABLE INSTEAD JUST PARTITION
    
    PROBLEM
    -------
    
    [1;31moptim[mize on partiton will recreate the whole table
    instead of just partition.
    
    ANALYSIS
    --------
    
    At present innodb doesn't support [1;31moptim[mize option ,so we do a rebuild of the
    whole table and then call analyze() on the table.Presently for any [1;31moptim[mize()
    option (on table or partition) we display the following info to the user
    
    "Table does not support [1;31moptim[mize, doing recreate + analyze instead".
    
    FIX
    ---
    
    It was decided for GA versions(5.1 and 5.5) whenever the user tries to
    [1;31moptim[mize a partition(s) we will will display the following info the user
    
    "Table does not support [1;31moptim[mize on partitions.
    All partitions will be rebuilt and analyzed."
    
    Earlier partitions were not analyzed.Now all partitions  will be analyzed.
    
    If the user wants to [1;31moptim[mize the whole table ,we will display the
    previous info to the user. i.e
    
    "Table does not support [1;31moptim[mize, doing recreate + analyze instead"
    
    For 5.6+ versions we will raise a new bug to support [1;31moptim[mize() options
    in innodb.

[33mcommit 44b905cedb54cbc20c107da4995e6f63a1200a86[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Nov 1 14:00:37 2012 +0100

    Adapt MySQL Cluster to "WL#6266 Make use of hidden key parts"
     - most member variables of the st_key struct which defines which keys
      used in a query has been renamed.
    - change uses of "key_parts" to "user_defined_key_parts",  used by engines
       not involved in this InnoDB specific [1;31moptim[mization.

[33mcommit 0321455f6bee212a3358fea343f480c1885a5ac3[m
Merge: 0b754ad57dd ae728586797
Author: Tatjana Azundris Nuernberg <tatjana.nuernberg@oracle.com>
Date:   Thu Oct 4 02:55:49 2012 +0100

    Bug#14619935: CRASH IN REGISTER_QUERY_CACHE_TABLE WHEN SELECTING FROM
    VIEWS
    
    In a set-up where a MERGE VIEW had an underlying TEMPTABLE view,
    we asked the (MyISAM-) engine whether we could cache a query.
    MyISAM decides this based on whether it thinks the table in question
    was changed since the query started. In the given setup, not only is
    the object in question a TEMPTABLE, we will also get to it on the 2nd
    iteration (of the loop testing all involved objects to see whether any
    prevents caching) anyway. We now skip the first test for efficiency.
    (In the example that means we don't test the TEMPTABLE of the TEMPTABLE
    VIEW while looking at the MERGE VIEW, but when we look at the TEMPTABLE
    VIEW. At that point, an existing [1;31moptim[mization will fire on account of
    the TEMPTABLE VIEW being materializeable.)
    This issue would only show up if the sandwiched table was a MyISAM
    temptable as then the MyISAM table would try to then try to deref the
    table info despite the handler not having been inited yet. For this bug
    to show, we'd need a TEMPTABLE (specifically asked for with ALGORITHM,
    or forced through use of LIMIT etc.), and it would have to be of MyISAM
    type (due do used data type). Lastly, the issue would not show if we
    weren't using the query-cache anyway (as in that case, we don't have to
    ask the handler whether storing the query is safe).

[33mcommit 1f52dc987c0bf037f3a1fa9f31bdf86fd3a858b7[m
Merge: d102dec9eb4 7e7d6dddd95
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Sep 21 07:57:38 2012 +1000

    Merge from mysql-trunk-wl6445.
    
    Allow InnoDB to work on read-only media e.g., DVD, CDROM . Introduce a new
    configuration parameter:
    
      --innodb-read-only
    
    When this configuration parameter is set:
    
     1. Open ALL the tables in read-only mode
    
     2. If the system was not shutdown cleanly then refuse to start.
    
     3. Disable all background threads except the IO read threads. In a 100%
        pure RO workload deadlocks should not happen.
    
     4. Disable writes to temp files, InnoDB uses temp files to log information
        like deadlocks, monitor output, SHOW ENGINE INNODB STATUS etc.
    
     5. If there are entries in the UNDO log  we should  still be able to start
        normal operation. Purge will not remove the delete marked entries from
        the tables and will not trim the UNDO logs because the purge thread(s)
        will be disabled.
    
     6. Refuse to start if there are entries in the change buffer. The user must
        do a slow shutdown or disable change buffering when creating the database
        that is going to be shipped on RO media.
    
        --innodb-change_buffering=OFF
    
     7. Users should ensure that they set the REDO log file to the smallest size
        possible (1M) before moving to read-only media. The REDO log is only used
        to check whether the database was shutdown cleanly or not.
    
     8. Some [1;31moptim[misations that are not done yet.
        1. Disable read view creation and deletion.
    
        2. Transaction start and commit, running in AUTO-COMMIT mode should
           achieve a similar effect for now.
    
     9. Should be possible to run multiple RO instances against the same
        set of (shared) RO files.
    
     10. This new parameter is independent of --read-only which is a server
        layer parameter
    
    Limitations:
       SHOW ENGINE INNODB STATUS; wont work, it will return an empty set.

[33mcommit 223dfa3b7b86bd3dba033d727738fa53ed1e7395[m
Author: Chaithra Gopalareddy <chaithra.gopalareddy@oracle.com>
Date:   Tue Sep 18 15:01:40 2012 +0530

    Bug#14338686: MYSQL IS GENERATING DIFFERENT AND SLOWER
                  (IN NEWER VERSIONS) EXECUTION PLAN
    PROBLEM:
    While checking for an index to sort for the order by clause
    in this query
    "SELECT datestamp FROM contractStatusHistory WHERE
    contract_id = contracts.id ORDER BY datestamp asc limit 1;"
    
    we do not calculate the number of rows to be examined correctly.
    As a result we choose index 'idx_contractStatusHistory_datestamp'
    defined on the 'datestamp' field, rather than choosing index
    'contract_id'. And hence the lower performance.
    
    ANALYSIS:
    While checking if an index is present to give the records in
    sorted order(datestamp), we consider the selectivity of the
    'ref_key'(contract_id here) using 'table->quick_condition_rows'.
    'ref_key' here can be an index from 'REF_ACCESS' or from 'RANGE'.
    
    As this is a 'REF_ACCESS', 'table->quick_condition_rows' is not
    set to the actual value which is 2. Instead is set to the number
    of tuples present in the table indicating that every row that
    is selected would be satisfying the condition present in the query.
    
    Hence, the selectivity becomes 1 even when we choose the index
    on the order by column instead of the join_condition.
    
    But, in reality as only 2 rows satisy the condition, we need to
    examine half of the entire data set to get one tuple when we
    choose index on the order by column.
    Had we chosen the 'REF_ACCESS' we would have examined only 2 tuples.
    Hence the delay in executing the query specified.
    
    FIX:
    While calculating the selectivity of the ref_key:
    For REF_ACCESS consider quick_rows[ref_key] if range
    [1;31moptim[mizer has an estimate for this key. Else consider
    'rec_per_key' statistic.
    For RANGE ACCESS consider 'table->quick_condition_rows'.

[33mcommit db6e6c0ea1400ec7be509f719d92185a09834bfe[m
Merge: b8042bb12cb a2e852a0d27
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Thu Sep 13 12:20:30 2012 +0200

    Bug#11750963: "RANGE CHECKED FOR EACH RECORD" IS NOT USED FOR
                  CONDITIONS WITH OUTER QUERY REFS
    
    When there are range predicates for a table 't1' that depend on
    a non-constant values, the range [1;31moptim[mizer is reexecuted after
    the table join order has been decided iff 't1' is not the first
    table. The range [1;31moptim[mizer may then use values read from all
    tables 'tx' earlier in the join sequence. This is called
    dynamic range access and is what happens when EXPLAIN shows
    'Range checked for each record'.
    
    However, it is also possible to do dynamic range access for the
    first table in a dependent subquery if one of the conditions
    for that first table depends on an outer reference. This change
    set implements support for using dynamic range access for the
    first table in a dependent subquery.

[33mcommit a2e852a0d2787e06cf48401e2749b558b7f92d0c[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Wed Sep 12 11:15:18 2012 +0200

    Bug#11750963: "RANGE CHECKED FOR EACH RECORD" IS NOT USED FOR
                  CONDITIONS WITH OUTER QUERY REFS
    
    When there are range predicates for a table 't1' that depend on
    a non-constant values, the range [1;31moptim[mizer is reexecuted after
    the table join order has been decided iff 't1' is not the first
    table. The range [1;31moptim[mizer may then use values read from all
    tables 'tx' earlier in the join sequence. This is called
    dynamic range access and is what happens when EXPLAIN shows
    'Range checked for each record'.
    
    However, it is also possible to do dynamic range access for the
    first table in a dependent subquery if one of the conditions
    for that first table depends on an outer reference. This change
    set implements support for using dynamic range access for the
    first table in a dependent subquery.

[33mcommit 3d5e6bfb1ae3c54baf9edcbd038b399e5076a5b2[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Sep 6 11:20:26 2012 +0200

    This commit will (again hopefully) fix the errors that prevent the ndb_big suite
    from running in this branch.
    Obsolete engine-condition-pushdown configuration variable was replaced by
    set [1;31moptim[mizer_switch = 'engine_condition_pushdown=on'

[33mcommit 254fd38d67941fd2e99ce179ba79620f64bc7cdd[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Sat Aug 18 11:47:31 2012 +1000

    WL#6445 - Disable the FTS [1;31moptim[mise thread if in read-only mode.

[33mcommit 4e941105671f8eb32a9d3484557f023d6977adff[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Wed Jul 4 10:15:39 2012 +0200

    After the fix for Bug#14145442 'INSERT IGNORE .. SELECT : PROTOCOL::END_STATEMENT(): ASSERTION `0' FAILED.',
    errors in JOIN::[1;31moptim[mize() are not ignorable anymore.
    The scenario of Bug#11747970 '34660: CRASH WHEN FEDERATED TABLE LOSES CONNECTION DURING INSERT ... SELECT'
    was that:
    - subquery's [1;31moptim[mization fails, no error is sent (statement has IGNORE)
    - this failure makes subquery's first execution be skipped
    - this failure is forgotten
    - subquery is evaluated a second time, second execution is
    attempted, and crashes.
    That cannot happen anymore as a failed [1;31moptim[mization is not ignored.
    So the fix is undone: it is a good thing because it makes one less
    member variable in subselect_single_select_engine.

[33mcommit 629dd406ac211a27e312485d33511711c259be42[m
Author: magnus.blaudd@oracle.com <>
Date:   Wed May 9 14:44:49 2012 +0200

    ndb
     - Turn off cmake's "transitive linking" for the share ndbclient library by
       using the LINK_INTERFACE_LIBARIES property. Previously anyone linking
       against shared ndbclient would also link against against the library which
       ndbclient was created from and thus resolve any unresolved depdendencies
       from those. That is no longer possible and causes a few link failures
       in our test* programs.
     - Add more references to ndbclient_exports.cpp to make more functions
       become exported from the shared ndbclient.
      - Note the number 37 passed to BitmaskImpl::setField, it has to be higher
       than 32 to avoid that BitmaskImpl::setFieldImpl() is [1;31moptim[mized away in
       release compile.
     - Add more code to ndbclient_link_test.cpp to detect any missing symbols
       early.

[33mcommit f12cb4c0a9482237f1fc13061db9d7c0e0042fbb[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu May 3 12:42:49 2012 +0200

    Backported some WL5940 related changes to 7.2-cluster:
    
      The handler extension ::test_push_flag() has now been removed.
      The two remaining HA_PUSH_<foo>-flags related to this extension
      has been made obsolete by:
    
      - HA_PUSH_BLOCK_CONST_TABLE is replaced with the 'test_flag'
        HA_BLOCK_CONST_TABLE. Furthermore it was identified that
        usage of this flag was obsolete at one of the places it was used.
        (As 'system' table [1;31moptim[mization is not possible for handlers
         with non-EXACT statistics)
    
      - HA_PUSH_MULTIPLE_DEPENDENCY has been entirely removed as this
        covered a corner case with questionable advantage of being pushed.
        Previous changes to join_no_more_records() has been reverted.

[33mcommit 08508cdf14fcab87aa7bb130f322363d08b502ec[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 25 11:34:51 2012 +0200

    SPJ Fix: Only try to 'make_pushed_join()' if there are multiple tables in query
    
    We want to keep a 'fast track' through the [1;31moptim[mizer for simple, single table queries.
    Thus we should avoid possible overhead from finding join-pushable parts of the query
    if there are only a single table.

[33mcommit 6bb31d68ccdef3080d2d09c1c09e8c17c03753e1[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Apr 18 11:32:17 2012 +0200

    Fix for bug#13901890 SQL NODE CRASHES DURING INSERT SELECT
    
    There was a bug in the lifetime handling of NdbQuery objects
    where the 'root' of the pushed join had 'type == const'
    
    As part of integrating the pushed join feature into the
    ha_ndbcluster handler implementation, the default implementation
    of handler::index_read_idx_map() was redefined. This method is
    used as the access function to read the (single) row when
    a table is const'ified by the [1;31moptim[mizer.
    
    Basically handler::index_read_idx_map() does (ignoring error handling):
    
      index_init(index, 0);  // Open table for access
      index_read_map();      // Virtual method reading single row
      index_end();           // Return table to 'closed' state + release
    
    As the above 'index_end' would also destruct the resultset from
    the pushed join, including the child result, we could not do that
    for a pushed join.
    
    Therefore, ha_ndblcluster::index_read_idx_map() was implemented *without*
    the final index_end() call - Our investigation at that time indicated that
    this could be omitted as the open tables was cleaned up anyway, either by the
    next operation on the handler instance, or when the NdbTransaction was terminated.
    
    However, this bugs uncovers that there are codepaths where we both:
    
    - Terminate NdbTransaction, which destruct the NdbQuery object.
    - Then call ha_ndbcluster::reset() to clean up the handler, which will
      result in NdbQuery::close() to be called -> Crash!
    
    Working with this problem I realized that:
    
    - In order to increase pushability we have already introduced the
      handler::test_push_flag(HA_PUSH_BLOCK_CONST_TABLE) inside the [1;31moptim[mizer
      which will block the const'ifying during [1;31moptim[mize.
    - It will then only be confusing to still explain these query plan
      having a type == 'const' as they are now really executed as an eq_ref
    - Override ::index_read_idx_map() to suite pushed join execution is
      obsolete if we instead handle these queries as an eq_ref.
    
    These changes does *not* affect the pushability of the queries, nor
    changes number of handler call required to retrieve the data.
    
    However, several EXPLAINS will change where 'type == const' will
    now show 'type==eq_ref' instead which IMHO is more correct.
    
    Also fixes incorrect destruction of NdbQueryImpl objects if
    construction of these fails in NdbQueryImpl::buildQuery().
    In order to ensure propper cleanup of these we should use
    NdbQueryImpl::release() instead of deleting them directly.

[33mcommit 3a0901e0202f15ea7bc1a438ea8e11a154da4ee1[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Mar 29 12:38:31 2012 +0200

    More WL5940 addaption of ORDER/GROUP BY handling in order to address
    comments from reviewers:
    
    In the case of a 'simple'(*) order by expression which can not
    be executed by reading from an ordered index, there is a conflict of
    interests between the [1;31moptim[mizer and the pushed join integration:
    
    For any 'simple' order by expression the [1;31moptim[mizer will establish
    the correct ordering of the first non-const table before starting the
    join. If there is no suitable ordered index, the table itself will be
    filesorted into an intermediate record store. As the table is now
    stored outside NDB, such a presorted table can not pe part of a
    pushed join.
    
    Before this fix this conflict was resolved in favour of maximizing
    usage of pushed joins: The Query Execution Plan (QEP) from the [1;31moptim[mizer was
    changes such that the presort was dropped. Instead the entire resultset
    was calculated (with pushed joins) and then written to a temporary file
    which was later filesorted.
    
    The reviewer commented that changing the QEP (above) as part of the pushed join
    integration was 'questionable, at best'.
    
    This fix changes this by extending our AQP (Abstract Query Plan) with
    functionality to detect when the [1;31moptim[mizer has decided to do
    'filesort_before_join'. If that condition is detected, this presorted
    table is excluded as non-pushable.
    
    To sum up:
    
    - We don't any longer modify QEP as part of pushed join integration.
    - This comes at the cost of some order/group by queries being less pushable.
    
    (*) A 'simple' ordering expression contain only column references to the
        first non-const table in the join_tab's.

[33mcommit 9f017698ea6921c6bcab30c974b850437325c837[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Fri Mar 23 14:33:58 2012 +0100

    SPJ: Addapt 'Abstract query plan' interface and SPJ-[1;31moptim[mizer integration to latest
    review comments. These changes has been made possible by rewrites introduced by
    WL5558 and adding native sorted-scan-scan support to the SPJ interface:
    
    - AQP::Join_plan* argument to several methods can now be made 'const'
      as AQP will not any longer modify the query plan.
    
    - Join_plan::group_by_filesort_is_skippable() & ::order_by_filesort_is_skippable()
      has become obsolete.
    
    - Integrate [1;31moptim[mizer code which sets up the pushed join version
      of *read_first_record accessor functions into pick_table_access_method().
    
    - Change [1;31moptim[mizer such that pick_table_access_method() is called at
      the end of the [1;31moptim[mizer phase when the complete query plan is known
      (Including pushed join execution)

[33mcommit 61236002b5555058e19cc843733effac56617733[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Mar 22 15:18:01 2012 +0100

    SPJ: Implement native support of pushed queries being sorted-scan-scan query.
    
    Until now we didn't allow a scan-scan query which required 'sorted' result
    set to be pushed to the SPJ block. Instead we supressed the 'sorted' request
    and instead dumped the resultset to a temporary file and 'filesorted' it.
    (Explained as 'Using tempfile; Using filesort')
    
    This has been commented as 'questionable as best' by Evgeny which is
    doing the review if WL5940: 'Integrating pushed join with [1;31moptim[mizer & handler interface'
    So we want to change it.....
    
    This fix implement native support of sorted scan-scan by setting the
    parent-scan batchsize to '1' when a sorted-scan-scan request is
    about to be sent to the SPJ block.
    (Dependant child scan/lookups are retreived with 'normal' batchsize)
    
    SPJ API has to NEXTREQ more result until all related child-scan result for
    the single parent row has been retrieved - This will guarantee sorted results
    as all child rows will be retrieved together with its related parent.
    (at the cost of some overhead though).

[33mcommit 883a0a52dc47cbce5225920eb0ba279b1a0d36a3[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Jan 18 13:35:51 2012 +0100

    ndb - use [1;31moptim[mized Bitmap search functions

[33mcommit f4546513aa409fa69232ee390abc227f4aca429c[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Jan 17 13:52:58 2012 +0000

      Refactor ndb_apply_status write_row generation
    
      Existing code splits row buffer initialisation from setting
      and writing.  Looks like an [1;31moptim[misation that's been broken.
    
      This patch pulls out the buffer preparation and writing into
      a method, and calls it from one place, chipping some code and
      locals off the Binlog Injector monolith.

[33mcommit 8e16679462d8071285435cf2760f89bdf5398867[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Wed Jan 11 11:49:05 2012 +0000

    Refactor ndb_apply_status write_row generation
    
    Existing code splits row buffer initialisation from setting
    and writing.  Looks like an [1;31moptim[misation that's been broken.
    
    This patch pulls out the buffer preparation and writing into
    a method, and calls it from one place, chipping some code and
    locals off the Binlog Injector monolith.

[33mcommit 97e73cc45c4e203aace27d437ba007cd852bdef9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 16 15:47:33 2011 +0100

    Added more variants of ndb_join_pushdown, testing it with different [1;31moptim[mizer settings.
    (Likely that even more variants will be added soon)

[33mcommit d7eb573c533a01344568362b151237e1c8d8cfd9[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Nov 15 15:45:16 2011 +0100

    MCP
     - backport latest version of "slave rbwr [1;31moptim[mization" to 5.5-cluster

[33mcommit ac2121835f2128cf46e2625231df9ca28e7f7129[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Thu Nov 10 14:33:56 2011 +0100

    ndb - add [1;31moptim[mizer_search_depth=10 to see if this helps daily RQG not to timeout

[33mcommit 49a6bec8ee517f57defdf0946650873c1fac16a7[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Nov 9 14:10:53 2011 +0100

    Fix for bug#13355055: CLUSTER INTERNALS FAILS TO TERMINATE BATCH AT MAX 'BATCHBYTESIZE'
    
      We have observed SCANREQs with a surprisingly small 'BatchSize' argument as part
    of debugging and tuning SPJ. Where we expected 'BatchSize=64' (Default) we
    have observed values around ~10. This directly translated into sub[1;31moptim[mal performance.
    
    When debugging this, we found the root cause in NdbRecord::calculate_batch_size(), which
    returns the batchsize (#rows) and  arguments for the SCANREQ signal.
    It contained the following questionable logic:
    
     1) Calculate the worst case record length based on that *all columns* are selected
        from a table, and all varchar() columns being filled to their *max limit*.
    
     2) If that record length is such that 'batchsize * recLength' > ,
        reduce batchsize such that batchbytesize would never be exceeded.
    
    This effectively put ::calculate_batch_size() in control of the batchbytesize
    logic. The negative impact if that logic was that 'batchsize' could be severely
    restricted in cases where we could have delivered a lot more rows in that batch.
    
    However, there are logic in LQH+TUP which are intended to keep the delivered batches
    withing the batchsize limits. This is a much better place to control this as
    LQH & TUP knows the exact size of the TRANSID_AI payload being delivered, taking
    actual varchar length and only the selected columns into acount.
    
    Debugging that logic, it turned out that it contained bugs in how the produced
    batchsize was counted: Actually a mixup between whether the 'length' was in
    specified in number of bytes or Uint32. - So the above questionable
    ::calculate_batch_size() logic seems to have been invented only to
    circumvent this bug......
    
    Fixing that bug allowed us to now leave the entire batch control to
    the LQH block.
    
    - ::calculate_batch_size could then be significantly simplified.
    - The specified BatchSize & BatchByteSize arguments could be used as
      specified directly as args in SCANREQ signals.
    - Will likely give better performance (larger effective batches) when
      scanning a table with 'max record length > BatchByteSize / BatchSize'
      (~500 bytes with default config)
    
    
    Fix number of bytes/Uint32 mixup in how m_curr_batch_size_bytes is counted
    ******
    Fix number of bytes/Uint32 mixup in how the SPJ adaptive parallelism count m_totalBytes
    ******
    Simplify ::calculate_batch_size() as LQH now correctly will stay within the specified batch_size rows/bytes limits
    ******
    Remove NdbRecord::m_max_transid_ai_bytes which is now obsolete
    ******
    Remove unused args from NdbRecord::calculate_batch_size()
    ******
    Fix SPJs adaptive paralellism logic to also handle batchsize termination due to BatchByteSize being exhausted

[33mcommit 71e24844e0362ea39cc1d7f674d30967f94efaca[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Oct 18 15:54:36 2011 -0700

    Reimplement query [1;31moptim[mization to improve performance:
    Analyze where clause when it is set on the query.
    Rank possible indexes by the number of terms.
    Highest rank for unique indexes (including PRIMARY)
    At query execution time, decide if a unique index is usable (no null parameters)
    If not, choose the ordered index based on whether the first comparison is usable.

[33mcommit d39e6b253ed1f65dd0af5d2211a55b7437f3066b[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Sep 27 15:33:51 2011 +0200

    Optimization of how parent operations are choosen in SPJ queries.
    
    Patch introduce usage of fanout-statistics in [1;31moptim[mization of
    how we select 'parent relations' for scan childs.
For keyword regression:
[33mcommit 4f3b53f5f7bda603298326bb12b0fdc7c65afc02[m
Author: Oystein Grovlen <oystein.grovlen@oracle.com>
Date:   Fri Sep 11 13:27:40 2015 +0200

    Bug#20701585  SEMI-JOIN DUPLICATE ELIMINATION GIVES LOWER COST THAN NO DUPLICATE ELIMINATION
    
    After the introduction of filtering estimation for conditions
    (WL#6635), the expected number of rows selected from a table may be
    between 0 and 1.  Some of our algorithms are not prepared for this.
    One observed effect is that semi-join DuplicateWeedout plans include
    tables in the duplicate-eliminating range that are not relevant to the
    semi-join (so-called nt tables).  This happens because extending the
    range with a table where the estimated number of rows are less than
    one make the cost go down.
    
    This patch solves this issue by ensuring that filtering estimates
    always gives at least one row.  I think there are several reasons to
    do this instead of trying to adapt our code to work with values below
    1:
    
     - WL#6635 already put such a lower limit on the filter effect of a
       single condition.  Hence, values lower than 1 may only happen where
       there are multiple conditions on a table.  However, I think the
       same reasoning could be made in case of multiple conditions.  That
       is, filtering estimates should assume at least 1 row regardless of
       number of conditions.
    
     - The assumption when computing filtering effect of multiple
       conditions is that conditions are non-correlated.  In actual
       databases, this is normally not the case.  Hence, the filtering
       estimates of multiple conditions will often be lower than what is
       actual the case.
    
     - I think it is more important to get an optimized query plan for the
       cases where the result is non-empty than for the case when no
       records are returned.  With very low numbers, one risk that the
       join order has very little impact on the total cost of the query.
       This means that for the cases when there is actual a match, the
       join order may not be optimal.
    
     - It is not trivial to adjust the optimizer code to cope with values
       lower than 1.
    
    Note: The existing limitation on filter, "filter*fanout >= 0.05", in
    calculate_condition_filter() is kept.  I am not convinced this
    limitation makes sense, but dropping it had negative effects on DBT-3
    query 8.
    
    This patch does not contain a new test case.  The "How-to-repeat" part
    of the bug report consists of an existing test case in subquery.inc,
    and the subquery_sj_dupsweed.test shows that an nt table is not longer
    included in the duplicate-producing range.  (See first hunk of
    subquery_sj_dupsweedout.result).
    
    File comments:
    (In addition comes result files where all changes are changes in
    filter and row estimates.  That is, there is no plan changes.)
    
    sql/sql_planner.cc
      Make sure calculate_condition_filter() never returns a number that
      corresponds to less than 1 row in the given table.
    
    mysql-test/include/explain_json.inc
      Added some rows to a table of a test case to get a plan which use
      materialization.  According to comment in the test, this is what was
      intended when test case was added.
    
    mysql-test/include/subquery_sj.inc
      Added some rows to a table of a test case to get same plan as before.
    
    mysql-test/r/derived.result
      Join order has changed, but this should not be relevant for the
      objective of the test.
    
    mysql-test/r/explain_json_all.result
      More records in table leads to different cost estimates.
      Test case now use materialization which was the original intention
    
    mysql-test/r/explain_json_none.result
      More records in table leads to different cost estimates, but no plan change.
    
    mysql-test/r/greedy_optimizer.result
      Query plans show different join order.  New join order is expected
      since tables with lowest filter estimate comes first.  (Previously,
      filter estimates were equal for the tables in question.)
    
    mysql-test/r/greedy_search.result
      One test case has a slight increase in number of partial query plans
      evaluated.
    
    mysql-test/r/partition_explicit_prune.result
      Changes in number of handler accesses.  All changed numbers are
      lower than previously.  (Neither the values before nor after match
      what comments in test says is exepected.)
    
    mysql-test/r/subquery_sj_all.result
    mysql-test/r/subquery_sj_all_bka.result
    mysql-test/r/subquery_sj_all_bkaunique.result
      @@ -3406,12 +3406,13 @@
        Changes from DupsWeedout => MatScan.  According to comment this
        test case is a [1;31mregression[m test case for MatScan.
      @@ -4067,12 +4068,12 @@
        According to comment in test, this test case is to reproduce an
        issue with MatLookup.  MatLookup plan is still used in
        subquery_sj_mat.test.  I doubt that it is possible to get MatLookup by
        default as long as condition filtering is used.
      @@ -4383,9 +4388,9 @@
        Added some rows to one table to get same plan as before.
      @@ -6215,11 +6220,11 @@
        New plan is the same plan as when the test case was pushed
      @@ -9675,11 +9680,11 @@
        Different join order from before, but neither old nor new
        results have plans that are similar to original plan.
    
    mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Changes are similar to subquery_sj_all.result except:
      @@ -4298,11 +4299,12 @@
      @@ -4434,11 +4440,12 @@
      @@ -4542,11 +4549,12 @@
      @@ -4632,11 +4640,12 @@
      @@ -4740,11 +4749,12 @@
        DupsWeedout => MatScan. According to test comment, MatLookup is
        what must be avoided
      @@ -6224,10 +6234,10 @@
        No longer DupsWeedout that covers both semi-join nests, but
        different algorithms for the two. This is a result of DupsWeedout
        no longer including more tables than necessary.  (Note that the
        tests where BNL is allowed has FirstMatch for both.)
    
    mysql-test/r/subquery_sj_dupsweed.result
    mysql-test/r/subquery_sj_dupsweed_bka.result
    mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Differences from subquery_sj_all.result:
      @@ -3378,12 +3378,12 @@
        This is the result change proves the point of this patch.  New
        plan does no longer include nt table in the temporary table range.
      @@ -6190,10 +6194,10 @@
        Different join order, but subquery_sj_all.test covers the original plan
    
    mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes that differ from subquery_sj_dupsweed_bka.result are changes
      to use same plan as subquery_sj_dupsweed_bka.result
    
    mysql-test/r/subquery_sj_firstmatch.result
    mysql-test/r/subquery_sj_firstmatch_bka.result
    mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
    mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      @@ -7066,11 +7070,11 @@
      @@ -7112,32 +7116,6 @@
        New join order, but but subquery_sj_all.test covers the old plan
      @@ -6173,8 +6177,8 @@
        New plan is came as query plan when test case was pushed
      @@ -11311,17 +11324,46 @@
        Table with increased filter estimate comes later in join order
    
    mysql-test/r/subquery_sj_loosescan.result
    mysql-test/r/subquery_sj_loosescan_bka.result
    mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
    mysql-test/r/subquery_sj_loosescan_bkaunique.result
    mysql-test/r/subquery_sj_mat.result
    mysql-test/r/subquery_sj_mat_bka.result
    mysql-test/r/subquery_sj_mat_bka_nixbnl.result
    mysql-test/r/subquery_sj_mat_bkaunique.result
    mysql-test/r/subquery_sj_mat_nosj.result
      Results differences is only in tests that use DupsWeedout because
      LooseScan or Materialization is not applicable.  Plans may be
      different from subquery_sj_dupsweedout.test since these tests have
      turned off DuplicateWeedout and that will effect plan pruning.
    
    mysql-test/r/view.result
    
      Tests get different join order due to different filter estimates for
      one table.  This should be OK since this does not seem to be a
      [1;31mregression[m test case.

[33mcommit 4c4d41c9435d562ce4b92c217080ddc181f6f0de[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Sep 11 09:34:20 2015 +0200

    Bug#21350125: Wrong results when ORDER BY is removed
    
    This is a [1;31mregression[m from the bug fix for bug no. 14358878.
    The fix for that bug was basically to add the nullable property
    for any Item_direct_view_ref object that would wrap an Item
    selected from a derived table/view reference placed on the
    inner side of an outer join, and letting the null value be
    calculated at run time by looking at the NULL status of the first
    table of the derived table.
    
    This works well when the derived table is a single table, or an
    inner join, or a left outer join. However, if the derived table
    is itself a right outer join, the first table of the derived table
    may be the inner table of the right outer join, and that table may
    contain a NULL row even though the derived table as a whole is
    producing a row.
    
    Thus, we can get a NULL indication even though the derived table
    is producing a row, if the inner table of the right outer join that
    is part of that derived table is null-extended.
    
    The solution to the problem is to locate a table within the
    derived table that is not outer-joined within the derived table,
    ie locate a table that does not have the outer_join property set.
    A new function TABLE_LIST::any_outer_leaf_table() is implemented
    to locate such a table.
    
    Notice that this bug is limited to derived tables: A view has the
    same properties as a derived table, but there is one important
    difference: When the view is created, its join nests are normalized
    so that right joins are converted into left joins. Hence, the
    first table of the derived table nest will always be an "outer" table,
    which is good for our calculation. An alternative solution might thus
    be to normalize derived tables like views.
    
    (Derived tables are indeed right-join normalized, but the normalization
    is limited to the join_list structures, and not the table_list
    structures, which are used when navigating tables contained in
    a view/derived table. Whereas views are normalized when written to
    stored dictionary and table_list is reconstructed based on the
    normalized form when view is included into a query.)

[33mcommit 86c19fe60bb54eb048f4b17f84d35072237520d7[m
Author: Dag Wanvik <dag.wanvik@oracle.com>
Date:   Wed Sep 9 16:42:21 2015 +0200

    Bug#17846246 ASSERTION FAILED: ENGINE_TYPE() != HASH_SJ_ENGINE
    
    The problem is related to the changing value of found_rows during the
    query.  It is evaluated more than once and the value in one case gives
    a NULL value for dayofmonth whereas another value gives a non-NULL
    value, which breaks things. In this case the subquery is materialized.
    
    The reason it changes is that initially, the value of found_rows is
    the the number of rows found during the previous query. Then, while
    evaluating the query, the sub-select modifies that value to the number
    of rows returned by the subquery. If those two values are sufficiently
    different (e.g. 101 in the first case and 1 in the second), the
    difference in the resulting null-ness of dayofmonth ensues.
    
    subselect_indexsubquery_engine::copy_ref_key has the
    following information:
    
    if (s_key->null_key) {
      /*
        If we have materialized the subquery:
        - this NULL ref item cannot be local to the subquery (any such
        conditions was handled during materialization)
        - neither can it be outer, because this case is
        separately managed in subselect_hash_sj_engine::exec().
      */
    
    The latter case is of interest here: in this case the search key was
    indeed *not* NULL before the materialization, but now it is NULL in
    the repro, because the number of rows returned from the subquery
    materialization is 1, not 101 as we had initially, and the former
    gives a NULL when fed into dayofmonth, whereas 101 gives 1.
    
    So, when we do the check above we enter and hit the assert. Because
    the code presumes a null value for the search argument would have been
    handled earlier, this case cannot be a case of HASH_SJ_ENGINE
    (asserted).
    
    The patch snapshots (into THD::previous_found_rows) the value of
    THD::limit_found_rows (renamed to current_found_rows) at the end of
    each query and consults that if/when FOUND_ROWS is performed
    throughout the next query.
    
    Also removed some inactive settings of limit_found_rows in
    opt_explain.cc
    
    The patch adds the repro test case to the [1;31mregression[m tests.

[33mcommit ab45d19c674341c8e44f73a6b1c5b5c39ee98f9b[m
Author: V S Murthy Sidagam <venkata.sidagam@oracle.com>
Date:   Wed Sep 9 23:05:09 2015 +0530

    Description:
    1. Hint commentaries are designed to behave like regular /* ... */
    commentaries:
      SELECT /*+ " */ 1; -- a valid SQL query
    However, the mysql client program waits for the closing doublequote
    character.
    2. There is a similar issue with statement delimiters like ';', ''', '`'
    
    Analysis:
    This [1;31mregression[m is introduced by the changes of wl#8016. When we are parsing the input string we need to consider '+' as hint character. Which is not handled.
    
    Fix: Introduced a hint flag for ss_comment and while parsing if we come across "+" we are making ss_comment as hint(SSC_HINT) and reseting it when we came across '*/'.

[33mcommit 4fcdb29c88f24239b2d563403a536054050b755e[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Sep 8 16:53:45 2015 +0200

    Bug#21574933 Extra rows with derived table in subquery + XOR
    
    This problem stems from the transformation of IN subqueries
    in Item_in_subselect::single_value_in_to_exists_transformer().
    
    The source of the [1;31mregression[m is the fix for bug no. 14358878, which
    wrapped all selected columns from a derived table that was on the
    inner side of an outer join in Item_direct_view_ref objects.
    The purpose of the wrapping was to make the columns nullable and
    nullability depending on some table from the derived table.
    
    single_value_in_to_exists_transformer() calculated orig_item from one
    such column. Prior to the wrapping, nullability information was present
    in orig_item. But after the bugfix, real_item() would pass the
    Item_direct_view_ref object and return the underlying Item object,
    which is not necessarily nullable.
    
    real_item() was originally used so that we do not build a permanent
    subquery transformation on top of a runtime item. The wrapped
    Item_direct_view_ref objects are runtime objects, meaning that
    they cannot be used in a permanent transformation.
    The solution is a kludge: If the Item_direct_view_ref object is
    nullable, we set the item being pointed to from the
    Item_direct_view_ref as nullable as well. This means that we pick up
    correct nullability, but the item does not correctly reflect the
    nullability of the datum. However, we have not been able to identify
    a situation where this causes erroneous or non-optimal execution.
    
    Further on, this means that NULL values from outer joined tables
    are included in the query result, meaning that the IN subquery
    can return NULL instead of FALSE in cases like this.

[33mcommit 25c82d63c7b43c686459bcf49631d9545e248705[m
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Fri Sep 4 11:27:48 2015 +0530

    Bug #19286708   REPLICATION BROKEN AFTER CREATION OF SCHEDULED EVENTS
    
    Reverting patch due to a [1;31mregression[m caused by this patch

[33mcommit f0b64bcc9b37a44ed209283f57357d2826fa65ea[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Thu Aug 20 15:16:10 2015 +0800

    BUG#21266784 - PATCH FOR 21052754 SHOWS UP TO AN 11% DROP IN PERFORMANCE
    FOR UPDATE OPS
    
    The patch would fix the L1 cache miss issue. The on/off check and counters
    per mutex should now be on the same cache line.
    
    The patch also fix an issue that we should distinguish BlockWaitMutex from
    WaitMutex.
    
    Finally, the patch does lots of code cleanups to improve the performance,
    by doing statstics in batch mode.
    
    The [1;31mregression[m observed is 4-5% now.
    
    The original patch is provided by Sunny.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 9608

[33mcommit 580b715a1fd720db29768cb4b6b01205daa3558a[m
Author: Christopher Powers <chris.powers@oracle.com>
Date:   Wed Aug 19 14:39:18 2015 -0500

    Bug#21506237 SIG11 AT QUERY_ARENA::STRMAKE IN SQL/SQL_CLASS.H:216
    
    Fixed [1;31mregression[m caused by Bug#20546175 "GET RID OF THE ER() MACRO".
    
    Function get_one_variable() resolves local and remote system vars by
    passing the local and remote THD pointers to sys_var::value_ptr().
    Bug#20546175 changed this so that only the remote THD is passed,
    resulting in a race condition.
    
    The THD pointer arguments to get_one_variable() are now unambiguous.

[33mcommit 38deb6a74c920f7a3b5e277cfc1e5c51c9bcad28[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Aug 17 13:46:03 2015 +0300

    Bug#21640085 RECV_PARSE_LOG_REC VIOLATES ITS CONTRACT RE. INCOMPLETE RECS
    FOR MLOG_CHECKPOINT
    
    When recv_parse_log_rec() encounters an incomplete redo log record,
    it should return 0 instead of the length of the redo log record.
    This was being violated for the MLOG_CHECKPOINT record.
    
    This is a [1;31mregression[m from the Bug#17798076 fix, which increased
    SIZE_OF_MLOG_CHECKPOINT from 1 to 9 bytes.
    
    recv_parse_log_rec(): Return 0 if an incomplete MLOG_CHECKPOINT record
    is encountered.
    
    recv_parse_log_recs(): Remove the check for incomplete MLOG_CHECKPOINT
    records. We will already have returned if recv_parse_log_rec() returned 0.
    
    RB: 9949
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>

[33mcommit 128a99a85a2b4d4267d6c26821fb1952d3bec98d[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Aug 7 12:56:42 2015 +0200

    Bug#21574096: ERROR LOG FILE DISABLED IF MYSQLD IS
                  STARTED AS A WINDOWS SERVICE
    
    The problem was that the error log file by mistake was disabled
    if mysqld was started as a Windows service. This was a [1;31mregression[m
    introduced by the patch for Bug#21328041.
    
    This patch fixes the problem by enabling the error log file on
    Windows unless --help or --console is used (i.e. regardless of
    if mysqld is started as a service or not).
    
    Note that this bug did not affect messages being written to
    the Windows event log.

[33mcommit 489e597d2d887f5c6fe1c015ef77e015a778b88a[m
Author: Guilhem Bichot <guilhem.bichot@oracle.com>
Date:   Fri Jul 31 15:06:50 2015 +0200

    Bug#21139402 ASSERTION FAILED: LENGTH > 0 && KEYPARTS != 0, CRASH IN JOIN::OPTIMIZE_KEYUSE
    
    select 1 from t1 where 1 in
    (select (t2.c is null) in (select t1.a from t1 where t1.b)  from t2);
    
    Scenario, in time order:
    
    1) FIXING 'c IS NULL':
    Item_func_isnull::fix_fields():
    c is a pk column, cannot be NULL, so this item is marked as constant
    with a value of FALSE:
      virtual void update_used_tables()
      {
        if (!args[0]->maybe_null)
        {
          used_tables_cache= 0;            /* is always false */
          const_item_cache= 1;
          cached_value= (longlong) 0;
        }
    
    2) DOING IN->EXISTS FOR SELECT#3:
    Item_in_subselect::single_value_transformer():
    "c IS NULL" wrapped into Item_ref which is is injected into WHERE:
    "WHERE b AND (c IS NULL)=t1.a"
    Item_ref is NOT marked as an outer reference because 'c IS NULL' is
    constant:
    
        // Make the left expression "outer" relative to the subquery
        if (!left_expr->const_item())
          left->depended_from= select->outer_select();
    
    (this if() is introduced by:
    Bug#16369522: Assertion failed: !tables ||
    thd->lex->is_query_tables_locked() with comment:
    "   Notice also that const items need not be designated as outer.
        Ignoring those occurrences was required to preserve multiple equality
        handling for equalities involving constant values.";
    it might be why this bug is a [1;31mregression[m)
    
    Same function does fix_fields() on this new WHERE, which does nothing
    as 'c is NULL' is already fixed.
    
    3) SELECT#2 IS SEMIJOIN-MERGED INTO SELECT#1
    
    We do fix_after_pullout on sj condition and thus on SELECT list of select#2
    and thus on select#3 and thus on WHERE of select#3 and thus on
    the previously injected 'c IS NULL'; this is fix_after_pullout for 'c
    IS NULL':
      if (arg_count)
      {
        for (arg=args, arg_end=args+arg_count; arg != arg_end ; arg++)
        {
          Item *const item= *arg;
          item->fix_after_pullout(parent_select, removed_select);
          used_tables_cache|=     item->used_tables();
          not_null_tables_cache|= item->not_null_tables();
          const_item_cache&=      item->const_item();
    
    'item' is t2.c. The last lines above declare that "IS NULL uses
    t2" and is not constant (anymore!).
    So we now have a non-constant item in select#3, not declared as outer
    reference, involving a table from select#1 (t2 is in select#1 now). This is
    wrong.
    
    Later we have a keyuse for select#3 for '(c IS NULL)=a',
    so keyuse->used_tables= used_tables of 'c is NULL' = map of t2, wrong,
    as t2 is not in select#3. So we try to access a join_tab for t2 in
    select#3 and crash.
    
    Fix:
    when we pull out an Item_func, if it's already constant there's no
    reason it could become non-const; and if it's constant it doesn't
    really use tables, so there's no reason to recalculate
    used_tables_cache and friends.

[33mcommit 9a0574411d5ec7de7fa6f61379315cd780e3fbdd[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Mon Jun 29 13:53:30 2015 +0530

    Bug#20444737 STRING::CHOP ASSERTS ON NAUGHTY TABLE NAMES
    
    DESCRIPTION:
    
    This is a [1;31mregression[m from the fix of Bug#16066637.
    Bug#16066637 introduced a character set conversion while writing
    to binlog. While calculating the length of the converted string,
    system default charset was being used instead of the new charset.
    This led to assert in debug build and crash in asan release build.
    
    FIX :
    
    Use the new charset to which string is converted to calculate the
    length of the string.

[33mcommit 25e1bdb96e862e76e58ef917726f92aa598dce8f[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Wed Jun 24 22:47:36 2015 +0800

    BUG#21255718 ASAN: MEMORY LEAK IN INNOCHECKSUM
    
    It's a [1;31mregression[m of WL#6045: Improve Innochecksum.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 9388

[33mcommit 02c05529e5ba03af2ba7bdd30c8a4f9ae7230693[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Thu Jun 18 13:53:42 2015 +0800

    Bug#21052754 ADD SHOW ENGINE INNODB MUTEX FUNCTIONALITY
    
    Follow up fix, we should check the latch level instead of level
    passed directly from callers in some LatchDebug functions.
    This is a [1;31mregression[m in previous push of this bug.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    over IM.

[33mcommit 14c8af9ece3e0efe1e61aad8b01e1c2598c53e17[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Jun 16 23:37:20 2015 +0200

    Temporarily change default MTR test config to use
    worker thread sending (No send threads) in order to
    get some [1;31mregression[m test coverage of part1
    patch for bug 18390321

[33mcommit 00ec81a9efc1108376813f15935b52c451a268cf[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu Jun 11 13:19:50 2015 +0300

    Bug#21198396 REINTRODUCE ADAPTIVE HASH INDEX FIELD PREFIXES
    TO SPEED UP SEQUENTIAL INSERTS
    
    This is a [1;31mregression[m from the fix of
    Bug#16852278 SIMPLIFY RECORD COMPARISONS
    which removed the n_bytes fields from the adaptive hash index.
    
    For certain workloads, such as LOAD DATA INFILE of sorted data,
    the ability of the adaptive hash index to build indexes on
    byte prefixes of arbitrary binary fields is essential.
    In the reported test case, there is an integer primary key,
    and we end up building adaptive hash indexes on the
    most significant bytes of this key.
    
    This patch will bring back the n_bytes fields by introducing
    variants of the low-level functions.
    The whole-field comparison functions will be left unchanged.
    
    btr_cur_t: Bring back up_bytes, low_bytes.
    btr_search_t: Bring back n_bytes.
    buf_block_t: Bring back n_bytes, curr_n_bytes.
    
    dtuple_fold(), rec_fold(), btr_search_build_page_hash_index():
    Bring back n_bytes.
    
    page_cur_search_with_match_bytes():
    A variant of page_cur_search_with_match().
    
    cmp_dtuple_rec_with_match_bytes():
    A variant of cmp_dtuple_rec_with_match().
    
    page_cur_try_search_shortcut_bytes():
    A variant of page_cur_try_search_shortcut().
    
    ut_pair_min(), ut_pair_cmp(): Bring back these.
    
    cmp_get_pad_char(): Bring back this.
    It was previously called dtype_get_pad_char().
    
    RB: 8961
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    Reviewed-by: Krunal Bauskar <krunal.bauskar@oracle.com>

[33mcommit 21ec004bdc507e0d6d1b2e29985e0e98d0d94196[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Tue Apr 28 16:40:30 2015 +0200

    BUG#20883676: ASSERT `THD->OWNED_GTID.SIDNO == THD::OWNED_SIDNO_ANONYMOUS' AT BINLOG.CC:1144
    
    Background:
    
    For 5.7 binary logs (after WL#7592), the slave applier thread sets
    GTID_NEXT according to the Gtid_log_event or Anonymous_log_event
    preceding every transaction.  For 5.6 binary logs using GTID_MODE=ON,
    it does the same.  But for 5.6 binary log using GTID_MODE=OFF, or for
    5.5 or earlier binary logs, there are no Gtid_log_events or
    Anonymous_gtid_log_event.  In order to set gtid_next correctly also
    for this case, we use a special mechanism: when any
    Format_description_log_event is applied, it sets
    thd->variables.gtid_next.type=NOT_YET_DETERMINED_GROUP.  The next
    statement to execute then sets the real value for gtid_next: if the
    next statement is SET GTID_NEXT, then gtid_next is set accordingly. If
    the next statement is anything else, it assumes that this is an old
    binary log and changes NOT_YET_DETERMINED_GROUP to ANONYMOUS_GROUP.
    
    The code that changes NOT_YET_DETERMINED_GROUP to ANONYMOUS_GROUP
    invoked for SQL statements (from mysql_parse). This normally covers
    also the RBR case, because RBR transactions begin with a
    query_log_event(BEGIN), so it will set gtid_next=ANONYMOUS for the
    BEGIN statement.
    
    However, if the applier thread begins execution in the middle of a
    transaction, it is possible that a row event is processed when
    gtid_next.type is NOT_YET_DETERMINED_GROUP.  This can happen if
    an explicit CHANGE MASTER TO RELAY_LOG_POS statement positions the
    applier thread after the BEGIN statement, or if an explicit CHANGE
    MASTER TO MASTER_LOG_POS statement positions the receiver thread in
    the middle of a transaction.  Therefore, even Rows_log_events need to upgrade
    NOT_YET_DETERMINED_GROUP to ANONYMOUS.
    
    Problem:
    
    Rows_log_events did not upgrade NOT_YET_DETERMINED to ANONYMOUS.  This
    was a [1;31mregression[m introduced by WL#7592 step 14.  Prior to WL#7592 step
    14, this logic was invoked from the function
    gtid_pre_statement_checks, which was invoked from Rows_log_event.
    However in WL#7592 step 14, gtid_pre_statement_checks was split into
    two functions, gtid_pre_statement_checks and
    gtid_pre_statement_post_implicit_commit_checks, where the latter
    contained the logic to upgrade NOT_YET_DETERMINED to ANONYMOUS.  The
    place where Rows_log_event calls gtid_pre_statement_checks was not
    updated so it still only called gtid_pre_statement_checks.
    
    Fix:
    
    Call gtid_pre_statement_post_implicit_commit_checks from
    Rows_log_event.

[33mcommit 91294afcb1a7aef32617846e5170c8912ded52cb[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Wed May 20 17:11:31 2015 +0800

    BUG#21111301 INNODB FTS: ERROR DUPLICATE ENTRY IN FTS_DOC_ID_INDEX ON UPDATE CASCADE
    
    It's a [1;31mregression[m of rb#2089: make cascade operation iterative.
    
    trx->fts_next_doc_id is used by two cascade operations in
    row_ins_cascade_calc_update_vec(), so we have duplicated
    entry error. The fix is simple: allocate a doc id from
    cascade heap for each operation.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 8973

[33mcommit af54cd7838b55c3dc8ded32096edf5138dd72f49[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon May 18 14:16:25 2015 +0200

    Fix [1;31mregression[m in debug build caused by fix for bug#20408733.
    
    Moved assertion from where row_ptr is assigned, to where it
    is used.

[33mcommit 52be064495e6f69180102dcdc0939857f23d977f[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed May 13 13:30:48 2015 +0100

    Bug#21074643: SERVER SETS OPEN_FILES_LIMIT UNCONDITIONALLY
    
    The problem was that when open_files_limit was set as part of
    server startup, the non-Windows check for if this value made
    sense didn't work. The problem is that the check only worked
    of RLIMIT_NOFILE was defined while not including the system
    header (sys/resource.h) that actually defines the symbol.
    This made it possible to e.g. set the value to 1000000 even
    on a system where the open files OS limit was 1024.
    
    This is a 5.7 [1;31mregression[m. On 5.6 the header file was
    included indirectly via other header files.
    
    This patch fixes the problem by including the missing header
    file + removing the check on if the symbol is defined so
    that similar [1;31mregression[ms in the future will give
    compilation error.

[33mcommit 34f306b5fd2b11f7c789e61f80623cd51a96a4c8[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Mon May 11 09:13:02 2015 +0800

    BUG#21041449 ASSERT IN I_INNODB.INNODB_BUG16244691
    
    It's a [1;31mregression[m of BUG#20926253 VALGRIND FAILURE IN INNODB
    .ALTER_MISSING_TABLESPACE(rb#8717).
    
    We allocated space from table heap in dict_load_foreign, but didn't
    add the length to dict_sys->size.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 8850

[33mcommit d69af47462b72b8de7bf9aacf17d41201613e394[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Sat Apr 11 00:06:45 2015 +0300

    rb8590 Fixes to sporadically failing on PB2 rpl_xa_survive_crash_debug
    
    The test fails due to not following a valid pattern of crash-simulation
    with the following restart.
    It's fixed to follow the pattern.
    Extensive run (1600 times) has not shown any [1;31mregression[m now.
    
    As a side effect a non-determinism of XA ROLLBACK is worked around
    to be fixed in a separate bug fixing as commented.

[33mcommit 504dfe6609b0e48afd87901ea89c34d6024a3984[m
Author: Kevin Lewis <kevin.lewis@oracle.com>
Date:   Tue Mar 24 14:20:49 2015 -0500

    Bug #20759613   ALTER TABLE CAN CRASH THE SERVER
    
    This is a [1;31mregression[m caused by
      Commit: 7e7d64a7e34a5c141b4f9f534ddd8320f3461f9d [7e7d64a]
      Author: Mattias Jonsson <mattias.jonsson@oracle.com>
      Date: March 20, 2015 at 11:00:12 AM CDT
      Bug#20554858: INNODB: ADD TABLESPACE SUPPORT FOR NATIVE PARTITIONING
    which was reviewed by me.  That patch split m_file_per_table into two
    booleans, but did not use them correctly.
    
    Here, those variables and related routines are renamed for clarity,
    the correct variables are used, and m_use_file_per_table is fully
    initialized in one place. The test added to create_tabespace.test
    fails without this code fix in the same way that the bug reports
    the failure in n_test.

[33mcommit 518d3e437ca603d697d7d18e454fe6e56bdd17d2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a [1;31mregression[m from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the flush-time check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    (cherry picked from commit fc1f91f396bc73fcea72f17d6c22c174ba057e9b)

[33mcommit c998472c0096f2102eaa5970d4ec0cacea1946b2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a [1;31mregression[m from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the flush-time check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>

[33mcommit 0b5894ce581d3a5c82847bd6e73448821b80de8c[m
Author: Gleb Shchepa <gleb.shchepa@oracle.com>
Date:   Tue Mar 10 12:36:30 2015 +0400

    Bug #20087571: 5.7 EASILY HITS ERROR 1436 (HY000): THREAD STACK OVERRUN ERRORS ON MANY EXPR'S
    
    This bug is a [1;31mregression[m of WL7200: that WL introduced an unconditional
    recursive processing of AND and OR parse tree nodes during the
    contextualization, when the original parser had some conditional
    flattening of nested AND/OR expression.
    That affected very long recursive AND/OR expression: instead of the normal
    execution the parser failed with a parse error: ER_STACK_OVERRUN_NEED_MORE.
    
    This bugfix re-introduces an optimization that flattens recursive AND/OR
    expressions at parse time (before the itemization/contextualization):
    
     (X1 AND X2) AND (Y1 AND Y2) ==> AND (X1, X2, Y1, Y2)
     (X1 AND X2) AND Y ==> AND (X1, X2, Y)
     X AND (Y1 AND Y2) ==> AND (X, Y1, Y2)
    
    and the same for OR.

[33mcommit 4e2b1fca6716f1c768c96a4873e8e36d7f553317[m
Author: Maitrayi.Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed Mar 4 17:33:25 2015 +0100

    Fix [1;31mregression[m caused by wl#7674 in testBasic -n RefreshTuple T6 D1
    
    After the wl, exceptional epochs are queued and thus pollEvents()
    returns 1 when there is empty epochs on the queue top. Instead of
    expecting pollEvents() return 0, call nextEvent()as usual when
    pollEvents() returns 1.  nextEvent() will handle exceptional epochs.

[33mcommit 6b96c8436bd57b22382e4beaa12caf4657c18817[m
Author: Maitrayi.Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed Mar 4 17:10:07 2015 +0100

    Fix [1;31mregression[m caused by wl#7674 in testDict -n Bug13416603 I2.
    
    After the wl, exceptional epochs are queued and thus pollEvents() returns 1 when there is empty epochs on the queue top. Instead of expecting pollEvents() return 0, call nextEvent()as usual when pollEvents() returns 1.  nextEvent() will handle exceptional epochs.

[33mcommit b5eb4a152a8d75718feeccf61b0ecf1abe3f3ad7[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Feb 24 09:14:51 2015 +0100

    This commit is a fix for bugs 20069617 (consider noofreplicas in
    dynamic max_failure_time calculation) and 20069624 (allow the 120s
    constant part of the gcp_save gcp_stop timeout to be configured).
    
    The patch makes the following changes:
    * The previously fixed 120s 'extra' time for GCP_SAVE is made configurable,
      with a minimum value of TimeBetweenEpochsTimeout.
    * The formula for calculating the maximal time to detect the failure of a
      set of nodes has been changed in two ways:
      1) The time needed to detect heartbeat failure is set at five rather than
         four heartbeat intervals, as up to five intervals may be needed.
      2) Only one round of arbitration is considered, rather than
         (no_of_nodes - 1), as there can be at most one (at least for
         NoOfReplicas<=2).
    
    The patch also adds a [1;31mregression[m test.

[33mcommit 822b4987bdbd158f5bbc7268085a40b01dfabe4d[m
Author: Maitrayi.Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Mon Feb 23 10:44:23 2015 +0100

    Bug#20539452 POSSIBLE MEMORY LEAK IN NDBEVENTBUFFER::ALLOC_MEM: fix test [1;31mregression[m

[33mcommit ec745f7a1f38f4876761d10d2c21c0d3e16e8e40[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Feb 13 15:44:32 2015 +0100

    Bug#20454533: Assertion failed: sargables == 0 || *keyfields ...
    
    The fix for "[1;31mregression[m from bug#19789450" had a problem for some
    non-SELECT statements: These statements call st_select_lex::prepare()
    after preparing some items specific to the UPDATE/DELETE operation,
    so the prepare() call is clearing cond_count set for such references.
    When a derived table is involved in the UPDATE, conditions counted in
    the derived table are propagated to the outer query block. This happens
    before cond_count is cleared in ::prepare().
    
    The fix is to move the resetting of cond_count, etc from ::prepare()
    to reinit_stmt_before_use(). Thus, the fields are reset by the
    st_select_lex constructor for the first ::prepare(), for the
    subsequent ::prepare()s, they are reset by reinit_stmt_before_use()

[33mcommit 4a5ee11ba80178bf777b5d55fa0f61f5e6b15f48[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Wed Feb 11 16:05:09 2015 +0100

    Bug#20456178 ASSERTION FAILED: (ENUM_SET_TYPELIB && GET_REAL_TYPE(ITEM) == MYSQL_TYPE_NULL)
    
    This is a [1;31mregression[m from
    WL#5275: Process subqueries in FROM clause in the same way as view
    
    The input argument to Item_type_holder::get_full_info
    can now be an Item_direct_view_ref
    
    Solution: use real_item() to locate the correct field and typelib.

[33mcommit c1125521641073b5ad1666edb89b07f6a2a3f5ba[m
Author: Lakshmi Narayanan Sreethar <lakshmi.narayanan.sreethar@oracle.com>
Date:   Tue Feb 3 02:20:21 2015 +0530

    BUG#14798022 : "CHAR (0)" DATATYPE CAUSING ERROR 1296 (HY000)
    PART III
    
    Fixes [1;31mregression[m caused by the original patch on SPARC machines.
    The actual patch gave wrong results when the condition for the char(0)
    field was pushed down to ndbd during select queries on SPARC machines.
    Root Cause : bit fields are not supported in condition pushdown.
    Since char(0) is treated as bit internally, the error occured.
    This patch fixes that by disabling char(0) condition push down.
    Changes:
      - ndb_serialize_cond now disallows pushing char(0) conditions
      - added test case

[33mcommit 3f5497cf1e85dbd18c7e428da8866e15ca64e410[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Wed Jan 28 18:23:09 2015 +0800

    BUG#20419582 INNODB.FIND_SPACE_ID: ASSERTION FAILURE IN THREAD
    11 IN FILE UT0UT.CC LINE 904
    
    It's [1;31mregression[m of BUG#20382921, and just a test case issue.
    The fix is to remove the test case, approved by Marko.

[33mcommit 0bf7bd860e2fe67f0c225a7711d6b50bde79e08f[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Tue Jan 27 12:27:05 2015 +0200

    WL#7307: Fold mysql_install_db into the server binary
    
    Implemented as per the worklog spec.
     - added an --initialize option that bootstraps the server
     - added an --initialize-insecure option to prevent random password for the root user
     - added a deprecation warning to --bootstrap and m_i_db
     - Changed sql/CMakeLists.txt to use the new initialization
     - added unit and [1;31mregression[m tests

[33mcommit 908e2e978042810eb075ab9e2611b484d0c8145f[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Jan 22 17:34:31 2015 +0100

    Bug#17579998 INVALID SUB_GCP_COMPLETE_REP DUE TO SUMA RESEND DATA FOR ALREADY COMPLETE EPOCH
    
    Post push fix of [1;31mregression[m introduced in previous patch.
    
    When a nodegroup was dropped, the SUB_GCP_COMPLETE_REP signals got
    a mismatch betweeen number of buckets to report and list of bucket
    identifiers in signal.
    
    This was due to that buckets dropped during the preparation of
    signal was not included in list of identifiers since it was no
    longer an active bucket.  A temporary bitmap keeping track of
    dropped buckets previously active was introduced and used to
    create the list of identifiers in signal.

[33mcommit 307afeb3f20abf85d47cef9561fc222e44c1bbe0[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 10:09:39 2015 +0100

    Fix of uncorrect merge (7.2 -> 7.3) of [1;31mregression[m fix for bug#19524096

[33mcommit 2bebdad2be65ff6649842b2d8c51c0f6828c15c9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:39:21 2015 +0100

    Fix [1;31mregression[m introduced by fix for bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    timed out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 [1;31mregression[m, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 60e1b37d9c904fb5cd24c7275e565cc41a6ffb99[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:37:53 2015 +0100

    Fix [1;31mregression[m introduced by fix for bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    timed out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 [1;31mregression[m, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 0c47952f2a467ce1a32ac8c6043424c5b26cba69[m
Merge: bd4a76f3fa3 90126a75b39
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:21:54 2015 +0100

    Fix [1;31mregression[m introduced by fix for bug#bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to simply remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Don't copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    Intend to push this 7.1 -> even if commiting to 7.2 now.
    
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    timed out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                 -> TransporterFacade::start_poll -
                    > lock TransporterFacade mutex (a global mutex)
    
      - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                 -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
      Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                    neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 [1;31mregression[m, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 90126a75b3948e7faee257dd5ac2c2665b73a6ad[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Jan 21 09:16:07 2015 +0100

    Fix [1;31mregression[m introduced by fix for bug#bug#19524096.
    
    That fix caused ndb_global_schema_lock_error to fail as mysqld
    ended up in a deadlock between TransporterFacade and ClusterMgr mutex
    in ClusterMgr::is_cluster_completely_unavailable(). There
    likely are other deadlock scenarios also.
    
    The fix is to remove the Guard locking clusterMgrThreadMutex.
    The rational and (lack of) correctness for this is discussed in
    a mail pasted below.
    
    There are also a few small improvements to ::is_cluster_completely_unavailable()
    
    1. Dont copy entire 'trp_node' struct but read it by refference instead.
    
    2. Replace usage of 'm_impl->m_transporter_facade' with 'getTransporter()'
       as this is the pattern used elsewhere in this code block.
    
    
    --------------- Pasted mail with some background ----------------
    
    Subject: (Dead-) locking of ClusterMgr::theNodes[] structures
    
    Hi
    
    Writing this as a note to myself and others after having analyzed
    the failure of ndb_global_schema_lock_error.test. That test
    timed out as mysqld ended up in a deadlock between
    ClusterMgr::clusterMgrThreadMutex and TransporterFacade::theMutexPtr
    
    ClusterMgr maintains node state & info in theNodes[]. From external
    components, this info is access through ClusterMgr::getNodeInfo().
    theNodes[] are only updated from within ClustMgr, all external access
    is read only.
    
    Updates to theNodes[] are partly done from withing several ClustMgr::exec<foo>
    methods, and partly from ClusterMgr::reportConnected() / ::reportDisconnected().
    All updates seems to be done with ClusterMgr::clusterMgrThreadMutex locked.
    
    Several ClusterMgr methods are available for inspecting node status
    from other components, these all use information from theNodes[].
    Some of the more commonly used of these methods are:
    
     - TransporterFacade::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - TransporterFacade::get_an_alive_node() (Implemented on top of ::get_node_alive(n))
     - NdbImpl::get_node_stopping(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get_node_alive(n) (Reads 'theClusterMgr->getNodeInfo(n)')
     - NdbImpl::get<foo> ...A lot more node state getters....
    
    The locking schema used to provide atomicity of theNodes[] for the above
    methods are .... mixed, and not really defined at all as far as I can tell.
    Some examples:
    
    - NdbDictInterface::dictSignal(), NdbDictInterface::forceGCPWait() & NdbDictInterface::listObjects():
      Before calling get_node_alive() / ::get_an_alive_node(), a PollGuard is set.
      PollGuard calls trp_client::start_poll() which is different pre/post 7.3:
      - Pre 7.3, trp_client::start_poll
                  -> TransporterFacade::start_poll -
                    -> lock TransporterFacade mutex (a global mutex)
    
       - 7.3 -> trp_client::start_poll, lock trp_client m_mutex.
                  -> TransporterFacade::start_poll ...no locking, and mutex gone in this version
    
     Observations: There are no locking of ClusterMgr::clusterMgrThreadMutex here,
                   neither pre/post 7.3 .
    
    - Ndb_cluster_connection::wait_until_ready(),
      Ndb_cluster_connection_impl::get_next_alive_node()
      Ndb_cluster_connection::get_no_ready():
      These all sets the TransporterFacadeFurthermore mutex.
    
    - Ndb::sendRecSignal
      Sets a PollGuard as above, which either lock the TransporterFacade or
      the trp_client mutex
    
    - Ndb::sendPrepTrans()
      Documents in comments that TransporterFacade mutex should be locked prior to call
    
    So this has become a total mess. It might seem like that it prior
    to 7.3 was the intention that TransporterFacade mutex should be held
    when accessing theNodes[], or any methods that access it itself.
    After 7.3 a mix of TransporterFacade and Trp_client mutex is effectively used
    
    Updating looks like it sets the ClusterMgr mutex to protect these, which will
    of course not work as the reader doesnt set this mutex. However, it could be
    that all updates happens at places where it is called from the TransporterFacade.
    Here we *used to* hold the TransporterFacade mutex prior to 7.3, which would make
    some sense. This all needs more investigation .... and some documentation in the code...
    In the current state there certainly are no effective concurrency protection of
    the node state info  in 7.3+ , It could be that it work in 7.1 & 7.2
    
    On top of this the fix for bug#19524096 introduced
    ClusterMgr::is_cluster_completely_unavailable() which is also based on
    the nodes status available in theNodes[]. Probably in good faith,
    backed by that updates of theNodes[] was protected with clusterMgrThreadMutex,
    that method grabs that lock before accessing theNodes[] info.
    
    Actually this method is *the only* node state getters which
    does any explicit locking. ... and it was doomed to fail as this
    was completely untested territory. See other mail about how
    it could deadlock with the TranporterFacade mutex.
    
    Sidenote: any other node state getters attempting to follow the
    same locking pattern had likely deadlocked the same way.
    
    ::is_cluster_completely_unavailable() is called from within code
    which also calls ::get_node_alive() and ::get_an_alive_node(),
    without using any locking protection for these. Based on this I will
    propose a patch for the bug#19524096 [1;31mregression[m, which simply
    removes the mutex locking from ::is_cluster_completely_unavailable().
    
    This will not be entirely kosher based on how the shared node state
    structs should have been mutex protected. However, based on my discussion
    above, there are already so many violations in this area that a single
    more should not matter. A bigger effort should be taken to clean up this entirely.

[33mcommit 1d25326831669d0810f556abed998608aeac5db2[m
Author: Allen.Lai <zheng.lai@oracle.com>
Date:   Thu Jan 15 11:40:59 2015 +0800

    Bug#20078646 CAN NOT STOP MYSQL WITH MEMCACHED PLUGIN
    
    It's a [1;31mregression[m introduced by fixing bug#18409840 MEMCACHED_SHUTDOWN
    IS CALLING "PLUGIN_DEL" WITHOUT ACQUIRING LOCK_PLUGIN MUTEX.
    The acquiring lock causes deadlock, we should move this acquiring lock after
    calling plugin_deinitialize.
    
    Reviewed-by: Jimmy Yang<jimmy.yang@oracle.com> via IM

[33mcommit 5a473a621d4149b4163c396998557ee759f8313f[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Fri Nov 28 14:57:54 2014 +0100

    Bug#19505175 REGRESSION IN Q21 OF DBT3 TEST FOR WL7339
    
    WL#7339 started to use more correct records per key estimates from
    InnoDB. This caused the cost estimate for the original query plan
    to become four times higher than previous, which caused another
    query plan to be selected.
    
    The new selected query plan does a table scan on the orders table.
    This table has 1.5 million records. On this table we have the
    following condition:
    
      orders.o_orderstatus = 'F'
    
    This is used for calculating the condition filter effect for this
    table. With the current guestimate for equality conditions of 0.005,
    this caused the estimated number of partial rows to be produced from
    this table to be only 7500. In reality, 730.000 rows were produced.
    
    The cause for this very wrong estimate is that the o_orderstatus
    column only contains three distinct values and almost half of the
    records have the value 'F'.
    
    To make the condition filter produce a better (more conservative)
    estimate for cases like this and to reduce the likelihood of
    similar [1;31mregression[ms, the fix for this problem is to increase the
    condition filter constant for equality estimates to 0.1. With this
    we estimate that 1 of 10 records will pass the filter (instead of
    only 1 out of 200).
    
    Changes in tests:
    
    @ mysql-test/r/compress.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/derived.result
       Mostly changes to filtered estimates. One change in query plan.
       This query now gets the same plan as it had before WL#6635.
    @ mysql-test/r/ds_mrr-big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/eq_range_idx_stat.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/explain_for_connection_rqg_json.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_for_connection_rqg_trad.result
       Mostly changes to filtered and rows estimates. One
       query has changes to its query plan.
    @ mysql-test/r/explain_other.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_big.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/filter_single_col_idx_small.result
       Mostly changes to filtered estimates. One query has
       changes to its query plan: changes from using FirstMatch
       to do materialization.
    @ mysql-test/t/filter_single_col_idx_small.test
       Added test to verify that the filter estimate for basic
       filter constants is not less than one row
    @ mysql-test/r/greedy_optimizer.result
       Mostly changes to filtered and cost estimates. Eight queries
       has changes in query plans. Four of these changes back to
       the query plan they had before WL#6335 was pushed. The four
       last had similar changes in query plans as for when WL#6635 was
       pushed.
    @ mysql-test/r/greedy_search.result
       Mostly changes to filtered estimates. Two queries has
       changes to its query plan. One of these are returned to
       what it was before WL#6635 was pushed. The other has
       similar changes to as what was introduced by WL#6335.
       The number of partial query plans for some queries are
       increased. With the exception of one of these, all
       new numbers are below what it was before WL#6335.
       The one the has a very high increas in the number of
       partial query plans, changes from 22201 to 5799004.
       Before WL#6635 this query considered 735518 partial
       query plans.
    @ mysql-test/r/group_by.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/group_min_max_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/heap_hash.result
       One change in query plan. The new plan is more similar
       to what the query plan was before WL#6635 was pushed. It has
       the same join order but uses table scan on the second
       table instead of ref access as it did before WL#6635.
    @ mysql-test/r/index_merge_innodb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_intersect_dml.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/index_merge_myisam.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/innodb_explain_json_non_select_none.result
       Changes in filtered, rows and estimates in explain output.
    @ mysql-test/r/innodb_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/innodb_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bkaunique.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_bnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_cache_nojb.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/join_outer_bka_nixbnl.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_all.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_json_non_select_none.result
       Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/r/myisam_explain_non_select_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_explain_non_select_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_all.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/myisam_icp_none.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/named_pipe.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/negation_elimination.result
       Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/null_key_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/order_by_all.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_icp_mrr.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/order_by_none.result
      One query has changes to query plan. The new query plan
      is the same as this query had before WL#6635 was pushed.
    @ mysql-test/r/partition_locking.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/partition_pruning.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_mrr_cost.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/range_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_icp_mrr_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/select_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/shm.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/ssl_compress.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_all_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_nomat_nosj_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_none_bka_nixbnl.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/subquery_sj_all.result
      Mostly changes to filtered, rows and cost estimates. Three queries
      has plan changes. Two of these returns to what the plan was before
      WL#6635. The last has changes that makes the plan look more like
      it was before WL#6635 but not identical.
    @ mysql-test/r/subquery_sj_all_bka.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Four of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_all_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Five of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_dupsweed_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_dupsweed_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_firstmatch_bka_nixbnl.result
      Changes to filtered, rows and cost estimates.
    @ mysql-test/r/subquery_sj_firstmatch_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. One
      query has real plan changes. The new query plan is the
      same as this query had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Two
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_loosescan_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Four
      queries have real plan changes. The new query plans are the
      same as these queries had before WL#6635.
    @ mysql-test/r/subquery_sj_mat.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bka_nixbnl.result
      Mostly changes to filtered, rows and cost estimates. Seven
      queries have real plan changes. Six of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_bkaunique.result
      Mostly changes to filtered, rows and cost estimates. Eight
      queries have real plan changes. Seven of these returns to
      the plan they had before WL#6635.
    @ mysql-test/r/subquery_sj_mat_nosj.result
      Changes to filtered and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bka_nixbnl.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subquery_sj_none_bkaunique.result
      Changes to filtered, rows and cost estimates in explain output.
    @ mysql-test/r/subselect_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/type_temporal_fractional.result
      Changes in filtered estimate in explain output.
    @ mysql-test/r/wl6711_heap_to_disk.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/innodb/r/innodb_lock_wait_timeout_1.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/general2_no_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/general2_ps_prot.result
      Changes in rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_no_prot.result
      Changes in filtered and rows estimates in explain output.
    @ mysql-test/suite/opt_trace/r/range_ps_prot.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_no_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/opt_trace/r/subquery_ps_prot.result
      Changes in filtered, rows and cost estimates in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_innodb.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/parts/r/partition_alter3_myisam.result
      Changes in filtered estimate in explain output.
    @ mysql-test/suite/perfschema/r/batch_table_io_func.result
      Changes in filtered estimate in explain output.
    @ sql/item.h
      Change the value for the condition filter constant COND_FILTER_EQUALITY
      from 0.005 to 0.1.
    @ unittest/gunit/item_filter-t.cc
      Change in unit test for condition filter for IN lists:
      Reduced from having six values in the IN list to four
      values. The reason is that with six values in the
      IN list the calculated condition filter will be larger
      than 0.5 and then rounded down to 0.5.

[33mcommit c866ba78b7ec78c2e86b24e829ad239ccecc5b83[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Dec 23 17:31:57 2014 +0800

    Fix test failures on pb2: innodb_zip.4k and innodb_zip.8k
    
    It's [1;31mregression[m of WL#5757 InnoDB: Support Page Sizes 32k and 64k
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>

[33mcommit 4f44fc95a00a6b6cd9860ca35be7f6715e3ce767[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Dec 17 10:32:22 2014 +0100

    Fix for bug#20112981
    
     TransporterFacade::reset_send_buffer might reset a send_buffer in use by 'send'
    
    This is a [1;31mregression[m introduced by WL#3860 (ATC patches)
    which redesigned the locking mechanism in the TransporterFacade.
    ...and thus introduced a possible race between sending sending
    buffered data, and another thread reseting the same buffer.
    
    TransporterFacade::reset_send_buffer() reset the two
    m_send_buffers[node] buffers: 'm_buffer' and 'm_out_buffer'.
    
    These are designed to be protected by :
    
    1)'m_buffer' should only be updated when holding the
       m_send_buffers[node].m_mutex lock.
    
    2)'m_out_buffer' is protected by 'm_send_buffers[node].m_sending'.
       When this flag is set, the buffer is 'owned' by a thread
       actively sending, and consuming the m_out_buffer contents.
       Thus this buffer should not be reset while this flag
       is set.
    
    ::reset_send_buffer didn't follow any of these rules.
    
    
    This fix locks the 'm_mutex' in ::reset_send_buffer.
    Furthermore it checks that 'm_sending==false' before
    it release the m_out_buffer.
    
    Iff 'm_sending==false', it sets the new 'm_reset' flag
    in order to signal a pending reset. That pending reset
    will then be server by ::do_send_buffer when sending
    completes, and it clears the 'm_sending_flag'.
    
    As part of the fix, all TransporterFacade 'send-code' has
    been refactored into the new ::do_send_buffer method.
    
    Furthermore, some very non-C++'ish memset/bzero of
    TFBuffer objects has been replaced by a new TFBuffer::clear()
    method.
    
    Also added several comments about how the locking protection
    of TFSendBuffer is intended to work
    
    Intended for push 7.3 ->
    
    ******
    Incremental addendum patch for bug#20112981
    
       TransporterFacade::reset_send_buffer might reset a send_buffer in use by 'send'
    
    Instead of introducing the method is_locked_send() used only in ::reset_send_buffer(),
    the patch is modified to temporary try_lock and unlock the 'send' lock.
    
    Improves the readability of the locking mechanism pr. reviewer comment.
    Performance should not be an issue in the reset eiter, called once at startup
    and disconnect/reconnect.
    ******
    Another addendum patch for bug#20112981, adressing comment from Mikael R.
    ******
    Adressing review comments in patch for bug#20112981

[33mcommit d89b8d681e2e57a13bcdb79707711139198c445f[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Dec 16 13:49:48 2014 +0100

    Fix for bug#20112981
    
     TransporterFacade::reset_send_buffer might reset a send_buffer in use by 'send'
    
    This is a [1;31mregression[m introduced by WL#3860 (ATC patches)
    which redesigned the locking mechanism in the TransporterFacade.
    ...and thus introduced a possible race between sending sending
    buffered data, and another thread reseting the same buffer.
    
    TransporterFacade::reset_send_buffer() reset the two
    m_send_buffers[node] buffers: 'm_buffer' and 'm_out_buffer'.
    
    These are designed to be protected by :
    
    1)'m_buffer' should only be updated when holding the
       m_send_buffers[node].m_mutex lock.
    
    2)'m_out_buffer' is protected by 'm_send_buffers[node].m_sending'.
       When this flag is set, the buffer is 'owned' by a thread
       actively sending, and consuming the m_out_buffer contents.
       Thus this buffer should not be reset while this flag
       is set.
    
    ::reset_send_buffer didn't follow any of these rules.
    
    
    This fix locks the 'm_mutex' in ::reset_send_buffer.
    Furthermore it checks that 'm_sending==false' before
    it release the m_out_buffer.
    
    Iff 'm_sending==false', it sets the new 'm_reset' flag
    in order to signal a pending reset. That pending reset
    will then be server by ::do_send_buffer when sending
    completes, and it clears the 'm_sending_flag'.
    
    As part of the fix, all TransporterFacade 'send-code' has
    been refactored into the new ::do_send_buffer method.
    
    Furthermore, some very non-C++'ish memset/bzero of
    TFBuffer objects has been replaced by a new TFBuffer::clear()
    method.
    
    Also added several comments about how the locking protection
    of TFSendBuffer is intended to work
    
    Intended for push 7.3 ->

[33mcommit 4d15983ec6619361b3f57132f34aa45a8635aa63[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Wed Dec 10 11:06:14 2014 +0800

    BUG#19316315 CRASH RECOVERY FAILS AFTER ONLINE ADD INDEX
    
    Analysis: It's a [1;31mregression[m of wl#7277. The root cause is we set
    checkpoint after row_log_apply for online add index.
    
    Solution: Flush dirty pages before row_log_apply, and avoid the
    end checkpoint for online add index and spatial index.
    
    rb#7498 approved by Marko.

[33mcommit e94de53af12578ef45b05a82109bc9f83297fec1[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed Dec 10 10:57:34 2014 +0100

    This commit is a followup to the fix for bug#19976428 ("reports failing
    with 'out of longmessagebuffer' error"). This commit updates a
    [1;31mregression[m test (testIndex/FireTrigOverload) so that it will expect the
    right error code (293 "Inconsistent trigger state in TC block" instead
    of 218 "Out of LongMessageBuffer").

[33mcommit 0e9a2da8f6cdf69195748c51cd0a65e3cdd86c36[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Nov 13 13:19:33 2014 +0100

    Fix [1;31mregression[m in AutoTest -n DropWithTakeover T1
    introduced by fix for bug#19874809:
    
    The above test started failing after pushing fix for 19874809 :
    'DICT operations during 'takeover' may crash new master'
    
    That fix changed how we handled a SCHEMA_TRANS_END_REQ on
    a new master still on the process of handling MASTER_TAKEOVER.
    
     - Previously a REF('Busy') were sent during takeover.
       The API client handled that REF as a Schema transaction failure
       instead of waiting for the final rollforward/backward in the
       takeover process. Thus, loosing the knowledge of the transaction
       outcome.
    
     - With the fix we now wait for the takeover to complete, and then
       sends a SCHEMA_TRANS_END_REP reporting either a commit or abort.
    
    The problem here is that the testcase 'DropWithTakeover' delays
    the DROP_TAB_REQ indefinitely. This breaks the requirement that
    every REQ should be either CONFed or REFed, or a NODE_FAILURE
    received.
    Furthermore, the testcase incorrectly required that 'dropTable()'
    failed. This used to be due to a REF(Busy) was interpretted as
    transaction failure. However, in this case the dropTable is rolled
    forward by the new master, and actually succeeds.
    
    Fix changes the delay of DROP_TAB_REQ from being indefinite to
    a 10s delay which should be sufficient for the master to crash.

[33mcommit 45d8c23cf2aa70adb117fe2d3c6adbbf344af521[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Nov 11 15:17:15 2014 +0100

    Fix Club test [1;31mregression[ms after backport of bug#19524096:
    
    Paste from a mail describing the issue below.
    As it is yet not clear wheteher this fix should also
    go into 7.3+ (as discussed below), it will for now
    only be pushed to 7.1 and 7.2 as a [1;31mregression[m fix.
    
    ....................................
    
    I have analyzed this and found the root cause to be the
    'lock()' set in ClusterMgr::is_cluster_completely_unavailable()
    
    ....
    bool
    ClusterMgr::is_cluster_completely_unavailable()
    {
      bool ret_code = true;
      lock();  <=
    
    ....
    
    
    In most circumstances 'is_cluster_completely_unavailable' is
    called while holding the poll right. As part of your (Mikael R)
    ATC patches (WL#3860) the implementation of the 'PollGuard'
    changed:
    
    Prior to WL#3860 the (global) TransporterFacade::theMutexPtr is held
    when owning the poll right. This WL changed that such that only
    the per-client mutex trp_client::m_mutex is locked.
    
    The above 'lock' in ::is_cluster_completely_unavailable() will call
    trp_client::lock(), which in 7.1 is implemented as:
    
    void
    trp_client::lock()
    {
      NdbMutex_Lock(m_facade->theMutexPtr); <- Already held
      ...
    }
    
    Thus, we will try to re-lock TransporterFacade::theMutexPtr
    which we already held, and will either block forever, or
    fail due to 'lock already held' (Os dependent?)
    
    In 7.3+, the 'lock' call will lock ClusterMgr::trp_client::m_mutex
    instead, which will be granted **if the current client is not ClusterMgr itself**.
    So this might not be entirely safe either, depending on the which
    trp_client calling checking is_cluster_completely_unavailable().
    AFAIK, no test failures observed yet though.
    
    Furthermore, looking at the implementation of ::is_cluster_completely_unavailable()
    I cant really find any reason for this 'lock' to be needed at all.
    What we need to protect is the 'NodeInfo' maintained in ClusterMgr::theNodes[].
    Thus, locking clusterMgrThreadMutex should be sufficient - both
    in order to fix the redness introduced in the 7.1-> backport,
    and as an fix/improvement to 7.3.

[33mcommit d305e2ee1497694f5b9687943208395ffdc82dd3[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Fri Nov 7 15:39:03 2014 +0530

    Bug#19418619 - SELECT QUERY WITH INVALID TIME WORKS FINE
                   BUT SP FAILS UNDER STRICT MODE
    
    Description:
    
    STRICT sql mode does not apply to SELECT statement except
    for invalid arguments to sleep(). SELECT statement gives
    warning for invalid data in STRICT mode, which is correct
    behavior. If same SELECT statement is executed from within
    Stored Procedure, it gives error and abort statement.
    Statement behavior should not change if executed from Stored
    Procedure. Same case exist for SET statement.
    
    Another issue, CREATE TABLE statement fails while CREATE
    INDEX statement passes for too long indexes in STRICT mode.
    Index creation should behave same if executed from
    CREATE TABLE or CREATE INDEX statement.
    
    Analysis:
    
    This is a [1;31mregression[m from WL#6891. Stored Procedures are
    executed in the sql mode they were created. When SP was
    created in STRICT mode, Strict_error_handler was pushed
    before executing statements inside the Stored Procedure.
    This upgrades warnings to error for every SELECT statement.
    Other side effects of above change are:
    - Changing STRICT sql mode inside Stored Procedure would
      have no effect.
    - As documented, evaluation of SP and SF parameters should
      depend on the current sql mode instead of the mode in
      which SP/SF was created. Behavior got changed with WL#6891
      and parameter evaluation started using sql_mode in which
      SP/SF were created.
    
    Strict_error_handler was missing check for CREATE INDEX
    statement. So, where CREATE TABLE will give error for too long
    indexes, CREATE INDEX statement will succeed with warning in
    STRICT mode.
    
    Fix:
    
    Fixed code to push Strict_error_handler when required.
    SELECT inside and outside Stored Procedure will behave
    same. Exception for this is when we do SELECT on variables
    declared inside Stored Procedure.
    Consider the procedure :
    
    create procedure proc1()
    begin
      declare div_zero integer;
      select 1/0 into div_zero;
    end
    
    Here, SELECT to assign invalid value to 'div_zero' will
    fail in STRICT mode as it follows the rules of assigning a
    value to INTEGER column in a table. There is no behavior
    change here.
    Same case applies to SET statement.
    
    Now STRICT mode setting can be changed inside Stored
    Procedures and parameter evaluation will depend on current
    sql mode.
    
    Added check for CREATE INDEX statement in Strict_error_handler.

[33mcommit cb896a3df94a8a3a2aa1f9d63ae60a83eac3acbd[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Nov 6 13:27:28 2014 +0900

    Bug#19803497 : SIGNAL 11 IN INNODB.INNODB_BUFFER_POOL_RESIZE_DEBUG
    
    buf_block_align() could be called during resizing buffer pool from debug assertion code.
    The buf_chunk_map access should be protected from deleting if debug build.
    
    (non debug build doesn't call buf_block_align() during resizing BP, because resizing BP disables AHI.)
    
    Not to cause [1;31mregression[m for non debug build, the rw_lock protection should be only for debug build.
    
    Approved by Sunny Bains in rb#7153

[33mcommit fc3c271adf2cac75fd3cc545047cf70b42c71788[m
Author: Knut Anders Hatlen <knut.hatlen@oracle.com>
Date:   Wed Nov 5 12:34:42 2014 +0100

    Bug#19931126: VALGRIND REPORTS USE OF UNINITIALIZED VALUE IN
                  MY_WILDCMP_BIN_IMPL
    
    The LIKE operator accepts ESCAPE clauses that contain expressions that
    are constant at execution time. However, it only evaluates the
    expression in the ESCAPE clause if its value is known at resolve time.
    If the ESCAPE clause contains an expression that is constant at
    execution time, but unknown at resolve time, the escape character will
    be uninitialized, and the LIKE operator will produce unreliable
    results. This is a [1;31mregression[m in 5.6.
    
    The fix factors out the code that initializes the escape character
    from Item_func_like::fix_fields() into a new function. If the escape
    expression is constant and known at resolve time, it is initialized by
    fix_fields() as before. If its value is not known at resolve time,
    Item_func_like::val_int() will now initialize the escape character the
    first time it is called.
    
    The fix also makes the range optimizer skip the optimization with LIKE
    if the escape character is unknown at resolve time. Otherwise, it
    would use the uninitialized value for the escape character and produce
    unreliable results.

[33mcommit 98fa455791b1ce5dca824f5df251cf7684be5b26[m
Author: bin.x.su@oracle.com <>
Date:   Sat Nov 1 11:22:36 2014 +0800

    Bug#19887285 - ASSERT 'LATCHES->EMPTY() || LATCH->M_LEVEL == ...' FAILED
    IN SYNCDEBUG::RELOCK
    
    There was an incorrect fix for bug#19703758, which introduced this [1;31mregression[m.
    After some checking, we can simply revert the problematic patch as we do not
    have to reserve the btr_search_latch nowadays.
    
    rb#7192, approved by Jimmy.

[33mcommit be031b4686faf6bed88fb0504115b0ffd2b8ab46[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Oct 30 20:38:04 2014 -0500

    Fix [1;31mregression[m in innodb.alter_table_redundant.test due to an assert
    in the patch for Bug#19884605 that does not hold during ALTER TABLE
    on REDUNDANT row formatted tables.

[33mcommit 2604e4eae6ab4c9f7aae5f185d94239f20c208e6[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Wed Oct 29 17:24:27 2014 +0530

    BUG#19873291 - MYSQL_EXECUTE_COMMAND(THD*): ASSERTION
                  `THD->IS_ERROR() || THD->KILLED' FAILED
    
    Executing DELETE on a table without key in safe update mode
    leads to error ER_UPDATE_WITHOUT_KEY_IN_SAFE_MODE. In debug
    build, using IGNORE does not set error status in Diagnostics
    Area. This leads to an assert when Diagnostics Area is checked
    for error status after statement execution.
    
    Code already got fixed as part of WL#6614, bzr revision no 8074.
    Test case added to verify fix and avoid [1;31mregression[m.

[33mcommit 9c8b7f32405f6fa727e78256bf84f84afa4839ae[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Oct 16 12:29:19 2014 +0200

    Fix for bug#19661543
    
       RACE CONDITION BETWEEN ::PERFORMRECEIVE AND ::UPDATE_CONNECTIONS()
    
    Fix a [1;31mregression[m introduced by WL#3860 (ATC patches). That WL
    removed the TransporterFacade::theMutexPtr which implictly
    provided protection between concurent calls of ::performReceive()
    and ::update_connections().
    
    Fix is in two parts:
    
    Part1, TransporterReceiverWatchdog.patch:
    
    Adds a DEBUG watchdog which will assert that there are no
    concurrent calls to ::performReceive() and ::update_connections().
    Doesn't fix anything, except guarding against reintroducing the
    same problem later.
    Running MTR tests with this patch will crash rather rapidly on >= 7.3
    Part1 is pushed 7.1 ->
    
    Part2: The real fix:
    This patch moves ::update_connections() calls inside do_poll()
    where it is now done (serialized) in a loop together with
    ::performReceive(). Part2 will be added to the merge 7.3 ->

[33mcommit 456f8275043a347fee79ab7b2846d2512ebda26c[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Oct 13 14:22:16 2014 +0200

    Merge of Bug #19582925 ENHANCED DETECTION AND DUMP OF CORRUPT OR UNSUPPORTED MESSAGES
    
    Test of io state is kept in outside loop of
    TransporterRegistry::unpack() to not introduce to much performance
    [1;31mregression[m.

[33mcommit 32842ba37b1ccd026aae40db3a9363d9f0ddc071[m
Author: Sunanda Menon <sunanda.menon@oracle.com>
Date:   Fri Oct 10 10:20:20 2014 +0200

    Applying the patch for [1;31mregression[m bug#19582807 for 7.1.33 cluster release

[33mcommit 1a49d4c4791d3daaeec9f95be59b89b121ff0654[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Oct 9 15:04:27 2014 +0200

    Cherrypicked
    
    revision-id: mauritz.sundell@oracle.com-20141009124636-dg0th9bzvr27r1i7
    parent: mauritz.sundell@oracle.com-20140930122950-gn1rl2yigc4s7ucu
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-7.1
    timestamp: Thu 2014-10-09 14:46:36 +0200
    message:
        Bug #19582807     MAKE SIGNAL DUMP IN TRACE FILES ALWAYS START FROM LATEST SIGNAL
    
        Removes a [1;31mregression[m introduced with patch for the above bug.
    
        During crash dumps there could be a segmentation fault or most recent
        signals could be dump as old signals or not at all.

[33mcommit 7eaa176fc34ddee697ecad6690cbc9d3df52344b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Oct 9 15:02:54 2014 +0200

    Cherrypicked
    
    revision-id: mauritz.sundell@oracle.com-20141009124636-dg0th9bzvr27r1i7
    parent: mauritz.sundell@oracle.com-20140930122950-gn1rl2yigc4s7ucu
    committer: Mauritz Sundell <mauritz.sundell@oracle.com>
    branch nick: mysql-7.1
    timestamp: Thu 2014-10-09 14:46:36 +0200
    message:
        Bug #19582807     MAKE SIGNAL DUMP IN TRACE FILES ALWAYS START FROM LATEST SIGNAL
    
        Removes a [1;31mregression[m introduced with patch for the above bug.
    
        During crash dumps there could be a segmentation fault or most recent
        signals could be dump as old signals or not at all.

[33mcommit 9d137d27c0f3fda2fcadc55425efaa0b533b80ee[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Thu Oct 9 14:46:36 2014 +0200

    Bug #19582807   MAKE SIGNAL DUMP IN TRACE FILES ALWAYS START FROM LATEST SIGNAL
    
    Removes a [1;31mregression[m introduced with patch for the above bug.
    
    During crash dumps there could be a segmentation fault or most recent
    signals could be dump as old signals or not at all.

[33mcommit 453dcc5c007cd208720b19104beae4f22c83a6ca[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Oct 7 16:22:45 2014 +0530

    Bug #19512199 FIX CLIENT PROTOCOL TRACING FOR NEW RESULT SET FORMAT.
    
    With the implementation of WL#7766 the result set format has been changed
    by removing EOF from metadata and replacing EOF with OK packet for data
    result set. However this implementation caused [1;31mregression[m in client protocol
    trace plugin.
    
    The fix is to handle this new row format in client protocol tracing plugin.

[33mcommit b73e418cf5c6ab46de1f9a7905b3ef09314a0071[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Oct 7 11:08:50 2014 +0200

    Bug#19730970: COMPILE ERROR WITH ENABLED_PROFILING=0
    
    This patch fixes two compilation errors when building with
    -DENABLED_PROFILING=0.
    
    The first is the missing inclusion of sys/resource.h in mysqld.cc which otherwise
    gets included via sql/sql_profile.h. This was a [1;31mregression[m introduced in 5.7.5 by
    the patch for Bug#18404381.
    
    The second is a missing field.h inclusion in the thread pool.

[33mcommit 827ad38d210dfa047e42398c68daffd5e527402f[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Tue Sep 30 20:58:41 2014 +0200

    The patch series for WL#7642 Ndb Active Active Binlog Read Tracking
    
    1) Empty update pass-through
    Allows defining Ndb API to allow empty update events to be passed
    through to allow monitoring changes of pseudo column AnyValue.
    
    By calling NdbEventOperation::setAllowEmptyUpdate(true) before
    calling nextEvent() will allow empty update events to be passed
    through.
    A test case is added to Ndb API test test_event and is added to
    be run in daily [1;31mregression[m tests.

[33mcommit c0c72692b25606d8ac367208cc9e89260a960e14[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Sep 24 10:02:24 2014 +0300

    Bug#18645050 WL#7142 CAUSED PERFORMANCE REGRESSION
    
    Reduce the use of space_id lookups. Remove the fil_system->mutex
    acquisitions from mtr_commit(), which is the likely primary cause of
    [1;31mregression[m.
    
    fil_space_get(): New function, to look up a tablespace by ID.
    This function can replace the use of existing functions that combine
    a lookup with a member access, such as fil_space_get_flags(),
    fil_space_get_latch(), fil_space-get_page_size().
    
    fil_system_t::named_spaces: Note that this is only protected by
    log_sys->mutex, not fil_system->mutex any more.
    
    fil_node_close_to_free(): Refactored from fil_node_free(). Prepares
    to free a file node object. The actual freeing will be done in
    fil_space_free_low().
    
    fil_space_detach(): Refactored from fil_space_free_low().
    Detaches a tablespace object from the common cache.
    After detaching, fil_space_free_low() may be called.
    
    fil_space_free(), fil_delete_tablespace(): First detach the tablespace
    and release the fil_system->mutex. If the tablespace was in the
    fil_system_t::named_spaces list, acquire log_sys->mutex for removing
    it from the list. Finally, invoke fil_space_free_low() to free the
    memory.
    
    fil_space_free_low(): Assume that fil_space_detach() has been invoked
    and that the tablespace is no longer in the fil_system_t::named_spaces
    list.
    
    fil_names_dirty(): Change the return type to void. This function is
    now only called by redo log scan, not during normal processing.
    
    fil_names_write_if_was_clean(), fil_names_dirty_and_write(): Replaces
    fil_names_dirty() and fil_names_write().
    
    fil_names_write_low(): Renamed to fil_names_write().
    
    mtr_t::m_user_space_id, mtr_t::m_user_space: Replaces mtr_t::m_named_space.
    
    mtr_t::m_undo_space, mtr_t::m_sys_space: New fields, for caching tablespace
    handles.
    
    mtr_t::set_sys_modified(): Looks up the system tablespace and caches it.
    In WL#7806 this would also flag the tablespace as modified.
    
    mtr_t::set_named_space(): Look up, cache and return the tablespace object.
    
    mtr_t::set_spaces(const mtr_t&): Copy the associated tablespaces from
    an existing mini-transaction.
    
    mtr_t::lookup_sys_space(), mtr_t::lookup_user_space(): Look up and cache
    a tablespace when it was not already cached.
    
    mtr_t::x_lock_space(), mtr_x_lock_space(space_id): New method, to X-latch a
    tablespace and to cache the tablespace pointer in the mini-transaction.
    
    mtr_t::Command::prepare_write(): Instead of optimistically calling
    fil_names_write() and then removing the records if they were not
    needed, invoke fil_names_write_if_was_clean() while holding
    log_sys->mutex. We no longer acquire fil_system->mutex here.
    
    mtr_write_log_t::m_len: Remove. We no longer need to omit some redo
    log records from the tail of the buffer.
    
    btr_root_adjust_on_import(), btr_validate_level(): Use fil_space_get()
    instead of fil_space_get_latch().
    
    btr_estimate_n_rows_in_range_on_level(), buf_dblwr_process(): Use
    fil_space_get() instead of fil_space_get_page_size().
    
    btr_free_externally_stored_field(): Use mtr_t::set_spaces() instead of
    mtr_t::set_named_space().
    
    rb#5918 approved by Yasufumi Kinoshita

[33mcommit 6298758116b2cb4efafde080ca066fe80d993426[m
Author: bin.x.su@oracle.com <>
Date:   Fri Aug 29 09:31:40 2014 +0800

    Revert the patch of bug#19450143 because there is a [1;31mregression[m after it.

[33mcommit a5ecc38f44abb66aa2024c70e37d1f4aa4c8ace9[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Aug 11 10:43:11 2014 +0300

    Bug#19330255 WL#7142 - CRASH DURING ALTER TABLE LEADS TO
    DATA DICTIONARY INCONSISTENCY
    
    The server crashes on a SELECT because of space id mismatch. The
    mismatch happens if the server crashes during an ALTER TABLE.
    
    There are actually two cases of inconsistency, and three fixes needed
    for the InnoDB problems.
    
    We have dictionary data (tablespace or table name) in 3 places:
    
    (a) The *.frm file is for the old table definition.
    (b) The InnoDB data dictionary is for the new table definition.
    (c) The file system did not rename the tablespace files yet.
    
    In this fix, we will not care if the *.frm file is in sync with the
    InnoDB data dictionary and file system. We will concentrate on the
    mismatch between (b) and (c).
    
    Two scenarios have been mentioned in this bug report. The simpler one
    first:
    
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    The files were not yet renamed in the file system.
    2a. The server is killed, without making a log checkpoint.
    3a. The server refuses to start up, because replaying MLOG_FILE_RENAME2
    fails.
    
    I failed to repeat this myself. I repeated step 3a with a saved
    dataset. The problem seems to be that MLOG_FILE_RENAME2 replay is
    incorrectly being skipped when there is no page-redo log or
    MLOG_FILE_NAME record for the old name of the tablespace.
    
    FIX#1: Recover the id-to-name mapping also from MLOG_FILE_RENAME2
    records when scanning the redo log. It is not necessary to write
    MLOG_FILE_NAME records in addition to MLOG_FILE_RENAME2 records for
    renaming tablespace files.
    
    The scenario in the original Description involves a log checkpoint:
    1. The changes to SYS_TABLES were committed, and MLOG_FILE_RENAME2
    records were written in a single mini-transaction commit.
    2. A log checkpoint and a server kill was injected.
    3. Crash recovery will see no records (other than the MLOG_CHECKPOINT).
    4. dict_check_tablespaces_and_store_max_id() will emit a message about
    a non-found table #sql-ib22*.
    5. A mismatch is triggering the assertion failure.
    
    In my test, at step 4 the SYS_TABLES root page (0:8) contains these 3
    records right before the page supremum:
    * delete-marked (committed) name=#sql-ib21* record, with space=10.
    * name=#sql-ib22*, space=9.
    * name=t1, space=10.
    space=10 is the rebuilt table (#sql-ib21*.ibd in the file system).
    space=9 is the old table (t1.ibd in the file system).
    
    The function dict_check_tablespaces_and_store_max_id() will enter
    t1.ibd with space_id=10 into the fil_system cache without noticing
    that t1.ibd contains space_id=9, because it invokes
    fil_open_single_table_tablespace() with validate=false.
    
    In MySQL 5.6, the space_id from all *.ibd files are being read when
    the redo log checkpoint LSN disagrees with the FIL_PAGE_FILE_FLUSH_LSN
    in the system tablespace. This field is only updated during a clean
    shutdown, after performing the final log checkpoint.
    
    FIX#2: dict_check_tablespaces_and_store_max_id() should pass
    validate=true to fil_open_single_table_tablespace() when a non-clean
    shutdown is detected, forcing the first page of each *.ibd file to be
    read. (We do not want to slow down startup after a normal shutdown.)
    
    With FIX#2, the SELECT would fail to find the table. This would
    introduce a [1;31mregression[m, because before WL#7142, a copy of the table
    was accessible after recovery.
    
    FIX#3: Maintain a list of MLOG_FILE_RENAME2 records that have been
    written to the redo log, but not performed yet in the file system.
    When performing a checkpoint, re-emit these records to the redo
    log. In this way, a mismatch between (b) and (c) should be impossible.
    
    fil_name_process(): Refactored from fil_name_parse(). Adds an item to
    the id-to-filename mapping.
    
    fil_name_parse(): Parses and applies a MLOG_FILE_NAME,
    MLOG_FILE_DELETE or MLOG_FILE_RENAME2 record. This implements FIX#1.
    
    fil_name_write_rename(): A wrapper function for writing
    MLOG_FILE_RENAME2 records.
    
    fil_op_replay_rename(): Apply MLOG_FILE_RENAME2 records. Replaces
    fil_op_log_parse_or_replay(), whose logic was moved to fil_name_parse().
    
    fil_tablespace_exists_in_mem(): Return fil_space_t* instead of bool.
    
    dict_check_tablespaces_and_store_max_id(): Add the parameter
    "validate" to implement FIX#2.
    
    log_sys->append_on_checkpoint: Extra log records to append in case of
    a checkpoint. Needed for FIX#3.
    
    log_append_on_checkpoint(): New function, to update
    log_sys->append_on_checkpoint.
    
    mtr_write_log(): New function, to append mtr_buf_t to the redo log.
    
    fil_names_clear(): Append the data from log_sys->append_on_checkpoint
    if needed.
    
    ha_innobase::commit_inplace_alter_table(): Add any MLOG_FILE_RENAME2
    records to log_sys->append_on_checkpoint(), and remove them once the
    files have been renamed in the file system.
    
    mtr_buf_copy_t: A helper functor for copying a mini-transaction log.
    
    rb#6282 approved by Jimmy Yang

[33mcommit cfdef95129f21b682956800b1fe57b3af3bc7a3c[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Wed Aug 6 14:17:27 2014 +0530

    Bug #16204823 INNODB WASTES 62 OUT OF EVERY 16384 PAGES
    
    Reverting my previous patch (rb#5559) because of [1;31mregression[ms.

[33mcommit 36fb419d39cd757f883feb6739ef65d54d36d95f[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Jul 31 14:12:46 2014 +0200

    This commit is a followup to the one below. It changes the fragment_num colum (in memory_per_fragment), such that it gives the global frgament id rather than a node-local sequence number. That way, it is possible to see where each fragment (or some replica of it) is located. This is consistent with the output from 'ndb_desc -pn -d <database> <table>.
    
    --------------------------------------------------------------------------
    #At file:///net/atum17/export/home/tmp/jw159207/mysql/repo/mysql-5.6-cluster-7.4/ based on revid:ole.john.aske@oracle.com-20140514124556-04t9xmxcwpndzi0x
    
     4333 Jan Wedvik      2014-05-14
          This commit implements WL#7375 (Ndbinfo: Per-fragment memory usage reporting). It provides a new view, 'ndbinfo.memory_per_fragment', that gives an overview of memory used per fragment replica, including fixed-element-size pages, var-element-size pages, rows, fixed-element-free-slots, var-element-free-bytes, hash index memory usage etc.
    
          In addition, the ndbinfo.ndb$pools has been extended with new rows for L2PMap memory usage.
    
          Finally, there are MTR [1;31mregression[m test cases for the new functionality.

[33mcommit 7d8ac9a74ee02d697a218bc927dd5bd50bb4c451[m
Author: Abhishek Ranjan <abhishek.ar.ranjan@oracle.com>
Date:   Wed Jul 9 17:30:15 2014 +0530

    Bug#18497612 - INCORRECT VIEW BEHAVIOR WHEN SEVERAL CHARACTER SETS
                   ARE USED
    
    Description:
    
    There are two issues as follows
    a)main.ddl_i18n_utf8.test and main.ddl_i18n_koi8r.test are skipped
      when tried with mtr.
    b)As test cases were skipped, 5.7 has got [1;31mregression[m with collations.
    
    Analysis:
    
    a) Due to change in 'show collation' command behavior, new records
       returned by 'SHOW COLLATIONS' started to mismatch what's stored
       within r/have_xxx.require. This caused the tests to skip.
    
    b) Regression was developed by the fix of the Bug#13520710.
       Fix made sure that while writing view statement, client character
       set is used. Regression arised in the case the view statement has
       different character set than client character set.
    
    FIX :
    
    a) Changed r/have_xxx.require files with the latest result of
       'SHOW COLLATIONS' statement. Test cases will not be skipped now.
       As the test case was disabled for a long time, result file got
       outdated. Updated result file.
    b) Fixed code to use the character set if provided in the statement.

[33mcommit 16827fcee42fdc668523375b42194a0cfb10b1bd[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Jul 9 15:33:20 2014 +0900

    Revert the following change, because the [1;31mregression[m was innocent about the Bug#11755438:Bug#47213 fix now.
    
    ------------------------------------------------------------
    revno: 8279 [merge]
    committer: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
    branch nick: mysql-trunk
    timestamp: Wed 2014-06-25 05:40:00 +0200
    message:
      Disable memory barrier only for Intel CPU, because performance [1;31mregression[m was observed at some conditions for Intel CPU.
      follow up for Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL

[33mcommit ce720cea7f254633a64f30fa68250669f64db3f0[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Wed Jul 9 05:19:51 2014 +0200

    WL#6711: This file is for big test. If --valgrind is used for [1;31mregression[m
    test, the test will last a long time which will result in Timeout for
    PB2 test. So I skip such a file test if in --valgrind mode.

[33mcommit f9b374da10438cb1ea3427ea547f11801adcb9f4[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Mon Jul 7 17:25:19 2014 +0800

    BUG#19149400 - ASSERT MTR->IS_ACTIVE() MYSQL_INPLACE_ALTER_TABLE()
    
    It's a [1;31mregression[m of Fix Bug#18674219 - IMPLEMENT ALTER TABLE...
    ALGORITHM=INPLACE FOR INNODB GIS INDEX CREATION.
    
    rb://5940 approved by Jimmy.

[33mcommit 74422f3b69a57fb9b2b7fe0e5d3c92a375fca246[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Fri Jul 4 12:43:02 2014 +0530

    Bug#18757964:RPL.RPL_CIRCULAR_FOR_4_HOSTS FAILS WITH RESULT
    MISMATCH NON DETERMINISTIC RESULTS
    
    Problem:
    ========
    rpl_circular_for_4_hosts  failing sporadically in pb2
    [1;31mregression[m runs. Test lead to non deterministic results in
    case of lower performance. If the lock of the table will be
    later than the inert in next session then the result may
    differ from the expected one.
    
    Analysis:
    ========
    This bug is clearly reproducible in windows with MTS
    and --slave-parallel-type=logical_clock.
    Test script has circular replication like 1->2->3->4->1.
    As part of test they do following two steps.
    
    1) Insert value 6 in the autoinc column on server_3 (and
    prevent it from replicating further using
    SQL_SLAVE_SKIP_COUNTER on server_4). Due to the
    auto_increment_offset setting, the autoinc value 6 is
    normally generated on server_2. When we later insert a row
    on server_2,we thus cause a duplicate key error on server_3.
    
    2) Reconfigure the topology to like this 1->2->4->1,2->3.
    On server 4 change master is executed by taking Server 3's
    Exec_master_log_pos and Master_Log_File. On Server 3
    Exec master position changes as per the order in which
    workers execute the following parallel transactions.
    
    use `test`; INSERT INTO t1(b,c) VALUES('B',2)
    use `mtr`; INSERT INTO test_suppressions (pattern) VALUES
    ( NAME_CONST('pattern',_latin1'Slave SQL.*Duplicate entry
    
    Example:
    Normal case:
    "Server 3 pos_c is ------3639"
    "Server 3 file_c is -----slave-bin.000001"
    
    slave-bin.000001 3554 Query  4 3639 COMMIT
    slave-bin.000001 3639 Query  2 3732 BEGIN
    slave-bin.000001 3732 Intvar 2 3764 INSERT_ID=6
    slave-bin.000001 3764 Query  2 3876 use `test`; INSERT
    INTO t1(b,c) VALUES('B',2)
    slave-bin.000001 3876 Query  2 3961 COMMIT
    slave-bin.000001 3961 Query  3 4047 BEGIN
    slave-bin.000001 4047 Query  3 4294 use `mtr`; INSERT INTO
    test_suppressions (pattern) VALUES ( NAME_CONST
    
    Error case:
    "Server 3 pos_c is ------4372"
    "Server 3 file_c is -----slave-bin.000001"
    
    slave-bin.000001 3554 Query  4 3639 COMMIT
    slave-bin.000001 3639 Query  2 3732 BEGIN
    slave-bin.000001 3732 Intvar 2 3764 INSERT_ID=6
    slave-bin.000001 3764 Query  2 3876 use `test`; INSERT
    INTO t1(b,c) VALUES('B',2)
    slave-bin.000001 3876 Query 2 3961  COMMIT
    slave-bin.000001 3961 Query 3 4047  BEGIN
    slave-bin.000001 4047 Query 3 4294  use `mtr`; INSERT INTO
    test_suppressions (pattern) VALUES ( NAME_CONST('pattern',
    _latin1'Slave SQL.*Duplicate entry .6. for key .PRIMARY.*
    Error_code: 1062' COLLATE 'latin1_swedish_ci'))
    slave-bin.000001 4294 Query 3 4372  COMMIT
    
    Because of this incorrect position in change master
    INSERT B gets missed in 4 & 1.
    
    Fix:
    ===
    Since the main intention is to generate duplicate key
    error on Server 3 to make test script more deterministic
    moved the mtr add suppression statement to a place bit
    ahead where we do a proper sync up of all servers.

[33mcommit b2adab2f07608231dc9793c4894c653dda9d6f9c[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Jul 1 08:19:52 2014 +0530

    - Follow-up performance fix for WL#7682.
    
      Intrinsic Table: Optimization to use dict_table_t for caching row_id/trx_id
      instead of innodb_session_t
    
      With WL#7682 we introduced intrinsic tables. Given that these tables has
      session scope row_id and trx_id needed for these tables were cached in
      innodb_session_t (session structure cached in thd).
    
      Unfortunately, accessing these ids through session is performance costly as
      it involves lookup into map using table name and loading of thd that in turn
      causes invalidation the CPU cache.
    
      We observed a [1;31mregression[m upto 35% with Wisconsin-DB queries.
    
      So we are moving back to original idea of caching these ids as part of
      dict_table_t.
    
      Approved by: Jimmy (rb#5881)

[33mcommit 0c6e277e00fddd30688227da7cbcbfa912927c24[m
Merge: 6944bdb1a42 5f280dd21dd
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Jun 25 05:40:00 2014 +0200

    Disable memory barrier only for Intel CPU, because performance [1;31mregression[m was observed at some conditions for Intel CPU.
    follow up for Bug#11755438: Bug#47213: INNODB MUTEX/RW_LOCK SHOULD BE CONSCIOUS ABOUT MEMORY ORDERING OTHER THAN INTEL

[33mcommit 43c524c163aed045ca47e4c8b79070b2c69bcb1d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Thu Jun 19 09:21:10 2014 +0200

    Fix for bug#18993037
    
      NODE RESTARTS WHILE ANOTHER NODE IS ALREADY STARTING MAY CRASH 'MASTER'
    
    This was a [1;31mregression[m introduced by the fix for bug#16007980
    That fix added the following code to Qmgr::failReportLab()
    
    +  /**
    +   * If any node is starting now (c_start.startNode != 0)
    +   *   sendPrepFailReq to that too
    +   */
    +  if (c_start.m_startNode != 0)
    +  {
    +    jam();
    +    cfailedNodes[cnoFailedNodes++] = c_start.m_startNode;
    +    c_start.reset();
    +  }
    
    This was to ensure that the (local) knowledge of 'startNode'
    was preparing to start, should not be lost in the case that it
    re-failed before it had reconnected.
    
    However, this also introduced a race condition where we either
    could, or could not, be notified through our normal 'channels'
    about the re-fail of this node. Thus, the above code could
    insert a duplicate of an already known failure in cfailedNodes[].
    
    Later, this 'list of nodes' was converted into a BitMask
    used in the PREP_FAILREQ-signal, and converted back into
    a 'list of nodes' by Qmgr::execPREP_FAILREQ(). During this
    BitMask conversion, the duplicated NodeId was eliminated.
    However, **the 'noOfNodes' count is kept unchanged**.
    Thus we end up with a materialized 'list of nodes' where
    the size is of-by-one, and the last list item contains garbage.
    
    This patch replaces the Qmgr internal usage of node lists with
    NodeBitMask. This natively takes care of duplicate removal, and
    simplifies the conversion to/from signal formats which use
    similar bitmaps.
    
    Furthermore, this patch also fixes an issue where 'c_start.m_startNode'
    could be reset to early. It should not be reset until
    Qmgr::execPREP_FAILREQ() has received a signal informing
    about the failure of the actual 'startNode'.

[33mcommit 61858e678802a722d3df16fdfd771d722a9cc612[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Jun 5 13:21:58 2014 -0500

    Bug #18684389-SPAM ERROR 'FILE NOT FOUND' MSG WHEN 'DATA DIRECTORY'
    CLAUSE IN 'CREATE TABLE'
    
    The patch for WL7142 caused this [1;31mregression[m in which an error message
    is written to the log at startup for every remote tablespace that was
    created with DATA DIRECTORY.  A 'strict' parameter was added to
    Datafile::open_read_only(strict) in order to determine if the message
    would be logged.  The function fil_open_single_table_tablespace() tries
    to find the file in three possible locations and it always looks at the
    default location.  If the file is not there, this strict parameter causes
    the spam message.
    
    But by the time the default-location filepath is attempted,
    fil_open_single_table_tablespace() has already tried the other two
    locations.  So the fix is to print the message only if the file is not
    found elsewhere.
    
    Approved by marko in RB#5536

[33mcommit 887aebeb4ac41c78bbc590cb9ee110a6e2452440[m
Author: kevin.lewis@oracle.com <>
Date:   Wed Jun 4 15:14:17 2014 -0500

    Bug#17713871-ASSERT ERROR == DB_SUCCESS, COMMIT_CACHE_REBUILD(),
    ALTER TABLE, PRBLEM RENAMING
    
    In order to add even more uniqueness to the temporary filename, this patch replaces the LSN in the filename with a more unique another number.  It is just a static global number that is initialized to a random distributed 32-bit number using ut_time() and ut_crc32().  It is then incremented atomically for each temporary file name assigned.  This should not cause performance [1;31mregression[m since an atomic_increment is nothing compared to a OS file rename.
    
    Approved by Marco in RB#5541

[33mcommit a8e5b2c550179aeb7956ce2778450930eb90c649[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Jun 2 11:35:56 2014 +0530

    - Bug#18808086: TEMP TABLE ASSERT LATCHES->EMPTY()
      || LATCH->M_LEVEL == SYNC_LEVEL_VARYING || LA
    
      Latching order during blob pessmistic insert is as follows:
      SYNC_FSP -> SYNC_NO_ORDER_CHECK -> SYNC_INDEX_TREE
    
      SYNC_NO_ORDER_CHECK is introduced by debug latch.
      For temporary tables we suppressed taking debug latch
      and so the latching order violating occured.
    
      Fixing the [1;31mregression[m by re-enabling debug latch for
      temporary tables.
    
      Original issue of latching still needs to be reviewed
      from broader and general perspective through a separate bug.
    
      Approved by: Sunny (rb#5576)

[33mcommit 24fbc2a8bd7361eddf9ef992235ecd2bab780401[m
Merge: 57bb3edf6eb 985f445c8d8
Author: Thirunarayanan B <thirunarayanan.balathandayuth@oracle.com>
Date:   Thu May 22 16:24:01 2014 +0530

    Bug #17657223   EXCESSIVE TEMPORARY FILE USAGE IN ALTER TABLE
            Reverting the patch for the above bug due to [1;31mregression[m.

[33mcommit 70f9ef4e1220827132b50275ca7272f2bcca1864[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed May 21 13:31:29 2014 +0300

    Bug#18755095 REDO LOG SIZE CHANGE AFTER CRASH RESULTS IN CHECKPOINT AGE
    ERROR MESSAGE
    
    This is a [1;31mregression[m from fixing
    Bug#18730524 REPEATED KILL+RESTART FAILS DUE TO MISSING MLOG_FILE_NAME RECORD
    
    innobase_start_or_create_for_mysql(): Invoke fil_names_clear() before
    creating the "checkpoint" when changing redo log files.
    
    Approved by Jimmy Yang on IM.

[33mcommit d3349098e6d820731e675989146acbfd0f91f923[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Fri May 16 08:33:00 2014 +0200

    This commit fixes two errors in the new [1;31mregression[m test in the commit below:
    
    --------------------------------------------------------------------------
    #At file:///net/atum17/export/home/tmp/jw159207/mysql/repo/mysql-5.6-cluster-7.4/ based on revid:ole.john.aske@oracle.com-20140514124556-04t9xmxcwpndzi0x
    
     4333 Jan Wedvik        2014-05-14
          This commit implements WL#7375 (Ndbinfo: Per-fragment memory usage reporting). It provides a new view, 'ndbinfo.memory_per_fragment', that gives an overview of memory used per fragment replica, including fixed-element-size pages, var-element-size pages, rows, fixed-element-free-slots, var-element-free-bytes, hash index memory usage etc.
    
          In addition, the ndbinfo.ndb$pools has been extended with new rows for L2PMap memory usage.
    
          Finally, there are MTR [1;31mregression[m test cases for the new functionality.

[33mcommit b4de827c692aea01de3445fd4f0b7b16e1b6dc0c[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed May 14 15:35:09 2014 +0200

    This commit implements WL#7375 (Ndbinfo: Per-fragment memory usage reporting). It provides a new view, 'ndbinfo.memory_per_fragment', that gives an overview of memory used per fragment replica, including fixed-element-size pages, var-element-size pages, rows, fixed-element-free-slots, var-element-free-bytes, hash index memory usage etc.
    
    In addition, the ndbinfo.ndb$pools has been extended with new rows for L2PMap memory usage.
    
    Finally, there are MTR [1;31mregression[m test cases for the new functionality.

[33mcommit cc2a5250d8bb11d2f237e2ff5d08b7d8e24b2bdc[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue May 13 10:00:20 2014 +0530

    Bug #18723872  FK: ASSERTION: *CURSOR->INDEX->NAME == TEMP_INDEX_PREFIX
    
    This is a [1;31mregression[m caused by the fix for Bug #11758237 INSERT ON DUPLICATE
    KEY UPDATE SOMETIMES WRITES BINLOG POSITION INCORRECT.  The trx->error_state
    will be modified when a thread is suspended.  But logic of placing just the gap
    locks on unique secondary indexes without actually performing the insert
    depends on the value of trx->error_state.  So the value of trx->error_state is
    restored for insert into every index.  Refer to rb#3196 for the original fix.
    
    approved by Marko over IM.

[33mcommit c1994dbefe8e26ed608497ee6a9faef04e0169f5[m
Author: Vinay Fisrekar <vinay.fisrekar@oracle.com>
Date:   Mon May 12 17:31:33 2014 +0530

    Disable federated test dur to [1;31mregression[m failure.

[33mcommit f0469306317804454a1ecbe2647e39faec871c14[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue May 6 13:51:25 2014 +0300

    Fix Bug#18704384 [1;31mregression[m (memory leak).
    
    dict_ind_init(): Do not create any objects for the removed dict_ind_compact.

[33mcommit 5cae974b008eb094b46059b12f289c1856ab6c1f[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Tue May 6 12:10:10 2014 +0200

    Bug#18486460 ASSERTION FAILED: N < M_SIZE AFTER FIX_INNER_REFS
    
    The function st_select_lex::setup_ref_array allocates a Ref_ptr_array
    which is too small. This is a [1;31mregression[m from
    WL#7200: True bottom-up server parser: refactoring of the SELECT statement
    
    Prior to that worklog, we would increment select->select_n_where_fields in
    Item_field::Item_field(Name_resolution_context *context_arg, ...)
    
    The parser now uses Item_field::Item_field(const POS &pos, ...)
    immediately followed by Item_field::itemize(...)
    and this function failed to increment select->select_n_where_fields
    when the parsing context is CTX_SELECT_LIST.

[33mcommit 4b798a9895090dfd25e70cc02e2ccea4987664a7[m
Author: Jorgen Loland <jorgen.loland@oracle.com>
Date:   Fri Apr 25 16:17:14 2014 +0200

    Bug#18561062: MYSQL-TRUNK REVNO 7736 CAUSED A -2% ~ -3%
                  PERFORMANCE REGRESSION (OLTP_RO)
    
    The [1;31mregression[m is caused by calculating the condition filtering
    effect for simple single table queries where condition filtering
    has no effect.
    
    Whether or not condition filtering shall be calculated is based
    on a number of conditions. One of these conditions is whether
    join buffering may be used for the table.
    
    Optimize_table_order::calculate_scan_cost() called
    calculate_condition_filter() with a wrong parameter that indicated
    that join buffering was used even when that was not the case.
    
    Fixed by passing "join_buffering_is_used" parameter instead of
    unconditional 'true'.

[33mcommit 1df69fbff27c7968271ce7ae8e31e5f9beedd50d[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Fri Apr 25 12:43:16 2014 +0200

    * Fixed [1;31mregression[m: return value of get_admin_credentials() wasn't checked for success, only for failure.

[33mcommit 88ed55f1439ec8935911e88e039cb7f663e298bd[m
Author: Kristofer Pettersson <kristofer.pettersson@oracle.com>
Date:   Thu Apr 24 14:48:53 2014 +0200

    * fixed [1;31mregression[m: if language dir was explicitly set the search path was set incorrectly.

[33mcommit 4178d7668110a673503ecc77fc621f0222ad7c7f[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Apr 23 09:53:52 2014 +0300

    Bug#18544317 ASSERT IN IBUF_MERGE_OR_DELETE_FOR_PAGE BUF_PAGE_GET_GEN
    
    Fix a [1;31mregression[m introduced in WL#7142.
    
    ibuf_merge_or_delete_for_page(): Even if block==NULL, do update
    the bitmap, in case update_ibuf_bitmap==true.
    
    Original patch provided by Michael Izioumtchenko, adjusted by me.
    Approved by Jimmy Yang.

[33mcommit 300e3d82d38287d7d9a2c2369eb733d6fcfc8f9c[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Apr 22 10:31:10 2014 +0200

    This commit is a fix for Bug#18550318 (HOST REBOOT CAUSES CRASH,
    INTERNAL ERROR, DBDIH (LINE: 18314) 0X00000002). This bug is a
    [1;31mregression[m caused by the fix for Bug#14220269 (CLUSTER WIDE SHUTDOWN
    POINTER TOO LARGE IN DBDIH (LINE: 9290) (TCKEYREQ)). That fix added
    a new dump handler using a dump code that was already in use (7019).
    This meant that 'dump 7019' would execute two different handlers with
    different semantics and different interpretation of the parameters.
    'Dump 7019' is called automatically from Qmgr::checkStartInterface()
    if node failure handling takes more than a minute. This triggered
    an ndbrequire in the new dump handler.
    
    The fix consists in allocating a new dump code for the new dump
    handler (7024). A DumpStateType enum literal for 7019 is also defined, to
    make similar errors less likely.

[33mcommit 78169fcd3c3e543bc8ef9792e7dd73dd3c251e2f[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Apr 1 18:39:12 2014 +0200

    Bug#18483776: ASSERTION FAILED: QUERY_SOURCE == 0,
                  FILE SQL_PROFILE.CC, LINE 297
    
    This assert was triggered since PROFILING::set_query_source() by
    mistake was called twice for a prepared statement. On release
    builds this could lead to memory leaks.
    
    This bug was a [1;31mregression[m introduced by the patch for Bug#17606098
    which moved the setting of the profiling query string from
    mysqld_stmt_execute() to Prepared_statement::execute_loop().
    This patch fixes the problem by moving the call back.

[33mcommit 8b7a460cbe5f104ea130c0be4c03e0773fbe21ff[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Apr 1 19:20:03 2014 +0300

    Fix Bug#71708 70768 fix perf [1;31mregression[m: high rate of RW lock creation
    and destruction
    
    Lazily create dict_table_t::stats_latch the first time it is used.
    It may not be used at all in the lifetime of some dict_table_t objects.
    
    Approved by:    Bin (rb:4739)

[33mcommit 2f280d1bf1163dd9e9d42ceefa2fed8626bd79ab[m
Author: Balasubramanian Kandasamy <balasubramanian.kandasamy@oracle.com>
Date:   Tue Apr 1 17:42:30 2014 +0200

    Applied patch for Bug#18426180 - ENHANCED DETECTION AND DUMP OF CORRUPT MESSAGES AND SIGNALS RECEIVED
    
    ndb - [1;31mregression[m: corrupt message detected with tcp checksum activated

[33mcommit 53c6f431412b730815665fd2f226e4fdf62cd65b[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Apr 1 16:37:12 2014 +0200

    ndb - cherrypicked fix
    
      revision-id: mauritz.sundell@oracle.com-20140401081334-nrve3gyqwgf2b6vx
      parent: frazer.clement@oracle.com-20140331161153-5b4yccimejdo13v9
      parent: mauritz.sundell@oracle.com-20140401081054-quhto0gfak217e1n
      committer: Mauritz Sundell <mauritz.sundell@oracle.com>
      branch nick: mysql-7.2
      timestamp: Tue 2014-04-01 10:13:34 +0200
      message:
        merge 7.1 -> 7.2
          ------------------------------------------------------------
          revno: 2555.840.20
          revision-id: mauritz.sundell@oracle.com-20140401081054-quhto0gfak217e1n
          parent: frazer.clement@oracle.com-20140331155509-nscusf07z7kjwp47
          committer: Mauritz Sundell <mauritz.sundell@oracle.com>
          branch nick: mysql-7.1
          timestamp: Tue 2014-04-01 10:10:54 +0200
          message:
            ndb - [1;31mregression[m: corrupt message detected with tcp checksum activated
            Bug #18426180    ENHANCED DETECTION AND DUMP OF CORRUPT MESSAGES AND SIGNALS RECEIVED

[33mcommit 19a6516a2e47912ad281df3d9d9bb0785d272fb8[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Tue Apr 1 10:10:54 2014 +0200

    ndb - [1;31mregression[m: corrupt message detected with tcp checksum activated
    Bug #18426180    ENHANCED DETECTION AND DUMP OF CORRUPT MESSAGES AND SIGNALS RECEIVED

[33mcommit 0c71dbda7f54bd66695ef2322f6b6062e6a63416[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Mon Mar 31 22:44:56 2014 +0200

    ndb - [1;31mregression[m: corrupt message detected with tcp checksum activated
    Bug #18426180    ENHANCED DETECTION AND DUMP OF CORRUPT MESSAGES AND SIGNALS RECEIVED

[33mcommit e138ec78cd08cb557c6fb0e382494356c7d44fe3[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Mar 28 17:00:01 2014 +0100

    Bug#18413646: SINCE 5.7 ERROR 1416 IS CONSIDERED FATAL, BUT IT SHOULDN'T BE
    
    The problem was that the handler error HA_ERR_NULL_IN_SPATIAL was
    considered fatal, which mean it was not possible to write a SP HANDLER
    which would trigger if this error was reported.
    
    This was a [1;31mregression[m introduced by the patch for Bug#16587369
    which changed how handler errors are determined to be fatal
    (or ignorable).
    
    This patch fixes the problem by adding HA_ERR_NULL_IN_SPATIAL
    to the list of non-fatal handler errors.

[33mcommit beb968157215721852e14feef3132bf1116adc67[m
Author: Marcin Babij <marcin.babij@oracle.com>
Date:   Wed Mar 26 14:49:32 2014 +0100

    Bug#18430704: MYSQLSLAP CRASHES BY FLOATING POINT EXCEPTION
    
    Iterations input variable is unsigned, but accept value of 0, which causes division by zero.
    
    Fix: --iterations argument have minimum value of 1. In case user specifies 0 or "" it shows a warning:
    mysqlslap: [Warning] option 'iterations': unsigned value 0 adjusted to 1
    
    Added [1;31mregression[m test.

[33mcommit 638eafa7eac2b48a9de4a991f08716ec57694b46[m
Author: bin.x.su@oracle.com <>
Date:   Wed Mar 26 09:29:46 2014 +0800

    BUG 18433658 - SYNC_CHECK_ENABLE() CALLED TOO LATE, ASSERT M_ENABLED
    
    This could be a [1;31mregression[m from #WL6501, so we should call
    sync_check_enable() earlier to fix it. This patch tries to fix this
    kind of issue thoroughly:
    
    1. Call sync_check_enable() before any thread(including io handler
       threads) is created, also before recovery.
    
    2. So SYNC_RECV_WRITER is given an explicit latch order level, since
       we would check it during recovery. And it should be included in
       dict_sync_check as well.
    
    3. We should x lock IBUF_SPACE_ID first, then enter ibuf_mutex,
       according to the latch order.
    
    4. No need to sleep in sync_check_enable()
    
    rb://5006, approved by Jimmy.

[33mcommit 56382b5c37332e5a935cd5a9f44a6f2be201e414[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Fri Mar 14 14:17:33 2014 +0700

    This patch fixes the bug#17864349 - CHANGING TRUNCATE TABLE TO
    DROP TABLE & CREATE TABLE MAKES TRIGGER.TEST FAIL
    
    The bug is a [1;31mregression[m from WL#6030 and relates to handling of
    NOT-NULL columns during execution of UPDATE statement when
    there is a BEFORE UPDATE trigger and table whose column is being
    updated is not a main table in the statement (that is the update
    operation is made using temporary table).
    
    The reason for this bug was that during copying a data of fields from
    temporary table to a destination table an information about the fact
    that a field was set to null temporary wasn't copied. Also in case
    when BEFORE-update trigger was executed against a not-main table in the
    query a checking of table fields for NOT-NULL wasn't done after trigger
    execution.

[33mcommit e40e9fa4266f1016ae2db95b152d43f4e022f660[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Feb 24 14:20:23 2014 +0100

    Fix for Bug#18293112
    
      NDB.HPP DEPENDS ON PORTLIB/NDBTICK.HPP THAT'S NOT INCLUDED IN BINARY DIST
    
    This patch fix a [1;31mregression[m introduced by the refactoring part of
    bug #17647637: 'Refactor of NdbTick as prep for real fix.'
    
    5252 Ole John Aske      2013-11-18
    Part1 of fix for bug#17647637: Refactor of NdbTick as prep for real fix.
    
    Thix patch reverts the introduction of 'NDB_TICKS' and 'portlib/NdbTick.h'
    in the API header files.

[33mcommit f5f54736a527768fa9796939c75d8213baa9863e[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Feb 11 15:25:08 2014 +0200

    Bug#18185930 UPD_NODE_INSERT_BLOB CAUSES BTR_EXTERN_OWNER_FLAG ASSERTION
    
    This is a [1;31mregression[m from
    Bug#14668683 ASSERT REC_GET_DELETED_FLAG(REC, PAGE_IS_COMP(PAGE))
    which just missed the 5.5.36 and 5.6.16 release window.
    
    In row_upd_clust_rec_by_insert(), we used to call
    btr_cur_disown_inherited_fields() after committing the mini-transaction for
    delete-marking the clustered index record. The bug fix made this atomic, but
    forgot to remove the status value UPD_NODE_INSERT_BLOB.
    
    UPD_NODE_INSERT_BLOB: Replace with UPD_NODE_INSERT_CLUSTERED.
    
    row_upd_clust_rec_by_insert_inherit_func(): When this is called after
    resuming from a lock wait, the BLOBs would already be marked disowned.
    Put back the assertion that was removed in the minimal fix, in a relaxed form.
    
    rb#4560 approved by Vasil Dimov

[33mcommit ecc955d2961d234efea46831ccd9b462db439275[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Jan 24 15:36:33 2014 +0100

    Bug#17641055: THR_LOCK_MUTEX IS USED BEFORE BEING INITIALIZED
    
    The problem was that during server startup, my_init() would call
    my_thread_global_init() before safe_mutex_global_init().
    
    The former initializes several mysys mutexes by calling mysql_mutex_init().
    With SAFE_MUTEX enabled (default on debug build), mysql_mutex_init()
    calls safe_mutex_init() which uses the THR_LOCK_mutex. However this mutex
    is only properly initialized when safe_mutex_global_init() has been called.
    
    This patch fixes the problem by calling safe_mutex_global_init()
    before my_thread_global_init(). It also adds an assert to guard
    against [1;31mregression[ms.
    
    This bug was only visible with SAFE_MUTEX which is default on for debug
    build and default off when building release build.

[33mcommit f07f000030521d6525b151deaf78de9a54695dbc[m
Author: kevin.lewis@oracle.com <>
Date:   Tue Jan 21 21:24:30 2014 -0600

    Follow up to wl7342 which refactored the tablespace class
    in InnoDB.  There were some [1;31mregression[ms in the embedded
    runs of pushbuild.  Most were unforseen changes in the
    testcases.  But there was one issure in the way InnoDB
    converted a filepath to lower case on Windows.  The solution
    is to store the filepath with the case provided and compare
    to other filepaths as case insensitive if
    lower_case_table_names is not 0.

[33mcommit 5d9d307f858a2d795c27f1cb14751ada0e8a655f[m
Author: Namit Sharma <namit.sharma@oracle.com>
Date:   Wed Dec 11 18:34:01 2013 +0530

    Running [1;31mregression[m tests on PB2 for WL#7299

[33mcommit 5821dacf3323f087718e9b44e3ca2737caca4feb[m
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Tue Nov 26 17:18:23 2013 +0530

    Bug#17458914 - crash in item_ref::walk, with case, subquery,values()
    Problem:-
    It is a [1;31mregression[m, cause by the patch for bug(14789787), where
    INSERT ... ON DUPLICATE KEY UPDATE statement, using VALUES()
    function with INSERT clause, causes valgrind warnings.
    
    In earlier patch, while resolving VALUES() function, we were checking
    every item in UPDATE value list and see whether this VALUES() function
    is in the UPDATE value list.
    
    But there may be chances that some items are not resolved in the UPDATE
    value list, which will result in an assert. This the case with the reported
    bug(17458914), where we are trying to reference the Item_ref in subquery.
    
    
    So now, instead of traversing the UPDATE item CLAUSE. We will put a new flag,
    which will be set when we are resolving the fields which are on duplicate
    updates.

[33mcommit 4750df9409c44b4be076eda3e8ee9d14ed8edf2a[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Tue Nov 26 09:35:59 2013 +0100

    Bug#17834434: sql/table.h: Assertion tbl->updatable && !tbl->multitable_view
    
    This was a [1;31mregression[m from WL#6987.
    It was forgotten to check for multi-table views, hence the assert fired.
    The fix is to check explicitly for !table_list->multitable_view.

[33mcommit b3b0a645fc3b93278f9e1cc312efaba50fe4f9e2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Nov 25 11:02:29 2013 +0200

    Bug#17543588 PERFORMANCE REGRESSION (9%) FOR DBT-3 QUERY 21
    
    This is an unexpected [1;31mregression[m from an earlier fix:
    
    Bug#16852278 SIMPLIFY RECORD COMPARISONS
    
    The reason of this [1;31mregression[m is that memcmp() can be slower than a for loop.
    If memcmp() is a library call, there is an overhead involved,
    which we can avoid by comparing the first few bytes with a loop.
    The built-in memcmp() of GCC on x86 and AMD64 (repz cmpsb) is known to
    perform worse than the GNU libc library function:
    
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43052
    
    cmp_data(): Use a for loop to compare some first bytes.
    Then invoke memcmp() for any remaining common bytes if needed.
    
    innodb.cmake: Pass -fno-builtin-memcmp when compiling rem0cmp.cc
    with GCC on x86 or AMD64.
    
    rb#3931 approved by Jimmy Yang

[33mcommit 0d2a88c18228c5e96a98f45058fd8c45b5ddd83c[m
Author: Nisha Gopalakrishnan <nisha.gopalakrishnan@oracle.com>
Date:   Thu Nov 21 10:21:30 2013 +0530

    BUG#17242996: ATOMIC/X86-GCC.H:MAKE_ATOMIC_CAS_BODY64 POTENTIAL
                  MISCOMPILATION BUG
    
    Analysis:
    --------
    The make_atomic_cas_body64 implementation on IA32 with GCC but
    without GCC builtins might get miscompiled due to a wrong
    constraint.
    
    make_atomic_cas_body64 implementation uses CMPXCHG8B with a
    single memory operand as input/output operand. However the
    constraint '+' was not used to indicate the same due to
    issue reported by Bug#11746008. Hence it could result in
    using two different memory locations, thus accessing an
    uninitialized memory with probable runtime crash.
    
    Fix:
    ---
    The assembly atomic implementation (x86-gcc.h) is not used on
    Windows and Solaris. The oldest version of GCC supported by
    Linux variants on any platform is GCC 4.1.2 which supports
    builtin atomics. On MAC, GCC versions higher than 4.1.2 or
    Clang is used which supports builtin atomics as well.
    
    Hence this patch removes assembly atomic implementation
    (x86-gcc.h) and relies on the GCC builtins. Upon using
    an older unsupported versions of GCC(A warning is flagged),
    we may see some performance [1;31mregression[ms since we fallback
    to rw locks.

[33mcommit ed3c4c96bfb286f6b40f44d32f8dc829fa495cb9[m
Author: Norvald H. Ryeng <norvald.ryeng@oracle.com>
Date:   Thu Nov 14 14:54:05 2013 +0100

    Bug#17727506 REGRESSION: CRASH IN JOIN::RECOUNT_FIELD_TYPES
    
    This is a [1;31mregression[m introduced by the fix for bug #16967281.
    
    Problem: Server may crash when executing an INSERT SELECT with UNION,
    ROLLUP and ON DUPLICATE KEY UPDATE with a subquery.
    
    When an INSERT SELECT statement has an ON DUPLICATE KEY UPDATE clause
    that contains a subquery, the SELECT_LEX_UNIT for that subquery is
    added as the slave of the last SELECT_LEX in the INSERT SELECT, even
    though it is not part of the query itself. This subquery in the update
    values list is not prepared before JOIN::optimize() on the INSERT
    SELECT.
    
    When JOIN::recount_field_types() tries to recursively call itself on
    subqueries, it also traverses the SELECT_LEX for the update subquery,
    which isn't prepared and doesn't have a JOIN object.
    
    Fix: Don't try to recount field types in unprepared subqueries.

[33mcommit 0efa750c9abb12f4a7eefc67cf571f2770f7cb83[m
Author: Tor Didriksen <tor.didriksen@oracle.com>
Date:   Fri Nov 8 09:36:41 2013 +0100

    WL#1509 Pack values of non-sorted fields in the sort buffer
    
    Fix performance [1;31mregression[m when sorting small/empty input tables.

[33mcommit 6cdca531f273160ceb468c69b220f4675e3fe5d7[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Nov 1 15:55:21 2013 +0100

    Bug#17720496: Test failure in innodb.innodb_mysql with query cache
    
    This is a [1;31mregression[m after WL#6987.
    
    The problem can be reproduced with the following test:
    
    ./mysql-test-run --mem main.view \
    --mysqld=--query_cache_type=1 --mysqld=--query_cache_size=1M
    
    The problem was a missing rewrite of a query cache update for LOAD
    statements, so that the query cache assumed a view was updated and
    not the underlying base table.
    
    The fix is to use the same query cache invalidate function as
    INSERT does.

[33mcommit caf0007212beeefdac0b3d0c9423209af86abf31[m
Author: Roy Lyseng <roy.lyseng@oracle.com>
Date:   Fri Nov 1 13:18:12 2013 +0100

    Bug#17713031: Assert 'table == tables[unique_counter] in multi_delete
    
    This is a [1;31mregression[m from the refactoring of multi_delete in WL#6987.
    
    The arrays tempfiles and tables in class multi_delete have one entry
    for each table to be deleted from, which is not deleted from immediately.
    (The deletes are collected inside a Unique object and deletes happen
    after the "immediate" deletes).
    
    However, the code that incremented the counter unique_counter which
    keeps track of the tempfile object for the current table being deleted
    from was placed in a wrong location, so that when a row from an
    outer-joined table was NULL-complemented, unique_counter was not
    incremented as it should.
    
    The fix is to place the increment operation earlier, just after
    detecting that we have a table that is deleted from, and the delete
    is not immediate. This placement of the code will also prevent other
    conditionals from interfering with this logic (such as e.g. an error
    in a triggered operation).

[33mcommit fb17145db8d218b55ee762f520a8c94bb8fc4cc8[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Oct 28 12:05:16 2013 +0530

    Bug #17665353 RECENT REGRESSION: FAILING ASSERTION:
    THR_GET_TRX(THR)->ERROR_STATE == DB_SUCCESS
    
    Problem:
    
    The fix for the Bug #11758237 "INSERT ON DUPLICATE KEY UPDATE SOMETIMES
    WRITES BINLOG POSITION INCORRECT" caused a [1;31mregression[m.  The fix for the
    bug#11758237 allows processing of unique secondary indexes even after
    the DB_DUPLICATE_KEY error has been detected.  Once the duplicate key
    error has been detected, no actual insert will be attempted, but just
    the necessary gap locks are taken.  The block that checks this was
    missed because of putting it as part of the else block.
    
    Solution:
    
    Once the duplicate key error is detected, we will not attempt actual
    insert of the data.  Instead only gap lock will be placed.  This
    code block is now independently placed.
    
    rb#3741 approved by Marko.

[33mcommit 42c84f502d3ccc76dda322218c75055bc2c27010[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Sat Oct 26 08:45:21 2013 +0200

    WL#7027: Deprecate EXTENDED and PARTITIONS flags of EXPLAIN
            Fix: Make daily_build stable and fix hudson [1;31mregression[m failures

[33mcommit 141afe536baec76b8f0bc91bb63198b412fe8d38[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Fri Oct 25 14:06:47 2013 +0200

    WL#7027: Deprecate EXTENDED and PARTITIONS flags of EXPLAIN
          Fix: Fix [1;31mregression[m test failure for daily test.

[33mcommit 1213767257f5e8ce0d153ff9c26a9d7081fc1a5d[m
Author: Benny.Wang <Benny.Wang@oracle.com>
Date:   Thu Oct 24 14:24:41 2013 +0200

    WL#7027:Deprecate EXTENDED and PARTITIONS flags of EXPLAIN
    post-fix: Fix [1;31mregression[m test failures for big-test introduced by WL7027.

[33mcommit 402fcb0a17406d3aeca052204f1fb75762558726[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Oct 17 19:05:54 2013 +0900

    WL#7050 - InnoDB: Refactor redo log write code for better performance
    
    - This is rewrite of log_write_up_to() to improve its performance in case where innodb_flush_log_at_trx_commit = 2.
    
    In log_write_up_to():
    
    * Remove wait mode. We always wait with one exception. And that is when doing log
    sync from master thread. It makes that synchronous as well because that happens
    only once per second.
    
    * Because we only have one log group therefore we don't need two flush_events.
    * Remove unnecessary fields like written_to_some_lsn, written_to_all_lsn.
    * If only write is requested we don't have to acquire the log_sys::mutex after we
    release it. We currently do that only to do event handling but event handling is
    really only needed in case where flush is requested i.e.: a thread should be
    waiting on the event iff it is interested in flushing. Writes are serialized under
    log_sys::mutex.
    
    This patch was originally written by Inaam Rana.
    
    rb#2389 Approved by Sunny and Yasufumi
    
    ===========
    
    Adjustment for performance was done therough inherited rb#3373
    
    - optimize log_write_up_to() more
            * remove the second log_sys->mutex obtain also for "innodb_flush_log_at_trx_commit = 1" path
            * remove unnecessary ut_memcpy. (because log_group_write_buf() is protected by log_sys->mutex)
            * remove dirty-read from flush_to_disk=true case. (to avoid [1;31mregression[m at some cases)
              (to keep current arbitration for write/fsync contention between log and data file)
            * fix wrong handling of O_DSYNC
    - revive log_buffer_sync_in_background(). (because it needs to be used)

[33mcommit 0d55a0761c9c7b8edeea4b9231ad931fa74fdb3e[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Thu Sep 26 14:04:57 2013 +0200

    Bug#17507853 PREVENT FALSE SHARING OF CPU CACHE LINE FOR SENSITIVE COUNTERS
    
    This fix is a code cleanup to avoid risks of performance degradations.
    
    In the performance schema code, several internal counters are highly
    sensitive, because they are used very often, and used with atomic
    operations.
    
    For these counters, it is critical that the counter is allocated on a CPU
    cache line, and that no other variable is allocated in the same memory area,
    which can end up in the same CPU cache line.
    
    This fix introduce PFS_cacheline_uint32 and PFS_cacheline_uint64,
    to enforce this property.
    
    Al internal counters related to allocation internal objects have been
    changed to use PFS_cacheline_uint32/64.
    
    Whether CPU cache line collision did happened in the past,
    causing performance degradations, is unknown, so this fix may not improve
    current performance.
    
    What this fix does however, is to make sure that cache line collisions are
    now impossible, to prevent any [1;31mregression[ms when using:
    - different compilers
    - different compiler options
    - different CPUs
    
    Also, the allocation algorithm for PFS_account, PFS_user and PFS_host have
    been changed to follow the mainline code.
    
    PFS_scan has been removed, this way of performing allocations is no longer
    needed.

[33mcommit 2b814f79de1ad84c3d82de08aa4bf15f8063d13a[m
Author: sayantan dutta <sayantan.dutta@oracle.com>
Date:   Thu Sep 19 11:54:32 2013 +0530

     WL #6944: MTR: Develop [1;31mregression[m tests for MTR framework

[33mcommit 8600a3976117f96ac005c1bcab504a68c0104923[m
Author: sayantan dutta <sayantan.dutta@oracle.com>
Date:   Wed Sep 18 14:09:49 2013 +0530

    WL #6944 : MTR: Develop [1;31mregression[m tests for MTR framework

[33mcommit 00dc6be986513b494ca8e14a8b117c583514b8c8[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Sep 11 09:48:59 2013 +0200

    Bug#17420682: SQL_KILL: ASSERTION FAILED: ! IS_SET(),
                  FILE SQL_ERROR.CC, LINE 382
    
    This assertion is triggered if a statement tries to send an OK
    packet to the client after the statement has reported an error.
    
    It was triggered in this case if KILL [CONNECTION | QUERY] was
    used with a thread_id argument that somehow gave an error.
    The implementation of KILL did not take this possibility into
    account. This patch fixes the problem by checking if val_int()
    has reported an error.
    
    No [1;31mregression[m test is included as the reported test first hits
    the assertion bug reported as Bug 11764690: SOME FUNCTIONS CAUSE
    ASSERTION WHEN GIVEN A ROW CONSTRUCTOR ARGUMENT. The patch was
    verified by temporarily disabling the Bug 11764690 assert.

[33mcommit 1232eb800c3c99a97c07e3e16194035fd7a91415[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Sep 5 08:39:37 2013 +0200

    Bug#17400687: DIAGNOSTICS_AREA::SET_ERROR_STATUS ASSERTION
    
    This assert is triggered if a statement tries to set an exception
    condition in a diagnostics area which already contains an
    exception condition or a completion condition.
    
    It was triggered in this case by a failed stored program trying
    to transfer its exception condition to the diagnostics area
    of the caller - which already contained an exception condition.
    Normally this should not happen, but there is a special case
    with multi update, non-transactional tables and triggers.
    
    If a multi-update has already updated a non-transactional
    table when it encounters an error updating another table,
    it cannot rollback the changes. In this case, it tries to
    proceed and may end up executing a trigger. If this trigger
    also fails, the assert was triggered.
    
    This patch fixes the problem by not copying exception condition
    from the diagnostics area of the stored program to the diagnostics
    area of the caller if the latter already contains an exception
    condition.
    
    This bug was (likely) a [1;31mregression[m introduced by WL#6406.

[33mcommit 49ef9a054475526f7e7107e87327b87d5bb838d4[m
Merge: 23c4a1161d8 0f432a86db3
Author: Neeraj Bisht <neeraj.x.bisht@oracle.com>
Date:   Fri Aug 23 19:27:44 2013 +0530

    Bug#17309863 AUTO RECONNECT DOES NOT WORK WITH 5.6 LIBMYSQLCLIENT
    Problem:
    A client program fails to reconnect to a mysql-5.6 server
    after a connection is killed by a KILL CONNECTION_ID()
    command. Any query executed after that returns
    "query failed, Lost connection to MySQL server during query".
    This is a [1;31mregression[m introduced by a patch for Bug#11762221
    (54790: USE OF NON-BLOCKING MODE FOR SOCKETS LIMITS PERFORMANCE)
    
    Solution:
    The patch is restoring the behavior of the previous versions.
    In net_serv.cc::net_clear(), setting net->error=2 flag if
    connection was terminated, allows the client to reconnect.
    We check if the peer is still connected (EOF not reached).
    Otherwise we set the error flag so that the connection might be
    re-established before we try to send a query to the server.

[33mcommit ee04b1d3dc00999fd62d5a5db7d9ee07b930df7d[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Thu Aug 22 15:22:35 2013 +0530

    Reverting the patch for Bug#17332603.
    Reason:
    The fixed is causing the [1;31mregression[m for
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH OLD INNODB
                 DATA FILES

[33mcommit c99a6774975285eb2627f83e68ade29ae62dff23[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Aug 21 15:45:56 2013 +0200

    Bug#17049537: ASSERT 'M_STATUS == DA_ERROR' FAILED IN
                  DIAGNOSTICS_AREA ON TRIGGER + KILL QUERY
    
    The problem was that if a statement was killed while executing
    a trigger, an assert could be triggered. The bug had no effect
    on release builds.
    
    Triggers are implemented as stored routines. When a stored
    routine ends with a failure, we copy error condition
    information from the diagnostics area used during the stored
    routine to the diagnostics area used by the caller. This assert
    checks that the source diagnostics area in fact contains an
    error.
    
    The problem was that in the case of kill, an error might
    not have been reported yet at this point. This patch fixes
    the bug by checking if an error condition is present before
    trying to copy it.
    
    This bug was a [1;31mregression[m introduced by WL#6406.

[33mcommit 0717ff9541610e68c269ab4b30b7e9b7c4b4bcff[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Thu Aug 1 12:03:38 2013 +0700

    This is patch for bug#16240526 - ASSERT FAILED IN FIELD::CHECK_CONSTRAINTS.
    
    The bug is a [1;31mregression[m from WL#6030.
    
    mysql server built with debug could be terminated abnormally if
    a user executes SQL statement LOAD DATA INFILE that is failed
    for some reason and after that user executes some other
    SQL statement.

[33mcommit 6d74dc4fa379fab063612b113c5ae851ae49dff6[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Tue Jul 16 16:17:21 2013 +0800

    Fix Bug#17057168 LARGE PERFORMANCE REGRESSION FOR INNODB
    GEOMETRY/SPATIAL INDEX LOOKUP
    
    For the new datatype DATA_GEOMETRY, comparison of this data always
    return 0. This caused it can't find the correct key range of the index
    on this data type, so the query on this type column will always do full
    index scan. That's why we got this performance [1;31mregression[m.
    The solution is: Since we still use still use BLOB as underlying
    datatype, so we should compare geometry data following the compare BLOB
    way.
    In this patch, I also fixed the occasionally failure of test case
    i_innodb.innodb_bug15963619. According to Krunal's suggestion, I
    increased the restart wait time from 10 to 60.
    
    rb#2890 Approved by Jimmy.

[33mcommit 085f27db6ef9b56238e2438c294fdb735d827c34[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Jun 10 12:24:00 2013 +0200

    This commit fixes bug#16853897 'NDB_JOIN_PUSHDOWN_BKA.TEST FAILS ON
    SOL10-SPARC'. This bug caused queries using BKA to give wrong results in
    certain cases. This happens if all of the following are true:
    - mysqld runs on a big-endian machine.
    - the table that is buffered in the BKA join cache use a storage
      engine with little-endian data format (e.g. InnoDB or MyISAM).
    - The next table use a storage engine with native-endian data format
      (i.e. ndb).
    
    The keys used for accessing the native-endian table is then in little-endian
    format, giving wrong results. This is caused by an optimization in the BKA
    code: when possible, it will use a pointer into the BKA chache as a key,
    instead of running the regular code for extracting fields from a record
    and building a key. There is a method called
    JOIN_CACHE_BKA::check_emb_key_usage() that checks if this optimization is
    possible. The fix consists in adding an extra test to this method, to check
    if both tables have the same endianness.
    
    Presumably the error would also happen if the two tables were switched, such
    that native-endian records were buffered in the BKA cahce and used for building
    keys for a little endian table. But I was not able to write a query that would
    use BKA this way.
    
    This bug breaks an exiting ndb [1;31mregression[m test. Unfortunately there does
    not seem to be any way to reproduce this error without ndb, since this
    is the only native-endian storage engine.

[33mcommit 255cafbe76bd2736cede1ec1991cc8d0deb420d5[m
Author: kevin.lewis@oracle.com <>
Date:   Sat Jun 8 02:11:55 2013 -0500

    Fix compillation errors when using WITH_PERFSCHEMA_STORAGE_ENGINE=0
    Fix MTR [1;31mregression[m of 4 innodb_fts tests involving a bad change to fts_bsearch().

[33mcommit 0c3b1de710d583834aa43bcced7397c918f4f48a[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Mon May 20 14:03:03 2013 +0200

    fix0 - bug#16834242
    
    This patch fixes a [1;31mregression[m in that data-nodes crash
    due to a data race in the time window between execNODE_FAILREP is recevied
    in QMGR and execNODE_FAILREP is recevied in DBLQH/DBTC.
    
    The race is that the version found in getNodeInfo(n).m_version
      return *global* information, in should only be dependant on
      in certain scenarios. (as far as i can tell usage was correct prior to this)
    
    This version of the patch changes so that m_version can be used
    up until all blocks has replied that they handled the node failure
    
    Prior to this patch, there was no real promises about when
    the information in getNodeInfo().m_version was consistent but
    no code prior to frazer.clement@oracle.com-20130123165838-rqdlut9tt7lco7uh
    depended on it in a dangerous way.
    
    Problem was introduces in
    2013-01-23 - frazer.clement@oracle.com-20130123165838-rqdlut9tt7lco7uh
    
    ---
     storage/ndb/src/kernel/blocks/qmgr/QmgrMain.cpp |   24 +++++++++++++++++-------
     1 file changed, 17 insertions(+), 7 deletions(-)

[33mcommit 08d17955ba9f598d43b9da9ef19822b2d3355c41[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu May 16 11:03:59 2013 +0300

    WL#6871: Fix a [1;31mregression[m from
    marko.makela@oracle.com-20130515123143-rrj6kmgz29wxx18a.
    
    Initialize n_ext=0 before calling rec_get_size_comp().

[33mcommit cc2ef7e64f557fa50d4a1b2ac32a68769bb35382[m
Author: kevin.lewis@oracle.com <>
Date:   Mon Apr 8 10:51:36 2013 -0500

    Fix testcase [1;31mregression[m from wl6742 that runs under big-test

[33mcommit 50e9946c20a4d19ab867739b43d529ff84c6ce57[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Apr 3 10:44:27 2013 +0200

    Bug#16522662: SHOW WARNINGS ISN'T RESETTING THE WARNINGS COUNT
    
    The problem was that the first EOF packet sent during execution of
    SHOW WARNINGS and SHOW ERRORS contained the number of conditions
    raised by the previous statement rather than the number of conditions
    raised by SHOW. This bug did not affect the second EOF packet sent
    after SHOW completes, only the first EOF packet sent directly after
    the initial result set metadata.
    
    This was a [1;31mregression[m introduced by WL#6406. This WL changed the
    behavior of SHOW WARNINGS | ERRORS so that a new diagnostics area is
    pushed on the diagnostics area stack before SHOW executes. This
    prevents any conditions raised during SHOW execution from modifying
    the diagnostics area we are inspecting, similarly to how
    GET DIAGNOSTICS works. The problem was that when the new diagnostics
    area is pushed and conditions copied from the old to the new area, the
    counter of conditions generated by the current statement was also
    updated. And since this new diagnostics area was the current area when
    the first EOF packet was sent, the number of conditions generated by
    the previous statement was by mistake sent instead of the number of
    conditions generated by SHOW.
    
    This patch fixes the problem by resetting the condition counter after
    pushing a new diagnostics area for SHOW WARNINGS | ERRORS. A similar
    change has been made for GET DIAGNOSTICS (for consistency only, there
    were no negative consequences).

[33mcommit 71c556bb7b3f73e7b90b59dc446a571b2df1b479[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Mar 12 15:47:02 2013 +0100

    Fix [1;31mregression[m after push of fix for
    bug16020945 HA_NDBCLUSTER ADAPTION TO 5.6 RBWR
    
    Partitioning columns need to be included in read_set
    for update & delete which use partition pruning.

[33mcommit 618ab4c226f829e8da45fc3676844526f43373d1[m
Author: kevin.lewis@oracle.com <>
Date:   Mon Mar 11 10:36:18 2013 -0500

    This patch pushed to mysql-trunk last week;
    
    revision-id: kevin.lewis@oracle.com-20130307172527-h2vtnnmd3w4004xr
    branch nick: mysql-trunk
    timestamp: Thu 2013-03-07 11:25:27 -0600
    message:
      Bug #11763660 - PLEASE ASSERT INSTEAD OF CALLING EXIT(1)
    
    caused a [1;31mregression[m in innodb_bug13450566 due to an error message format change.
    It now needs to be suppressed.  This mtr.add_suppression() is added to
    innodb_bug13450566 along with some other test case cleanup where this message
    is suppressed.
    
    Approved by Vasil on IM

[33mcommit 8ac4a3e7d355950b906807077da420328f82b0af[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Wed Mar 6 16:32:20 2013 +0100

    Bug#16429216: SP_NOTEMBEDDED IS UNSTABLE ON PB2
    
    sp_notembedded.test was unstable because of the [1;31mregression[m test for Bug#47736.
    This [1;31mregression[m test is supposed to kill a select from a view while the view
    was processing a function. This was implemented by having a function that executed
    sleep(60). The problem was that there was no synchronization ensuring that sleep(60)
    was actually executing when KILL was issued.
    
    This patch fixes the problem by reimplementing the stored function to use
    user-level locks for synchronization. The patch also removes an extra
    connection that was not needed.

[33mcommit 65c147bf5a98ca8b311c161651050e5c66795ac3[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Mar 1 13:24:46 2013 +1100

    WL#6044 - Spin before calling the system mutex "enter". This is to test if it
    makes any difference in perfromance. Last set of results indicate that the
    Sys mutexes on Linux 1K OLTP RW Sysbench tests show [1;31mregression[m compared to
    mysql-trunk.

[33mcommit fa4d6f3411fb46f549d16f74920b8eb1d8d23468[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Feb 14 10:14:23 2013 +0100

    This commit is a fix for bug Bug#16187976 "NDBD NODE FAILS TO START WITH
    ILLEGAL SIGNAL RECEIVED (GSN 121 NOT ADDED)". This commit ensures that ROUTE_ORD
    signals will be sent to TC rather than the SPJ block. It also adds a [1;31mregression[m test.
    Finally it adds extra checks to ensure that TRANSID_AI_R signals will not be sent to
    SPJ.

[33mcommit 7c8c87460e2e7064e7a2b57f8c984f2e43524248[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Feb 12 12:38:32 2013 +0100

    Previous commit had an error. Reverting it for now.
    
    Previous commit:
    
    4170 Jan Wedvik 2013-02-12
          This commit is a fix for bug Bug#16187976 "NDBD NODE FAILS TO START WITH
          ILLEGAL SIGNAL RECEIVED (GSN 121 NOT ADDED)". This commit ensures that ROUTE_ORD
          signals will be sent to TC rather than the SPJ block. It also adds a [1;31mregression[m test.
          Finally it adds extra checks to ensure that TRANSID_AI_R signals will not be sent to
          SPJ.

[33mcommit 3d2a7a6f45b48842dac2cf6bd833b7bab46ae5a8[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Feb 12 11:12:56 2013 +0100

    This commit is a fix for bug Bug#16187976 "NDBD NODE FAILS TO START WITH
    ILLEGAL SIGNAL RECEIVED (GSN 121 NOT ADDED)". This commit ensures that ROUTE_ORD
    signals will be sent to TC rather than the SPJ block. It also adds a [1;31mregression[m test.
    Finally it adds extra checks to ensure that TRANSID_AI_R signals will not be sent to
    SPJ.

[33mcommit f6ee0ede8329072ac3180f86b2264f8f29ec45fe[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Tue Jan 29 10:42:26 2013 +0100

    Fix for Bug#14826522 REF ACCESS ON STRING DATA TYPES RETURNS
                         NON-MATCHING RECORDS
    
    This bug was caused by the fix for Bug 14682735. The fix for Bug
    14682735 was for ref access on string data types and tried to avoid
    having the evaluate the ref condition as part of the WHERE condition
    after the records were read from the storage engine. This worked in
    most cases but there are at least two cases where this caused
    non-matching records returned from the storage engine to not be filter
    out:
    
    1. If a string field is compared against a string constant that is
       longer than the field, then ref access will be done on a prefix of
       the string constant and can thus return non-matching records. These
       were no longer filtered away.
    
    2. For BINARY/VARBINARY fields with equality against a string constant,
       ref access can return non-matching records even in the case where
       the string constant is shorter than the field. This is due to not
       padding the string to the width of the field when setting up ref
       access.
    
    This patch fixes the [1;31mregression[m caused by Bug 14682735 by reverting
    most of the changes done in the patch. Thus, after this change we will
    again evaluate the ref condition for string data types after the
    records are read from the storage engine.

[33mcommit c10846736c166c35917410abe1ae8132aacda5e9[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Mon Dec 10 12:09:22 2012 +0000

    Bug #15953730   NDB : SLAVE_ALLOW_BATCHING BROKEN IN 7.2
    
    This commit adds a new testcase, ndb_rpl_batch which aims to
    show that slave_allow_batching=ON has a positive effect.
    This is committed to 7.0 and merged up, giving some protection
    against future [1;31mregression[ms there.
    In 7.2 it will be joined by the fix.

[33mcommit e9ea1fb30db8102bbe23e24cb7641fba28b17cdd[m
Author: Dmitry Shulga <Dmitry.Shulga@oracle.com>
Date:   Wed Dec 5 23:56:42 2012 +0600

    This patch fixes bug#14729757 - MY_HASH_SEARCH(&XID_CACHE,
                                    XID_STATE->XID.KEY(),
                                    XID_STATE->XID.KEY_LENGTH())==0
    
    This bug is a [1;31mregression[m of bug#11759534 - 51855: RACE CONDITION
                                               IN XA START.
    
    The reason for [1;31mregression[m is that the changes that fixes the original
    bug wasn't merged from mysql-5.1 into mysql-5.5 and mysql-trunk.
    Only null-merge was done for the patch changeset.
    
    To incorporate lost changes the manual merge have been done.
    
    Additionally the call of trans_rolback() was added into trans_xa_start()
    in case if xid_cache_insert is failed() after transaction has been started.
    If we don't call trans_rollback() we would never reset the flag
    SERVER_STATUS_IN_TRANS in THD::server_status and therefore all subsequent
    attempts to execute XA START in the connection where the error was occurred
    will be failed since thd->in_active_multi_stmt_transaction() will return
    the true every time when trans_xa_start is called.
    
    The latest changes were absent in patch for mysql-5.1

[33mcommit 78ef54a78cb46c5f9e8b9c3bbfc1f1e3924ebd58[m
Author: Pekka Nousiainen <pekka.nousiainen@oracle.com>
Date:   Wed Oct 17 13:44:50 2012 +0300

    wl#5929 sp_fix01.diff
    bug#14772503 ndb_apply_status [1;31mregression[m

[33mcommit 6bcfe8d4878c3f904c1f97ff547b81e92cdb4cfe[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Mon Oct 15 13:46:54 2012 +0200

    This commit fixes Bug#14647210 "CAN CRASH ALL NODES EASILY WHEN
    RESTARTING MORE THAN 6 NODES SIMULTANEOUSLY":
    
    * It implements Dbdict::execGET_TABINFOREF(). This function will resend
    GET_TABINFOREQ if the DICT master was busy.
    
    * It removes local queueing of GET_TABINFOREQ signals (via delayed
    signals) for signals coming from another DICT block.
    
    * It implements a [1;31mregression[m test for this bug.
    
    * It adds a version check, such that GET_TABINFOREF will not be sent
    to DICT blocks where the software is too old to handle that signal.

[33mcommit 6e07e5ae8cdbd5fc06a7cf61ec838c6b7f8eb5c8[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Fri Sep 21 09:24:28 2012 +0200

    This commit is a backport of the fix for Bug#14644936
    "INEFFICIENT AND INCORRECT EXECUTION OF PUSHABLE OUTER JOINS".
    
    This commit also adds a [1;31mregression[m test,
    since this bug can only be reproduced in cluster.

[33mcommit 9d9be9ff38e5256652e1046b436fbc735376aa03[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Thu Sep 13 21:18:47 2012 +0100

    Bug #14472648   CONFIGURED DISKCHECKPOINTSPEED EXCEEDED WHEN BACKUPMAXWRITESIZE SET TO HIGH VALU
    
    The DiskCheckpointSpeed mechanism is implemented using 100 millisecond
    periods, which each have 1/10th of the configured quota available.
    
    A period is allowed to overflow its quota, with the excess being taken
    from the next period's quota.
    
    However, this overflow was limited to the next period, after that, any
    further overflow was ignored.
    
    In cases where large overflows were possible, relative to the 1/10
    DiskCheckPointSpeed quota, this could result in excessive disk writing,
    and CPU overhead as a result.
    
    Setting a larger-than standard MaxBackupWriteSize is the primary means
    of causing larger-than 2* quota overflows and triggering this bug.
    
    This bug is fixed by using as many subsequent periods as necessary to
    'pay off' the quota overflow.
    
    This will result in the data node staying within its quota.
    
    This fix may result in slower LCP in some systems, and reduced CPU usage
    during LCP.
    
    A testcase, and an internal DiskCheckPointSpeed verification mechanism
    are added to avoid future [1;31mregression[ms.

[33mcommit 83891927f0efc125ed6a2e896f7c70f94085e30f[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Aug 30 10:40:49 2012 +0200

    This commit is a followup to
    revno 4960 / revid:jan.wedvik@oracle.com-20120822104942-q95h5zi9m729lkkk
    which was a fix for 'Bug#14190114: CLUSTER CRASH DUE TO NDBREQUIRE IN
    ./LOCALPROXY.HPP DBLQH (LINE: 234)'.
    
    When running ndbautottest, the [1;31mregression[m test for bug 14190114
    (runDropTakeoverTest()) would always fail. The reason for this was that
    another [1;31mregression[m test (runBug13416603()) would leave some data nodes
    in a state where they would not start again after stopping because
    of an error insert. This commit fixes this problem by adding cleanup
    code to runBug13416603().

[33mcommit 90897fc9cc1e54213b5b515001cba3896413162b[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Wed Aug 22 12:49:42 2012 +0200

    Bug#14190114: CLUSTER CRASH DUE TO NDBREQUIRE IN ./LOCALPROXY.HPP DBLQH (LINE: 234)
    
    This patch fixes a set of errors that causes node failure (or block new
    dictionary operations) if the master node crashes in certain states of a drop
    table operation. This covers bug 14190114 and some related errors that showed
    up when running the [1;31mregression[m test:
    
    1)
    This patch fixes the direct cause of bug 14190114. The patch ensures that
    the master will reject SCHEMA_TRANS_BEGIN_REQ and SCHEMA_TRANS_END_REQ messages
    while there are outstanding DICT_TAKEOVER_REQs. If SCHEMA_TRANS_BEGIN_REQ was
    allowed, the system could end up in a situation where two transactions had
    outstanding DROP_TAB_REQs at the same time. This caused bug 14190114.
    Likewise, SCHEMA_TRANS_END_REQ cannot be processed before the new master knows
    the state of the transaction (i.e. after it has receibed the
    DICT_TAKEOVER_CONFs).
    
    2)
    This patch fixes an error in the construction of the CONTINUB message that
    DICT sends to itself if it receives a DICT_TAKEOVER_REQ while it still
    has active operations.
    
    This patch also substitutes sendSignal with sendSignalWithDelay. This is done
    for two reasons:
    I) To avoid waisting CPU cycles by doing busy wait.
    II) To prevent CONTINUB messages from filling the jam trace buffer (this made
    the error report fro the customer harder to analyze.)
    
    3)
    This patch disables counting of SCHEMA_TRANS_IMPL_CONF and
    SCHEMA_TRANS_IMPL_REF messages during takeover. Normally the master counts
    these to know when all participants have completed an operation. But during
    a takeover, the new master will not know the number of outstanding messages
    until it has received DICT_TAKEOVER_CONF.
    
    4)
    This patch ensures that drop table operations are set to start OS_COMPLETED
    after finishing RT_COMPLETE requests. As it was, these would remain in state
    OS_COMPLETING. This meant that DICT could never send DICT_TAKEOVER_CONF, since
    this can only be done when all operations are in 'passive' states.

[33mcommit 59058bd4806bfb114e85ca6373500d8dcabdd326[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Tue Aug 14 13:23:36 2012 +0200

    Fix for Bug#14489398 CLUSTER BACKUP FAILS WHEN USING NDBMTD AND CONFIGURED MULTIPLE LQHS
    
    Fix [1;31mregression[m introduced by push:
    
    3923 Ole John Aske      2012-05-23
          This is the improved 'save_mem.patch' from Mikael R. patch set
    
    That push tried to save memory used for interthread communication buffers
    by not allocated buffers between those threads which was assumed to not communicate.
    LDM <-> LDM communication was one of the places where such buffers where removed.
    
    However, the BACKUP-block is part of the LDM thread, and during backup the
    first BACKUP instance in LDM thread#1 will act as client/coordinator. Thus
    it *will* communicate with other LDM threads!
    
    This fix introduce special handling of LDM thread#1, and 'opens up' communication
    between that thread and other LDM threads. Instead of writing a completely new
    testcase to cover this fix, I have introduced 'ndb_restore_misc.cnf' which overrides
    'MaxNoOfExecutionThreads=4' - Such that at least one of the existing backup-restore
    tests are run with multiple LDM threads.
    
    If the BACKUP is later made full multithreaded, the changes to mt.cpp in
    this patch should be reverted.

[33mcommit 5fa4cb4c41ee4ca95c4b7fc00fcb969ab8822022[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Tue Jun 26 11:00:13 2012 +0200

    Patch updated according to suggestions from Magnus Blåudd and Fraqzer Clement.
    
    This is a fix for bug#11748194 "TRANSACTION OBJECT CREATED AND UNRELEASED BY
    EXTRA CALL TO NEXTRESULT()".
    
    This bug had the following effect: If an API application tried calling
    NdbScanOperation::nextResult() once more when the previous call had retuned
    end-of-file (return code 1), the api would leak a transaction object.
    
    This commit fixes that problem. Ndb::closeTransaction() will now put the
    extra transaction object associated with the scan back in the idle list
    for the right TC node.
    
    Also, calling NdbScanOperation::nextResult() once too much will now set
    a proper error code (4210), instead of the undefined code -1.
    
    Finally, this commit adds a [1;31mregression[m test that will trigger the bug if the
    bugfix is not applied.

[33mcommit 0ddda2319c25a4cb4b9fb0302ed0fa4a73838970[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Jun 21 12:31:29 2012 +0200

    This is a fix for bug#14208924 . It is based on an initial patch written by
    Frazer Clement.
    
    If TC decides to abort a transaction in the 'prepared' state, it may leak
    ApiConnectRecord objects that was allocated via Dbtc::seizeApiConnectCopy().
    These are normally released by Dbtc::sendApiCommit() followed by
    Dbtc::releaseApiConCopy(). If the transaction aborts, these methods are not
    called.
    
    This patch fixes the above issue by checking if the ApiConnectRecord object
    has an apiCopyRecord when the transaction aborts. If so, the copy object is put
    back in the free list.
    
    The patch also adds a [1;31mregression[m test case (using an error insert).

[33mcommit c48dedf4f951d91cb70a5f7c64c06dd6d74b196c[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Wed Mar 14 20:21:12 2012 -0700

    Revsion 3634 (Nov 7, 2011; 7.2.2) introduced a major performance [1;31mregression[m
    in memcached.  The symptom of this was a decline in throughput as memslap
    connections increase from 4 to 6.  The cause seems to be a change in which
    SimpleRead PK reads are executed NoCommit rather than Commit; this patch
    fixes it.

[33mcommit 4e811a397e1a1dddaede0cb593792a1a0f9ffd6f[m
Author: Jonas Oreland <jonas.oreland@oracle.com>
Date:   Wed Jan 25 15:29:38 2012 +0100

    ndb - fix [1;31mregression[m introduced in fix for bug-13602508

[33mcommit 8d09602c123caa09a4e127fa5c3bbddbf5521170[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Oct 20 14:52:58 2011 +0200

    Added [1;31mregression[m test cases for the commit below:
    ---------------------------------------------------
    
    This commit concerns SPJ (i.e. pushed queries).
    
    The commit fixes two errors that may happend when there is a pruned child
    scan operation:
    1. The api did not set SIP_PRUNE_PARAMS as it should.
    2. There was an error in Dbspj::parseScanIndex().  When it called
    Dbspj::expand() to process prune keys, it used to start reading
    query parameters from after whatever parseDA() had consumed. Now it starts
    from the beginning of the query parameters (for that operation). In other
    words, the parameter values are only sent once, and these values are used for
    building both the scan bounds and the prune key.

[33mcommit 6762100fb52a3ff1abd3f97ff0f43451faebd8ff[m
Author: Jan Wedvik <jan.wedvik@oracle.com>
Date:   Thu Sep 29 15:11:52 2011 +0200

    Adding [1;31mregression[m test for the following commit:
    
    "4562 Jan Wedvik 2011-09-29
    This patch will make the SPJ block fetch all rows for some non-root index scans
    in one batch rather than two.
    This will happen if the first batch reads from a subset of the fragments and
    receive few rows. The SPJ block will then
    try to read from the remaining fragments before finishing the batch.
    This is especially useful when doing bushy scans. If there are more branches in
    the bushy scan, then these will have to be
    repeated for each batch of the current scan."
For keyword speed:
[33mcommit ef258e0670343d19cc0650f398880450dfe1013a[m
Author: Mikael Ronström <mikael.ronstrom@oracle.com>
Date:   Fri Jun 19 17:26:24 2015 +0200

    BUG#20904721: Part 9: Implementing the adaptive LCP [1;31mspeed[m using bounded delay concepts and A-level signals

[33mcommit e3c524a886dcb867cc7b8b6872993b07e92c8825[m
Author: Mikael Ronström <mikael.ronstrom@oracle.com>
Date:   Mon May 18 11:43:47 2015 +0200

    BUG#20904721: WL#8525: Part 3, use prefetch to [1;31mspeed[m up scan processing for LCP scans and also other full table scans, such as node recovery scans and user level full table scans

[33mcommit 39a34107820565aff4d317c73bf390098ba930b8[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Thu Nov 20 14:02:58 2014 +0100

    WL#7509: Tweaked the adaptive LCP [1;31mspeed[m parameters to be a bit slower in changing

[33mcommit 75d7f28c07edc7e18b2f190771c42bcd47775296[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Nov 14 23:47:32 2014 +0100

    BUG#19795108: Fix of node restart status for adapting [1;31mspeed[m of LCP disk write [1;31mspeed[m

[33mcommit a8891c7c7814909c0b5466466a2124176e271c05[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Tue Oct 14 16:21:56 2014 +0200

    BUG#19795072: Fix problems related to ndbinfo tables for disk write [1;31mspeed[m

[33mcommit 236220987e1cbbdb33ce44638ec210bfb2e5b84d[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Thu Sep 4 15:59:59 2014 +0200

    WL#7509: Introduced more configurability of disk write [1;31mspeed[ms and added a number of new ndbinfo tables to track this new more adaptive behaviour

[33mcommit 87c69291df1d422f9041645d9d27d2bccf6ff8a2[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jul 1 22:39:42 2014 +0300

    WL#7777 Integrate PFS memory instrumentation with InnoDB
    
    Move the PFS keys/names that are used in auto tracing (when the caller
    does not provide a key, then we automatically pick one based on the file
    name of the caller) into a separate array to ease management and to
    [1;31mspeed[m up search (std::map is used instead of a linear array).

[33mcommit 430abf9c7731a26b6ae2ce9c5fb0b5db94c654b8[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Fri Mar 14 23:25:20 2014 +0100

    WL#7509: Increase parallelism at same disk checkpoint [1;31mspeed[m to fully utilize all LDM threads in parallel when running LCPs

[33mcommit 0b56f8cb6084e443421488902bc57b102683cd5b[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Tue Mar 4 12:12:46 2014 +0100

    WL#7504: Multi-thread touch of memory for memory allocation to [1;31mspeed[m up restarts

[33mcommit 4667319a774a652de6241e8df8e45b4a73a47970[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Feb 21 20:18:19 2014 +0200

    Convert the debug-only "[1;31mspeed[mo" code into a C++ class and rename it to
    "chrono".

[33mcommit 678ea5c5c362ebed2a07ca60eb19be715a239edd[m
Author: Mikael Ronstrom <mikael.ronstrom@oracle.com>
Date:   Tue Feb 18 17:26:14 2014 +0100

    WL#7532: Scan optimisations, asynch signals -> synch signals, use of local variables to assist compiler, improved checksum algorithm, prefetch of fixed part of rows, many new extensive comments, execute up to 3-4 rows before sending asycnh signal in LQH scanning, removal of lots of dead code, minor jam improvements, turned bit variables into 32-bit variables for higher [1;31mspeed[m

[33mcommit 071fc37e92dc2557754fa57a93b606b5f9dafb2c[m
Merge: 337305ae9a9 1cf157b4122
Author: Anirudh Mangipudi <anirudh.mangipudi@oracle.com>
Date:   Thu Feb 6 11:26:04 2014 +0530

    Bug#14211271 ISSUES WITH SSL ON DEBIAN WHEEZY I386 AND KFREEBSD-I386
    
    Problem:
    It was reported that on Debian and KFreeBSD platforms, i386 architecture
    machines certain SSL tests are failing. main.ssl_connect  rpl.rpl_heartbeat_ssl
    rpl.rpl_ssl1 rpl.rpl_ssl main.ssl_cipher, main.func_encrypt were the tests that
     were reportedly failing (crashing). The reason for the crashes are said to be
    due to the assembly code of yaSSL.
    
    Solution:
    There was initially a workaround suggested i.e., to enable
    -DTAOCRYPT_DISABLE_X86ASM flag which would prevent the crash, but at an expense
    of 4X reduction of [1;31mspeed[m. Since this was unacceptable, the fix was the
    functions using assembly, now input variables from the function call using
    extended inline assembly on GCC instead of relying on direct assembly code.

[33mcommit 21bae833da77fe56f8972db01bba1e8726a268df[m
Author: Shubhangi Garg <shubhangi.garg@oracle.com>
Date:   Mon Jan 27 14:04:18 2014 +0530

    ISSUE
    =====
    The previous implementation for fetching the type_code for an event was:
    get_type_code() was a pure virtual function in the class Log_event, which was
    implemented by all the event subclasses, and returned the enum value,
    Log_event_type.
    To fetch the type_code, a call to the method was made using a Log_event pointer.
    
    The changed implementation  removes the method from from Log_event and its
    subclasses, and fetching of event type code is done using 'type_code',
    a member variable of class Log_event_header.
    
    RATIONALE
    =========
    - While decoding/encoding an event, the type_code  of an event is stored
      as a part of the event header, and not the event body.
      An event is represented as:
       class Binary_log_event
       {
       public:
          ~Binary_log_event()= 0;
       private:
          Log_event_header m_header;
          Log_event_footer m_footer;
       }
      Since the event type is always available in the type_code field of the class
      Log_event_header, it is not required to have a separate virtual function in
      the event superclass, and the implementation in the subclasses.
      Type code is taken as a parameter in the constructor of the event, and is set
      while creating the Log_event_header object, m_header.
    
    - Also, replacing a virtual method with a member function improves [1;31mspeed[m.
    
    CHANGES
    =======
    @libbinlogapi/include/binlog_event.h:
       - Added a parameter of type enum Log_event_type to the constructor of
          class Log_event_header, which sets the type_code.
       - Added a parameter of type enum Log_event_type to the constructor of
          class Log_event_header, which defaults to ENUM_END_EVENT.
    
       - Modified the constructor for Log_event_header to take and binlog_version
         as parameters instead of a description_event, for only the binlog_version
          is required
       - Modified the constructors of event subclasses to pass the type code to
         Binary_log_event in the constructor.
       - Modified the constructors of events in which is a superclass to some other
         event ( for ex: Binary_log_event <- Start_event_v3 <- FDE), to take type code
         as a parameter.
         This allows subclasses to pass the event type code to the super classes,
         which can call Binary_log_event constructors to set the type code.
         For example. Start_event_v3 takes an argument of Log-event_type in
        the constructor (which defaults to START_EVENT_V3). When called by FDE
        constructors, it passes FORMAT_DESCRIPTION_EVENT as the type_code.
    
    
    @sql/log_event.h
       - Removed the virtual method get_type_code() from Log_event and other
         subclasses.
       - Replaced calls to get_type_code() by  using the variable
         common_header->type_code
    
    @sql/log_event.cc
       - Replaced calls to get_type_code() by using the variable
         common_header->type_code
       - Set the type_code in the method Create_file_log_event::write_base,
         where we the event pretends to be a load event.
    
    @all other files:
       - Replaced calls to get_type_code() by using the variable
         common_header->type_code

[33mcommit 54dfa8c0a1547dcfd634f9e2867eb7c24786dd1d[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Mon Jan 6 09:53:38 2014 +0100

    Fix for Bug#17857442: MULTITHREADED JOB SCHEDULER SHOULD HANDLE LEAPS IN CLOCK TIME
    
    We have seen customer bug reports, like bug 17475425, where it
    looks like delayed signal processing from the 'time_queue' comes
    completely out of control: There seems to be two scenarios:
    
     - Processing of delayes signals stops completely for a long periode,
       or forever. This will cause issues like the disk write [1;31mspeed[m throttling
       to completely halt any disk activity (Local checkpoints stop)
    
     - We see a burst of signals which should have been delayed relatively
       to each other, executed withing the same clock second.
    
    Inspecting the mt-scheduler code, we find that this may happen if the
    NdbTick clock moves forward or backwards in large leaps. When the time
    moved backwards, it will stop the time_queue handling until the
    same amount of clock time has passed again. A forward leap will force
    the scheduler to simulate that the same amount of time passing by expiring
    all time_queue event in the elapsed periode.
    
    Such timer leaps might happen if the platform does not support monotonic
    timers, or we didn't implement the usage of these on the specific platform.
    Windows used to be such a platform prior to fixing bug 17647637. However,
    there are still such platforms, OSX seems to be one of these, and possible
    some Linux variants. Furthermore there seems to be several OS/HW/VM bugs
    related to monotonic timers not always being monotonic after all. And
    there might be time leaps due to CPU starvation such that we are stuck
    for a long time - So this has to be expected and handled somehow.
    
    Looking at the single threaded scheduler we find that it actually handle
    such leaps - see ThreadConfig::scanTimeQueue(). This logic doesn't seem
    to have been ported to the mt-version !! The single threaded handling
    is to accept a backtick as the new current time. A forward leap of more
    than 1500ms will be consumed by resetting the current time to 'now-1000'ms
    and continue from there.
    
    This fix introduce the same handling in the MT-scheduler.
    
    Furthermore, we also found that the mt-scheduler allowed itself to
    yield CPU, or wait for receiving signals, even if it has possibly
    expired event to handle from the time_queue.
    (see: 'lagging_timers')
    
    We are not sure about whether this has any ill effects - However,
    it doesn't seem like a good idea to let possible expired signals
    linger any longer than necessary - also fixed.
    
    Also removed a ndbout_c() which was a leftover from previous debugging.

[33mcommit 07bb83ae15047c0c9868c44e4ce6e05390a96506[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Fri Oct 18 09:24:05 2013 +0200

    Bug#17497869: NEW DEPRECATION WARNINGS INTRODUCE UNWANTED
                  PERFORMANCE DEGRADATION SIDE-EFFECT
    
    This patch changes the implementation of deprecation warnings
    to avoid using current_thd. This will slightly [1;31mspeed[mup
    generation of such warnings.
    
    But note that the generation of deprecation warnings is
    not free (due to e.g. memory allocation and localization).
    So deprecation a function will make it slower.

[33mcommit 48132cbedb4d7557056faa7d7540e2d457fe9723[m
Author: magnus.blaudd@oracle.com <>
Date:   Thu Apr 11 15:21:03 2013 +0200

    ndb_backup_rate
    
     - [1;31mspeed[m up the inserts by caching 8192 rows in a temporary table, thus avoiding
      scaning t1 for each insert.
    - correct second error message to say "Failed waiting for redo log having space"

[33mcommit 727d9ec99fad05fd16cdc3dfe4caa93855d49da0[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Mar 12 19:36:24 2013 -0700

    Refactor NdbOperation.prepare() for [1;31mspeed[m
For keyword delayedInnoDBflush:
For keyword flush:
[33mcommit af0acedd885eb7103e319f79d25fda7386ef1506[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri Sep 11 12:29:37 2015 +0300

    WL#8845 Implement an InnoDB redo log format version identifier
    
    InnoDB has several times changed its redo log format by introducing
    new redo log record types. Format changes would lead to misleading
    reports of redo log corruption when processing individual redo log
    records.
    
    In the redo log header (start of the ib_logfile0 file), we will introduce
    a format version identifier and textual representation of the software
    version that created the redo log files.
    
    Furthermore, we change the checksum of redo log checkpoint pages, so that
    older versions of MySQL will refuse to start up on redo log files that
    were created with a MySQL server that includes this fix.
    
    We will also remove a number of unused fields from the redo log
    header and checkpoint pages (pages 0, 1, and 3).
    
    Some tests will be expanded, because with this fix,
    the server must refuse to start up with older redo log files,
    unless they are dirty.
    
    We will also replace the configuration parameter
    innodb_log_checksum_algorithm with the Boolean parameter
    innodb_log_checksums.
    We make CRC-32C the only checksum on the InnoDB redo log pages when
    innodb_log_checksums=ON (the default). Checksums on the header page
    and the checkpoint pages are never disabled.
    
    innodb_log_checksums_func_update(), innodb_log_checksums_update():
    Update triggers for the new global Boolean variable innodb_log_checksums.
    
    innodb_log_checksum_func_update(), innodb_log_checksum_algorithm_update():
    Removed along with the global parameter innodb_log_checksum_algorithm.
    
    Removed definitions:
    LOG_MAX_N_GROUPS
    log_group_read_checkpoint_info()
    log_checkpoint_get_nth_group_info()
    log_checkpoint_set_nth_group_info()
    log_block_calc_checksum_innodb()
    log_block_calc_checksum_crc32_legacy_big_endian()
    recv_check_cp_is_consistent()
    log_block_checksum_weak_validation()
    log_block_checksum_what_matches()
    log_block_checksum_fail_fatal()
    log_block_checksum_is_ok_or_old_format()
    LOG_CHECKPOINT_OFFSET_LOW32
    LOG_CHECKPOINT_ARCHIVED_LSN
    LOG_CHECKPOINT_GROUP_ARRAY
    LOG_CHECKPOINT_ARCHIVED_FILE_NO
    LOG_CHECKPOINT_ARCHIVED_OFFSET
    LOG_CHECKPOINT_ARRAY_END
    LOG_CHECKPOINT_CHECKSUM_1
    LOG_CHECKPOINT_CHECKSUM_2
    LOG_CHECKPOINT_FSP_FREE_LIMIT
    LOG_CHECKPOINT_FSP_MAGIC_N
    LOG_CHECKPOINT_FSP_MAGIC_N_VAL
    LOG_CHECKPOINT_OFFSET_HIGH32
    LOG_CHECKPOINT_SIZE
    LOG_GROUP_ID
    LOG_FILE_START_LSN
    LOG_FILE_NO
    LOG_FILE_WAS_CREATED_BY_HOT_BACKUP
    LOG_FILE_ARCH_COMPLETED
    LOG_FILE_END_LSN
    
    Changed definitions:
    LOG_CHECKPOINT_LOG_BUF_SIZE
    
    Added definitions:
    innodb_log_checksums: The current value of the SET GLOBAL variable.
    LOG_CHECKPOINT_OFFSET
    LOG_HEADER_FORMAT (repurposing LOG_GROUP_ID which was always 0)
    LOG_HEADER_PAD1 (unused 4 bytes, zero-initialized)
    LOG_HEADER_START_LSN
    LOG_HEADER_CREATOR (renamed from LOG_FILE_WAS_CREATED_BY_HOT_BACKUP)
    LOG_HEADER_CREATOR_END
    LOG_HEADER_CREATOR_CURRENT
    LOG_HEADER_FORMAT_CURRENT
    log_group_t::format
    
    log_block_calc_checksum_format_0(): Renamed from
    log_block_calc_checksum_innodb(). This is only used when upgrading the
    redo log from non-tagged format.
    
    recv_find_max_checkpoint_0(): New function, used when upgrading the
    redo log from non-tagged format.
    
    recv_log_format_0_recover(): New function, used when upgrading the
    redo log from non-tagged format. Checks if the redo log is clean.
    
    log_group_header_read(): Replaces log_group_read_checkpoint_info().
    Also used for reading the log header page (page 0).
    
    recv_check_log_header_checksum(): Replaces recv_check_cp_is_consistent().
    Also used for checking the log header page (page 0).
    
    log_block_checksum_is_ok(): Checks a log block checksum. It must either
    be CRC-32C, or we must have innodb_log_checksums=OFF.
    
    Changed functions:
    
    log_group_file_header_[1;31mflush[m(): Always zero-initialize the buffer
    and initialize all LOG_HEADER_ fields.
    
    log_group_checkpoint(): Zero-initialize the checkpoint buffer,
    and write the checkpoint in the new format, with CRC-32C checksum.
    
    recv_find_max_checkpoint(): Support both the old log format
    (if the old-format redo log is logically empty) and the new format.
    
    recv_scan_log_recs(): Clean up the logic a bit. Display a message
    when encountering (and terminating log parsing due to) invalid
    log blocks. For now, keep the existing behaviour and do not display
    a message about log block header mismatch.
    
    recv_recovery_from_checkpoint_start(): Replace the whole "ibbackup" label.
    Check if a log upgrade or a normal recovery is needed.
    
    srv_prepare_to_delete_redo_log_files(): Display a message about
    upgrading the redo log.
    
    Other changes to startup:
    
    If we are going to upgrade the redo log, we must avoid writing any
    new redo log records before we have replaced the redo log.
    
    dict_check_sys_tablespaces(), dict_check_sys_tables(): Avoid
    updating SYS_DATAFILES if we are going to upgrade the redo log.
    
    dict_create_or_check_sys_virtual(): Do not modify anything if we are
    running in --innodb-read-only or --innodb-force-recovery=6 mode.
    
    RB: 10096
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Annamalai Gurusami <annamalai.gurusami@oracle.com>

[33mcommit c8b6a6b4e0574910822945be1b041407a822f2c1[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Sep 9 11:55:01 2015 +0300

    Bug#21796691 INNODB REDO LOG DOES NOT INDICATE WHEN REDO LOGGING IS SKIPPED
    
    As part of WL#7277 Bulk Load for Create Index,
    InnoDB disabled redo logging of index page writes during DDL operations.
    
    InnoDB crash recovery is not affected by this, because there will be
    extra page [1;31mflush[mes before the DDL operation is committed.
    
    But, external hot backup tools will have no way of detecting if the
    redo logging was disabled, and they can thus create a corrupted backup
    without noticing it, if there was concurrent DDL.
    
    After successfully completing the extra [1;31mflush[m and before re-enabling
    redo logging, we will issue a new redo log record
    MLOG_INDEX_LOAD(index->space,index->page,index->id).
    
    When backup tools see this record in the redo log, they can either
    retry copying the tablespace (if it is small) or abort the backup,
    asking the user to avoid concurrent DDL.
    
    recv_parse_or_apply_log_rec_body(): Parse and ignore the MLOG_INDEX_LOAD
    record.
    
    row_merge_write_redo(): New function, to write the MLOG_INDEX_LOAD
    record after the non-logged changes have been [1;31mflush[med.
    
    RB: 10168
    Reviewed-by: Shaohua Wang <shaohua.wang@oracle.com>
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>

[33mcommit e5ae7aa96eca9f9f2f6f76dda226bec60b1a18f6[m
Author: Libing Song <libing.song@oracle.com>
Date:   Mon Jul 20 12:23:42 2015 +0800

    BUG#21045848 XA+MEMORY TABLE: POST SERVER RESTART 'XA COMMIT'
                 IS OVERRIDDEN BY 'DELETE' CMD
    
    ANALYSIS
    ========
    Memory table's data will be lost after server restarts. To keep the data
    consistency between master and slave, it just binlogs
    'DELETE FROM `db`.`table_name`' when the memory table is locked first time.
    So DELETE statement could be binlogged in many statements.
    
    Because it was put into binlog statement cache of current session,
    so it could cause below troubles:
    - COM_FIELD_LIST
      it doesn't [1;31mflush[m binlog cache to binlog file. So the DELETEs were not
      binlogged in COM_FIELD_LIST, it was binlogged with next statement together.
    
    - CREATE TABLE ... LIKE memory_table
    - CREATE TABLE ... SELECT memory_table
      They were binlogged like:
      GTID_log_event | Anonymous_gtid_log_event
      DELETE FROM memory_table
      CREATE TABLE ... LIKE memory_table | CREATE TABLE ... SELECT memory_table
    
      Both statements shared the same gtid event. That was not correct.
    
    - DELETE was binlogged without BEGIN and COMMIT
      It caused some DML binlogged without BEGIN and COMMIT
      e.g.
      INSERT INTO myisam_t1 SELECT * FROM memory_table
      it was binlogged as:
      GTID_log_event | Anonymous_gtid_log_event
      DELETE FROM memory_table
      INSERT INTO myisam_t1 SELECT * FROM memory_table
    
    FIX
    ===
    To fix it, each DELETE will be binlogged separately and wrapped by
    BEGIN/COMMIT pair. It is [1;31mflush[m to binlog immediately after it is
    written into binlog statement cache. E.g.
    
    INSERT INTO myisam_t1 SELECT * FROM memory_table will be binlogged as:
    
    GTID_log_event | Anonymous_gtid_log_event
    Query_log_event BEGIN
    Query_log_event DELETE FROM memory_table
    Query_log_event COMMIT
    GTID_log_event | Anonymous_gtid_log_event
    Query_log_event BEGIN
    Query_log_event INSERT INTO myisam_t1 SELECT * FROM memory_table
    Xid_log_event
    
    It also added a debug option build_completion_hash to mysql for test purpose.

[33mcommit 69d4e72cb397d3e27a76458e989f440fba049f4f[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Fri Aug 28 12:05:29 2015 +0530

    Bug#20901025: SLAVE ASSERTION IN UNPACK_ROW WITH ROLLBACK TO
    SAVEPOINT IN ERROR HANDLER
    
    Problem:
    =======
    
    When issuing a ROLLBACK TO SAVEPOINT within an error
    handler, the slave asserts as follows:
    
    mysqlcom-pro-5.6.24/sql/rpl_record.cc:246:
    int unpack_row(const Relay_log_info, TABLE, uint,
    const uchar, const MY_BITMAP, const uchar, ulong, const
    uchar): Assertion `table_found' failed.
    
    Analysis:
    ========
    
    The above assert is caused in two cases.
    
    CASE 1:
    SAVEPOINT and ROLLBACK TO SAVEPOINT are inside a trigger and
    followed by other DML statements.
    
    SAVEPOINT, ROLLBACK TO SAVEPOINT statements are logged into
    binary log as Query_log_events.
    Query_log_event::do_apply_event() wipes out table map.
    Events that follow the above Query_log_event will not find
    table map event and it results in the above assertion.
    
    CASE 2:
    
    SAVEPOINT and ROLLBACK TO SAVEPOINT are inside a trigger and
    and they does not follow with any DML statement.
    
    In the above scenario On master during execution of
    savepoint inside the trigger
    "mysql_bin_log.write_event(&qinfo)" is called, which intern
    invokes thd->binlog_[1;31mflush[m_pending_rows_event() call with
    "end_stmt" = false. This causes the peding event to be [1;31mflush[med
    to the IO_CACHE without a STMT_END_F flag. Since it does't
    follow with any DML stements no proper clean up is done. i.e
    table maps are not cleared this causes the next query that
    follows to be logged as part of query1 without its own table
    map event resulting in same assert as mentioned in the bug.
    
    Fix:
    ====
    
    When a SAVEPOINT is executed inside a stored
    function/trigger we force the pending event to be [1;31mflush[med
    with a STMT_END_F flag and clear the table maps as well to
    ensure that following DMLs will have a clean state to start
    with. In the same way when we are rolling back to a
    SAVEPOINT we should not only set the position back to
    SAVEPOINT but also we must ensure that the same clean state
    is preserved. In order to do that we clean the table maps
    during rolling back to savepoint.

[33mcommit d84945f82aad0619c11f85910f640cc81f5260d0[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Wed Aug 26 10:46:16 2015 +0800

    WL#8756  Deprecate binlog_max_[1;31mflush[m_queue_time in 5.7  - post fix
    
    On some platforms, the assert_grep.inc in sys_vars/t/binlog_max_[1;31mflush[m_queue_time_basic.test
    is failing, because it requires enabled binlog and an error log named mysqld.1.err exactly.
    
    To fix the problem, move the test case with assert_grep.inc into replication binlog suite.

[33mcommit 9b0e016889c73b8d9db1837f97e03f8ee20fbf19[m
Author: Venkatesh Duggirala <venkatesh.duggirala@oracle.com>
Date:   Tue Aug 25 09:12:33 2015 +0530

    BUG#16418100 ERROR "WHEN GTID_NEXT IS SET TO A GTID" ROW BASED REPLICATION
    
    Problem & Analysis:
    ===================
    When relay log info repository is configured to be persisted in a
    table, it is updated when transaction commits or explicit [1;31mflush[m is called,
    like on relay log rotate. If a transaction that uses non-transactional
    engine is split across multiple relay logs, on relay log [1;31mflush[m
    it will be partially committed. If GTID_MODE=ON and when partial transaction
    in first relay log is committed, it uses the current GTID value and sets
    GTID value to UNDEFINED GROUP. Hence it causes 'ER_GTID_NEXT_TYPE_UNDEFINED_GROUP'
    error when it is trying to commit the rest of the transaction.
    It also causes many other problems if the repository is updated in between
    the transaction and it should be avoided.
    
    Fix: relay log info repository should be updated on relay log
    rotate. But when the transaction is split across two relay logs,
    update the repository will cause unexpected results and should
    be postponed till the 'commit' of the transaction is executed.
    
    Introduced a flag that set to 'true' when this type of 'forced [1;31mflush[m'(at the
    time of rotate relay log) is postponed due to transaction split
    across the relay logs. And the flag will be checked in [1;31mflush[m_info function
    to see if there is a [1;31mflush[m that got postponed earlier. If so, it will
    force the [1;31mflush[m now irrespective of sync_relay_log_info value.

[33mcommit c7991b854331c90db641f51c60eb2ae67ef27efe[m
Author: Daogang Qu <bill.qu@oracle.com>
Date:   Thu Aug 20 10:54:58 2015 +0800

    WL#8756  Deprecate binlog_max_[1;31mflush[m_queue_time in 5.7
    
    The variable binlog_max_[1;31mflush[m_queue_time was introduced in 5.6
    in order to fine-tune the Binlog Group Commit. Due to changes
    in 5.7, the option has no effect any more. Therefore we
    deprecate the variable in 5.7 and plan to remove it in 5.8.
    
    This worklog will generate a deprecation warning in 5.7 and 5.8.
    It will not remove the variable in 5.8.

[33mcommit 6e94cc9c6d31ea74ffe05ac45f0e0777fd82db25[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Tue Aug 18 15:18:15 2015 +0530

    Bug#21451922 - FOUND A MISMATCH PAGE, EXPECT PAGE - ASSERTION
    
    Problem:
    -------
    During truncate, we intentionally crash (DBUG_SUICIDE()) after updating
    SYS_INDEXES. In recovery, we skip all redo on the tablespace that was
    truncated. All the LSNs > MLOG_TRUNCATE LSN are also skipped if the
    tablespace is marked for truncate. Because of this, tablespace pages
    are inconsistent state (depends on [1;31mflush[m order).
    
    Since we crashed when inserting into SYS_INDEXES, this transaction has to be
    rolledback. After finishing recovery, we try to rollback the DD
    transaction. When undoing the changes to SYS_INDEXES, we drop the index
    of a tablespace first and undo the modification on SYS_INDEXES table.
    
    The drop of index on the truncated tablespace causes assert because the
    tablespace is in inconsistent state (the truncate fixup is yet to be done).
    
    Fix:
    ----
    During rollback, do not try to drop the index of tablespace if it is
    marked for truncate. The tablespace will be fixed by the truncate fixup
    action.
    
    Also fixed the wl#6501 testcases to use .inc files
    
    Reviewed-by: Krunal Bauskar <krunal.bauskar@oracle.com>
    RB: 9961

[33mcommit 44315b92aa92eef967eeb21bd7b90fcd7ce1791f[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Tue Aug 18 09:56:38 2015 +0200

    Bug#21296553: WARNINGS SENT TO STDERR/OUT EVEN WHEN LOGGING SET TO SYSLOG
    
    Follow-up patch: Add [1;31mflush[ming for error log in a few cases
    where we don't call unireg_abort(). Remove key_memory_buffered_logs
    since it is now unused.

[33mcommit 2af84d97175932f27a3cffb279a5ec732de46c49[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Sun Jul 19 21:06:26 2015 +0800

    BUG#20582189 - INNODB.DOUBLEWRITE FAILS DUE TO DATABASE CORRUPTION
    
    Follow-up patch for this bug.
    
    After previous fixes, current failure is due to the design of the test case.
    This test case could not make sure that the page it modifes when the server
    is down has a correct copy in doublewrite buffer. The test case always [1;31mflush[m
    the the to be modified page first, and then start a transaction to update
    some other pages. It's possible that pages changed by the transaction would
    occupy the same doublewrite buffer page with the one used by the to be
    modifed page, so server may reports the corrupted page. We can simply always
    [1;31mflush[m the to be modified page at last to fix this issue.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    rb: 9270

[33mcommit 1f4f1353f5adc09d45f35b169bcd134a47dc25fb[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Wed Jul 15 10:32:25 2015 +0800

    Follow-up patch for Bug#20582189 INNODB.DOUBLEWRITE FAILS DUE TO DATABASE
    CORRUPTION
    
    This patch should make sure that when setting innodb_buf_[1;31mflush[m_list_now = 1,
    server will return until dirty pages are [1;31mflush[med out.
    
    Approved by Jimmy over IM.

[33mcommit 07cad65513349795322da022b4f6ab17c2c71e70[m
Author: Olav Sandstaa <olav.sandstaa@oracle.com>
Date:   Sun Jun 28 11:38:26 2015 +0200

    WL#7340 IO aware cost estimate function for data access
    
    This worklog extends the optimizer cost model to take into account
    whether data and indexes must be read from disk or are likely already
    present in a memory buffer.
    
    -The cost model will use estimates from the handler/storage engines
     about how much of tables and indexes are present in memory (see
     WL#7168) in its cost calculations.
    
    -A new cost constant named "memory_block_read_cost" representing the
     cost of accessing data that is in a memory buffer inside the storage
     engine. This constant corresponds to the existing cost constant
     "io_block_read_cost" that is used when a block needs to be read from
     disk.
    
    In this initial version the default value for memory_block_read_cost
    is the same as the default value for io_block_read_cost.
    
    Per-file comments for some of the test changes:
    
    main.index_merge_delete:
      One query now merges three indexes instead of two due to changes in the
      model for disk sweeps used by index merge.
    main.index_merge_myisam:
      Minor change in cost estimates in JSON output due to changes to cost
      model for disk sweeps used by index merge.
    main.innodb_explain_json_non_select_all
    main.innodb_explain_json_non_select_none
    main.innodb_explain_non_select_all
    main.innodb_explain_non_select_none main.myisam_explain_json_non_select_all:
    main.myisam_explain_json_non_select_none:
    main.myisam_explain_non_select_all.result
    myisam_explain_non_select_none.result
      Minor changes in cost estimates in JSON output due to changes to cost
      model for disk sweeps used by index merge and DS-MRR.
      Added more data to two queries to preserve original query plan as index
      merge.
    main.partition_index_innodb:
      Increase number of records in table in order to preserve original query
      plan. This was needed due to changes in cost model for disk sweeps used
      by index merge.
    main.subquery_sj_all:
    main.subquery_sj_all_bka:
    main.subquery_sj_all_bka_nixbnl:
    main.subquery_sj_all_bkaunique:
      Two queries changes from using FirstMatch to use Materialization due
      to a small change in the cost estimate for DS-MRR.
    opt_costmodel.result:
      Re-recorded to reflect that the engine_cost table now has a new entry
      for memory_block_read_cost.
    opt_costmodel_[1;31mflush[m.test:
    opt_costmodel_[1;31mflush[m.result:
      Replaced use of io_block_read_cost with memory_block_read_cost since
      the test tables are so small that they are in the InnoDB buffer.
      Changing the value for io_block_read_cost do not longer have any
      impact on the test queries.
    opt_costmodel_restart.test:
    opt_costmodel_restart.result:
      Replaced use of io_block_read_cost with memory_block_read_cost since
      the test tables are so small that they are in the InnoDB buffer.
      Changing the value for io_block_read_cost do not longer have any
      impact on the test queries.
    opt_costmodel_warnings.result:
      Re-recorded to reflect that the engine_cost table now has a new entry
      for memory_block_read_cost.
    suite/opt_trace/r/bugs_no_prot_all.result:
    suite/opt_trace/r/bugs_ps_prot_all.result:
    suite/opt_trace/r/range_no_prot.result:
    suite/opt_trace/r/range_ps_prot.result:
      Minor changes in cost estimates optimizer trace due to changes to cost
      model for disk sweeps used by index merge and DS-MRR.
    unittest/gunit/faketable.h:
      Initialize the handler object's table pointer to point to the table.
    unittest/gunit/opt_costconstants-t.cc:
      Added unit tests for the new SE_cost_constants::memory_block_read_cost()
      function.
    unittest/gunit/opt_costmodel-t.cc:
      Added unit tests for the new functions added to the Cost_model_table class.

[33mcommit 9b5f1b810715cf64448b87f4316006da380931b0[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Wed Apr 22 20:03:55 2015 +0300

    Bug #20920851   ASSERTION `IS_STARTED()' IN HA_TRX_INFO::NEXT() AT TRANSACTION_INFO.H:138
    
    The assert is hit when XA transaction updated only a non-transactional table and
    went to prepare stage.
    At that time MYSQL_BIN_LOG::write_binlog_and_commit_engine() is invoked where
    the trx should not attempt any committing. But that's happened.
    The binlog "engine" managed to commit_low() in conditions of the reported case
    which led to the assert few instructions later
    
    #7  0x00007fad151dbe42 in __GI___assert_fail (assertion=0x1e37d16 "is_started()", file=0x1e37c68
    #8  0x0000000000fafcf7 in Ha_trx_info::next (this=0x7fac8c002028) at
    #9  0x0000000000f9dfa9 in ha_prepare (thd=0x7fac8c000bb0)
    
    More analysis proved there's an "inverted" related issue in the rollback branch.
    When the pure non-transactional engine xa transaction is rolled back, this time, it
    misses to execute rollback_low() method to leave a screwed state which
    the following query discovers hiting against an assert.
    
       bool trans_commit_stmt(THD*): Assertion
       `thd->in_active_multi_stmt_transaction() || thd->m_transaction_psi
       == __null' failed.
    
    And finally, testing revealed a case of no test coverage so far in combination
    of XA ROLLBACK, no transactional tables involved and no XA PREPARE.
    In such case logging was just incorrect mixing XA START and ROLLBACK.
    
    The original issue is fixed with making
    MYSQL_BIN_LOG::write_binlog_and_commit_engine() to compute a local boolean flag `skip-commit'
    correctly based on the value of the XA state.
    Commit is disallowed when the state is Prepared.
    To satisfy to the ONE-phase XA, the committing XA is also made
    to receive XA_PREPARED status, as intermediate, right after the prepare phase is done.
    in a general commit handler of ha_commit_trans().
    
    The second issue of the rollback part is fixed with relocating an existing explicit
    rollback_low() for xa-rollback to a safer point.
    
    And the final third issue is fixed with augmentment of
    ending_trans()'s the trans_cannot_safely_rollback(thd) branch to
    compose an appropriate Query-log-event.
    Logics of preventing second time do_binlog_xa_commit_rollback()
    invocation that is actual to the "externally" committing XA is
    simplified.
    The former idea was in that the first invocation of
    do_binlog_xa_commit_rollback() in the "external" XA commit branch
    would necessarily turn the cache from empty, asserted, to not empty.
    On the other hand, at running do_binlog_xa_commit_rollback() for the
    local xa the cache must be empty (because it should've been [1;31mflush[med at
    prepare), asserted in the rollback case too.
    Hence the state of the cache checking was correct: (the local xa go
    through, the external xa goes through once which is first time).  Now
    instead of the above deduction the 1st invocation is just gets
    explicitly flagged. And because we would like to preserve signature of
    MYSQL_BIN_LOG class methods the flag is made to pass as a new member
    of `bool binlog_cache_mngr::has_logged_xid'
    
    Mixing transactional and non-transactional tables in
    rpl.rpl_xa_survive_disconnect_mixed_engines reveal one issue in MTS
    grouping. An XA transaction "prepare" group can be closed
    with XA-ROLLBACK query which was previously missed to capture.
    A use case for that is mixed transactional and non-transactional updates.
    It's been corrected now.
    
    As a side effect is_loggable_xa_prepare() had to be refined to satisfy
    @c simulate_commit_failure.

[33mcommit 868d3a1ff096e00d86a501f3b485053ebd5ceef8[m
Author: Nuno Carvalho <nuno.carvalho@oracle.com>
Date:   Wed Jul 1 22:28:35 2015 +0200

    BUG#21348278: ENABLE SLAVE PARALLEL APPLIER RECOVERY GTID BASED
    
    Parallel applier recovery, which happens after the sequence:
    STOP SLAVE, START SLAVE; is based on master log name and position,
    on Group Replication we have several masters what makes impossible
    to recover parallel applier from that information.
    Since we always have GTID_MODE=ON on Group Replication, we can
    ignore the positions completely, seek the current relay log to the
    beginning and start from there. Already applied transactions will be
    skipped due to GTIDs auto skip feature and applier will resume from
    the last applied transaction.
    
    This patch fix the above issue and also introduce some more changes:
     1) Parallel applier recovery based on GTIDs instead of
        file+position.
        This change will only affect Group Replication applier.
        Files changed: rpl_slave.cc
    
     2) Sequential execution of transactions on logical clock parallel
        applier, by setting transaction logical timestamps to (0,0).
        This was already implemented on server, but since it was not a
        valid code path, there was a warning. Now we have a valid code
        path, so warning was removed.
        Files changed: log_event.cc, rpl_mts_submode.cc
    
     3) Adapt UNTIL_SQL_VIEW_ID to parallel applier.
        Parallel applier cannot be stopped on a ongoing transaction, so
        UNTIL_SQL_VIEW_ID condition, the stop condition for Group
        Replication distributed recovery, was adjusted to take that into
        consideration.
        Files changed: rpl_rli.h, rpl_rli.cc, rpl_slave.cc
    
     4) Disallow FLUSH RELAY LOGS FOR CHANNEL "group_replication_applier".
        To avoid that DBA splits transactions among relay logs, the
        command FLUSH RELAY LOGS was disabled for channel
        "group_replication_applier".
        Files changed: rpl_slave.cc, share/errmsg-utf8.txt
    
     5) Extend Trans_context_info structure with parallel applier
        options.
        Group Replication doesn't support database parallel applier, in
        order to prevent its use, it will check if it is in use and
        error out.
        Files changed: replication.h, rpl_group_replication.cc,
                       rpl_handler.cc
    
     6) Add channel_[1;31mflush[m() function to rpl_channel_service_interface
        Provide a API function to [1;31mflush[m relay logs from within a plugin,
        so that START GROUP_REPLICATION can rotate applier relay log.
        Files changed: rpl_channel_service_interface.h,
                       rpl_channel_service_interface.cc

[33mcommit d084f68b918d37f013931f29edc91bcf70a842c7[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Jun 18 07:04:52 2015 -0700

    induce memcached to [1;31mflush[m its log file at end of mtr testing

[33mcommit 3495047c5dcd819f173bc35dcaabb0dd49023a78[m
Author: Bin Su <bin.x.su@oracle.com>
Date:   Wed Jun 17 16:33:31 2015 +0800

    BUG#20582189 - INNODB.DOUBLEWRITE FAILS DUE TO DATABASE CORRUPTION
    
    This bug only happens sporadically on PB2, and mainly in Windows.
    It's hard to reproduce. The test case writes some junk, such as 0,
    to a data page to test if double write buffer works well. It looks like
    the 'corrupted' page may not be [1;31mflush[med to disk immediately but buffered
    in OS cache in Windows and the server would use a DIRECTIO to load page,
    which makes the test case failure. So we just try to [1;31mflush[m the data ASAP
    in the test case, and try to print the page infomation on error.
    
    RB: 9270
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>

[33mcommit a84eb899a88b13303e14d4d5e36bdfda9ec1eda2[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Fri Feb 13 11:56:20 2015 +0000

    BUG#20451386 SQL THREAD CRASH: LOG-SLAVE-UPDATES OFF, RELAY LOG ENDS
                 WITH GTID_LOG_EVENT
    
    A replication slave running with GTIDs, no binary log and relay info
    repository set to TABLE is hitting an assert (assertion "is_started()"
    at Ha_trx_info::is_trx_read_write) when applying a transaction that
    spanned across distinct relay log files.
    
    This is happening because the slave SQL thread is [1;31mflush[ming the relay
    log info to TABLE while in the middle of a transaction (or, better,
    right after the GTID of the transaction) because of the rotation of
    the relay log.
    
    After setting the GTID next to the SQL thread thd, the slave was
    saving the rpl_info on table and calling gtid_state->save that
    "committed" the GTID set (but not used yet).
    
    The fix for this issue skips the gtid_state->save for operations
    that will save the rpl_info on table.

[33mcommit 05d1ad13c3e4f07484a36b9183f9f8b7ac2571bc[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Fri May 8 12:56:48 2015 +0530

     - Bug#21053486: TRUNCATE_RECOVER FAILING IN MYSQL-TRUNK
    
       While recovering undo tablespace from crash during truncate
       of undo-tablespace, undo-tablespace is re-created and pages
       for it are written/[1;31mflush[m to disk immediately.
    
       Flush to disk logic regressed after introducing observer
       in trx object which was not initialized in the path mentioned
       above which caused non-[1;31mflush[ming of undo-tablespace and related
       metadata.
    
       trx object is not needed while force [1;31mflush[ming undo-tablespace
       during recovery so removed requirement for the same and
       now started passing NULL.
    
       Approved by: Sunny Bains (sunny.bains@oracle.com)
       RB: over IM

[33mcommit 058b6e71872c662954684bbd5301f9308f4dcdf6[m
Author: Daogang.qu <bill.qu@oracle.com>
Date:   Wed May 6 16:29:25 2015 +0800

    Bug #20592961  'FLUSH LOGS' POST FAULT INJECTION HITS ASSERT `! IS_SET()' AT SQL_ERROR.CC:381 - post fix
    
    The binlog.binlog_check_[1;31mflush[m_log_assertion.test is failing with
    valgrind and failing with query 'FLUSH LOGS' succeeded - should
    have failed with errno 1598...
    
    Post-fix: Do not run the test with valgrind, since the server may
    intentionally crash in the test. And handle the successful case
    of 'FLUSH LOGS'.

[33mcommit 809bc0e2bfda1033210a7b14265d984a7736ec65[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Apr 28 14:17:05 2015 +0800

    Bug#19865673 DDL LIKE ADD INDEX IS VERY SLOW IN 5.7.5
    
    Analysis:
    We have a checkpoint at the end of bulk load to [1;31mflush[m all dirty pages to disk,
    because we disable redo logging for bulk load. It works well for big table, but
    not for small table in heavy work load.
    
    Solution:
    Only [1;31mflush[m the dirty pages modified by the bulk load. the main idea is to set a
    [1;31mflush[m observer to these dirty pages when committing mtr during a bulk load. When
    a page is submitted to [1;31mflush[m or a page is [1;31mflush[med to disk, it notifies the
    observer. We scan [1;31mflush[m list in buffer pool and [1;31mflush[m dirty pages with the
    observer at the end of the bulk load, and wait for all the [1;31mflush[mes to finish.
    
    Reviewed-by: Jimmy Yang <jimmy.yang@oracle.com>
    RB: 8238

[33mcommit 1dcad1c1ea9b19ce58a15776e0628b4337f0c7e2[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Fri Mar 27 08:17:16 2015 +0530

      Related to bug#20390670 - Following adjustment for WL#7868
    
      - Small adjustments for more optimial behavior
      - Revive IO burst at checkpoint around max_modified_age_sync
      - added bool option innodb_[1;31mflush[m_sync = [ON|OFF] to choose the behavior.
        ON is the default to keep previous behavior for upgraded users
    
        static MYSQL_SYSVAR_BOOL([1;31mflush[m_sync, srv_[1;31mflush[m_sync,
        PLUGIN_VAR_NOCMDARG,
        "Allow IO bursts at the checkpoints ignoring io_capacity setting.",
        NULL, NULL, TRUE);
    
      Reviewed by: Sunny Bains (sunny.bains@oracle.com)
      RB: 7849
      Created by: Yasufumi

[33mcommit 518d3e437ca603d697d7d18e454fe6e56bdd17d2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a regression from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the [1;31mflush[m-time check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>
    (cherry picked from commit fc1f91f396bc73fcea72f17d6c22c174ba057e9b)

[33mcommit c998472c0096f2102eaa5970d4ec0cacea1946b2[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 17 13:53:08 2015 +0200

    Bug#20691930 5.7.6 CRASH WHEN RUNNING MYSQL_UPGRADE AFTER BINARY UPGRADE
    This is a regression from
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH
    OLD INNODB DATA FILES
    
    This affected a production environment where the data files were
    originally created with MySQL 5.0 or earlier. Originally, InnoDB only
    initialized FIL_PAGE_TYPE on two types of pages:
    
    #define FIL_PAGE_INDEX          17855   /*!< B-tree node */
    #define FIL_PAGE_UNDO_LOG       2       /*!< Undo log page */
    
    When files were allocated in the file system, the field was
    initialized to 0. When a page was initialized in the buffer pool, the
    field would be left uninitialized, reusing whatever value happened to
    be at that address (typically one of the 3 values).
    
    In the originally reported incident, page 32768 in the system
    tablespace is an allocation bitmap page, but the uninitialized
    FIL_PAGE_TYPE field on it happened to be FIL_PAGE_INDEX, which caused
    the [1;31mflush[m-time check to fail.
    
    Our fix comprises the following parts:
    
    1. Reset wrong page type on allocation bitmap pages and change buffer bitmap
    pages based on the page number, without checking the page contents and
    without writing redo log.
    
    #define FIL_PAGE_IBUF_BITMAP    5       /*!< Insert buffer bitmap */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    2. On database startup, reset the page types on the following pages
    in the system tablespace, writing redo log:
    
    #define FSP_IBUF_HEADER_PAGE_NO 3       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_TRX_SYS_PAGE_NO     5       // init to 7=FIL_PAGE_TYPE_TRX_SYS
    #define FSP_FIRST_RSEG_PAGE_NO  6       // init to 6=FIL_PAGE_TYPE_SYS
    #define FSP_DICT_HDR_PAGE_NO    7       // init to 6=FIL_PAGE_TYPE_SYS
    
    3. Whenever we modify other types of pages, we reset the FIL_PAGE_TYPE
    within the same mini-transaction, to one of the following values:
    
    #define FIL_PAGE_INODE          3       /*!< Index node */
    #define FIL_PAGE_TYPE_SYS       6       /*!< System page */
    #define FIL_PAGE_TYPE_FSP_HDR   8       /*!< File space header */
    #define FIL_PAGE_TYPE_XDES      9       /*!< Extent descriptor page */
    
    Note: Some page types are initialized immediately after page
    allocation, and the pages are not modified further without changing
    the page type first.  Nothing needs to be done for these page types,
    if the requirement is to have valid page type when we are writing back
    pages from the buffer pool to files.
    
    #define FIL_PAGE_IBUF_FREE_LIST 4       /*!< Insert buffer free list */
    #define FIL_PAGE_TYPE_BLOB      10      /*!< Uncompressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB     11      /*!< First compressed BLOB page */
    #define FIL_PAGE_TYPE_ZBLOB2    12      /*!< Subsequent compressed BLOB page */
    
    Because MySQL does not officially support upgrade following by a
    server crash, there should be no legitimate usage scenario where such
    pages with an incorrect page type would be written out as a result of
    applying redo log during crash recovery.
    
    BLOB pages created before MySQL 5.1 could carry any page type
    (including FIL_PAGE_INDEX), but this should not be an issue, because
    existing BLOB pages are never updated in place. The BLOB columns are
    always updated by copy-on-write, with a valid FIL_PAGE_TYPE.
    
    Note: InnoDB never modifies BLOB pages in place. If BLOB data is modified,
    entirely new pages will be initialized and rewritten. Thus, no logic
    is implemented to update FIL_PAGE_TYPE on BLOB pages. This means that
    even after this fix, subsequent versions of MySQL must be prepared
    to read BLOB pages that contain anything in FIL_PAGE_TYPE.
    
    RB: 8314
    Reviewed-by: Kevin Lewis <kevin.lewis@oracle.com>
    Reviewed-by: Vasil Dimov <vasil.dimov@oracle.com>
    Reviewed-by: Sunny Bains <sunny.bains@oracle.com>

[33mcommit 820db843d248e7755229206b298239de59ee8b3a[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Mar 11 16:01:58 2015 +0100

    Bug#20685859 ENABLE STAGES WITH PROGRESS BY DEFAULT FOR EASE OF USE
    
    Before this fix, every "stage/%" instrument is disabled by default.
    
    With this fix, stage instruments that provide online progress monitoring
    are enabled and timed by default.
    
    The list of stage instruments with online progress is currently:
    
    - stage/sql/copy to tmp table
    - stage/innodb/alter table (end)
    - stage/innodb/alter table ([1;31mflush[m)
    - stage/innodb/alter table (insert)
    - stage/innodb/alter table (log apply index)
    - stage/innodb/alter table (log apply table)
    - stage/innodb/alter table (merge sort)
    - stage/innodb/alter table (read PK and internal sort)
    - stage/innodb/buffer pool load
    
    (cherry picked from commit 818f9f4660ae2d9257429e1ed025fd54dd075be1)

[33mcommit e8ffcf5e251572648725ee124220809f556bcd48[m
Author: Marc Alff <marc.alff@oracle.com>
Date:   Wed Mar 11 16:01:58 2015 +0100

    Bug#20685859 ENABLE STAGES WITH PROGRESS BY DEFAULT FOR EASE OF USE
    
    Before this fix, every "stage/%" instrument is disabled by default.
    
    With this fix, stage instruments that provide online progress monitoring
    are enabled and timed by default.
    
    The list of stage instruments with online progress is currently:
    
    - stage/sql/copy to tmp table
    - stage/innodb/alter table (end)
    - stage/innodb/alter table ([1;31mflush[m)
    - stage/innodb/alter table (insert)
    - stage/innodb/alter table (log apply index)
    - stage/innodb/alter table (log apply table)
    - stage/innodb/alter table (merge sort)
    - stage/innodb/alter table (read PK and internal sort)
    - stage/innodb/buffer pool load

[33mcommit f4c37f7aea732763947980600c6882ec908a54a0[m
Author: Andrei Elkin <andrei.elkin@oracle.com>
Date:   Mon Feb 2 12:26:28 2015 +0200

    WL#6860 Binlogging XA-prepared transaction
    
    ChangeLog
    ---------
    Fri Mar  6 17:44:42 EET 2015
    
    Resolving conflicts at merging to 5.7.
    
    Fri Mar  6 13:59:13 EET 2015
    
    - fixing innodb transaction freeing that was missed out in a case of
      1phase xa through mysqlbinlog applier.  Restructuring an if to
      comply to innodb style, as earlier requested by Marko.  Fixing an
      assert, not dealing directly with the WL per him as well.
    
    Thu Mar  5 08:50:38 EET 2015
    
    - Compilation in innodb got broken on some platforms with the last patch. Fixed now.
    
    Wed Mar  4 19:13:52 EET 2015
    
    - XA COMMIT ONE PHASE at replaying from binlog made
      binlog.binlog_xa_prepared_disconnect to cause failures in other
      tests masquerading to bug19502202. Yet the reason turned out to be
      missed re-attachment of storage engine trans context, that matches
      previous detachment.
      Fixed and the test is augmented to catch the failure alone.
      Small changes are done to innodb.cc and handler.cc.
    
    Tue Mar  3 21:35:28 EET 2015
    
    - fixing more non-deterministic cases that PB2 reveals
    
      1.  main.xa: I left disconnect to run wo/ generally needed synchronisation
                   with connection that runs XA recover. I did not like to contrain
                   the test with P_S dependency. Added warnings and todo, if the warning
                   will come true;
      2.  rpl_xa_survive_crash_debug: found a reason of result mismatch, fixed *it*, but
                                      the test fails sporadically. I suspect --error 2013;
      3.  sys_vars.innodb_support_xa_func: fixed
    
    - ending cleanup requested by Dmitry at today's review of xa.cc and sql_plugin.*
      changes. Hunks from the latter pair of files are relocated onto xa.*.
    - adding support and test for mysqlbinlog recovery for XA-COMMIT-ONE-PHASE;
    
    Tue Mar  3 16:59:14 EET 2015
    
    - determinism in binlog_xa_prepared_disconnect (gtid on/off), and
      rpl_xa_survive_crash_debug (binlog format, synchronisation)
    - fixes to Bug20608551 (LiBing's contribution).
    
    Tue Mar  3 11:59:43 EET 2015
    
    - Changes are done to support "mysqlbinlog | mysql | mysqld" recovery,
      making couple of log_event.cc static functions to turn to public into
      sql_plugin.cc,h and applier_reset_xa_trans() is defined in xa.cc.
      binlog_xa_prepared.test is extended to cover the case;
    - XA COMMIT/ROLLBACK vs GTID_NEXT testing is added;
    - Bug20616249 fixes are incorporated with the WL patch;
    - cleanup in done in xa.cc,h
    
    Fri Feb 27 18:30:28 EET 2015
    
    - Incorrect ordering of rollback to binlog and SE is fixed. XA-rollback
      is written first now. That must be the most probable reason of bug20616249;
    - set @@gtid_next and following XA-commit,rollback worked incorrectly in few ways:
      a. mysql.gtid_executed on slave missed records
      b. set @@gtid_next could not be set to another "manual" value 'cos
         of uncleared status of previous assignment done to the first logging phase
         of XA START..PREPARE
      Tests are extend to cover b.
    - Crash simulation in the middle of XA-rollback is added to prove the existing
      policy of survival of the prepared XA is not gone, as well as to prove
      correct logging ordering (see above).
    
    Tue Feb 24 12:46:33 EET 2015
    
    Updating few test result files:
     - XA basic framework main.xa complained about mismatch
       expectedly 'cos of having disconnect of a prepared transaction;
     - binlog_xa_prepared.test is slightly cleaned not to depend
       on preceeding tests that may contribute to binlog which is
       printed out to show new valid logging pattern.
    Fixing compilation on win;
    
    Todo: rpl_gtids_table_disable_binlog_on_slave seems to have caught
    mysql.gtid_executed updating by two-phased logged XA, under investigation.
    
    Mon Feb 23 21:24:48 EET 2015
    
    Fixing failed tests on hundson.
    
    i_binlog.binlog_loose_XA_trans.test  did not expect new XA-START query, corrected;
    binlog.binlog_xa_prepared_disconnect failed 'cos of a previous test, its
      correctness verification clause is refined to account that;
    
    rpl.rpl_gtids_table_disable_binlog_on_slave did not expect two-phase
      logging, GTID_NEXT handler did not let to change the value after
      XA-PREPARE which must be done each time a "manual" GTID_NEXT
      prepands XA-START.  check_super_outside_prepared_trx_outside_sf() is
      introduced to refine a former GTID_NEXT check function.  Also a
      minor bug in updating GTID_EXECUTED is fixed when XA COMMIT is
      invoked from another connection (through "recovery" interface).  The
      test was extensively cleaned from all way using (incl copy-pasting)
      integer constants.
    
    compilation on embedded is fixed.
    
    Fri Feb 20 20:06:24 EET 2015
    
    Addressing Shiv's final notes dealing with cleanup
    and undoing a result file.
    
    Fri Feb 20 13:22:11 EET 2015
    
    Cleanup is do to
     - replacing xid methods with more appropriate, adding a new m_is_binlogged
       to the XID_STATE constructor initializer list;
     - dismantling st_replace_native_trx_args;
     - restoring help--no-win results.
    
    Thu Feb 19 19:05:49 EET 2015
    
    --xa_survive_disconnect option is removed;
    changes around m_native_trx_ptr that is turned into
    THD::ha_data.ha_ptr_backup to provide one-to-many association THD to
    
    SE (multiple engines in on XA transaction) rather than 1-to-1;
    removing unneeded tests, renaming in remained;
    fixing rpl_trx_boundary_parser_warning (announced but slipped from
    yesterday patch);
    
    various consmetics: empty new lines for coding standard (announced but
    slipped from yesterday patch); renaming, few cosmentics
    
    Wed Feb 18 20:08:20 EET 2015
    
    Cosmetics requested by Luis and Dmitry: renaming, adding empty line as separator throughout
    the patch,
    rpl_trx_boundary_parser_warning references to explicit log_pos
    are elminated 'cos the test started failing due to FD's grown one byte by the fixes.
    
    todo: sort out need of --xa_survive_disconnect option, raised by Luis. Upon that I'll take
    care of tests that are new-option sensitive.
    
    Wed Feb 18 09:43:15 EET 2015
    
    A bug found out in that updating slave_relay_log_info table at XA_prepare handling
    ended up in locking the table for following transactions.
    It's fixed with refining [1;31mflush[m_info() conditions to chose the file repo branch
    for the method invocation in this XA_prepare case.
    
    Fri Feb 13 20:25:36 EET 2015
    
    Addressed
    
     - the final Marko's notes (tests related)
     - few notes from Dmitry
     - renaming a new transaction_ctx member
    
     - cleanup in trans_xa_prepare() as well as removed open questions
       to refer to new reported bug.
       In the bug report it's suggested to correct the error code and specify details
       of prepare failure.
       We won't automatically add XA ROLLBACK in this unlikely case.
    
    Thu Feb 12 14:36:29 EET 2015
    
    xa_prepared_binlog_off is fixed in the assert part
    which could not run correctly due to log_bin is OFF in this test.
    Cosmetic changes in few tests, and picking up Marko's assert.
    
    Fri Feb  6 18:13:37 EET 2015
    
    Refining the last commit's solution to syncronize the exiting connection's
    THD::cleanup with a follower connection through SELECTing from P_S.threads.
    That makes the test available wo/ DBUG, but requirs P_S.
    
    xa.h's start_recovery_xa() is made of two arguments. The new one
    carries m_is_binlogged value.
    
    Fri Feb  6 12:06:57 EET 2015
    
    Changes in tests motivated by
    1.  It turned that there's no SQL way of learning by the killer or by
        an external connection that a prepared trx of one that is supposed
        to be gone is available.  Methods based on `show processlist' or
        SELECT from I_S do *not* work!  The exiting connect is not in the
        result list, but its THD may not have been passed through
        cleanup() method.
        I worked it around with dbug_sync. Here and in other place incl other tests.
    2.  gtid asserts added to main.xa_prepared_binlog_off
        and binlog.binlog_prepared_disconnect
    
    XID::m_is_binlogged flagging is fixed. In the previous vesion the member
    may be left uninitialized.
    
    Thu Feb  5 11:44:41 EET 2015
    
    Fixing some tests (not full addressing yet) per Shiv's notes;
    making 2nd of two ser_buf_size declarations compilable everywhere,
    the issue was spotted by Dmitry.
    da
    todo: address the rest of sticking out notes, by Marko and Shiv,
          in the following commit.
    
    Wed Feb  4 22:22:26 EET 2015
    
    opt_xa_prepared_rollback_at_disconnect is renamed to
    opt_xa_survive_disconnect;
    due to semantics got inverted, its value had to be inverted in sources;
    innodb cleanup is done;
    binlog_xa_prepared_do_and_restart.inc refactored to introduce
    another sourced include, extended;
    tests renaming is done per reviewers and the renaming above.
    
    Wed Feb  4 14:40:20 EET 2015
    
    Asserting the new and old trx properties in
    innodb_replace_trx_in_thd. It's also revealed a flaw in arguments
    passing (fixed: see changes around get_slave_trx_orig());
    fixed visibility in control_events.h per Dmitry's request, and
    compilation of static const XID_t::ser_buf_size;
    perfected binlog_xa_prepared_do_and_restart.inc to invoke
    a new
      extra/binlog_tests/binlog_xa_prepare_connection.inc
    to initiate connections with certain properties.
    
    Tue Feb  3 15:38:43 EET 2015
    
    Fixing Marko's style complains, extending tests per his suggestion.
    An dbug-sync assert was found in innodb, explained and worked around.
    Potential non-determinism of XA RECOVER in tests like
    binlog_xa_prepared_disconnect is eliminated.
    
    Mon Feb  2 12:25:28 EET 2015
    
    Cleanup, merge with trunk.
    
    Addressed remained notes by Marko.
    Futher extension to xa_prepared_binlog_off and binlog_xa_prepared_disconnect
    is done to disconnect prepared XA with the server shutdown.
    
    Thu Jan 29 15:01:48 EET 2015
    
    Replaced --sleep 1 in binlog_xa_prepared and eliminated the same race
    in rpl_xa_old_logging;
    
    Found an issue and corrected XA COMMIT ONE PHASE. It's fixed
    with making XA_prepare_log_event to record the value of ONE_PHASE option.
    It case the new bool member is true the event commits the current
    group.
    Added tests for this case (rpl_xa_old,new_logging).
    
    Fixed compilation issue due to a incomplete renaming done in innodb;
    
    Extended rpl_xa_old,new_logging with random XID generation and made
    more XA transaction to disconnect.
    
    --
    TODO:
    
    Self-review, and
    hand to reviewers to finish their work.
    
    Tue Jan 27 22:35:06 EET 2015
    
    Addressing all Marko's notes.
    Adding up 2pc deregistration into the "read-only" XA prepared
    branch of innobase_close_connection() (Marko, plz assess);
    Extending testing base with adding more tests to
    extra/binlog_tests/binlog_xa_prepared.test, a common source for either
    --skip-log-bin and --log-bin top-level test files of
    xa_prepared_binlog_off and binlog_xa_prepared_disconnect.
    
    There're already notes from Dmitry who left me them on IM,
    to be addressed in tomorrow patch.
    
    TODO:
    1. replace --sleep 1 made in binlog_xa_prepared.test
       to avoid race of connection close and XA COMMIT from another connection.
    2. eliminate the same race in rpl_xa_old_logging
    3. Address Dmitry's comments and get it fully reviewed.
    
    Tue Jan 27 10:10:44 EET 2015
    
    After --skip-log-bin support opened by the previous patch an issue was revealed
    in that "read_only" prepared XA transation cleanup was not conducted properly
    to assert in innodb
         assert ( rw_trx_list.len >= n_prepared_trx).
    The "read-only" trx must've been rolled back and get to proper state.
    This patch corrects that issue.
    
    Few review notes (by Marko) are addressed as well. These include inlines
    and white space consmetics.
    
    Fri Jan 23 14:30:43 EET 2015
    
    The final notes are cleared.
    
    Fri Jan 23 12:44:04 EET 2015
    
    Addressed Neha's documentation and cleanup related comments.
    
    Thu Jan 22 12:51:15 EET 2015
    
    Addressed all points by Neha (related to WL7440).
    
    Wed Jan 21 17:05:33 EET 2015
    
    Merging with libbinlogevent is done (uint4korr() replaced).
    
    Wed Jan 22
    
    This patch merges few WL:s pushed to the trunk
    as well addresses few notes made by reviewers.
    The external WL that caused changes are wl7440, wl7592, trx_boundary_parser.
    
    This patch does not fix two items present on RB:7755 atm:
    
    TODO/FIXME: trx_t::is_recovered life time to settle down.
    
    The WL6860 agenda
    -----------------
    
    The WL addresses long time standing limitation of XA support
    in the server. When the server runs with replication (binary log) turn on
    prepared XA transaction had to be rolled back at disconnection
    Bug#11745231 (bugs.mysql.com/12161) contains the whole story of complains.
    
    This worklog adds support for XA-transactions to make them sustaining
    connection close or server restart without any harm to replication.
    For backward compatibility a new server option/global-read-only-var is introduced:
    --xa_prepared_rollback_at_disconnect
    to have the default value to comply with the rollback "old" behavior.
    
    The Worklog patch consists of
    
    - binary logging extension to write prepared XA and its Commit or Rollback decision
      as separate group of events into the binary log.
      That makes XA-binlogging possibly interleaving yet without any harm to
      data consistency after replaying on the slave.
    
    - slave applier extension to handle XA-prepared and its termination (Commit or Rollback) event
    
    - extension to XA recovery implementation in that connection closing leaves
      a prepared XA in the transaction cache as well as specially marked in
      Innodb. Such prepared XA can be discovered and terminated as the user
      wishes, as well as by the slave applier
    
    - extension to handlerton interface to add up interface allowing
      attach and detach a SE "internal" transaction from the server level transaction
      handle
    
    - augmentment to connection close logics in Innodb as well as
      changes to maintain disconnected transaction's state sane to survive
      the server restart.
    
    Conflicts:
            sql/xa.h

[33mcommit 6ee43801b3518418ad4fa2c2863b080388848a29[m
Author: pedro.gomes <pedro.gomes@oracle.com>
Date:   Mon Mar 2 17:56:47 2015 +0000

    Bug #20577209 ADD TRANSACTIONAL SUPPORT TO VIEW CHANGE EVENT
    
    Not being transactional, View Change events always suffered with
    problems when we wrote them to the binlog and relay log.
    
    On Relay log:
    When the View Change event was the last event in the
    log, this meant that the relay log positions wouldn't be update.
    Upon plugin restart this meant that the events would be again written
    to the binlog.
    
    On Binlog:
    When written to the binlog the events would not any support to allow
    skipping them when we connected to the same master twice for example.
    Other problems were that we are not able to track when the event was
    written to relay_log/binlog without checking log positions or thread
    states.
    MTS issues are also predictable in the near future
    
    Solution:
    When queued to the relay log, the View Change event will be surrounded
    by a transactional block.
    
    GTID
    BEGIN
    VIEW CHANGE
    COMMIT
    
    This need changes to the event writing routines as it now needs to be
    written to the binlog cache that will be [1;31mflush[med by the commit.

[33mcommit ae3689364562b7fcf1862419926afb4e1e5daf38[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Feb 17 13:55:27 2015 +0530

    Bug #20537246 SERVER CRASH WHILE CONNECTING WITH CLEARTEXT-PLUGIN USER
                  WITH BLANK PWD
    When user is created with blank password, NULL value gets stored in
    authentication_string column of mysql.user. When we do [1;31mflush[m privileges
    this NULL value gets loaded into acl_user.auth_string. Thus during
    authentication there is a strcmp for external plugins (like cleartext
    password, authentication_pam) for auth_string which causes segmentation
    fault.
    Fix is to set the auth_string to empty string when there is a NULL value
    in authentication_string column of mysql.user table.
    
    (cherry picked from commit 7fe41b58ef86059b737f42d1c1960aa5958d8f01)

[33mcommit e96a0104da8446893513683b3f1321b0a70ed67e[m
Author: Bharathy Satish <bharathy.x.satish@oracle.com>
Date:   Tue Feb 17 13:55:27 2015 +0530

    Bug #20537246 SERVER CRASH WHILE CONNECTING WITH CLEARTEXT-PLUGIN USER
                  WITH BLANK PWD
    When user is created with blank password, NULL value gets stored in
    authentication_string column of mysql.user. When we do [1;31mflush[m privileges
    this NULL value gets loaded into acl_user.auth_string. Thus during
    authentication there is a strcmp for external plugins (like cleartext
    password, authentication_pam) for auth_string which causes segmentation
    fault.
    Fix is to set the auth_string to empty string when there is a NULL value
    in authentication_string column of mysql.user table.

[33mcommit f8d205a3d2d3043e2d62a4e90f5c5d89ada1c07b[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Mon Feb 2 11:22:20 2015 +0200

    Fix determinism in alter_table_stage_progress.test
    
    The performance_schema.events_stages_history_long contains events from
    all threads and it can be polluted from previous tests that were
    executed.
    
    Thus use performance_schema.events_stages_history instead which only
    contains stages from the currently executing thread.
    
    But by default only the last 10 events are displayed in
    performance_schema.events_stages_history and the events this test is
    looking for (LIKE 'stage/innodb/alter table%') could be [1;31mflush[med down the
    drain before it gets a chance too peek them. Thus increment the default
    value of 10 to 1000 via alter_table_stage_progress-master.opt (the limit
    is a readonly option and cannot be increased with SET GLOBAL).
    
    Reviewed-by:    Marko (via IM)

[33mcommit f723e9d65d15aabeeefad369ec71e810bfcc9cfb[m
Merge: a4021179e47 d847fe695a0
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Jan 29 16:09:38 2015 +0200

    Merge branch 'mysql-trunk-wl5889' into mysql-trunk
    
    * mysql-trunk-wl5889:
      Remove temp aux scripts used to assess WL#5889
      Instrument the log apply when adding index
      Non-functional: rename phase "log" to "log table"
      Non-functional: convert 2 functions to new style
      Non-functional: break long lines
      Elaborate a comment
      Rename two phases of ALTER TABLE progress
      Avoid rounding-skew with the recs_per_key estimate
      Non-functional: move the typecast to the constant
      Clarify comments wrt [1;31mflush[ming and ALTER progress
      Put more weight on the read pk phase
      s/my_pthread.h/my_thread.h after a rename
      Copy Alter_info::se_blob even if PFS is disabled
      Account memory allocated in handler0alter.cc
      Add a comment when setting m_n_recs_per_page
      - Updated the copyright year in the welcome message for MySQL - Fix mysql_config_editor test after new copyright year - Removed --rpm options from mysql_install_db
      Fix a compilation failure after a merge from trunk
      Initialize ut_stage_alter_t::m_progress to NULL
      Fix typo: s/m_sort_multi_factor/sort_multi_factor
      Fix compilation error on Windows
      Fix compilation error on Windows
      Fix a compilation error on Windows
      Use 1.0 instead of 1 when comparing with "double"
      Handle edge cases of log2(0) and log2(1)
      Add a simple mtr test to exercise bits of WL#5889
      Replace two magic numbers with named constants
      Instrument the [1;31mflush[m during index creation
      Whitespace fixup
      Improve ut_stage_alter_t's readability
      Check if stage is NULL before using it in commit
      Fix a UT_NEW_NOKEY() / delete mismatch
      Use m_cur_phase to determine the current phase
      Adjust the estimate of pages to be [1;31mflush[med
      Remove redundant call to begin_phase_end()
      Document the stage parameter in all functions
      Tidy the ut_stage_alter_t interface and document
      Set completed=estimated at the end of ALTER TABLE
      Add ut_stage_alter_t noop stubs when PSI is off
      Revert unneeded changes and whitespace fixups
      Isolate the progress-accounting logic into a class
      Handle NULL progress in ut_stage_new_estimate()
      Prevent underscores to cause subscript in gnuplot
      Progress-instrument the log phase of ALTER TABLE
      Handle NULL return values from mysql_set_stage()
      483af0c followup: tune the num of indexes to sort
      Improve the progress estimate in ALTER TABLE
      Don't dereference progress if it can be NULL
      Whitespace and other fixup
      Add aux scripts to gather/visualise ALTER TABLE
      Recalculate the work estimated before [1;31mflush[m
      Add a basic progress reporting to InnoDB ALTER
      Add a progress member to Alter_info
      Add mysql_stage_get_work_(completed|estimated)
      WL#5889 Add InnoDB events to Performance Schema's Event Stage table
      WL#5889 Add InnoDB events to Performance Schema's Event Stage table
      Fixed branch name for commit emails
      WL#5889 Add InnoDB events to Performance Schema's Event Stage table
      WL#5889 Add InnoDB events to Performance Schema's Event Stage table

[33mcommit 2ebff72bb226b69c578cf0820112c25b05677bd9[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Mon Nov 17 10:47:47 2014 +0100

    WL#7592 step 13. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix binlogging of strange SQL statements.
    
    A few SQL statements have strange semantics that causes trouble for GTIDs. This includes DROP TABLE with multiple tables, CREATE TABLE ... SELECT, DROP DATABASE that fails on rmdir, OPTIMIZE/REPAIR/ANALYZE/CHECKSUM TABLE, CREATE TEMPORARY/DROP TEMPORARY, DROP TEMPORARY generated by client disconnect, and statements/transactions that mix both transactional and non-transactional updates.
    
     1. Background:
        When DROP TABLE is used with multiple tables, and the tables are of
        different types (transactional/non-transactional or
        temporary/non-temporary), tables of the same type get grouped
        together and each group is logged as a separate statement. For
        example:
          DROP TABLE temporary, non_temporary
        gets logged as
          DROP TABLE temporary; DROP TABLE non_temporary.
    
        When GTID_MODE = ON, each such statement is assigned its own GTID.
        In order to generate the GTID, mysql_rm_table_no_locks calls
        mysql_bin_log.commit for each group of tables.
    
        Problem:
    
         1. mysql_bin_log.commit is only called when gtid_mode !=
            OFF. When gtid_mode == OFF, all the statements are written to
            the binary log in one operation. So after WL#7592 there is
            only one Anonymous_gtids_log_event, instead of one for each
            DROP TABLE.
    
         2. If GTID_NEXT='ANONYMOUS', Gtid_state::update_on_commit
            releases anonymous ownership. But the statement should hold
            anonymous ownership until it completes.
    
        Fix:
    
         1. Call mysql_bin_log.commit unconditionally.
    
            Note: inside a transaction, if only temporary tables are
            dropped, we should not call mysql_bin_log.commit, since the
            transactional context must remain open in this case.
    
         2. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
     2. Background:
        Prior to this patch, when binlog_format=row, CREATE...SELECT gets
        written to the binary log as
          BEGIN
          CREATE
          row events
          COMMIT
        CREATE...SELECT is not allowed when gtid_mode=on (in fact, not when
        enforce_gtid_consistency=1).
    
        Problem:
    
         1. Although CREATE without SELECT has an implicit commit, it
            appears in the middle of a transaction on the slave. Thus,
            after this worklog and prior to this patch, it gets logged as:
    
              Anonymous_gtids_log_event
              BEGIN
              CREATE
              row events
              COMMIT
    
            This causes problems on an MTS slave.
    
         2. If GTID_NEXT='ANONYMOUS', Gtid_state::update_on_commit
            releases anonymous ownership. But the statement should hold
            anonymous ownership until it completes.
    
        Fix:
    
         1. Call mysql_bin_log.commit after writing the CREATE statement.
            This causes CREATE...SELECT to be logged like:
    
            Anonymous_gtids_log_event
            CREATE
            Anonymous_gtids_log_event
            BEGIN
            row events
            COMMIT
    
         2. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
        A side-effect of this is that a CREATE...SELECT statement that
        fails in the SELECT part is already logged when the error happens,
        but the error causes the statement to be rolled back.  To make
        this case work correctly, we log a compensatory DROP statement if
        the SELECT part fails.
    
     3. Background:
        If DROP DATABASE fails after dropping some tables (e.g., if there
        are extra files in the database directory), then it writes a
        DROP TABLE statement that lists all the tables that it dropped.
        If there are many tables, this statement gets long. In this case,
        the server splits the statement into multiple DROP TABLE statements.
    
        Problem:
    
         1. If the statement fails when GTID_NEXT='UUID:NUMBER', then
            there is no way to log this correctly. So we must generate an
            error, log nothing, and not add GTID to GTID_EXECUTED.
    
         2. If the statement fails and generates multiple transactions
            when GTID_NEXT='AUTOMATIC' or 'ANONYMOUS', then we must
            generate multiple transactions, and each should have its own
            Gtid_log_event or Anonymous_gtid_log_event.
    
         3. If GTID_NEXT='ANONYMOUS', we must hold anonymous ownership
            until all transactions have been written to the binary log.
    
        Fix:
    
         1. Introduce a new error code,
            ER_CANNOT_LOG_PARTIAL_DROP_DATABASE_WITH_GTID, and generate
            the error if GTID_NEXT='UUID:NUMBER' and DROP DATABASE fails.
    
         2. Call mysql_bin_log.commit after writing DROP TABLE statements.
    
         3. Set thd->is_commit_in_middle_of_statement before calling
            mysql_bin_log.commit. This tells
            Gtid_state::update_on_rollback to not release anonymous
            ownership.
    
     4. Background:
        OPTIMIZE/REPAIR/ANALYZE/CHECKSUM TABLE are written to the binary
        log even if they fail, after having called trans_rollback.
    
        Problem:
        trans_rollback calls gtid_state::update_on_rollback, which normally
        releases GTID ownership. But we must not release ownership before
        writing to the binary log.
    
        Fix:
        This was already fixed for the case gtid_mode=on; for that case we
        set a special flag in the THD object which tells
        gtid_state::update_on_rollback to not release ownership. Now we need
        to fix the case gtid_mode=off, so we set the flag in this case too.
    
     5. Background:
        CREATE TEMPORARY and DROP TEMPORARY behave very strange. If executed
        outside transactional context, they behave as DDL: they get logged
        without BEGIN...COMMIT and cannot be rolled back. If executed in
        transactional context, they behave as non-transactional DML: they
        get logged inside BEGIN...COMMIT, leave the transactional context
        open, but cannot be rolled back.
        Before this patch, CREATE TEMPORARY and DROP TEMPORARY call
        gtid_end_transaction unconditionally.
    
        Problem:
        gtid_end_transaction ends the transactional context and releases
        ownership. This was not a problem before WL#7592 since
        gtid_end_transaction could only be called when gtid_mode=on, and
        when gtid_mode=on we disallow CREATE TEMPORARY and DROP TEMPORARY
        inside transactional context. However, after WL#7592, we call
        gtid_end_transaction also when gtid_mode=off, and
        gtid_end_transaction releases anonymous ownership.
    
        Fix:
        Do not call gtid_end_transaction for CREATE TEMPORARY and
        DROP TEMPORARY inside transaction context.
    
     6. Background:
        When a client that has open temporary tables disconnects, the
        temporary tables are dropped and DROP TEMPORARY is written to the
        binary log.
    
        Problem:
        After WL#7592 and before this patch, if a client disconnects when
        GTID_NEXT='ANONYMOUS', the client would not hold anonymous ownership
        when writing to the binary log, which would trigger an assertion in
        write_gtid.
    
        There was no problem when GTID_NEXT='UUID:NUMBER', since this case
        was taken care of already before WL#7592. In this case, we set
        GTID_NEXT='AUTOMATIC' before dropping any tables.
    
        Fix:
        Set GTID_NEXT='AUTOMATIC' regardless of GTID_MODE.
    
     7. Background:
        Anything that is written to the binary log is first written to a
        thread-specific IO_CACHE. There are two such IO_CACHES: the
        transaction cache and the statement cache. Transactional updates are
        written to the transaction cache and non-transactional updates are
        written to the statement cache. (Under certain conditions, such as
        when a non-transactional update appears after a transactional update
        within the same transaction and
        binlog_direct_non_transactional_updates=0, non-transactional updates
        are written to the transaction cache, but that is not relevant to
        the current discussion.)
    
        The statement cache is [1;31mflush[med when the statement ends and the
        transaction cache is [1;31mflush[med when the transaction ends.
    
        It is possible that both caches are non-empty if a single
        transaction or a single statement updates both transactional and
        non-transactional tables. This is not allowed when GTID_MODE=ON.
        If both caches are non-empty, an autocommitted transaction [1;31mflush[mes
        first the statement cache and then the transaction cache (see
        binlog.cc:binlog_cache_mngr::[1;31mflush[m). Both happen in the BGC [1;31mflush[m
        stage.
    
        Problems:
         7.1. When [1;31mflush[ming both caches in an autocommitted transaction,
              and GTID_NEXT=AUTOMATIC, Gtid_state::generate_automatic_gtid
              is called once for each cache.
              Gtid_state::generate_automatic_gtid acquires anonymous
              ownership, but it also assumes (and asserts) that no ownership
              is held when entering the function. Thus, before this patch
              the assert would be raised when [1;31mflush[ming the transaction
              cache, since the previous [1;31mflush[m of the statement cache did
              acquire ownership.
    
         7.2. When the statement cache is [1;31mflush[med in the middle of a
              transaction (due to a non-transactional update happening
              in the middle of the transaction), and GTID_NEXT=ANONYMOUS,
              before this patch ownership was released. This breaks the
              protocol that anonymous ownership must be held for the
              duration of the transaction when GTID_NEXT=ANONYMOUS.
    
         7.3. Any statement calls gtid_reacquire_ownership_if_anonymous
              before it executes (mysql_execute_statement calls
              gtid_pre_statement_checks which calls
              gtid_reacquire_ownership_if_anonymous). This is important
              because when GTID_NEXT=ANONYMOUS, anonymous ownership must
              be held from when the transaction begins to execute.
              Therefore, when the transaction commits it also asserts that
              if GTID_NEXT=ANONYMOUS, it holds anonymous ownership.
    
              For the same reason, Rows_log_event::do_apply_event calls
              gtid_pre_statement_checks which calls
              gtid_reacquire_ownership_if_anonymous.
    
              However, before this patch the call to
              gtid_reacquire_ownership_if_anonymous was missing in
              Xid_log_event::do_apply_event.
    
              There is no existing case when GTID_NEXT=ANONYMOUS and
              anonymous ownership is not held when
              Xid_log_event::do_apply_event is called. However, it could
              potentially happen if a future version has a bug that sends
              a lonely Xid_log_event to the slave, or if the relay log is
              generated by something else than mysql, which has a bug. So
              we should call gtid_reacquire_ownership_if_anonymous at the
              beginning of Xid_log_event::do_apply_event too (just like
              we would do in a Query_log_event that contains a COMMIT
              query).
    
        Fix:
         7.1. When both caches are non-empty, and GTID_NEXT='AUTOMATIC',
              call thd->clear_owned_gtids between the two cache
              [1;31mflush[mes. This releases anonymous ownership so that the
              call to Gtid_cache::generate_automatic_gtid for the second
              [1;31mflush[m is done without holding any ownership.
    
         7.2. In Gtid_state::update_gtids_impl, do not release ownership
              if the transaction cache is nonempty.
    
         7.3. Call gtid_reacquire_ownership_if_anonymous from
              Xid_log_event::do_apply_event.
    
    @mysql-test/extra/binlog_tests/drop_tables_logical_timestamp.inc
    - Remove this file. See binlog_mts_logical_clock.test for explanation.
    
    @mysql-test/extra/binlog_tests/logical_timestamping.inc
    - Use assert_logical_timestamps.inc instead of grep_pattern.inc See
      binlog_mts_logical_clock.test for explanation.
    
    @mysql-test/extra/rpl_tests/rpl_gtid_drop_multiple_tables.inc
    - New auxiliary file used by
      rpl_gtid_drop_multiple_tables_in_multiple_ways.inc.
    
    @mysql-test/extra/rpl_tests/
    rpl_gtid_drop_multiple_tables_in_multiple_ways.inc
    - New auxiliary test file used by rpl_split_statements.test.
    
    @mysql-test/extra/rpl_tests/rpl_split_statements.test
    - New test case.
    
    @mysql-test/include/assert_binlog_events.inc
    - New test utility. This is needed by rpl_split_statements.inc.
    
    @mysql-test/include/assert_grep.inc
    - New test utility. This is needed by assert_logical_timestamps.inc.
    
    @mysql-test/include/assert_logical_timestamps.inc
    - New auxiliary test utility used by binlog_mts_logical_clock.test.
    
    @mysql-test/include/grep_pattern.inc
    - Add comment suggesting to use assert_grep.inc instead.
    
    @mysql-test/include/gtid_step_assert.inc
    - Add $gtid_step_gtid_mode_agnostic, needed by rpl_split_statements.test
    - Update test utility due to changes in gtid_utilities.inc.
    
    @mysql-test/include/gtid_utils.inc
    - Add new utility functions.
    
    @mysql-test/include/gtid_utils_end.inc
    - Drop the new functions introduced in gtid_utilities.inc.
    
    @mysql-test/include/rpl_connect.inc
    - Make it easier to map mysqltest connections to thread numbers in
      server debug traces.
    
    @mysql-test/include/rpl_connection.inc
    - Add $rpl_connection_silent_if_same.
    
    @mysql-test/include/rpl_get_end_of_relay_log.inc
    - New auxiliary test utility. This is needed by
      rpl_skip_to_end_of_relay_log.inc.
    
    @mysql-test/include/rpl_init.inc
    - Set some more mtr variables for convenience.
    
    @mysql-test/include/rpl_skip_to_end_of_relay_log.inc
    - New auxiliary test utility. This is needed by
      rpl_gtid_split_statements.test.
    
    @mysql-test/include/rpl_stop_slaves.inc
    - Correct a typo.
    
    @mysql-test/include/save_binlog_position.inc
    - New auxiliary test script, useful in combination with
      include/assert_binlog_events.inc.
    
    @mysql-test/include/set_gtid_next_gtid_mode_agnostic.inc
    - New auxiliary test utility. This is needed by
      rpl_split_statements.test.
    
    @mysql-test/include/wait_for_status_var.inc
    - Fix broken implementation of $status_fail_query.
    
    @mysql-test/suite/binlog/r/binlog_gtid_utils.result
    - Update result file for modified test.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock.result
    - Update result file due to change in test.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock_gtid.result
    - Update result file due to change in test.
    
    @mysql-test/suite/binlog/t/binlog_gtid_utils.test
    - Add tests for new utility function.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock.test
    - This test started failing because DROP TABLE is now logged
      differently.
      The failure was in 'grep_pattern.inc' executed just after
      drop_tables_logical_timestamp.inc. This was fixed by changing the test
      assertion.
    - The test assertion following drop_tables_logical_timestamp.inc belongs
      inside drop_tables_logical_timestamp.inc. Moved the test assertion
      there.
    - The tests in drop_tables_logical_timestamp.inc were located in this
      file because they differed between gtid_mode=on and gtid_mode=off. But
      because of the change in logging of DROP TABLE, the test works in the
      same way regardless of gtid_mode. Therefore, the contents of
      drop_tables_logical_timestamp.inc was moved into
      logical_timestamping.inc and drop_tables_logical_timestamp.inc as
      removed.
    - This file used grep_pattern.inc to check assertions. This made the
      test very verbose and hard to read and understand. Also, it was
      imprecise since any future server bug that introduces a logical
      timestamp that does not match the given pattern would go unnoticed by
      the test. Therefore, replaced all grep_pattern.inc by
      assert_logical_timestamps.inc.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock_gtid.test
    - See binlog_mts_logical_clock.test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_create_select.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_disconnect_drop_temporary_table.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_gtid_split_statements.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_no_gtid_split_statements.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_semi_sync.result
    - When semisync is enabled, there will be one ack for each transaction.
      Since we now have more 'transactions' due to the extra
      Anonymous_log_event preceding each DROP statement, this changes the
      result file.
    
    @mysql-test/suite/rpl/t/rpl_drop_db.test
    - This test can now run regardless of GTID_MODE.
    
    @mysql-test/suite/rpl/t/rpl_gtid_create_select.test
    - New test to verify that CREATE ... SELECT is logged as expected.
    
    @mysql-test/suite/rpl/t/rpl_gtid_disconnect_drop_temporary_table.test
    - New test file to verify that DROP TEMPORARY generated by client
      disconnect is logged correctly.
    
    @mysql-test/suite/rpl/t/rpl_gtid_split_statements.test
    - New test file.
    
    @mysql-test/suite/rpl/t/rpl_invoked_features.test
    - Explain why not_gtid_enabled is used.
    
    @mysql-test/suite/rpl/t/rpl_mixed_drop_create_temp_table.test
    - Explain why not_gtid_enabled is used.
    
    @mysql-test/suite/rpl/t/rpl_no_gtid_split_statements.test
    - New test file.
    
    @mysql-test/suite/rpl/t/rpl_stop_slave.test
    - Explain why not_gtid_enabled is used.
    
    @sql/rpl_gtid.h
    - Add need_lock parameter to Sid_map::sidno_to_sid,
      Gtid::to_string, Gtid::dbug_print, Gtid_specification::to_string, and
      Gtid_specification::dbug_print, to simplify the use of these
      functions.
    - Simplify the definition of Gtid_specification::MAX_TEXT_LENGTH.
    - Make gtid_reacquire_ownership_if_anonymous extern so that it can be
      called from log_event.cc.
    
    @sql/rpl_gtid_execution.cc
    - Make gtid_reacquire_ownership_if_anonymous extern so that it can be
      called from log_event.cc.
    - Move parts of gtid_pre_statement_checks into an own function,
      gtid_reacquire_ownership_if_anonymous. This is just to make
      the logic more easy to follow.
    - Make gtid_reacquire_ownership acquire anonymous ownership in case
      GTID_NEXT='ANONYMOUS'. This is needed for cases like:
        SET AUTOCOMMIT=1;
        SET GTID_NEXT='ANONYMOUS';
        INSERT;
        BEGIN;
      The first INSERT will commit the transaction and therefore release
      anonymous ownership. Then the BEGIN has to re-acquire anonymous
      ownership so that it can execute correctly.
    
      (The logic to reacquire ownership in this case is needed because we
      allow user to set GTID_NEXT='ANONYMOUS' and then execute multiple
      transactions. This differs from the case of GTID_NEXT='UUID:NUMBER',
      where we generate an error if user tries to execute a second
      transaction without setting GTID_NEXT again. The reason we want to
      generate an error in this case is that otherwise the GTID autoskip
      feature would silently skip the transaction and user would not notice
      it.)
    
    - Move the call to gtid_reacquire_ownership_if_anonymous
      earlier in gtid_pre_statemen_checks. This is needed for the
      statements BEGIN/ROLLBACK/COMMIT in two cases.
    
      Case 1:
        User executes:
          SET GTID_NEXT='ANONYMOUS';
          BEGIN;
          INSERT;
          COMMIT;
          BEGIN;
      Here the second BEGIN needs to acquire anonymous ownership so that
      anonymous ownership is held during the entire transaction.
    
      Case 2:
        The relay log comes from an old master that does not generate
        Anonymous_gtids_log_event, so it begins with:
          Format_desc
          Previous_gtids
          BEGIN;
        Then the Format_desc will set GTID_NEXT=NOT_YET_DETERMINED, and the
        BEGIN needs to acquire anonyous ownership.
    
      The only case where we should not acquire anonymous ownership is for a
      SET statement that does not invoke a stored function. This is
      important in case a client processes output from mysqlbinlog, for a
      binary log generated with GTID_MODE=ON. Then, the server will first
      execute a Format_description_log_event, which will set
      GTID_NEXT='NOT_YET_DETERMINED', followed by a
      SET GTID_NEXT='UUID:NUMBER' statement. If it would try to reacquire
      anonymous ownership in this case, and gtid_mode=on, an error would be
      generated since anonymous transactions are not allowed when
      gtid_mode=on.
    
    @sql/rpl_gtid_misc.cc
    - Add need_lock parameter to Gtid::to_string, to simplify the use of
      this function.
    - Allow Gtid::to_string to take sid_map==NULL in debug mode, so that
      debug printouts can show the sidno.
    
    @sql/rpl_gtid_specification.cc
    - Add need_lock to Gtid_specification::to_string.
    
    @sql/rpl_gtid_state.cc
    - Print any change of thd->owned_gtid to the debug trace.
    - Implement thd->is_commit_in_middle_of_statement.
    - Return early from update_gtids_impl to avoid releasing ownership, if
      the transaction cache is nonempty and GTID_NEXT=ANONYMOUS.
    
    @sql/share/errmsg-utf8.txt
    - New error message.
    
    @sql/binlog.cc
    - Release anonymous ownership between [1;31mflush[ming the statement cache
      and [1;31mflush[ming the transaction cache, if GTID_NEXT=AUTOMATIC.
    
    @sql/log_event.cc
    - Call gtid_reacquire_ownership_if_anonymous in
      Xid_log_event::do_apply_event.
    
    @sql/sql_admin.cc
    - Administrational statements (OPTIMIZE TABLE, REPAIR TABLE,
      CHECKSUM TABLE, ANALYZE TABLE) behave strange if they fail: they first
      call trans_rollback and then write the statement to the binary log.
      This causes problems for GTIDs, since ha_rollback eventually calls
      gtid_state::update_on_rollback, which releases GTID ownership. Then
      no GTID is owned when it comes to writing the statements to the binary
      log.
    
      This was handled when GTID_NEXT='UUID:NUMBER' by setting the flag
      thd->skip_gtid_rollback before calling ha_rollback.
      gtid_state::update_on_rollback checks the flag and if the flag is set,
      it does not release ownership.
    
      After WL#7592 we need to hold anonymous ownership in case
      GTID_NEXT='ANONYMOUS'. Therefore we set the flag also in this case.
    
    @sql/sql_base.cc
    - When a client which has open tables disconnects, DROP TEMPORARY TABLE
      is written to the binary log. Before doing that, we must set
      GTID_NEXT='AUTOMATIC', so that it generates new GTIDs correctly.
    
      This was done only in the case of GTID_MODE=ON. Now we need to do it
      regardless of GTID_MODE.
    
      Moreover, in case a GTID is owned already, we must release ownership
      before setting GTID_NEXT='AUTOMATIC'. Thus we call
      gtid_state->update_on_rollback() first.
    
    @sql/sql_class.cc
    - Print any change of thd->owned_gtid to the debug trace.
    - Initialize new THD member variable.
    
    @sql/sql_class.h
    - Clarify life cycle of THD::owned_gtid.
    - Print any change of thd->owned_gtid to the debug trace.
    - Add THD::is_commit_in_middle_of_statement.
    
    @sql/sql_db.cc
    - If DROP TABLE fails, so that it generates DROP TABLE statements in the
      binary log, then we need to:
      - Call mysql_bin_log.commit after each statement. If we did not do
        this, all DROP TABLE statements would be written to the same
        statement cache, so there would just be one
        Anonymous_gtid_log_event. We need each DROP TABLE to have its own
      - If GTID_NEXT='UUID:NUMBER', and multiple DROP TABLE are needed, then
        there is no way to log this correctly. Thus, we generate
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS. This is
        a new error code.
        To handle this case correctly, we must also postpone the generation
        of ER_DB_DROP_RMDIR. If we would not move this error until later,
        then ER_DB_DROP_RMDIR would be generated before
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS, so then
        the user would never see
        ER_CANNOT_LOG_FAILED_DROP_DATABASE_WITH_MULTIPLE_STATEMENTS.
      - If GTID_NEXT='ANONYMOUS', it is not a problem that we generate
        multiple transactions. However, we must set
        thd->is_commit_in_middle_of_statement in order to hold
        anonymous ownership for the duration of the statement.
    
    @sql/sql_insert.cc
    - For CREATE ... SELECT, write the CREATE as a non-transactional
      statement, directly to the binlog.
      This prevents BEGIN and COMMIT statements from being generated around
      the CREATE statement.
    - Call mysql_bin_log.commit after writing the CREATE statement. This
      causes the row events to be written as a separate transaction, so they
      will be preceded by an Anonymous_gtid_log_event.
    - Do not release anonymous ownership in the middle of the statement.
    - Log a compensatory DROP TABLE statement if the CREATE...SELECT fails
      on the SELECT part.
    
    @sql/sql_parse.cc
    - CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE are very strange
      statements. When executed in transactional context, they behave like
      MyISAM: they leave the transaction context open and get logged between
      BEGIN and COMMIT, but cannot be rolled back. When executed outside
      transactional context, they get logged as DDL, without BEGIN/COMMIT.
    
      If we would call gtid_end_transaction without ending the transaction,
      then ownership would be released too early. This would cause an
      assertion in write_gtid, where it is expected that the thread holds
      ownership of whatever GTID_NEXT is set to (unless
      GTID_NEXT='AUTOMATIC').
    
      This was not a concern before WL#7592, since gtid_end_transaction was
      only called if gtid_mode=ON, and CREATE TEMPORARY/DROP TEMPORARY are
      disallowed in transactional context when gtid_mode=ON.
    
      Now that we can call gtid_end_transaction also when gtid_mode=OFF,
      CREATE TEMPORARY/DROP TEMPORARY must only invoke gtid_end_transaction
      if executed outside transaction context.
    
    @sql/sql_table.cc
    - Call mysql_bin_log.commit regardless of gtid_mode, only avoid it
      between temporary tables inside a transaction.
      The code has three blocks:
       1. nontransactional temporary tables
       2. transactional temporary tables
       3. non-temporary tables
      Before, the commit was at the end of block 1 and 2. We moved it
      to the beginning of block 2 and 3 instead, to simplify the condition.
      This does not change the logic for writing to the log: in both
      cases the point is that we do the commit *between* two calls to
      thd->binlog_query.
    - Fix some comments and re-wrap some long lines.
    - In parameters to mysql_bin_log.commit, use true/false instead of
      TRUE/FALSE, and add comments to clarify the meaning of the parameters.
    
    @mysql-test/suite/binlog/r/binlog_row_binlog.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/binlog/r/binlog_row_insert_select.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/binlog/r/binlog_row_mix_innodb_myisam.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/r/rpl_non_direct_row_mixing_engines.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/r/rpl_row_mixing_engines.result
    - Update result file due to design change for logging CREATE ... SELECT in RBR.
    
    @mysql-test/suite/rpl/t/rpl_stm_start_stop_slave.test
    - Suppress a warning, which is generated from its included file
      rpl_start_stop_slave.test
    
    @mysql-test/suite/rpl/r/rpl_stm_start_stop_slave.result
    - Update result file to remove a redundent suppression warning.

[33mcommit 9ad6b77365292af944226ff03bbb1e63189e5c6f[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Thu Nov 13 13:17:02 2014 +0100

    WL#7592 step 10. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Fix failing tests.
    
    This patch fixes all test cases that fails due to the previous two
    patches. In addition it fixes two code bugs that were exposed/introduced
    due to the previous two patches, and which caused some of the test
    failures.
    
    - Fix bug in START SLAVE UNTIL MASTER_LOG_POS logic.
      The problem was: some Rotate_log_events in the relay log are generated
      on the slave, not on the master. Thus, their end_log_pos field is
      relative to the slave relay log. Since MASTER_LOS_POS is relative to
      the master binary log, we must not evaluate the MASTER_LOS_POS
      condition for such slave-generated Rotate_log_events. But the logic to
      skip the until check for slave-generated events was missing, and this
      caused tests to fail.
    
      The fix is to avoid evaluating the until condition for slave-generated
      events. This is easy because such events are easily distinguishable
      since their server_id is zero. So we check if the server_id==0, and in
      that case we don't evaluate the until condition.
    
      This did not cause any tests to fail before this worklog, because the
      events appeared so early in the relay log that their positions would
      be smaller than the position specified by MASTER_LOG_POS. However,
      after this patch, the events appear after Previous_gtids_log_event,
      which moves the position forward so much that it causes the slave
      thread to stop before the rotate event, which causes the test to fail.
    
      This was also not triggered by running the suite with gtid_mode=on,
      because the test was using include/not_gtid_enabled.inc.
    
    - Fix bug in sql_slave_skip_counter with GTIDs.
      sql_slave_skip_counter did not compute transaction boundaries
      correctly in the presence of Gtid_log_events. This did not cause any
      problems before this patch since sql_slave_skip_counter is not allowed
      when gtid_mode=on.
    
      sql_slave_skip_counter is supposed to decrease for every event
      processed, except it should not decrease down to 0 in the middle of a
      group. This ensures that the applier thread does not stop in the
      middle of a transaction. However, the applier thread did not consider
      Gtid_log_event to be part of a group, and therefore it could stop
      after the Gtid_log_event.
    
      The problem was that Gtid_log_event implemented a specialized
      do_shall_skip function. This caused it to decrease the counter down to
      zero. The fix is to implement Gtid_log_event::do_shall_skip and make
      it call continue_group.
    
    - Fix simplified-binlog-recovery.
      Writing Previous_gtids_log_event always broke the logic for
      simplified-binlog-recovery.
      Background:
      Before this patch, simplified-binlog-recovery would avoid
      iterating over multiple binary logs only in the case that the
      binlog lacks a Previous_gtids_log_event.
      Problem:
      Since we now generate Previous_gtids_log_event always,
      recovery would iterate over all binary logs even when
      simplified-binlog-recovery was enabled.
      Fix:
      Make it so that simplified-binlog-recovery skips the rest
      of the binary logs also in the case that the first binary log
      contains a Previous_gtids_log_event and no Gtid_log_event.
    
    @mysql-test/extra/binlog_tests/binlog.test
    - Use show_binlog_events.inc instead of SHOW BINLOG EVENTS, so that
      Gtid/Anonymous events gets masked appropriately.
    - This particular test requires that columns 1, 2, and 5 are masked out
      (so that server_id is not masked out). However, show_binlog_events.inc
      only masks columns 2, 4, 5. Changed show_binlog_events.inc so that it
      allows user to specify the set of columns to be masked.
    
    @mysql-test/extra/binlog_tests/binlog_mysqlbinlog_row.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/ctype_ucs_binlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/drop_tables_logical_timestamp.inc
    - Update test because grep_pattern.inc was altered.
    
    @mysql-test/extra/binlog_tests/logical_timestamping.inc
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/extra/binlog_tests/mix_innodb_myisam_binlog.test
    - Test was failing because it was expecting an exact number of events in
      the binlog. Fixed by adding an auxiliary test script
      include/get_row_count.inc that computes the number of events in the
      binlog.
    - While I was here, also changed to use assertion.
    
    @mysql-test/extra/binlog_tests/mysqlbinlog_row_engine.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/binlog_tests/mysqlbinlog_start_stop_2.inc
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/rpl_tests/check_type.inc
    - Provide more debug info if $rpl_debug is set.
    
    @mysql-test/extra/rpl_tests/create_recursive_construct.inc
    - The test expected that an empty binlog would contain 2 events. Changed
      this to 3.
    - The test expected that the third event of a binlog containing only one
      DML transaction in row format would be a Table_map. Changed third to
      fifth.
    - Simplify code to use assertions, to make it more readable and produce
      more debug output on failure.
    
    @mysql-test/extra/rpl_tests/rpl_implicit_commit_binlog.test
    - Fix test failure:
      This test executed some statements and then asserted that there was a
      COMMIT after a specific number of events in the binary log. Since
      we now write more events to the binary log, we have to increase this
      number in order for the test to succeed.
    - Use assert.inc instead of manual 'if' statements.
    - Rename variable $ok to $check_position_of_commit_event, since that
      explains better what the variable does.
    - Clarify purpose of the test.
    
    @mysql-test/extra/rpl_tests/rpl_insert_ignore.test
    - Test failure fix:
      The test asserted that there was a query_log_event after a specific
      number of events in the binary log. Since the number of events has
      changed, this number has to be updated.
    - The test case used to have a special case for gtid_mode=on, handled
      by extra/rpl_tests/rpl_insert_ignore_gtid_on.inc
      Since there is now no difference in event count between gtid_mode=on
      and gtid_mode=off, we can hardcode the number again and do not need
      the include file. Removed the include file.
    
    @mysql-test/extra/rpl_tests/rpl_insert_ignore_gtid_on.inc
    - Remove this file as it is not needed any more.
      See changeset comment for
      mysql-test/extra/rpl_tests/rpl_insert_ignore.test
    
    @mysql-test/extra/rpl_tests/rpl_log.test
    - Update the LIMIT clause for SHOW BINLOG EVENTS because
      the number of events in the binlog has changed. Now we can
      unify this instead of having different cases for GTID_MODE=ON
      and GTID_MODE=OFF.
    
    @mysql-test/extra/rpl_tests/rpl_row_show_relaylog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_show_binlog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_show_log_events_with_varying_options.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/rpl_sp.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/extra/rpl_tests/rpl_start_stop_slave.test
    - Disable a part of the test that crashes MTS, which will be fixed in a
      separate bug.
    
    @mysql-test/extra/rpl_tests/rpl_stm_mix_show_relaylog_events.inc
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/extra/rpl_tests/type_conversions.test
    - Provide more debug info if $rpl_debug is set, but do not output lots
      of junk to the result file if $rpl_debug is unset.
    
    @mysql-test/include/assert_grep.inc
    - New auxiliary test script to be used instead of
      include/grep_pattern.inc
    
    @mysql-test/include/assert_gtid_mode_on.inc
    - New auxiliary file that causes the test to fail if gtid_mode is not
      ON. This was added to avoid similar bugs to the one found in
      rpl_incompatible_gtids_in_relay_log.test (see commit comments for that
      file for details). This file is sourced from the auxiliary files
      include/sync_*.inc
    
    @mysql-test/include/filter_file.inc
    - Add parameter to allow masking a given set of columns. This is needed
      to implement the new $show_binlog_events_mask_columns parameter of
      show_binlog_events.inc.
    
    @mysql-test/include/get_row_count.inc
    - New auxiliary file used by
      mysql-test/extra/binlog_tests/mix_innodb_myisam_binlog.test.
    
    @mysql-test/include/grep_pattern.inc
    - Add $grep_output parameter, to allow different modes of output. This
      was needed in order to fix a bug in
      mysql-test/extra/rpl_test/rpl_large_serverid.inc
      (see changeset comment for that file for details).
    - Move the file to mysql-test/include. This is a generic utility, and
      as such it belongs to mysql-test/include. mysql-test/extra/... is
      generally used for test-specific includes.
    - Use mtr variables instead of environment variables for parameters.
    - Remove extra newline that was printed after each output row.
    - Improve the output: s/Occurrences of the $pattern/Occurrences of
      '$pattern'/
    - Suggest using include/assert_grep.inc
    
    @mysql-test/include/mysqlbinlog.inc
    - New test framework file to filter out nondeterministic output.
      (Before, similar filter regexes were repeated in lots of places all
      over the test suite.)
    
    @mysql-test/include/rpl_change_topology_helper.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    - Provide more debug info.
    
    @mysql-test/include/rpl_init.inc
    - Allow user to override $use_gtids=1 when gtid_mode=on.
    
    @mysql-test/include/save_io_thread_pos.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/save_master_pos.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/show_binlog_events.inc
    - Add missing documentation for existing parameter
      $mask_binlog_commit_events.
    - Add documentation for new parameters $show_binlog_events_verbose and
      $show_binlog_events_mask_columns (see show_events.inc for details).
    
    @mysql-test/include/show_events.inc
    - Change SQL commands to UPPERCASE.
    - Add $show_binlog_events_verbose parameter that will print statement
      with positions and filenames masked.
    - Add output of full statement if $rpl_debug is set.
    - Add parameter to allow masking out a given set of columns. This
      is needed in order to fix and simplify
      mysql-test/extra/binlog_tests/binlog.test
    - Mask out Anonymous_Gtid so that output is the same whether GTID_MODE
      is ON or OFF.
    - Correct a variable name, s/sidno/gno/.
    
    @mysql-test/include/show_rpl_debug_info.inc
    - Select from P_S tables.
    - Add $rpl_topology to output.
    
    @mysql-test/include/sync_slave_io.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    
    @mysql-test/include/sync_slave_sql.inc
    - Assert that gtid_mode=on if $use_gtids is set
      (see motivation in changeset comment for assert_gtid_mode_on.inc).
    - Fix typo to produce correct output on timeout.
    
    @mysql-test/include/wait_for_query_to_succeed.inc
    - Document the file.
    - Write debug info when it fails.
    
    @mysql-test/include/write_result_to_file.inc
    - Print perl's error text ($!) on error.
    - Print $stmt on error.
    
    @mysql-test/r/[1;31mflush[m_block_commit_notembedded.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/r/mysqlbinlog.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/r/user_var-binlog.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row_innodb.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_row_myisam.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_mysqlbinlog_start_stop.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_row_ctype_ucs.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_gtid_stm_ctype_ucs.result
    - Update result file because test now uses include/mysqlbinlog.inc.
    
    @mysql-test/suite/binlog/r/binlog_hexdump.result
    - Update result file because output of mysqlbinlog has changed.
    
    @mysql-test/suite/binlog/r/binlog_implicit_commit.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_innodb.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_mts_logical_clock_gtid.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_innodb.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_myisam.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_row_trans.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_mysqlbinlog_start_stop.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-
    test/suite/binlog/r/binlog_mysqlbinlog_start_stop_slave_server_id.result
    - Update result file since the test uses include/mysqlbinlog.inc instead
      of $MYSQL_BINLOG.
    
    @mysql-test/suite/binlog/r/binlog_rewrite_suppress_use.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/binlog/r/binlog_row_binlog.result
    - Update result file since we now use show_binlog_events.inc instead of
      SHOW BINLOG EVENTS. show_binlog_events.inc filters out
      Format_description_log_events.
    
    @mysql-test/suite/binlog/r/binlog_row_ctype_ucs.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_row_insert_select.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_row_mix_innodb_myisam.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_stm_binlog.result
    - Update result file because of changes in test.
    
    @mysql-test/suite/binlog/r/binlog_stm_ctype_ucs.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/binlog/r/binlog_stm_insert_select.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/binlog/r/binlog_stm_mix_innodb_myisam.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    - Also an assert was added.
    
    @mysql-test/suite/binlog/r/binlog_unsafe.result
    - Update result file because assert was added.
    
    @mysql-test/suite/binlog/t/binlog_killed.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    - Removed useless replace_result.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_mts_logical_clock_gtid.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_mysqlbinlog_concat.test
    - This test started failing because of this worklog. The test generates
      three binary logs: one with GTID_MODE=ON, one with GTID_MODE=OFF, and
      one with GTID_MODE=ON. Then it runs mysqlbinlog and executes the
      output from mysqlbinlog on a server that uses GTID_MODE=ON.
      Before this worklog, this did not cause any problems, because the
      binary log generated with GTID_MODE=OFF did not contain any GTIDs.
      Now, it contains Anonymous_gtid_log_event, which is not allowed when
      GTID_MODE=ON.
      To fix this, we use the --skip-gtids with mysqlbinlog when processing
      the second binary log file.
    
    @mysql-test/suite/binlog/t/binlog_mysqlbinlog_row_trans.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/binlog/t/binlog_rewrite_suppress_use.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/binlog/t/binlog_row_query_log_events.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/binlog/t/binlog_server_id.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/rpl/r/rpl_begin_commit_rollback.result
    - Updated result file because of changes in test file.
    
    @mysql-test/suite/rpl/r/rpl_do_db_filter.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_filter_warnings.result
    - Update result because output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_gtid_binlog_errors.result.THIS
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/r/rpl_gtid_mode.result
    - Update result file because of changes in test file and in output
      from SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/r/rpl_gtid_row_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_gtid_stm_mix_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_ignore_db_filter.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_loaddata_s.result
    - Update result file because output from SHOW BINLOG EVENTS has changed.
      Before, the result set was empty. Now, it contains one row with a
      Previous_gtids_log_event. But the row is filtered out by
      show_binlog_events.inc. The end result is that the column headers are
      added.
    
    @mysql-test/suite/rpl/r/rpl_mixed_implicit_commit_binlog.result
    - Update result file.
    
    @mysql-test/suite/rpl/r/rpl_mts_logical_clock_timestamping.result
    - Update result since output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_mts_logical_clock_wrong_start_pos.result
    - Update result because of change in test.
    
    @mysql-test/suite/rpl/r/rpl_mysqlbinlog_gtid_on.result
    - Update result.
    
    @mysql-test/suite/rpl/t/rpl_recovery_replicate_same_server_id.result
    - Update result file because of clarifications in the main test file.
    
    @mysql-test/suite/rpl/r/rpl_replicate_same_server_id.result
    - Update result file because of clarifications in the main test file.
    - Rename the file since this really tests replicate-same-server-id.
    
    @mysql-test/suite/rpl/r/rpl_replication_observers_example_plugin.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-
    test/suite/rpl/r/rpl_replication_observers_example_plugin_io.result
    - Update result because grep_pattern.inc output was changed.
    
    @mysql-test/suite/rpl/r/rpl_row_event_max_size.result
    - Update result file because binlog now contains
      Anonymous_gtids_log_event.
    
    @mysql-test/suite/rpl/r/rpl_row_ignorable_event.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_row_implicit_commit_binlog.result
    - Update result file because assert was added.
    
    @mysql-test/suite/rpl/r/rpl_row_mts_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_row_mysqlbinlog.result
    - Update result because test uses the new mysqlbinlog.inc file.
    
    @mysql-test/suite/rpl/r/rpl_row_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_row_until.result
    - Update result file because of simplification in main file.
    
    @mysql-test/suite/rpl/r/rpl_server_id.result
    - Renamed this file. No need to have a numeric suffix.
    
    @mysql-test/suite/rpl/r/rpl_server_uuid.result
    - Update result file because test file was changed.
    
    @mysql-test/suite/rpl/r/rpl_show_relaylog_events.result
    - Result file for new test.
    
    @mysql-test/suite/rpl/r/rpl_skip_slave_err_warnings.result
    - Update result since output from grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/r/rpl_sp_innodb.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_sp_myisam.result
    - Update result file because output of mysqlbinlog has changed and we
      now use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG'.
    
    @mysql-test/suite/rpl/r/rpl_stm_implicit_commit_binlog.result
    - Update result file because assert was added in the test file.
    
    @mysql-test/suite/rpl/r/rpl_stm_mix_mts_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_stm_mix_show_relaylog_events.result
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/r/rpl_typeconv.result
    - Update result file because the test file was simplified.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback-master.opt
    - Rename database to make test more readable.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback-slave.opt
    - Rename database to make test more readable.
    
    @mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
    - Test failure fix:
      The test did a START SLAVE UNTIL MASTER_LOG_POS, where
      MASTER_LOG_POS was one byte into a transaction. With no
      Gtid_log_event, that will round the position up so that the entire
      transaction is exeucted on slave. However, if the position is one byte
      into a Gtid_log_event, it will instead stop before the transaction has
      been committed. This caused the test to fail. The fix is to set
      MASTER_LOG_POS to one byte into the BEGIN query_log_event instead.
    - Cosmetic fix:
      added comments to explain what the test does
    - Improve debugging:
      use assertions instead of 'eval SELECT $result as 'Must be 0'
    - Cleanup:
      The test did:
        echo [on master]
        SET SESSION AUTOCOMMIT=0;
      Since there was no semicolon after the echo, the SET statement
      was never executed, only echoed. This was very confusing, and
      the test would fail if we actually set autocommit=0 here. So
      removed this.
    - Cosmetic fix:
      removed redundant DROP DATABASE IF EXISTS at the beginning of the test
    - Cosmetic fix:
      renamed databases to make test more readable:
      db1 -> replicate_do_db
      db2 -> binlog_ignore_db
    - Cosmetic fix:
      replaced
        connection master;
        echo [on master];
      by
        --source include/rpl_connection_master.inc
    - Cosmetic fix:
      use uppercase for SQL in some cases
    
    @mysql-test/suite/rpl/t/rpl_binlog_errors.test
    - Change LIMIT clause of SHOW BINLOG EVENTS to compensate for adding two
      more events.
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_binlog_errors.test
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_dual_pos_advance.test
    - Fix failing test:
      because the number of events has changed, we change the LIMIT options
      for SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/t/rpl_filter_warnings.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_grant_plugin.test
    - The code assumed a specific number of events in the binlog, and had a
      case distinction between GTID_MODE=ON or OFF. Simplified this since we
      now expect the same number of events always.
    
    @mysql-test/suite/rpl/t/rpl_gtid_binlog_errors-master.opt
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_gtid_binlog_errors.test
    - Now, rpl_binlog_errors.test and rpl_gtid_binlog_errors.test
      have identical result files. Having different result files was
      the only reason for separating them into different tests. Thus,
      reconciled them into one test by removing the .test files (which
      were just wrappers), replacing them by the .inc file, and removing
      all the rpl_gtid_binlog_error* files.
    
    @mysql-test/suite/rpl/t/rpl_gtid_drop_table.test
    - Fix typos in comment.
    - Simplify test assertion.
    
    @mysql-test/suite/rpl/t/rpl_gtid_mode.test
    - Fix failing test:
      because the number of events has changed, we change the LIMIT options
      for SHOW BINLOG EVENTS.
    
    @mysql-test/suite/rpl/t/rpl_gtid_row_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_gtid_stm_mix_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_mts_logical_clock_timestamping.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_mts_logical_clock_wrong_start_pos.test
    - Mask out positions from result file.
    
    @mysql-test/suite/rpl/t/rpl_mysqlbinlog_gtid_on.test
    - Add comments explaining the purpose of the test.
    - Remove last part of the test. The tested behavior is not intended, and
      this part of the test failed.
    
    @mysql-test/suite/rpl/t/rpl_recovery_replicate_same_server_id.test
    - Use include/assert_grep.inc instead of grep_pattern.inc since
      grep_pattern.inc has changed.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id-master.opt
    - Rename this file to better reflect what is being tested.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id-slave.opt
    - Rename this file to better reflect what is being tested.
    
    @mysql-test/suite/rpl/t/rpl_replicate_same_server_id.test
    - Rename this file to better reflect what is being tested.
    - Add comments to explain what is being tested.
    - Upgrade to our current coding standards.
    - Remove a possible race.
    
    @mysql-test/suite/rpl/t/rpl_replication_observers_example_plugin.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/rpl/t/rpl_replication_observers_example_plugin_io.test
    - Update test since grep_pattern.inc has moved and changed.
    
    @mysql-test/suite/rpl/t/rpl_row_ignorable_event.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/rpl/t/rpl_row_mts_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_row_mysqlbinlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/suite/rpl/t/rpl_row_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_row_until.test
    - Read master position using SHOW MASTER STATUS instead of
      SHOW BINLOG EVENTS, because SHOW BINLOG EVENTS requires that you
      give the exact number of events.
    - Use replace_result to filter out exact positions, instead of echoing
      the filtered statement and disabling the query log while executing the
      statement.
    
    @mysql-test/suite/rpl/t/rpl_server_id.test
    - Improve and clarify the comment.
    - Change the name of the test.
    
    @mysql-test/suite/rpl/t/rpl_server_uuid.test
    - Improvements in readability, used when debugging the test. (Eventually
      the fix was elsewhere but we may as well keep the improvements.)
    
    @mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    - Simplify the test suite:
      This family of tests contained a lot of duplications and
      confusions:
      - The following tests existed in the suite:
          rpl.rpl_stm_mix_show_relaylog_events
          rpl.rpl_stm_mix_gtid_show_relaylog_events
          rpl.rpl_stm_mix_mts_show_relaylog_events
          rpl.rpl_row_show_relaylog_events
          rpl.rpl_row_gtid_show_relaylog_events
          rpl.rpl_row_mts_show_relaylog_events
      - The *_mts_* tests were not specific to mts, they were specific to
        innodb. The only difference was that their result files differed on
        the commit events, which were Xid_log_events for MTS and
        query_log_event(COMMIT) events for non-MTS.  There is a flag in
        show_binlog_events.inc to mask Xid events so that they look like
        query_log_events, so we can merge them to one file.
      - The *_gtid_* tests were only different because there was a
        Gtid_log_event when GTID_MODE=ON and no event when GTID_MODE=OFF.
        Now we always have an event, so we can merge the files together.
      - The *_stm_mix_* and *_row_* tests were only different because there
        was DML in the binlogs.  There is no need for DML in this test case,
        it's enough to test with DDL, so we can merge these files together
        too.
      - So now that the output has been unified, we only need one test file
        and one result file.
      - The test used four levels of include files. This was redundant
        and made it very difficult to follow the logic. The test now
        invokes include/show_relaylog_events.inc directly, which
        makes it easier to understand what is going on.
      - The test used show_binlog_events.inc. This was unnecesary since
        the purpose is to test SHOW RELAYLOG EVENTS. Other tests test
        SHOW BINLOG EVENTS. So I have removed SHOW BINLOG EVENTS.
    - Fix test failure:
      The tests were failing because the output of SHOW RELAYLOG EVENTS
      has changed because Gtid_log_event is now generated.
    
    @mysql-test/suite/rpl/t/rpl_skip_slave_err_warnings.test
    - Update test because grep_pattern.inc was moved and altered.
    
    @mysql-test/suite/rpl/t/rpl_stm_mix_mts_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/suite/rpl/t/rpl_stm_mix_show_relaylog_events.test
    - Delete this file. See changeset comment for the file that is
      being renamed from
      mysql-test/extra/rpl_tests/rpl_show_relaylog_events.inc to
      mysql-test/suite/rpl/t/rpl_show_relaylog_events.test
    
    @mysql-test/t/mysqlbinlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @mysql-test/t/sp_trans_log.test
    - Change LIMIT clause of SHOW BINLOG EVENTS since binlog now
      contains two new events.
    
    @mysql-test/t/user_var-binlog.test
    - Use mysqlbinlog.inc instead of 'exec $MYSQL_BINLOG', to filter out
      GTIDs and other nondeterministic output.
    
    @sql/binlog.cc
    - Make simplified-gtid-recovery stop iteration over binary logs in the
      case that the first binary log contains a Previous_gtids_log_event.
    - Improve English in a comment.
    
    @sql/log_event.cc
    - Document logic for sql_slave_skip_counter.
    - Remove printing of logical timestamps from Query_log_event. The code
      for these timestamps will be removed in the next patch, but we remove
      the output already in this patch so that the necessary upates of
      tests get included in this patch.
    - Fix bug in Gtid_log_event for sql_slave_skip_counter, by implementing
      Gtid_log_event::do_shall_skip and make it call continue_group.
    
    @sql/log_event.h
    - Implement Gtid_log_event::do_shall_skip.
    
    @sql/rpl_rli.cc
    - Fix bug in START SLAVE UNTIL MASTER_LOG_POS.
    
    @sql/rpl_rli.h
    - Document logic and purpose of is_in_group.
    
    @mysql-test/suite/engines/funcs/t/disabled.def
    - Disable rpl_row_until due to BUG#20365935 (which is unrelated to this worklog).

[33mcommit aaabe8c5ce88fe3b56b46f07a6cfa32cc8b84bd6[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:12:46 2014 +0100

    WL#7592 step 8. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Generate Gtid_log_event_always.
    
    After all the previous preparation steps, we can now generate
    Gtid_log_event always. This does change server behavior. Moreover, it
    breaks many existing test cases that assume there is no Gtid_log_event.
    However, since we have another big change to make (generate
    Previous_gtids_log_event always), we do not touch tests in this patch.
    Later patches will fix the tests.
    
    This patch has three primary goals:
    
     1. Remove Gtid_log_event from the statement/transaction cache.
     2. Generate Gtid_log_event just before [1;31mflush[ming the
        statement/transaction cache. Store the event in a separate buffer.
     3. Generate Gtid_log_event even when GTID_MODE=OFF.
    
    In order to make this work, we have to do the following additional
    tasks:
    
     4. Previously, MTS logical timestmaps were generated when [1;31mflush[ming the
        cache. Since Gtid_log_event is not part of the cache now, we
        generate them in Gtid_log_event.
     5. Since Anonymous_log_events now exist, a case must be added to
        MTS code so that it reads the logical timestamps from the
        ANONYMOUS_GTID_LOG_EVENT just like it reads from the GTID_LOG_EVENT.
    
    In addition we do the following cleanup tasks:
    
     6. Remove binlog.cc:gtid_before_write_cache. This functionality is now
        in the new function MYSQL_BIN_LOG::write_gtid. In addition, we move
        the part of gtid_before_write_cache that deals with generating the
        GTID into the new function Gtid_state::generate_automatic_gtid.
     7. Avoid taking a lock in Gtid_log_event constructor.
     8. Remove the '[commit=yes/no]' output when mysqlbinlog prints a
        Gtid_log_event. This was only relevant for a pre-GA version of
        the GTID feature.
    
    @sql/binlog.cc
    - Fix 4: Remove generation of commit_seq_no from Binlog_event_writer.
      This means we do not need the member variables
      Binlog_event_writer::thread_cache or
      Binlog_event_writer::is_first_event any more so we remove them too.
    - Fix 1: Remove generation of Gtid_log_event from
      binlog_cache_data::write_event.
    - Fix 6: Implement MYSQL_BIN_LOG::write_gtid, which does the following
      things:
      - Generate the GTID by calling Gtid_state::generate_automatic_gtid.
        generate_automatic_gtid replaces logic that was previously located
        in gtid_before_write_cache and Group_cache::generate_automatic_gno.
      - Generate Gtid_log_event, which was previously done in
        binlog_cache_data::write_event.
      - Generate the logical timestamps for MTS, and pass them to the
        Gtid_log_event constructor. The logic for generating commit_seq_no
        was previously located in binlog_cache_data::finalize.
    - Fix 2,3: Replace call to gtid_before_write_cache by call to
      MYSQL_BIN_LOG::write_gtid.
    - Fix 4: Remove generation of MTS logical timestamps from
      binlog_cache_data::finalize.
    
    @sql/binlog.h
    - Add member function write_gtid.
    
    @sql/log_event.cc
    - Fix 7: Change Gtid_log_event::Gtid_log_event(THD*...) constructor to
      avoid taking lock, to rely on what is owned by the thread rather than
      a given Gtid_specification.
    - Fix 4: Change Gtid_log_event::Gtid_log_event(THD*...) to take the MTS
      logical timestamps as parameters.
    - Fix 8: Remove "[commit=yes/no]" output when mysqlbinlog prints a
      Gtid_log_event.
    - Fix 4: In Gtid_log_event::write_data_header, do not store
      commit_seq_offset in IO_CACHE::commit_seq_offset. Also, instead of
      storing placeholders for the logical timestamps, store the correct
      values directly. (Previously, the logical timestamps were not known at
      the time Gtid_log_event::write_data_header_to_memory was called. Now
      that we moved the writing of the Gtid_log_event to the end of the
      transaction it is known). Also remove the IO_CACHE* parameter.
    
    @sql/log_event.h
    - Clarify a comment.
    - Fix 4: Update the prototype for Gtid_log_event constructor.
    - Fix 4: Remove the IO_CACHE* parameter to write_to_memory. This was
      only needed because the logical timestamps were stored in the
      IO_CACHE; since we do not store thm in the IO_CACHE any more, this
      parameter is not needed now.
    - Fix 2: Define the constant Gtid_log_event::MAX_EVENT_LENGTH, which is
      used in MYSQL_BIN_LOG::write_gtid for the size of the buffer that
      holds the Gtid_log_event.
    
    @sql/rpl_gtid.h
    - Fix 6: Declare the new function generate_automatic_gtid.
    
    @sql/rpl_gtid_state.cc
    - Fix 6: Implement the new function generate_automatic_gtid. This was
      previously done in binlog.cc:gtid_before_write_cache, but it is
      better to have it here since the function is not dependent on the
      binary log.
    
    @sql/rpl_mts_submode.cc
    - Fix 5: Now that ANONYMOUS_GTID_LOG_EVENT is generated, MTS has
      to be able to read the commit_seq_no from it.

[33mcommit 33ef855d4aaee178dfbbcd4a51ba18c611c88c71[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:10:40 2014 +0100

    WL#7592 step 7. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Refactor MYSQL_BINLOG::do_write_cache.
    
    This patch rewrites do_write_cache so that it becomes more maintainable
    and so that we can write out-of-cache events to the binary log. This
    does not change server behavior, so it is not possible to have a test
    case for this.
    
    Rationale 1:
    Before this patch, do_write_cache mixes two tasks:
     1. It reads from the statement or transaction cache and assembles
        pieces of events that are split over multiple pages in the
        IO_CACHE.
     2. It processes the event and writes it to the binary log.
    
    In WL#7592, we need to store the Gtid_log_event outside the statement
    and transaction caches. The event needs the processing in (2), but not
    that in (1). Hence, we de-couple these two tasks. Task (2) is
    implemented by a new class and task (1) remains in do_write_cache.
    
    Rationale 2:
    
    Before this patch, do_write_cache was very difficult to understand, as
    it kept lots of state in variables that were updated throughout the
    function. The reason was that it was organized in an unpractical way:
    the outermost loop iterated over pages of the IO_CACHE and tried to keep
    various pieces of state of half-written events in local variables. This
    state information was updated all over the function, which made it
    difficult to understand what were the loop invariants.
    
    This patch changes so that the outermost loop iterates over events. This
    makes the state information is much more short-lived. For instance,
    state related to the beginning of an event is only needed at the
    beginning of the iteration and can be stored in a very short-lived
    variable. So there is less state kept between iterations, and this makes
    the loop invariants much simpler.
    
    @sql/binlog.cc
    - Add auxiliary class Binlog_event_writer, which handles fixing the
      fields before writing to the binlog.
    - Remove fix_log_event_crc, which was only doing trivial things and
      is now part of Binlog_event_writer.
    - Add auxiliary function read_cache_page used by do_write_cache.
    - Change do_write_cache to use Binlog_event_writer and be more
      maintainable.
    - Declare the Binlog_event_writer a couple of levels up in the call
      stack, in binlog_cache_data::[1;31mflush[m, and pass it as a parameter to
      write_cache and do_write_cache. The Binlog_event_writer is needed
      in binlog_cache_data::[1;31mflush[m in the next patch.
    - Move DBUG_EXECUTE_IF("simulate_binlog_[1;31mflush[m_error", ...) out of
      write_cache, into the caller. This is important since this is
      supposed to be executed prior to writing anything in the transaction.
      Before the patch, write_cache was the only function that wrote the
      transaction. However, after next patch, Gtid_log_event will live
      outside the cache and will be written to the binary log before we call
      write_cache, so then the error simulation must happen before writing
      the Gtid_log_event.
    
    @sql/binlog.h
    - Fix whitespace in declaration of wait_for_update_bin_log.
    - Make do_write_cache private since there is no reason for it to be
      public.
    - Add Binlog_event_writer parameter to do_write_cache and write_cache.

[33mcommit f0033795cd21eeb56927867731704d0de66c4c5e[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:08:54 2014 +0100

    WL#7592 step 6. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Allow writing Log_event header to memory.
    
    This is a small refactoring to allow writing Gtid_log_event to a memory
    buffer rather than to an IO_CACHE. This does not change the behavior of
    the server, so it is not possible to have a test case for this.
    
    When, in a later patch, we write Gtid_log_event always, the
    Gtid_log_event will live outside the transaction cache. Before the event
    is [1;31mflush[med to the binary log, it has to be stored in a memory buffer.
    However, there was no function for writing log events to a memory
    buffer. This patch adds the function Log_event::write_header_to_memory
    that writes the 19 byte Log_event header to a memory buffer, and
    Gtid_log_event::write_to_memory that writes an entire Gtid_log_event to
    a memory buffer.
    
    @sql/log_event.cc
    - Move code for writing the Log_event header out of Log_event::write and
      into the new function Log_event::write_header_to_memory.
    - Move body of Gtid_log_event::write_data_header into
      new function Gtid_log_event::write_data_header_to_memory.
    - Simplify the logic for clearing flags&~LOG_EVENT_BINLOG_IN_USE_F
      before generating the checksum.
    
    @sql/log_event.h
    - Add Log_event::write_header_to_memory.
    - Add Gtid_log_event::write_header_to_memory.
    - Add Gtid_log_event::write_to_memory.
    - Make Log_event::write_header, Log_event::write_footer,
      Log_event::need_checksum, and Gtid_log_event::print private,
      since there is no reason for them to be public.

[33mcommit 08e3bd904b9185c464cd9b249e32a744ba12cf86[m
Author: Sven Sandberg <sven.sandberg@oracle.com>
Date:   Wed Nov 12 15:05:38 2014 +0100

    WL#7592 step 3. GTIDs: Generate Gtid_log_event and Previous_gtids_log_event always. Clean up binlog.cc:gtid_empty_group_log_and_cleanup.
    
    This only refactors and cleans up the function, to make it easier to
    understand what it does. This does not change the server behavior, so it
    is not possible to have a test case for this.
    
    Address multiple issues in this function:
    - Add comments.
    - The function was only called under the condition that
      thd->owned_gtid.sidno != 0 &&
      thd->variables.gtid_next.type==GTID_GROUP &&
      opt_log_bin && opt_log_slave_updates. These checks were repeated in
      all places where the function was called, so moved the checks into the
      function.
      The check that gtid_next.type==GTID_GROUP was redundant, since
      owned_gtid.sidno is only != 0 if gtid_next.type==GTID_GROUP, so
      changed that into an assertion in the function.
    - This function was called from sql_class.cc. Here, next to the call,
      there was some logic for inserting GTIDs into table in case the binary
      log is off.
      Moved this logic into the function, because (1) this is not a
      replication file, so better to minimize replication logic in it; (2)
      this handles the case of empty transactions, so it seems natural to do
      it inside the function.
    - Rename the function and move it into the MYSQL_BIN_LOG class.
    - Remove the call to gtid_before_write_cache(): this is done in any
      case, at a later point in time, since gtid_empty_group_log_and_cleanup
      calls
        mysql_bin_log.commit(THD*,bool), which calls
        MYSQL_BIN_LOG::ordered_commit(THD*,bool,bool), which calls
        process_[1;31mflush[m_stage_queue(my_off_t*,bool*,THD**), which calls
        [1;31mflush[m_thread_caches(THD*), which calls
        binlog_cache_mngr::[1;31mflush[m(THD*,my_off_t*,bool*), which calls
        binlog_cache_data::[1;31mflush[m(THD*,my_off_t*,bool*), which calls
        gtid_before_write_cache().
    
    @sql/binlog.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    
    @sql/binlog.h
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Comment the function.
    
    @sql/log_event.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Move the checks for thd->variables.gtid_next.type,
      thd->owned_gtid.sidno, opt_bin_log, and opt_log_slave_updates
      into gtid_end_transaction.
    
    @sql/sql_parse.cc
    - Replace gtid_empty_group_log_and_cleanup by
      MYSQL_BIN_LOG::gtid_end_transaction.
    - Move the checks for thd->variables.gtid_next.type,
      thd->owned_gtid.sidno, opt_bin_log, and opt_log_slave_updates
      into gtid_end_transaction.
    - Move the code for storing GTIDs into table into gtid_end_transaction.

[33mcommit e9d61075009b33dfa2a137e8e3f42f7f2dadc3c3[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Jan 20 10:39:05 2015 +0200

    Clarify comments wrt [1;31mflush[ming and ALTER progress
    
    WL#5889 Add InnoDB events to Performance Schema's Event Stage table

[33mcommit 9dab9dad975d09b8f37f33bf3c522d36fdf1d0f9[m
Author: Joao Gramacho <joao.gramacho@oracle.com>
Date:   Sun Jan 11 18:40:01 2015 +0000

    BUG#17943188 SHOW SLAVE STATUS/RETRIEVED_GTID_SET MAY HAVE PARTIAL TRX
                 OR MISS COMPLETE TRX
    
    Problem:
    =======
    
    The SHOW SLAVE STATUS command contains the column RETRIEVED_GTID_SET.
    This is supposed to contain the set of GTIDs that exist in the relay
    log. However, the field is updated when the slave receiver thread
    (I/O thread) receives a Gtid_log_event, which happens at the beginning
    of the transaction.
    
    If the I/O thread gets disconnected in the middle of a transaction,
    RETRIEVED_GTID_SET can contain a GTID for a transaction that is only
    partially received in the relay log. This transaction will
    subsequently be rolled back, so it is wrong to pretend that the
    transaction is there.
    
    Typical fail-over algorithms use RETRIEVED_GTID_SET to determine which
    slave has received the most transactions to promote the slave to a
    master. This is true for e.g. the mysqlfailover utility.
    
    When RETRIEVED_GTID_SET can contain partially transmitted transactions,
    the fail-over utility can choose the wrong slave to promote. This can
    lead to data corruption later.
    
    This means that even if semi-sync is enabled, transactions that have
    been acknowledged by one slave can be lost.
    
    Fix:
    ===
    
    It was implemented a transaction boundaries parser that will give
    information about transaction boundaries of an event stream based on
    the event types and their queries (when they are Query_log_event).
    
    As events are queued by the I/O thread, it feeds the Master_info
    transaction boundary parser. The slave I/O recovery also uses the
    transaction parser to determine if a given GTID can be added to the
    Retrieved_Gtid_Set or not.
    
    When the event parser is in GTID state because a Gtid_log_event was
    queued, the event's GTID isn't added to the retrieved list yet.
    It is stored in an auxiliary GTID variable.
    
    After [1;31mflush[ming an event into the relay log, the IO thread verifies if
    the transaction parser is not inside a transaction anymore (meaning
    that the last event of the transaction has been [1;31mflush[med).
    
    If transaction parser is outside a transaction, the I/O thread
    verifies if a GTID was stored in the start of the transaction, adding
    it to the retrieved list, ensuring that all the transaction has arrived
    and was [1;31mflush[med to the relay log.
    
    Also, before this patch, after the I/O thread [1;31mflush[med a single received
    event into the relaylog, it was possible to rotate the relaylog if the
    current relaylog file size exceeded max_binlog_size/max_relaylog_size.
    After this patch, when GTIDs are enabled we only allow this rotation by
    size if the transaction parser is not in the middle of a transaction.
    
    Note: The current patch removed the changes for BUG#17280176, as it
          also dealt with similar problem in a different way.

[33mcommit acb1193b3753dad0931d8eef45e8a348823bc9c2[m
Merge: 20a884833f6 6ca9b51d2f7
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu Jan 15 11:52:15 2015 +0200

    Merge commit 'e90f0f0' into mysql-trunk-wl5889
    
    * commit 'e90f0f0':
      WL#7868: InnoDB: Improvements around [1;31mflush[ming for proper performance
    
    Conflicts:
            storage/innobase/log/log0log.cc

[33mcommit 6ca9b51d2f749b10b9de3fcf3c0b15a056a4df1c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Jan 15 13:16:38 2015 +0900

    WL#7868: InnoDB: Improvements around [1;31mflush[ming for proper performance
    
    1: Implementing improvements to the adaptive [1;31mflush[ming algorithm
    
    1-a: should consider the distribution about the oldest LSN of the pages
    in the
    end of the [1;31mflush[m_list
    
    1-b: should consider [1;31mflush[ming balance for each buffer pool instances
    
    2: Setting a thread priority for the page_cleaner (if the platform
    allows and
    authorized to set)
    
    3: Proper [1;31mflush[m waiting when the max modified LSN age is around
    max_modified_age_sync
    
    3': should block the checkpoint LSN overwritten
    
    Reviewed-by: Sunny Bains <Sunny.Bains@oracle.com>
    Reviewed-by: Shaohua Wang <shaohua.wang@oracle.com>
    
    RB: 7444 (4881 for bzr)

[33mcommit a70baeceeebce19702dbe397b0efa26c88f1f251[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Dec 17 14:31:35 2014 +0200

    Instrument the [1;31mflush[m during index creation
    
    WL#5889 Add InnoDB events to Performance Schema's Event Stage table
    
    This is an adjust after 51ebea7 was merged into the mysql-trunk-wl5889
    branch.

[33mcommit b52a531b4da50993ad158592fab77a787096626f[m
Author: Robert Golebiowski <robert.golebiowski@oracle.com>
Date:   Mon Dec 15 15:55:19 2014 +0100

    Bug #19309652 MAGICALLY REAPPEARING GRANT OPTION FOR PROXY
    
    Description:
    -----------
    The GRANT PROXY syntax is overloaded to accept a WITH GRANT OPTION
    modifier,
    which allows the specified user to issue GRANT PROXY privileges for the
    proxied user account.  When GRANT PROXY is issued against an existing
    proxy/proxied user combination, the operation appears to succeed, but
    apparently only affects the in-memory privileges.  Consequentially,
    while the
    WITH GRANT OPTION appears to have been honored, it is reverted the next
    time
    FLUSH PRIVILEGES is issued, or the server is restarted
    
    Fix:
    ------------
    Option with_grant was not saved to the disk  when the proxy grant was
    updated. This is why it was as if it had not changed at all after
    [1;31mflush[ming the privilages. The fix makes sure that with_grant gets stored
    on the disc.
    
    Testing:
    ------------
    Added new test: i_main.grant-bug19309652

[33mcommit 570cbd5bedc54c96bbea6287883ea3683e54bc85[m
Author: Krunal Bauskar <krunal.bauskar@oracle.com>
Date:   Mon Dec 15 15:14:11 2014 +0530

     Bug#20137435 ASSERT UT_LIST_GET_LEN(BUF_POOL->FLUSH_LIST) == 0 IN SHUTDOWN
    
     The last [1;31mflush[ming loop on shutdown is missing buf_[1;31mflush[m_wait_LRU_batch_end()
     that waits for [1;31mflush[m to complete.
    
     Reviewed by: Yasufumi Kinoshita (yasufumi.kinoshita@oracle.com)
     Approved over IM

[33mcommit f0c0859ffbdd54cceb5940647d89e7df7692c9d7[m
Author: Maitrayi Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed Dec 10 19:00:15 2014 +0100

    Bug 19895502 -  BINARY LOG GROUP COMMIT CAUSES REGRESSION FOR NDB REPLICATION.
    
    A call to ha_[1;31mflush[m_logs - introdueced by WL5223 in MYSQL_BIN_LOG::new_file_impl-
    causes a deadlock in the Ndb handler implementation of
    ha_[1;31mflush[m_logs(), because it is now called after Binlog is locked
    (Lock_log). This lock prevents ha_[1;31mflush[m_logs() from writing to Binlog
    causing deadlock.
    
    ha_[1;31mflush[m_logs is now used as storage engine checkpointing and thus not relevant
    for Ndb. Removing it.

[33mcommit a14762f1ed6245e0bb414309291b7d0f60fd6833[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Dec 10 13:31:57 2014 +0200

    Adjust the estimate of pages to be [1;31mflush[med
    
    WL#5889 Add InnoDB events to Performance Schema's Event Stage table
    
    Testing shows that pk_pages/2 is a better estimate than pk_pages.

[33mcommit 5817aa0fe864a4600005d787a7ec32b52674e628[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Fri Nov 14 11:41:28 2014 +0200

    Recalculate the work estimated before [1;31mflush[m
    
    Before starting the [1;31mflush[m we know how many pages are we going to [1;31mflush[m,
    and given that the estimate's units are pages, we reset the total work
    estimated to: work completed so far + pages to [1;31mflush[m.
    
    Also, update the estimate after reading PK after
    row_merge_read_clustered_index() has finished, not inside it.
    
    WL#5889 Add InnoDB events to Performance Schema's Event Stage table

[33mcommit e524829b3b650ae08d761d53993393e91611a561[m
Author: Daogang.qu <bill.qu@oracle.com>
Date:   Fri Nov 14 12:15:50 2014 +0800

    Bug#19424075    WRITE/SYNC REDO LOG BEFORE FLUSH THREAD CACHE TO BINLOG
    
    Post-fix: Rename 'rpl_group_commit_binlog_[1;31mflush[m_crash-master.opt'
    with 'binlog_group_commit_[1;31mflush[m_crash-master.opt'.

[33mcommit e669ad6dcf132e7998e480184e8676cec7ded6f7[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Nov 11 16:50:41 2014 +0800

    Bug#19424075    WRITE/SYNC REDO LOG BEFORE FLUSH THREAD CACHE TO BINLOG
    
    Removing system variable 'binlog_max_[1;31mflush[m_queue_time' broke
    sys_vars.session_track_system_variables_basic in some platforms.
    
    Post-fix: Restore the system variable 'binlog_max_[1;31mflush[m_queue_time'.

[33mcommit 437900222b5c9a0a89c038d68dd84763b26df701[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Wed Nov 5 17:06:00 2014 +0200

    Bug#19949683 SPEED UP FSP_GET_AVAILABLE_SPACE_IN_FREE_EXTENTS()
    
    fil_space_t: Add size_in_header, free_len, free_limit
    to cache FSP_SIZE, the length of the FSP_FREE list, and FSP_FREE_LIMIT.
    
    fsp_get_available_space_in_free_extents(): Instead of accessing
    the tablespace header page in the buffer pool, access the cached fields.
    This avoids I/O on tablespaces that are rarely written to.
    
    fsp_fill_free_list() and some other functions: Take a fil_space_t
    pointer instead of a numeric tablespace identifier. This avoids some
    lookups.
    
    fil_space_extend(): Renamed from fil_extend_space_to_desired_size()
    and simplified the API.
    
    undo::Truncate::was_tablespace_truncated(): A new method, to avoid
    a consistency check before [1;31mflush[ming a truncated undo tablespace file.
    
    rb#7224 approved by Jimmy Yang and Yasufumi Kinoshita

[33mcommit c60ac300ad2e5d1396694fb9f642b78b10bc3c93[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Tue Nov 4 10:28:26 2014 +0800

    Bug#19424075    WRITE/SYNC REDO LOG BEFORE FLUSH THREAD CACHE TO BINLOG
    
    According to the crash recovery logic of mysql server, we only
    need to guarantee that write/sync prepared transaction to InnoDB
    redo log before writing to binary log. And prepared records of
    transactions were not committed into InnoDB redo log in a group.
    
    To improve performance, write/sync prepared records of transactions
    to InnoDB redo log in a group right before writing to binary log
    during binlog group commit [1;31mflush[m stage.

[33mcommit 8561fa558e835f106c83395349da78f1fc0c0e30[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Thu Oct 30 13:23:41 2014 +0800

    BUG#19658698 - CORRUPTION IN COMPRESSED PAGE BUF_FLUSH_INIT_FOR_WRITING
    
    Regression of Bug#17332603 REMOVE BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH.
    Use skip_[1;31mflush[m_check other than setting FIL_PAGE_TYPE to avoid [1;31mflush[m check;
    N-fix incomplete page to avoid eviction.
    
    rb#7154 approved by Marko & Jimmy.

[33mcommit 9012c63ca6781a5d53a00873acc958cecbce271e[m
Author: Maitrayi Sabaratnam <maitrayi.sabaratnam@oracle.com>
Date:   Wed Oct 15 17:28:35 2014 +0200

    bug#19793475 - NO WAIT FOR EPOCHS THAT HAVE NO DB CHANGES MADE BY THIS NDB THD TO BE IN BINLOG
    
    Ensure the most recent data changes made by the current thread to Ndb
    (session_last_committed_epoch) instead of those made by all threads
    (server_last_committed_epoch) to be [1;31mflush[med to Binlog.

[33mcommit 4cdb5ab3cb99d710c521d668cd72b83300f02be9[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 24 15:37:38 2014 +0200

    Patch 2 of 3 for WL#7654: "Reduce lock contention in NDB mt-scheduler"
    
    Reduce of 'send_lock' contentions when worker thread
    is responsible for sending. (No dedicated send threads)
    
    The previous usage of send_lock, was to protect both
    the send_buffers, and the send-transporters while they are
    sending the send_buffer contents. This caused contention
    when we had to grab the lock for packing send buffers, while
    another thread were sending on the transporter.
    
    This has been refactor by introducing an additional lock
    which protects the send buffers (only). The usage of
    the existing lock is changed to only protect the transporter
    and buffers 'given' to it.
    
    In order to support the above, we also introduce another
    list of available send_buffers: 'struct thr_send_buffer m_sending'.
    Buffers in this list is 'owned' by the send transporter, and protected
    by the send_lock.
    
    When we prepare for sending, available send buffers are collected,
    and moved from 'm_buffer' to 'm_sending'. (Requires both lock)
    The 'm_buffer_lock' can then be released prior to sending which
    removes the contention from buffer packing.
    
    Furthermore, contention on the 'm_send_lock' is already (mostly)
    avoided by doing 'trylock' on it.
    
    The pressure on the buffer_lock has been further reduced by reducing
    the amount of work we do when holding it for 'packing'. There
    are basically two cases:
    
    - [1;31mflush[m_send_buffer() are out of buffer slots. This is solved by
      moving send buffers from thread local buffer list, to the global
      send_buffer with link_thread_send_buffers(). In addition we used
      to pack the buffers here. However, this is not necessary in order
      to solve the real problem at this point, and has been removed.
    
    - The worker thread is low on locally available send buffers and
      initiate packing of all send buffers in an attempt of freeing some:
      As no specific send buffers has to be packed - we are happy with
      any - we introduced 'trylock' semantics in pack_send_buffers and
      renamed accordingly.
    
    NOTE: As send_lock is not held anymore during 'pack', the
      checking of 'm_force_send' -> try_send() inside
      pack_send_buffer() became obsolete. The remaining logic from
      pack_send_buffer() is now inlined as part of pack_send_buffers().

[33mcommit b6b256b6271ec127ad4a94b6e4181a653210a484[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Wed Sep 24 16:35:57 2014 +0300

    Fix Bug#19472028 BUG#18663997 WAS NOT (FULLY) FIXED, STILL: CANNOT CLOSE
    FILE: MOD CNT!=FLUSH CNT
    
    Remove the mtr testcase import_[1;31mflush[m_temp.test and the changes in the code
    that were made solely for this test. The test is too aggressive in trying
    to reproduce Bug#18663997 that even the error printout is emitted for tables
    like mysql.innodb_table_stats that are not temporary and have never been
    imported.
    
    Approved by:    Marko (via IM)

[33mcommit 16d498f22f33201d0c3603f31a96255ea50db60f[m
Author: Ole John Aske <ole.john.aske@oracle.com>
Date:   Wed Sep 24 15:31:51 2014 +0200

    Patch 1 of 3 for WL#7654: "Reduce lock contention in NDB mt-scheduler"
    
    This part reduce lock contentions on the 'jba_write_lock'
    
    When a signal has been sent on the JBA queue, a wakeup is
    signaled to any waiters. However, this wakeup was signaled
    when still holding the 'jba_write_lock'. The following
    scenario was then quite likely:
    
     - As wakeup signal is sent by thread t1, the waiter, t2 is
       prepared for reschedulling.
    
     - The OS may conclude that t1 has consumed its CPU quota.
       As t2 is now waiting for the CPU, the control is
       given to this thread (t2).
    
     - One if the first thing the t2 will do, is to check its
       'delayed-queues'. Any expired signals will
       then be sent on the 'JBA queue', which require grabbing
       the jba_write_lock.
    
     - As t1 holds this lock, t2 will be blocked, and has to wait
       for t1 to be rescheduled such that the lock can be released.
    
    This fix defers sending of the wakeup signal to after jba_write_lock
    has been released by using the 'non signaling' [1;31mflush[m_write_state().
    The previous [1;31mflush[m_write_state_wakeup has become obsolete, and
    is removed.
    
    The [1;31mflush[m_write_state-codepath itself has become slightly
    less optimized as a result of this fix. However, avoiding two
    extra thread switches should more than make up for this.

[33mcommit d003dd23cb571d049f22da476150133c62934211[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Sep 17 15:22:30 2014 +0900

    It is revive of yasufumi.kinoshita@oracle.com-20140916075634-3jkbtq9ql7hrevm6 with fixed.
    
    The problem was missing log_mutex_exit() before log_write_[1;31mflush[m_to_disk_low().

[33mcommit 8a32da2e1bbd65e2cbdde6380dddfae1635a2079[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Sep 17 13:22:41 2014 +0900

    follow up for yasufumi.kinoshita@oracle.com-20140916075634-3jkbtq9ql7hrevm6
    
    Fixed problem. It was missing log_mutex_exit() before log_write_[1;31mflush[m_to_disk_low().

[33mcommit 98c891c56d7a4f8c9f1cf27f0373436b70ca951a[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Tue Sep 16 16:56:34 2014 +0900

    Bug#19068569 : UNNECESSARY REDO LOG WRITING OCCASIONALLY IN FUNCTION LOG_WRITE_UP_TO (Bug#73109)
    
    log_write_up_to() always causes log writing when [1;31mflush[m_to_disk==true.
    If no lsn progressed, we can skip the writing before [1;31mflush[ming to disk.
    
    Approved by Vasil in rb#6254

[33mcommit 6a019e4d9e635dd432020d4458087e60b139d994[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Fri Sep 12 10:39:17 2014 +0300

    Follow-up to Bug#17332603 REMOVE BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH
    
    In WL#7277, we may [1;31mflush[m invalid pages to the data files while the
    index creation is in progress. Tag these pages with a different page type.
    Also, add assertions that the code is only used for B-tree indexes,
    not R-tree indexes.
    
    Approved by Shaohua Wang.

[33mcommit 2b22d9909690b58227ed20486037f5b1c70c97a9[m
Author: Marko Makela <marko.makela@oracle.com>
Date:   Thu Sep 11 19:03:46 2014 +0300

    Bug#17332603 REMOVE BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH
    Bug#17345513 CHECKING FIL_PAGE_TYPE BREAKS COMPATIBILITY WITH OLD
    INNODB DATA FILES
    
    The field block->check_index_page_at_[1;31mflush[m was needed in the bad old
    days when InnoDB did not initialize FIL_PAGE_TYPE when creating a
    page. It would only initialize this field to FIL_PAGE_INDEX when it
    was about to [1;31mflush[m a B-tree page. For other page types, InnoDB would
    leave garbage in the field. That garbage could also be FIL_PAGE_INDEX
    by chance. There was no problem with this in the past, because we
    would only invoke the B-tree validation functions when
    block->check_index_page_at_[1;31mflush[m had been set by some B-tree operation
    while the page was in the buffer pool.
    
    Since MySQL 5.1, InnoDB initializes the FIL_PAGE_TYPE for every
    page. But, there could be some old data files that contain garbage
    FIL_PAGE_TYPE values in some pages. This garbage must be reset when
    [1;31mflush[ming the page.
    
    buf_block_t: Remove check_index_page_at_[1;31mflush[m.
    
    buf_dblwr_check_block(): Reset unknown FIL_PAGE_TYPE values to
    FIL_PAGE_TYPE_UNKNOWN if the tablespace flags are 0.
    For newer tablespaces, the flags never are 0, and we will complain
    about a corrupted page.
    
    FIL_PAGE_TYPE_UNKNOWN: New constant (13) for FIL_PAGE_TYPE.
    
    i_s_page_type[]: Map I_S_PAGE_TYPE_UNKNOWN to FIL_PAGE_TYPE_UNKNOWN.
    Both will be reported as "UNKNOWN".
    
    I_S_PAGE_TYPE_BITS: A new macro, to make compile-time checks more localized.
    
    With this fix, it is possible that some non-index page that happens
    to be tagged as FIL_PAGE_INDEX or FIL_PAGE_RTREE will start to trigger
    failures in page_simple_validate_new() or page_simple_validate_old().
    This situation can be handled by manually patching the data file to replace
    the FIL_PAGE_TYPE on that page with FIL_PAGE_UNKNOWN. The error
    message will identify the tablespace ID and page number.
    
    rb#6604 approved by Vasil Dimov

[33mcommit a0fc855ec110269940a365f2e33c4cd9568b45f7[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Aug 20 15:50:06 2014 +0300

    Correct incorrect comments abuot innobase_[1;31mflush[m_logs.
    
    The function has never made an InnoDB redo log checkpoint,
    which would involve [1;31mflush[ming data pages from the buffer pool
    to the file system. It has only ever [1;31mflush[med the redo log buffer
    to the redo log files.
    
    The actual InnoDB function call has been changed a few times
    since the introduction of InnoDB to MySQL in 2000, but the semantics
    never changed.
    
    Approved by Vasil Dimov

[33mcommit c4faa86c97726d2a961382c8814c6586ac347922[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue Aug 12 18:50:52 2014 +0300

    Fix Bug#18663997 INNODB: TOO MANY FILES STAY OPEN
    
    Flushing is not done for FIL_TYPE_TEMPORARY files (at the beginning
    of fil_[1;31mflush[m()):
    
            if (space == NULL
                || space->purpose == FIL_TYPE_TEMPORARY
                || space->stop_new_ops
                || space->is_being_truncated) {
                    mutex_exit(&fil_system->mutex);
    
                    return;
            }
    
    but FIL_TYPE_TEMPORARY and FIL_TYPE_IMPORT were considered equivalent
    and thus [1;31mflush[ming was also not done for FIL_TYPE_IMPORT.
    
    This lead to the problem that modification counter > 0 and
    [1;31mflush[m counter == 0 for to-be-imported tablespaces and it was left like
    that after the tablespace got converted to normal FIL_TYPE_TABLESPACE
    in fil_space_set_imported().
    
    Thus, when [1;31mflush[ming do not skip the to-be-imported tablespaces.
    
    Approved by:    Marko (rb:6318)

[33mcommit 59708a25bac93a8f0132ecb7bad267b6239d6e9e[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Tue Jul 29 15:28:45 2014 -0700

    Bug 19028487 NPE when scanning rows with blob or text columns
    
    BLOB/TEXT columns were not properly handled when scanning tables via
    query.
    
    AbstractClusterJTest:
      add compareBytes method to compare two byte arrays
    
    AbstractQueryTest:
      make resultList protected and available to subclasses after the query
    
    FindByPrimaryKeyTest:
      add test for row not found
    
    QueryBlobIndexScanTest:
      new test case queries table with a blob column
      scan using filter
        update a row with a blob column
        delete a row with a blob column
    
    QueryTextIndexScanTest:
      new test case queries table with a text column
      scan using filter
        update a row with a text column
        delete a row with a text column
    
    ClusterTransactionImpl:
      change the autocommit [1;31mflush[m abort option to false
      this allows autocommit find of rows with blob columns to fail the operation
        but not fail the transaction on row not found
        (which will result in returning null to the application instead of
        throwing an exception)
    
    NdbRecordBlobImpl:
      create a new constructor for use with scans
      the new NdbRecordBlobImpl gets a new NdbRecordOperationImpl
        but retains the original data and storeColumn info
      if the NdbRecordBlobImpl is used in a subsequent operation
        the new operation replaces the original operation
        this allows the NdbRecordBlobImpl to be used with several operations
          in sequence (e.g. find, update, delete; or scan, update)
    
    NdbRecordIndexScanOperationImpl:
      once the operation is completely defined, activate the blob/text columns
    
    NdbRecordKeyOperationImpl:
      load blob values after the row is retrieved
    
    NdbRecordOperationImpl:
      change the constructor used with SmartValueHandler to copy the blob
        data so it can be updated in a subsequent operation
      for update operations, calculate which blob/text columns have been updated
        so the blobs can be activated for update
      add method isColumnSet to determine whether a column is in the mask
    
    NdbRecordScanResultDataImpl:
      during scans, read-only operations might have been upgraded to read lock
        if a blob/text column is present
      if scanning with blob/text, only upgrade the lock if actually scanning
        with read or write lock; otherwise skip the upgrade to avoid NPE later
      if scanning with blob/text, fetch the blob/text values during the scan
    
    NdbRecordSmartValueHandlerImpl:
      if updating a blob/text field, set the mask for the field
    
    NdbRecordUniqueKeyOperationImpl:
      when reading row with blob/text columns, load the blob values
        after the row is read

[33mcommit 3ae9bc804685838b222ba783bfd19bcb264d4eaf[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Mon Jul 14 11:24:14 2014 +0200

    Bug #18756233 ASSERT !HAS_DONE_RESERVATION, FSEG_ALLOC_FREE_PAGE_LOW(),
    BTR_STORE_BIG_REC_EXTE
    
    Post push fix.  The test case uses innodb_buf_[1;31mflush[m_list_now which is
    a debug only variable.  So adding have_debug.inc in the test case.
    
    approved by Jimmy over IM.

[33mcommit 010811852fb66d5bccd42ebd84c263aba0157f9c[m
Author: Dmitry Lenev <Dmitry.Lenev@oracle.com>
Date:   Wed Jul 2 22:07:50 2014 +0400

    WL#1159 "Allow multiple locks in GET_LOCK()".
    
    This patch is based on contribution from Konstantin Osipov.
    
    The goal of this patch is to allow user to acquire multiple
    user-level locks in the same connection via serie of calls to
    GET_LOCK() function. Also it makes possible to take the same
    lock twice in connection (i.e. locks became recursive).
    GET_LOCK() no longer implicitly releases the previous lock held
    by the connection. To release all locks in connection one can
    use new RELEASE_ALL_LOCKS() function.
    
    The above is achieved by deleting all old code which implements
    user-level locks, and forwarding user-level lock requests to the
    metadata locking subsystem. A new metadata type was introduced
    for user-level locks - "user". Instances of "USER" locks are
    mutually exclusive.
    
    Possible deadlocks between multiple locks taken in different order,
    as well as between user-level locks and metadata/waits for table
    [1;31mflush[mes are detected and resolved using the MDL deadlock detector.
    Waits for user-level locks are preferred as victim over waits
    for locks typically acquired by DDL, but waits for locks typically
    acquired by DML are preferred over waits for user-level locks.
    
    Wait for user-level lock which is aborted due to deadlock is
    reported as ER_USER_LOCK_DEADLOCK error. No transaction rollback
    happens in this case.
    
    MDL subsystem was extended to support the old behavior when waits
    for user-level locks are automagically aborted if connection which
    requested them disconnects. However now this situation is handled
    similarly to KILL statement rather than to lock wait timeout.
    
    Since IS_USED_LOCK() function returns id of connection which owns
    the lock the MDL API had to be extended with capability to query
    lock owner.
    
    Note that with old implementation when a wait for user-level lock was
    aborted due to query or connection being killed GET_LOCK() function
    always returned NULL and statement using this function might have
    succeeded or failed with ER_QUERY_INTERRUPTED error, depending if
    statement tried to do anything else after calling GET_LOCK().
    With new implementation statement which called GET_LOCK() and was
    killed will always fail with ER_QUERY_INTERRUPTED due to slightly
    different KILL error handling by MDL subsystem.
    
    Since MDL subsystem imposes limits on the length of key used to
    identify objects new implementation introduces limit on user-level
    lock name length. New limit is 64 characters.
    ER_USER_LOCK_WRONG_NAME error is emitted when one of functions
    accepting user-level lock name as argument gets name which is
    longer than 64 character. Also behavior is changed to return the
    same error for NULL or empty ('') lock name.
    Also new implementation will always convert lock name to utf8 from
    its original charset and perform case-insensitive comparison.
    
    Existing test coverage for user-level lock functionality has been
    extended and moved from main.func_misc to main.user_lock test.
    
    New rpl.rpl_user_lock test covers replication of statements using
    user-level lock functions.
    
    Some other existing tests had to be adjusted to the new user-lock
    behavior including rpl.rpl_err_ignoredtable test. The latter was
    additionally fixed to really test what it was intended to test.
    
    Tests in perfschema test suite which used mutex used in user-level
    lock implementation to test aggregation of wait events were changed
    to use different mutex. Tests which used user-level locks to test
    aggregation for memory events we adjusted to new implementation
    memory usage. Coverage for new MDL namespace was added to
    perfschema.mdl_func test.
    
    Unit tests covering MDL API/code changes were added.

[33mcommit 8406d6aac3be6353bf693159ea0a5163edae0179[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Jul 2 10:26:42 2014 +0300

    WL#7990 Repurpose FIL_PAGE_FLUSH_LSN
    
    The field FIL_PAGE_FLUSH_LSN is consulted during InnoDB startup.
    
    At startup, InnoDB reads the FIL_PAGE_FLUSH_LSN from the first page of
    each file in the InnoDB system tablespace. If there are multiple
    files, the minimum and maximum LSN can differ. These numbers are
    passed to InnoDB startup.
    
    Having the number in other files than the first file of the InnoDB
    system tablespace is not providing much additional value. It is
    conflicting with other use of the field, such as on InnoDB R-tree
    index pages.
    
    The FIL_PAGE_FLUSH_LSN was also being written to InnoDB undo
    tablespace files, even though the fields are not going to be consulted
    on crash recovery.
    
    This worklog will stop writing FIL_PAGE_FLUSH_LSN to other files
    than the first file of the InnoDB system tablespace (page number 0:0).
    
    fil_write_[1;31mflush[med_lsn(): A new function, merged from
    fil_write_lsn_and_arch_no_to_file() and
    fil_write_[1;31mflush[med_lsn_to_data_files().
    Only write to the first page of the system tablespace (page 0:0),
    and invoke fil_[1;31mflush[m_file_spaces(FIL_TYPE_TABLESPACE) afterwards.
    
    fil_open_single_table_tablespace(): Remove output of LSN, because it
    was only valid for the system tablespace and the undo tablespaces, not
    user tablespaces.
    
    Datafile::m_[1;31mflush[med_lsn: Remove.
    
    Datafile::validate_first_page(),
    SysTablespace::read_lsn_and_check_flags():
    Add an output parameter for [1;31mflush[med_lsn.
    
    SysTablespace::read_lsn_and_check_flags(): Only read the first file.
    
    SysTablespace::open_or_create(): Replace the output parameters
    min_lsn,max_lsn with [1;31mflush[m_lsn.
    
    Tablespace::m_min_[1;31mflush[med_lsn: Remove.
    Tablespace::m_max_[1;31mflush[med_lsn: Remove.
    
    recv_recovery_from_checkpoint_start(): Replace the two parameters
    min_[1;31mflush[med_lsn, max_[1;31mflush[med_lsn with one: [1;31mflush[m_lsn.
    
    innobase_start_or_create_for_mysql(): Remove a message that would
    detect FIL_PAGE_FILE_FLUSH_LSN mismatch between multiple files of the
    system tablespace.
    
    rb#5882 approved by Jimmy Yang

[33mcommit 2a1c1a2e9326954ef8d56aa5eb6ca3aaa90704fc[m
Author: Luis Soares <luis.soares@oracle.com>
Date:   Mon Jun 2 16:13:36 2014 +0100

    WL#7742: BGC: waiting more transactions entering bgc queues
    
    Adds two new options to introduce an artificial delay to make the
    binary log group commit procedure wait. This gives a chance that
    more transactions are [1;31mflush[med and synced together to disk, thus
    reducing the overall time spent to commit a group of transactions
    (the bigger the groups the less number of sync operations).
    
    These options are named: binlog-group-commit-sync-delay and
    binlog-group-commit-sync-no-delay-count. The former takes as
    input the number of microseconds to wait. The latter, takes as
    input the number of  transactions that the server waits for,
    before deciding to abort the waiting.

[33mcommit 51c58965234585286dbfddbcc4de24d4ba42384a[m
Merge: 1c95f9b7852 6b13a95d113
Author: Anirudh Mangipudi <anirudh.mangipudi@oracle.com>
Date:   Fri May 30 18:22:31 2014 +0530

    Bug #17076131 MYSQLDUMP FAILS WHEN ERR LOG ON RUNNING SERVER IS DELETED ON WINDOWS
    
    Problem:
    As the title mentions, on windows mysqldump tool is creating the dump file
    without the content (tables, views etc) of a database that is dumped when the
    error log file is deleted (by the user). Similar behaviour is not happening
    on Linux flavours. The difference is due to the difference in handling of the
    deleted files. In Windows, the line "if ([1;31mflush[m_logs || opt_delete_master_logs)"
    in main.c of mysqldump is returning error whereas on linux this line is passing
    
    Solution:
    The core problem is that, in Windows, the new error log file is being created but
    the old file descriptor which is associated with both stdout and stderr stream
    is not being closed by the program. So a retry logic has been implemented where
    we close the error log's file descriptor (associated open fds of stderr and
    stdout) in the first attempt and open a new file and associating it with
    stdout/stderr streams.

[33mcommit 8c2627a03a454229de149c9f35b36f398b06440f[m
Author: Anirudh Mangipudi <anirudh.mangipudi@oracle.com>
Date:   Tue May 27 09:42:00 2014 +0530

    Bug #17076131 MYSQLDUMP FAILS WHEN ERR LOG ON RUNNING SERVER IS DELETED ON WINDOWS
    
    Problem:
    As the title mentions, on windows mysqldump tool is creating the dump file
    without the content (tables, views etc) of a database that is dumped when the
    error log file is deleted (by the user). Similar behaviour is not happening
    on Linux falvours. The difference is due to the difference in handling of the
    deleted files. In Windows, the line "if ([1;31mflush[m_logs || opt_delete_master_logs)"
    in main.c of mysqldump is returning error whereas on linux this line is passing
    
    Solution:
    The core problem is that, in Windows, the new error log file is being created but
    the old file descriptor which is associated with both stdout and stderr stream
    is not being closed by the program. So a retry logic has been implemented where
    we close the error log's file descriptor (associated open fds of stderr and
    stdout) in the first attempt and open a new file and associating it with
    stdout/stderr streams.

[33mcommit 300cc3f33fad5a1a6ff6adcc4f1efd581a7176de[m
Merge: ec2ca9f6f2f 820ecb0c43f
Author: Anirudh Mangipudi <anirudh.mangipudi@oracle.com>
Date:   Mon May 26 20:45:26 2014 +0530

    Bug #17076131 MYSQLDUMP FAILS WHEN ERR LOG ON RUNNING SERVER IS DELETED ON WINDOWS
    
    Problem:
    As the title mentions, on windows mysqldump tool is creating the dump file
    without the content (tables, views etc) of a database that is dumped when the
    error log file is deleted (by the user). Similar behaviour is not happening
    on Linux falvours. The difference is due to the difference in handling of the
    deleted files. In Windows, the line "if ([1;31mflush[m_logs || opt_delete_master_logs)"
    in main.c of mysqldump is returning error whereas on linux this line is passing
    
    Solution:
    The core problem is that, in Windows, the new error log file is being created but
    the old file descriptor which is associated with both stdout and stderr stream
    is not being closed by the program. So a retry logic has been implemented where
    we close the error log's file descriptor (associated open fds of stderr and
    stdout) in the first attempt and open a new file and associating it with
    stdout/stderr streams.

[33mcommit 30a08cf60cfa08655f553da2795b68eee8dba53c[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Thu May 15 15:07:12 2014 +0300

    Fix Bug#18253089 BUF_POOL->FLUSH_RBT IS CREATED WHEN RECOVERY IS NOT NEEDED,
    THEN NEVER FREED
    
    * recv_recovery_from_checkpoint_start() is only called from
      innobase_start_or_create_for_mysql() and
      recv_recovery_from_checkpoint_start() calls recv_sys_create()&recv_sys_init()
      but those two functions have already been called earlier in
      innobase_start_or_create_for_mysql(). Those two functions return immediately
      if recv_sys is created/initialized. From this follows that their invokations
      from recv_recovery_from_checkpoint_start() are noops and thus I am removing
      them.
    
    * The code used to initialize buf_pool->[1;31mflush[m_rbt (buf_[1;31mflush[m_init_[1;31mflush[m_rbt())
      from recv_sys_init(). Change this so that it is initialized directly from
      recv_recovery_from_checkpoint_start(). This is the only functional change
      in this patch - buf_pool->[1;31mflush[m_rbt will now be initialized later in the
      startup code path.
    
    * The code used to free buf_pool->[1;31mflush[m_rbt (buf_[1;31mflush[m_free_[1;31mflush[m_rbt()) from
      recv_sys_debug_free(). Change this so that the free is done from
      recv_recovery_from_checkpoint_finish(), which is the only caller of
      recv_sys_debug_free(). This is a noop, but made for consistency wrt where
      alloc/free is done - call
      buf_[1;31mflush[m_init_[1;31mflush[m_rbt() from recv_recovery_from_checkpoint_start() and
      buf_[1;31mflush[m_free_[1;31mflush[m_rbt() from recv_recovery_from_checkpoint_finish().
    
    This way the code will be restored as of before
    annamalai.gurusami@oracle.com-20140210082850-vqlzadis1asdws0e (the fix of
    Bug#18144349 INNODB CANNOT USE THE DOUBLEWRITE BUFFER FOR THE FIRST PAGE OF
    SYSTEM TABLESPACE) wrt buf_pool->[1;31mflush[m_rbt initialization.
    
    Approved by:    Yasufumi (rb:5409)

[33mcommit be3dd2cbb0a2f9f92bb60cf8c8af0a302c824bfa[m
Author: Georgi Kodinov <georgi.kodinov@oracle.com>
Date:   Tue May 13 18:42:21 2014 +0300

    Bug#12368203 - 60878: mysqladmin doesn't support [1;31mflush[ming
      specific logs
    
    mysqladmin currently supports a command '[1;31mflush[m-logs'
    to [1;31mflush[m all log types. However, mysqladmin offered
    no support to [1;31mflush[m specific logs.
    
    Added support for additional arguments to
    mysqladmin fllush-logs. The syntax is :
    
    [1;31mflush[m-logs [specific specific ...]
    specific = binary
                    | engine
                    | error
                    | general
                    | relay
                    | slow
    
    Any other "specific" is interpretted as the next mysqladmin
    command.
    
    This results in COM_QUERY (vs. COM_REFRESH that
    was used before, but found lacking in functionality).
    
    Test case added.

[33mcommit 4160346efc87b6d5aeac7619e99513baa8f93b84[m
Author: Vasil Dimov <vasil.dimov@oracle.com>
Date:   Tue May 13 16:30:17 2014 +0300

    Non-functional change around buf_pool->[1;31mflush[m_rbt:
    use NULL when comparing and remove UNIV_LIKELY_NULL.

[33mcommit bad2da00a7d81da850abfffd9f128536a6a2cd81[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Fri May 9 11:22:52 2014 +0300

    Bug#18730524 REPEATED KILL+RESTART FAILS DUE TO MISSING MLOG_FILE_NAME RECORD
    
    There were several spurious test failures on PB2 where InnoDB failed to start
    due to missing MLOG_FILE_NAME record:
    
    ... [ERROR] [FATAL] InnoDB: Missing MLOG_FILE_NAME or MLOG_FILE_DELETE for
    redo log record ...
    
    I analyzed one of them (innodb.innodb-double-write on daily-trunk 2014-05-07
    carefully. I was only able to repeat the bug by tweaking code in debugger:
    
    create table ...; insert lots of data;
    [kill the server]
    Set a breakpoint on log_checkpoint()
    restart with --debug=d,ib_log
    
    Observe the LSN of the last MLOG_FILE_NAME record (rec 55) for this
    tablespace.
    
    When the breakpoint on log_checkpoint() is hit
    (the master thread does periodic checkpoints,
    even if there is no user activity),
    tweak buf_[1;31mflush[m_ready_for_[1;31mflush[m() to always return false.
    
    Check the value of
    
            oldest_lsn = log_buf_pool_get_oldest_modification();
    
    With our tweak to buf_[1;31mflush[m_ready_for_[1;31mflush[m(), this should be not later than
    the MLOG_FILE_NAME record, which is fine.
    
    When buf_[1;31mflush[m_ready_for_[1;31mflush[m() is returning suitable values, the
    oldest_lsn could end up being somewhere between the LSN of the
    MLOG_FILE_NAME record and the last logged modification for the
    tablespace.
    
    In my manual repeat, oldest_lsn happened to be exactly the LSN of the
    MLOG_FILE_NAME record. I faked oldest_lsn to be immediately after the
    MLOG_FILE_NAME record, and let the execution continue.
    
    Immediately after log_checkpoint() returns, kill and restart the
    server. You should get the message:
    
    ... [ERROR] [FATAL] InnoDB: Missing MLOG_FILE_NAME or MLOG_FILE_DELETE for
    redo log record ...
    
    The fix:
    
    recv_init_crash_recovery_spaces(): Invoke fil_names_dirty() on the
    tablespaces for which MLOG_FILE_NAME records were scanned during crash
    recovery.
    
    recv_recovery_from_checkpoint_start(): Initialize log_sys->lsn a
    little earlier, so that fil_names_dirty() will get the correct value.
    
    Patch approved by Jimmy Yang on IM.

[33mcommit 38144ed4cf6bc285cad6a8b7a72ad198c586587c[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Fri May 9 11:49:44 2014 +0530

    BUG#18642372 - I_INNODB.INNODB-LOG-FILE THROW TIMEOUT ERROR ONLY ON WINDOWS
    
    Problem:
    
    The reason for hang is because DeleteFile() hangs on ibtmp1.
    The file is opened by the page cleaner thread. The handle is not in
    Datafile Object but it is stored in fil_node_t.
    
    The explicit close() on Datafile will not solve the issue because the
    handle is not opened by Datafile Object. It is stored in fil_node_t by page
    cleaner thread.
    
    Fix:
    Exit [1;31mflush[m thread early incase of startup failure. This will make sure
    that we don't have handle open on 'ibtmp1' by the page cleaner thread.
    Without any open handles, deletion of ibtmp1 will be successful
    
    Note:
    Although it->close() in Tablespace::delete_files() will not solve the
    current hang problem, it will make sure that we close the file before deleting.
    
    Approved by Kevin,Yasufumi,Krunal rb#5258

[33mcommit 03d1ccca36c7b55e03fb662c9e7a6cc885b4a87d[m
Merge: 5fcf360fd50 adf46af75a2
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Wed May 7 19:37:54 2014 +0530

    BUG#17798076 - BUG#16418661 CLEANUP: REMOVE UNNECESSARY BUF_FLUSH_LIST()
                   DURING RECOVERY
    
    Why and when the extra buf_[1;31mflush[m_list() was added?
    ---------------------------------------------------
    The extra buf_[1;31mflush[m_list() was added as part of
    Bug#16418661 CHANGING NAME IN FOR INNODB_DATA_FILE_PATH
    SHOULD NOT SUCCEED WITH LOG FILES.
    The testcase for rb#2293 failed randomly reporting inconsistent number
    of undo tablespaces.
    MTR didn't provide a way to use custom innodb_data_file_path,
    innodb_undo_tablespaces. See Bug#17059436.
    So the workaround is to use "--exec $MYSQLD --args my.cnf" on a empty
    data directory. This will create necessary files and aborts after that.
    
    The extra [1;31mflush[m made sure that correct the number of undo tablespaces
    reported. This is not correct because the changes are already covered
    by redo.
    
    So the the ideal solution would be have been to run "--exec $MYSQLD"
    on the datadir again to apply redo or allow the bootstrapping to
    finish properly and exit.
    
    Fix:
    ----
    Remove uncessary [1;31mflush[m. Fix the inconsistent number of undo
    tablespaces issue by passing '--skip-grant-tables
    --innodb-unkown-parameter' to $MYSQLD. This would allow the
    bootstrapping to finish and properly exit.
    
    Another side effect of the unnecessary [1;31mflush[m is, it masked a problem
    in WL#7142:
    
    Side effect Issue:
    ------------------
    An MLOG_CHECKPOINT record for an earlier checkpoint can be mistaken as
    the MLOG_CHECKPOINT record for the latest checkpoint.
    
    Fix:
    ----
    The fix is to grow the MLOG_CHECKPOINT record size from 1 to 9 bytes,
    so that it will contains the checkpoint LSN. On the first scan of the
    redo log, we will ignore MLOG_CHECKPOINT records, unless the LSN
    matches the latest checkpoint.
    
    SIZE_OF_MLOG_CHECKPOINT: Grow from 1 to 9.
    
    mtr_t::commit_checkpoint(), recv_parse_log_recs(), recv_scan_log_recs():
    Add the parameter checkpoint_lsn.
    
    Approved by Marko, Kevin. rb#5234 rb#5320

[33mcommit a97f39ba905a507caa32067a3f5ab0ee0885f745[m
Author: Sivert Sorumgard <sivert.sorumgaard@oracle.com>
Date:   Tue Apr 29 10:37:07 2014 +0200

    WL#7593: New data dictionary: don't hold LOCK_open while
    reading table definition
    
    While retrieving a table definition when getting a table
    share, the LOCK_open mutex may be temporarily released.
    This allows a higher degree of concurrency when opening
    tables, and is needed for the new data dictionary where
    dictionary tables may need to be opened recursively.
    
    The following changes have been implemented:
    
    1. A new TABLE_SHARE member m_open_in_progress is introduced
    to flag that a share is being initialized. Concurrent threads
    opening the same share must check this flag to see whether
    they must wait for the share to be initialized.
    
    2. A condition variable COND_open is introduced for threads
    that need to wait for a share which is being opened.
    
    3. We must wait for the share to be completely opened
    before returning from get_cached_table_share().
    
    4. There are loops iterating over all elements in the
    table definition cache. One loop is in close_cached_tables(),
    this is safe to keep as is because shares being opened are
    handled. The other loop is in list_open_tables(), here we found
    it better to ignore shares being opened to avoid situations
    where we would incorrectly present a table as being open
    (e.g. if the opening actually failed).
    
    5. We must assert that the newly inserted share is actually
    present in the cache before returning from get_table_share().
    
    6. Shares being opened (m_open_in_progress == true) in
    close_cached_connection_tables() are skipped. Otherwise, due
    to the way TABLE_SHARE::connect_string is initialized in
    open_binary_frm(), there is a risk that connect_string.length
    is initialized while connect_string.str is not, hence
    making us read into uninitialized memory further down in
    close_cached_connection_tables().
    
    Thus, in theory, there is a risk that shares are left in the
    cache that should really be closed (matching the submitted
    connection string), and this risk is already present since
    LOCK_open is unlocked before calling
    close_cached_connection_tables().
    
    Now, close_cached_connection_tables() is called as the final
    step of DROP/ALTER SERVER, so its goal is to [1;31mflush[m all tables
    which were open before DROP/ALTER SERVER started. Thus, if a
    share gets opened after close_cached_connection_tables() has
    started, the information about the server has already been
    updated, so the new table will use the new definition of the
    server.
    
    It might have been an issue, however if one thread started
    opening a federated table, read the old server definition into a
    share, and then a switch to another thread doing ALTER SERVER
    happened right before setting m_open_in_progress to false for
    the share. Because in this case ALTER SERVER would not [1;31mflush[m
    the share opened by the first thread as it should have been. But
    luckily, server definitions affected by * SERVER statements are
    not read into TABLE_SHARE structures, but are read when we
    create the TABLE object in ha_federated::open().
    
    This means that in close_cached_connection_tables(), ignoring
    shares that are in the process of being opened is safe, because
    such shares don't have TABLE objects associated with them yet.

[33mcommit 92969b7878632318b7f073e0177842f9d8de39d5[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Wed Apr 23 09:51:39 2014 +0300

    Remove an invalid sentence from a comment that was introduced in WL#7142.
    
    During a log checkpoint, there may exist dirty blocks in the buffer pool.
    What matters is that before the checkpoint completes, we will have
    [1;31mflush[med all blocks up to the checkpoint LSN, and no blocks with a newer LSN.
    
    In the event of a crash, we will recover the system as it was at the
    checkpoint LSN, after applying any redo log that has been
    successfully written since the latest checkpoint.

[33mcommit 6d7df62a40eb968826d53d9ad889a99299463037[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Apr 11 18:16:58 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - Corrected shutdown state to check while forcing buffer to [1;31mflush[m.

[33mcommit f0c5773e87f297297bd48d2c46029ca9b921c2fd[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Apr 11 12:19:46 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - During shutdown don't demand even if buf-fix-count > 0 continue to [1;31mflush[m it.
        This is safe given that it is for temporary tablespace only (where-in we
        can avoid [1;31mflush[m completely on shutdown) + user threads are already done
        performing the action.

[33mcommit 7fcb3aaaf6f171138b9833a99fa8bdc8afd3cbb9[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Wed Apr 9 14:21:34 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - Make sure you acquire block_mutex before changing fix_count
        as it is being used to synchronized [1;31mflush[m thread and user thread.

[33mcommit dbeace6d5d1e9c6edb6c8e62b3795f5a64c1c635[m
Author: Shaohua Wang <shaohua.wang@oracle.com>
Date:   Tue Apr 8 20:14:03 2014 +0800

    Remove row threshold and [1;31mflush[m threshold parameters

[33mcommit c2fb0291a18e3a4673345c195a70cfb0597f0459[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Mar 28 18:56:28 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - During log-checkpoint it needs all the pages to be [1;31mflush[med
        before progressing checkpoint.
        User thread might still hold reference to pages residing in
        shared/system temporary tablespace because we cache the last
        inserted position.
      - These pages are not [1;31mflush[med if fix_count > 0 and so log-checkpoint
        can't proceed.
      - Avoid considering these pages and let log-checkpoint to proceed
        as anyway there is no redo logging associated with these pages.

[33mcommit 0378d16ff972ee4feb93e1910ea318f7943670b7[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Mar 27 14:13:00 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - Fixed issues where-in if block with fix_count > 0 should never
        be selected for [1;31mflush[m.

[33mcommit 0c61b8d440bfc86370cd48c3213acd3445d38297[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Mar 27 11:09:34 2014 +0530

    - WL#7682: Optimize InnoDB temporary tables.
      - For all the tables residing in system temporary tablespace
        sync between [1;31mflush[m list and user thread is done using IO_FIX
        state.

[33mcommit dd2e9e77872f73347106fc4c8101ab70cb18ff14[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Mar 24 10:57:16 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Fixed race condition between User/Connection thread
        and background [1;31mflush[m thread in accessing the block.
      - For normal block this is handled using SX-latch.

[33mcommit 853fe41791949581d83877c77af93cbad3fbe855[m
Author: Martin Skold <Martin.Skold@oracle.com>
Date:   Fri Mar 21 17:21:22 2014 +0100

    WL#6146 Extend Ndb conflict exceptions table, adding [1;31mflush[ming of logs to make test ndb_rpl_conflict_epoch_extended_exceptions_table more predictable

[33mcommit ef22dbc883bb8d366026e0135d308428ba010377[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Mar 20 10:45:23 2014 +0530

    - WL#7682: Optimizing InnoDB temporary tables.
      - Corrected bug that avoided [1;31mflush[ming of modified page.
        During insert if we hit duplicate then we mark the record deleted.
        For intrinsic table we don't latch the page and so even though
        record on the page was delete-marked it was not being scheduled for
        [1;31mflush[ming.

[33mcommit 91955348ab477198d4605a1e23145ae4a2a30322[m
Author: Libing Song <libing.song@oracle.com>
Date:   Wed Mar 19 13:13:28 2014 +0800

    WL#6813: Intra-schema MTS: ordered commits (sequential consistency)
    
    DESCRIPTION
    ===========
    This worklog ensures that the commits by slave worker threads running in
    parallel are done in the same order as on the master. This also means that
    the slave will never externalize a database state which was never externalized
    by the master.
    
    DESIGN
    ======
    - slave_preserve_commit_order
      The behavior is optional. Users can control it through a new system variable
      named slave_preserve_commit_order.
    
    - Commit_order_manager
      We implement a commit order manager which maintains a queue of transactions
      which are applying by workers.
    
      Coordinator registers them into the queue when dispatching them to a worker.
      So the transactions in the queue have the order as they are in the relay log.
    
      All the transactions should go into the [1;31mflush[m stage(see Binlog Group commit)
      in the queue sequence. When a worker commits its transaction(before the
      transaction goes into [1;31mflush[m stage), it must wait until it becomes the queue
      head. It does means all transactions before it have entered into [1;31mflush[m stage.
    
      Once the transaction enters [1;31mflush[m stage, It is popped from the queue
      and signals the next transaction to go ahead.
    
    The feature is for only intra-schema multi-threaded slave, but not database
    partitioned multi-threaded slave. And it is not supported when log_bin or
    log_slave_updates is OFF.

[33mcommit ec76332a67b1a18bef78f7f1e273d3c243e83f19[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 18 21:12:03 2014 +0200

    Bug#18417128 MINOR REFACTORING OF REDO LOGGING AND RECOVERY CODE NEEDED
    
    Simplify the code flow in InnoDB redo logging and crash recovery, and
    remove some unused definitions.
    
    Use log_mutex_enter(), log_mutex_exit(), log_mutex_own() instead of
    accessing log_sys->mutex directly.
    
    log_release(): Replace by log_mutex_exit().
    
    buf_page_is_corrupted(): Remove type casts.
    
    log_reserve_and_write_fast(), log_reserve_and_open(): Assume that
    log_mutex_enter() has already been invoked.
    
    log_buffer_extend(): Expose to mtr_t::Command, which is invoked as
    part of mtr_commit(). This must be called before log_mutex_enter().
    
    log_write_checkpoint_info(): Renamed from
    log_groups_write_checkpoint_info(). Add a Boolean parameter to
    indicate whether to wait for the checkpoint to be persistent.
    This will invoke log_mutex_exit().
    
    log_checkpoint(): Replace ibool with bool, clarify some comments, and
    move some code to log_write_checkpoint_info().
    
    log_make_checkpoint_at(): Replace ibool with bool, and clarify the
    function description.
    
    log_checkpoint_margin(): Replace ibool with bool.
    
    log_check_margins(): Use a loop instead of goto.
    
    logs_empty_and_mark_files_at_shutdown(): Remove a duplicated
    log_mutex_exit() call.
    
    log_archive_do(), log_archive_stop(), log_archive_start(),
    log_archive_noarchivelog(), log_archive_archivelog(),
    log_archived_file_name_gen(): Remove. These were orphan declarations,
    with no corresponding function definition.
    
    log_group_write_buf(): Make this a static function.
    
    log_t::check_[1;31mflush[m_or_checkpoint: Replace ibool with bool.
    
    recv_sys_t: Remove the unused fields lsn, last_log_buf_size.
    
    recv_synchronize_groups(): Remove some code that is now in
    log_write_checkpoint_info().
    
    recv_recover_page_func(): Add some debug assertions. Add DBUG_PRINT
    output.
    
    recv_recovery_from_checkpoint_start(): Add a missing space to an error
    message. Remove an unreachable condition.
    
    recv_recovery_from_checkpoint_finish(): Move some code to
    innobase_start_or_create_for_mysql().
    
    mtr_t::Command::write(): Split to prepare_write() and finish_write().
    
    rb#4954 approved by Vasil Dimov

[33mcommit 74a37ff6b1cd152113b478f57bb96c875a3810ea[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Mar 18 21:09:25 2014 +0200

    Bug#18354821 LOG0LOG.H COMMENTS DISAGREE WITH LOG_WRITE_UP_TO()
    
    Clarify the comments about the concurrency protection around
    log_sys->[1;31mflush[m_event and log_sys->n_pending_[1;31mflush[mes.
    
    Remove some type casts.
    
    rb#4953 approved by Sunny Bains and Vasil Dimov

[33mcommit 6e3d936182e3821c78a45b8cb014734aceb61daf[m
Author: Aditya A <aditya.a@oracle.com>
Date:   Wed Mar 12 20:38:25 2014 +0530

    Bug #13029450  OFF BY ONE ERROR IN INNODB_MAX_DIRTY_PAGES_PCT LOGIC
    
    If the percentage of dirty pages in the buffer pool
    exceeds innodb_max_dirty_pages_pct (set by the user)
    then we [1;31mflush[m the pages.If user sets
    innodb_max_dirty_pages_pct=0,then the [1;31mflush[ming mechanism
    will not kick in unless the percentage of dirty pages
    reaches at least 1%.For huge buffer pools even 1% of the
    buffer pool can be a huge number.
    
    FIX
    ---
    
    Flush the dirty pages in buffer pool if percentage of dirty
    pages equals innodb_max_dirty_pages_pct or exceeds it. We have
    changed the innodb_max_dirty_pages_pct and
    innodb_max_dirty_pages_pct_lwm to double value,so that user
    can fine tune the percentage of dirty pages in the buffer pool.
    
    [ Approved by vasil #rb4778 ]

[33mcommit b989866a6303a8cfd36c2dbf2ea936b41081120e[m
Author: Frazer Clement <frazer.clement@oracle.com>
Date:   Tue Mar 11 12:52:39 2014 +0000

    Bug #18375920   NDB TESTING : SYNC_SLAVE_WITH_MASTER CAN TIMEOUT AND CONTINUE
    
    This fix changes mysqltest's sync_slave_with_master
    ndb-binlog-injector-sync timeout from 30s to 150s, and
    makes timeout a hard test failure.
    
    This causes the 'fishy' ndb_rpl_commit_after[1;31mflush[m testcase to fail, as
    it relied on a timeout+pass behaviour from sync_slave_with_master.
    
    This testcase is removed and a new testcase is added to replace it:
    ndb_binlog_[1;31mflush[m_tables_with_read_lock.test.
    
    The new testcase verifies the basics of the behaviour, with 2 MySQLDs
    attached to a cluster.

[33mcommit df6c8179669425bdefbc603404f66aca608ee2be[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Thu Mar 6 15:05:04 2014 +0200

    Bug#18345004 CLEAN UP SOME REDO LOG WRITING CODE
    
    There is some unused and unreachable code in the InnoDB redo log
    subsystem.
    
    Second, the DBUG_PRINT("ib_log", ...) statements should be more
    comprehensive and follow a common pattern.
    
    srv_master_thread: Call my_thread_init() and DBUG_ENTER(), so that
    DBUG_PRINT messages will be displayed also in the InnoDB master
    thread.
    
    srv_unix_file_[1;31mflush[m_method, srv_win_file_[1;31mflush[m_method: Declare these
    as enum types, depending on whether _WIN32 is defined.
    
    log_do_write: Remove. This was always set to TRUE. If really needed,
    the redo log writing could easily be disabled by tweaking fil_io(),
    skipping the writes if OS_FILE_LOG is set.
    
    log_mutex_enter(), log_mutex_exit(), log_mutex_own(): Simplify the
    macros. Always use these instead of referring to log_sys->mutex.
    
    log_block_init_in_old_format(): Enclose in #ifdef UNIV_HOTBACKUP.
    
    log_init(): Remove some initialization of individual fields to zero,
    because log_sys was already initialized by ut_zalloc(). Do not acquire
    and release the newly created log_sys->mutex.
    
    log_block_init(): Remove an assertion about holding log_sys->mutex.
    This function is only being called in two places, one of them being
    log_init().
    
    log_io_complete(): Enumerate all values of srv_unix_file_[1;31mflush[m_method.
    
    log_group_file_header_[1;31mflush[m(): Use memset instead of memcpy.  Make a
    DBUG_PRINT statement more uniform with others.
    
    log_complete_checkpoint(): Add DBUG_PRINT.
    
    log_io_complete_checkpoint(): Call MONITOR_DEC while not holding a
    mutex.
    
    log_group_checkpoint(): Move a DBUG_PRINT statement from
    log_checkpoint(). Remove some local variables.
    
    recv_sys_var_init(): Remove some redundant initializations.
    
    recv_sys_add_to_parsing_buf(): Change the return type to bool.
    
    recv_find_max_checkpoint(), recv_recover_page_func(): Make DBUG_PRINT
    statements more uniform.
    
    recv_scan_log_recs(): Simplify some conditions.
    
    rb#4838 approved by Vasil Dimov

[33mcommit 5c7569ab2dbba15876649c3cb2b459680039f99f[m
Merge: 3ee7857f7ca 8f63c056fb2
Author: Libing Song <libing.song@oracle.com>
Date:   Tue Feb 25 09:44:42 2014 +0800

    Manual Merge
    
    BUG#17632285 SLAVE CAN'T CONTINUE REPLICATION AFTER MASTER'S
                 CRASH RECOVERY
    
    Binary events might be sent to slaves before they are [1;31mflush[med
    to disk on master, even sync_binlog is set to 1. It can cause
    two problems if the master restarts after an OS crash.
    * Replication cannot continue because the slaves are
      requesting to replication the events don't exist on master.
    * Data exists on slaves, but not exists on the master.
    
    The problems are expected on less durable settings(
    sync_binlog != 1), but it should not happen on durable
    setting(sync_binlog = 1).
    
    Since 5.7 LOCK_log is removed from dump thread. Dump threads
    can read the binary events before binlog_end_pos simultaneously,
    even it is not synced to disk.
    
    To fixing the problem on durable setting, binlog_end_pos is updated
    after binlog is [1;31mflush[med to disk.

[33mcommit 7c701ef68deb60d574e45bd3bd5b45163f613091[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Mon Feb 24 13:39:33 2014 +0200

    Bug#18236692 REDUCE FLUSHING OF TEMPORARY TABLESPACES
    
    Currently, InnoDB does not distinguish temporary and persistent tablespaces
    at the tablespace level.
    It uses space->purpose == FIL_TABLESPACE for both.
    As a result, it is generating unnecessary writes and [1;31mflush[ming for temporary
    tablespaces.
    
    fil_type_t: An enum of FIL_TYPE_TEMPORARY, FIL_TYPE_TABLESPACE, FIL_TYPE_LOG.
    
    fil_open_single_table_tablespace(): Add the parameter "purpose"
    (FIL_TYPE_TABLESPACE or FIL_TYPE_TEMPORARY).
    Only IMPORT will use FIL_TYPE_TEMPORARY.
    We initially flag to-be-imported tablespaces as temporary, because we will
    not write redo log for them until the import has been completed.
    
    fil_space_set_imported(): New function, to change a tablespace from
    FIL_TYPE_TEMPORARY to FIL_TYPE_TABLESPACE once the space has been imported.
    
    Tablespace::open_or_create(): Add the parameter is_temp.
    
    fil_[1;31mflush[m(): Never [1;31mflush[m (fsync) FIL_TYPE_TEMPORARY tablespaces.
    
    rb#4657 approved by Kevin Lewis

[33mcommit e2585072f261f707fc214180176c1774399d7af1[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Feb 21 10:36:48 2014 +1100

    Bug#18069105 - ADD FUSIONIO ATOMIC WRITE SUPPORT FOR LINUX
    
    Follow up fix, [1;31mflush[med was uninitialised in the else branch. Some minor
    formatting changes.

[33mcommit da04521e7aa52d487975c3b3feb2e257a516c8dc[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Mon Feb 17 10:49:14 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Don't save gtid into table for the following case, if the transaction
    is being rolled back and contains changes that cannot be rolled back,
    the trx-cache's content is [1;31mflush[med.

[33mcommit e6321ee1dc09d63c5f36323094c723a43f295639[m
Author: Annamalai Gurusami <annamalai.gurusami@oracle.com>
Date:   Tue Feb 11 10:37:17 2014 +0530

    Fixing a pb2 failure.  The os_file_[1;31mflush[m() call must not be made when the file
    is opened in read-only mode.

[33mcommit c3ca36817ea04c3b91ae078e834f3e516f6a97fc[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Sun Feb 9 10:11:51 2014 +0800

    WL6559    Optimize GTIDs for passive slave - store GTIDs in table
    
    Add transaction's gtid into executed_gtids at binlog [1;31mflush[m time when
    binlog is enabled and at right after commit time when binlog is
    disabled.

[33mcommit 676263d7307312b0445b2709948d762058e83469[m
Author: magnus.blaudd@oracle.com <>
Date:   Tue Feb 4 10:03:50 2014 +0100

    Bug#18165088 FAILURE TO SETUP EVENT LISTENER DETECTED BY NDB_RESTORE_COMPAT_COMPRESSION
    
    Problem:
    - The ndb_mgmd processes the  "listen event" command, writes the reply "listen
    event" to m_output and then adds the new listener to the list of active
    listeners. But since m_output is buffered(using BufferedSockOutputStream)
    there is a probability that check_socket_listener() is called before the
    output is [1;31mflush[med. This caused the client to receive the string "<PING>" instead
    of the expected "listen event".
    - Actually it's also possible for a new log event (from Ndb_mgmd_event_service::log() ) to
      be written to the socket while pending reply was in the output buffer.
    
    Solution:
    - Flush the reply from buffer before adding the new event listener to the list
    of active listeners.
    - Unlock m_mutex while [1;31mflush[ming to avoid holding the mutex while doing
    network IO. This is smilar to the other place where m_oputput is [1;31mflush[med and
    thus the same logic should apply here.

[33mcommit 7dd4661b07a3a9a22c5f67be6d4d305e071ed870[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Jan 10 12:52:48 2014 +0900

    WL#6642 - Follow up fix, buf_dblwr_[1;31mflush[m_buffered_writes() opportunity should not be reduced.
    
      * WL#6642 - InnoDB: multiple page_cleaner threads
    
      * 'innodb_page_cleaners' option is added to perform [1;31mflush[ming dirty pages
      * and keeping free pages for each buffer pool instance in parallel.
      * Increasing the value of the options increases scalability of the activity.
    
      * This patch was originally written by Inaam Rana,
      * and refactored by Yasufumi Kinoshita.
    
      * rb#3004 Approved by Kevin Lewis and Marko Makela

[33mcommit 5aad1c4ed57fb8fe1b701584d5c6a3eda43ae806[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Dec 27 22:46:12 2013 +0900

    WL#6642 - InnoDB: multiple page_cleaner threads
    
    'innodb_page_cleaners' option is added to perform [1;31mflush[ming dirty pages
    and keeping free pages for each buffer pool instance in parallel.
    Increasing the value of the options increases scalability of the activity.
    
    'innodb_page_cleaners'
    Variable Scope   | Global
    Dynamic Variable | No
    Type             | numeric
    Default          | 1
    Range            | 1 .. 64
    
    This patch was originally written by Inaam Rana,
    and refactored by Yasufumi Kinoshita.
    
    rb#3004 Approved by Kevin Lewis and Marko Makela

[33mcommit 8f261a2a7842441c5998e59866aeda0bba1bd945[m
Author: Craig L Russell <Craig.Russell@oracle.com>
Date:   Fri Dec 6 11:43:30 2013 -0800

    iBug 17885485 - SETPARTITIONKEY() FAILS AFTER PREVIOUS TRANSACTION FAILS
    
    After a failed commit, the session transaction state becomes idle
    but the cluster transaction is not cleaned up. A subsequent setPartitionKey
    fails because there is still a cluster transaction.
    
    SessionImpl:
      change commit to fail the transaction on errors
      change executeCommit to call [1;31mflush[m with the commit flag
      change [1;31mflush[m to allow the caller to executeCommit instead of executeNoCommit
        followed by executeCommit
        serendipity: this has a slight performance benefit
    
    TransactionErrorSetPartitionKeyTest
      Add four tests for setPartitionKey after a failure to insert a duplicate record
        commit: this is the original bug report
        [1;31mflush[m/commit: [1;31mflush[m the failed change and then commit the transaction
        [1;31mflush[m/rollback: [1;31mflush[m the failed change and then roll back the transaction
        rollback: roll back the failed change

[33mcommit ffa40f322af998eaeb20adaa7c4d2515b15b3412[m
Author: Neha Kumari <neha.n.kumari@oracle.com>
Date:   Wed Nov 27 00:38:32 2013 +0530

    BAPI 85 : Moving Previous_gtids_log_event
    
    Description:
                -This is the subclass if Previous_gtids_event and Log_event
                -It is used to record the Gtids executed in the last binary log file,
                 for ex after [1;31mflush[m logs, or at the starting of the binary log file
    
                 The inheritance structure is as follows
    
                        Binary_log_event
                              /   \
                             /     \
                     <<vir>>/       \<<vir>>
                           /         \
         B_l:Previous_gtids_event   Log_event
                           \         /
                            \       /
                             \     /
                              \   /
                       Previous_gtids_log_event
    
      B_l: Namespace Binary_log
    
      TODO: Remove virtual inheritance once all the events are implemented in
            libbinlogapi

[33mcommit 8dc03bee3ade2edcc53a3a257346f4a0a9f0b44c[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Wed Nov 20 17:02:24 2013 +0900

    Bug #17824101 : WL#7050 CAUSES RW PERFORMANCE REGRESSION AT SOME WINDOWS ENVIRONMENT
    
    Write requests during fil_[1;31mflush[m() might not be good for some Windows.
    Disabled the path to cause write during [1;31mflush[m.
    
    Approved by Sunny directly in mail

[33mcommit 5b72a2ed78e58bbdbe0faa537b4b9f0e3381157f[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Fri Oct 18 13:47:54 2013 +0900

    WL#7050 - Follow up fix.
    race condition around log_sys->[1;31mflush[m_event state change
    
    Approved by Sunny directly

[33mcommit 402fcb0a17406d3aeca052204f1fb75762558726[m
Author: Yasufumi Kinoshita <yasufumi.kinoshita@oracle.com>
Date:   Thu Oct 17 19:05:54 2013 +0900

    WL#7050 - InnoDB: Refactor redo log write code for better performance
    
    - This is rewrite of log_write_up_to() to improve its performance in case where innodb_[1;31mflush[m_log_at_trx_commit = 2.
    
    In log_write_up_to():
    
    * Remove wait mode. We always wait with one exception. And that is when doing log
    sync from master thread. It makes that synchronous as well because that happens
    only once per second.
    
    * Because we only have one log group therefore we don't need two [1;31mflush[m_events.
    * Remove unnecessary fields like written_to_some_lsn, written_to_all_lsn.
    * If only write is requested we don't have to acquire the log_sys::mutex after we
    release it. We currently do that only to do event handling but event handling is
    really only needed in case where [1;31mflush[m is requested i.e.: a thread should be
    waiting on the event iff it is interested in [1;31mflush[ming. Writes are serialized under
    log_sys::mutex.
    
    This patch was originally written by Inaam Rana.
    
    rb#2389 Approved by Sunny and Yasufumi
    
    ===========
    
    Adjustment for performance was done therough inherited rb#3373
    
    - optimize log_write_up_to() more
            * remove the second log_sys->mutex obtain also for "innodb_[1;31mflush[m_log_at_trx_commit = 1" path
            * remove unnecessary ut_memcpy. (because log_group_write_buf() is protected by log_sys->mutex)
            * remove dirty-read from [1;31mflush[m_to_disk=true case. (to avoid regression at some cases)
              (to keep current arbitration for write/fsync contention between log and data file)
            * fix wrong handling of O_DSYNC
    - revive log_buffer_sync_in_background(). (because it needs to be used)

[33mcommit a0f965eddc048e8700d76fbe1c2d4f5da54fdebe[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Oct 16 20:08:31 2013 +1100

    Bug#17516062 - BUF_FLUSH_EVENT IS CREATED TOO LATE, MAY CAUSE RECOVERY FAILURE
    
    Create the [1;31mflush[m event at server startup not at [1;31mflush[m thread startup. Also
    don't set the event if --innodb-read-only is set because the event is
    not created in read-only mode.
    
    Approved by Marko over IM.

[33mcommit 962f313c9038fcb2f59600d9885eaf7c0245f28a[m
Author: Pavan Naik <pavan.naik@oracle.com>
Date:   Wed Sep 25 12:19:24 2013 +0530

    Bug#17468295   ADD MISSING OUT-OF-BOUND TESTS FOR VARIABLES IN SYS_VAR SUITE
    
    Fix:
    ----
    This bug is a repreat of Bug#13875534. But there were missing tests in Bug#13875534.
    Reviewed the entire sys_vars suite and added missing testcases in following tests.
    
      mysql-test/suite/sys_vars/t/auto_increment_increment_basic.test
      mysql-test/suite/sys_vars/t/auto_increment_offset_basic.t     Bug#17468295   ADD MISSING OUT-OF-BOUND TESTS FOR VARIABLES IN SYS_VAR SUITE
    
          Fix:
          ----
          This bug is a repreat of Bug#13875534. But there were missing tests in Bug#13875534.
          Reviewed the entire sys_vars suite and added missing testcases in following tests.
    
            mysql-test/suite/sys_vars/t/auto_increment_increment_basic.test
            mysql-test/suite/sys_vars/t/auto_increment_offset_basic.test
            mysql-test/suite/sys_vars/t/binlog_max_[1;31mflush[m_queue_time_basic.test
            mysql-test/suite/sys_vars/t/connect_timeout_basic.test
            mysql-test/suite/sys_vars/t/host_cache_size_basic.test
            mysql-test/suite/sys_vars/t/innodb_adaptive_[1;31mflush[ming_lwm_basic.test
            mysql-test/suite/sys_vars/t/innodb_adaptive_max_sleep_delay_basic.test
            mysql-test/suite/sys_vars/t/innodb_api_bk_commit_interval_basic.test
            mysql-test/suite/sys_vars/t/innodb_max_dirty_pages_pct_basic.test
            mysql-test/suite/sys_vars/t/interactive_timeout_basic.test
            mysql-test/suite/sys_vars/t/key_cache_block_size_basic.test
            mysql-test/suite/sys_vars/t/lock_wait_timeout_basic.test
            mysql-test/suite/sys_vars/t/read_rnd_buffer_size_basic.test
            mysql-test/suite/sys_vars/t/table_open_cache_basic.testest
      mysql-test/suite/sys_vars/t/binlog_max_[1;31mflush[m_queue_time_basic.test
      mysql-test/suite/sys_vars/t/connect_timeout_basic.test
      mysql-test/suite/sys_vars/t/host_cache_size_basic.test
      mysql-test/suite/sys_vars/t/innodb_adaptive_[1;31mflush[ming_lwm_basic.test
      mysql-test/suite/sys_vars/t/innodb_adaptive_max_sleep_delay_basic.test
      mysql-test/suite/sys_vars/t/innodb_api_bk_commit_interval_basic.test
      mysql-test/suite/sys_vars/t/innodb_max_dirty_pages_pct_basic.test
      mysql-test/suite/sys_vars/t/interactive_timeout_basic.test
      mysql-test/suite/sys_vars/t/key_cache_block_size_basic.test
      mysql-test/suite/sys_vars/t/lock_wait_timeout_basic.test
      mysql-test/suite/sys_vars/t/read_rnd_buffer_size_basic.test
      mysql-test/suite/sys_vars/t/table_open_cache_basic.test

[33mcommit f7e62bb9f1d86e510f892a3c237918fcae556037[m
Author: Marko Mäkelä <marko.makela@oracle.com>
Date:   Tue Sep 24 09:20:49 2013 +0300

    Bug#17492627 ABORT() SHOULD BE INLINED IN UT_DBG_ASSERTION_FAILED
    
    The macros ut_a and ut_error in InnoDB would invoke ut_dbg_assertion_failed()
    followed by ut_abort(). The function ut_abort() is not invoked from anywhere
    else than these macro expansions.
    
    Furthermore, on some platforms, ut_a() would dereference a dummy variable
    ut_dbg_zero, which is always zero.
    
    ut_abort(): Remove.
    ut_dbg_zero: Remove. This was an always-zero global variable.
    
    ut_dbg_assertion_failed(): Invoke f[1;31mflush[m() and abort() at the end.
    
    rb#3396 approved by Kevin Lewis

[33mcommit 56ab8c9bf0a0e117feaab2fa1e389141141b9965[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Tue Aug 27 09:59:52 2013 +0530

    - Bug #17349975 ASSERT SUCCESS, FLUSHING, EXTENDING TEMPORARY TABLESPACE,
      BAD LOG MESSAGES
    
      DB Init thread assumes that it is only thread doing buffer [1;31mflush[ming.
    
      This assumption doesn't hold if there are un-committed transactions that need
      to be undone after recovery. If the redo log file size is small, the UNDO of
      recovered transactions can cause a log-checkpoint which in turn can invoke
      buffer [1;31mflush[ming. This violates the assumption of main db-init thread.
    
      The fix is for the DB init thread to [1;31mflush[m the buffer pool but if it fails
      then it will wait for the signal that will indicate completion of buffer
      pool [1;31mflush[m by the other thread.
    
      ------
    
      Also fixed the log message. After the uncommitted transaction was undone
      trx->id was reset to 0. Cache trx_id before it is reset to 0 for printing.
    
      Approved by: Sunny (rb#3192)

[33mcommit 34c079684f9a97b91206744a4adc517536f0d574[m
Author: Anil Toshniwal <anil.toshniwal@oracle.com>
Date:   Thu Aug 22 10:13:16 2013 +0530

    Bug#17332603 -  REMOVE UNNECESSARY MEMORY ACCESS AS
                    BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH
    Fixed:
    1) Removing unnecessary memory use and accesses for
       BUF_BLOCK_T::CHECK_INDEX_PAGE_AT_FLUSH
    2) Validating FIL_PAGE_TYPE when [1;31mflush[ming.
    
    Approved by Jimmy and Ima. (rb#3132)

[33mcommit 6ef8c343445a26aaf9ebd76d72cf57db44b481f5[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Fri Aug 9 08:37:45 2013 +1000

    WL#7047 - Optimize buffer pool list scans and related batch processing code
    
    Reduce excessive scanning of pages when doing [1;31mflush[m list batches. The
    fix is to introduce the concept of "Hazard Pointer", this reduces the
    time complexity of the scan from O(n*n) to O(n).
    
    The concept of hazard pointer is reversed in this work.  Academically a
    hazard pointer is a pointer that the thread working on it will declare as
    such and as long as that thread is not done no other thread is allowed to
    do anything with it.
    
    In this WL we declare the pointer as a hazard pointer and then if any other
    thread attempts to work on it, it is allowed to do so but it has to adjust
    the hazard pointer to the next valid value. We use hazard pointer solely for
    reverse traversal of lists within a buffer pool instance.
    
    Add an event to control the background [1;31mflush[m thread. The background [1;31mflush[m
    thread wait has been converted to an os event timed wait so that it can be
    signalled by threads that want to kick start a background [1;31mflush[m when the
    buffer pool is running low on free/dirty pages.
    
    This patch was originally written by Inaam Rana.
    
    rb#2367 Approved by Jimmy Yang and Kevin Lewis.

[33mcommit 59da279dfb4107db1b73ea3dda5106bbad945beb[m
Author: Allen lai <zheng.lai@oracle.com>
Date:   Thu Aug 8 11:13:00 2013 +0800

    Bug#17214191 MEMCACHED ADD/SET ARE SLOWER THAN INSERTS
    
    rb://2993 approved by Jimmy.
    
    This bug is because when we do inserts through memcached, we skip a
    srv_active_wake_master_thread() embedded in innodb_commit() in ha_innodb.cc.
    This srv_sys->activity_count.inc() is checked by BUF FLUSH LIST [1;31mflush[m.
    If we do not increment it, then the background buffer [1;31mflush[m do extra [1;31mflush[mes.
    The master thread thought the server is quiet and no activity, so it does
    buffer pool preemptive [1;31mflush[m.  It brings more extra I/O.
    
    Add srv_active_wake_master_thread() to ib_cursor_insert_row(), then
    things come to normal.
    
    In this patch, we also remove the unnecessary memcpy for inserting row,
    and fixed a legacy memory leak in innodb_get.

[33mcommit 8041698bd9c910ee50ddf9445d54de902ad87672[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Tue Jun 4 23:27:05 2013 -0700

      Add "stats log" command.
      This is essentially a hack that causes the server's debug log to be [1;31mflush[med,
      used for diagnosing failures from mtr (because mtr sends kill -9 to memcached
      before any useful log information can be written).

[33mcommit c548bbfcec64d9145e94c0c21c0d22391cbf057a[m
Author: Libing Song <libing.song@oracle.com>
Date:   Wed May 8 19:11:43 2013 +0800

    WL#6355 Semisync: externalize transactions only after ACK received
    
    Postfix
    There was a assert failure in Binlog_storage_delegate::after_sync.
    It asserts that the argument log_pos is never 0. But It happened
    if the last thd in the commit queue encountered an [1;31mflush[m error.
    
    To fix the problem, the log_pos of the last thd without error
    is passed to after_sync.

[33mcommit 3dfa4d40fa8679ae09ac676f339064e6ade4f746[m
Author: Satya Bodapati <satya.bodapati@oracle.com>
Date:   Thu Apr 18 10:43:17 2013 +0530

    Bug#16566877 - BUF_FLUSH_LIST() ALWAYS DOES ASYNCHRONOUS FLUSHING
    
    Refactor buf_[1;31mflush[m_list() and buf_[1;31mflush[m_wait_batch_end() to
    buf_[1;31mflush[m_sync_all_buf_pools()
    
    Approved by Inaam. rb#2268

[33mcommit 7a86c8fa6eaa9f72f9847813e01a4fac8e6287bf[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Apr 12 16:15:27 2013 +0530

    - WL#6915: fixed tc output as it is dependent on whether page is [1;31mflush[med on hitting debug crash pt

[33mcommit c8e5ee6aeef08f5f047fbf675f5856dbb01aa08c[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Apr 12 15:04:03 2013 +0530

    - WL#6915: correct TCs to supress warning. Warning depends on whether undo logs is [1;31mflush[med to disk while crash happens (behavior same as mysql-trunk)

[33mcommit 2b20f2981de602d5c9f5bf4851e12b0f36b4547f[m
Author: kevin.lewis@oracle.com <>
Date:   Thu Apr 11 12:36:05 2013 -0500

    Bug #16609256 - INLINE FFLUSH CALL CAUSES DROP IN SYSBENCH PERFORMANCE
    
    The patch for 11763660 expanded the macro for UT_DBG_PANIC which was
    defined as abort(). This macro is part of every call to ut_a(), ut_ad()
    and uterror.  So this expanded about 7500 lines of code.  The result
    was a performance drop because of a generally larger codebase.
    
    So this patch replaces UT_DBG_PANIC with a function called ut_panic().
    This patch [1;31mflush[mes both stderr and stdout as a precaution before calling
    abort.  The previous patch added f[1;31mflush[m(stderr) to make sure that all
    error messages were written before crashing.
    
    Approved by Marko in http://rb.no.oracle.com/rb/r/2290/

[33mcommit b1ca61ec7aa454644255e649043e3f755099b25d[m
Author: Inaam Rana <inaam.rana@oracle.com>
Date:   Mon Mar 25 17:07:01 2013 -0400

    Bug#16477781 SINGLE PAGE FLUSHING CAUSES FSYNC() OF ALL DATAFILES
    
    Approved by: Kevin Lewis rb://2174
    
    In cases where threads are forced to do single page [1;31mflush[ming we call
    buf_[1;31mflush[m_sync_datafiles() after queueing the IO asynchronously.
    buf_[1;31mflush[m_sync_datafiles() wakes up IO helper threads, then waits for
    all the IO requests to be serviced and then trigger an fsync() on all
    data files.  The minimal we need is that the IO request for the given
    page has been serviced and the data file that it belongs to have been
    synced to the disk.
    
    * Explicitly pass 'sync' value to buf_[1;31mflush[m_page(). The new scheme is:
      -- All batches happen asynchronously (same as before)
      -- Single page [1;31mflush[mes to find a victim are synchronous
      -- Single page [1;31mflush[mes for export are asynchronous
    * Flush only the datafile that is touched during the single page [1;31mflush[ming
    * Change handling of single page [1;31mflush[m dblwr slot to IO completion
    routine i.e., now it will happen in IO helper thread in case of async
    request
    * Remove polling from dblwr buffer. Introduce two separate events,
    one for batch [1;31mflush[ming and one for single page [1;31mflush[ming

[33mcommit 1ff551ee095c6abbb89e8185063c033d6f49c349[m
Author: Raghav Kapoor <raghav.kapoor@oracle.com>
Date:   Mon Feb 25 14:07:35 2013 +0530

    BUG#11790057 KILL QUERY WILL AFFECT THE NEXT COMMAND INCORRECTLY
    Bug#11754124:KILL QUERY NOT BEHAVING CONSISTENTLY AND WILL HANG IN SOME CASES
    
    BACKGROUND OF BUG#11790057
    SCENARIO 1:
    When a very long SELECT is running in one connection and it
    is just [1;31mflush[ming its output on the console and, In another
    connection a KILL QUERY command is issued, the next query
    executed in the first connection would throw
    ER_QUERY_INTERRUPTED which is wrong.
    
    SCENARIO 2:
    When an INSERT query is running in one connection and it is
    in final stage to commit itself by acquiring MDL locks and
    waiting for the lock (i.e it is blocked because Flush Tables
    With Read Lock (FTWRL) is issued in another connection) and,
    In another connection, a KILL QUERY command is issued to kill
    that blocked INSERT query, it would not result a Query
    Interrupted Error.
    Rather the next query issued after INSERT in the connection
    would throw error ER_QUERY_INTERRUPTED.
    This behaviour is wrong.
    
    ANALYSIS OF BUG#11790057:
    SCENARIO 1:
    In this scenario, SELECT has cleared all the stages and it is
    just [1;31mflush[ming its output on the console through send() system
    call which is non blocking.
    Here if a user issues kill query in another connection,
    in the kill reception code there was no check to determine if
    the command is finished executing and it should not set the kill
    flag for that thread.
    
    SCENARIO 2:
    The state of blocked INSERT query is set to MDL_wait::KILLED due
    to KILL QUERY being issued in another session. No error was thrown.
    The state of INSERT executing thread would be set to THD::KILL_QUERY
    by kill reception code and it would affect the next command incorrectly.
    
    FIX FOR BUG#11790057:
    SCENARIO 1:
    A check has been added in the kill reception code that if any
    kill request comes in after the command is processed but
    before the next command is received, it should not set the kill
    flag for that thread because there is no active command being
    processed.
    SCENARIO 2:
    When the query waiting on MDL Lock is killed, its state is set to
    MDL_WAIT::KILLED. As a fix, an error ER_QUERY_INTERRUPTED is thrown
    from there.
    After the error is thrown from inside trans_commit_stmt() function,
    the thread state is reset to NOT_KILLED, by rearranging the code
    inside if (!thd->sub_stmt) so that next command does not get affected.
    
    BACKGROUND OF BUG#11754124:
    This bug is pretty similar or we can say duplicate of Bug#11790057,
    Only difference is more number of scenarios of unexpected behaviour
    of KILL QUERY are presented in this bug than Bug#11790057, and they
    are pretty much clear from the bug description.
    
    ANALYSIS OF BUG#11754124:
    All the scenarios mentioned in this bug are already been taken care
    by the patch submitted for Bug#11790057. Only Additional test cases
    are written to test the scenarios mentioned in this bug.
    DETAILED ANALYSIS OF SCENARIOS:
    1. Scenario 1, 2, 4 and 8 mentioned in the bug report are reproducible
       and are fixed with the current patch.
    2. Scenario 3 is Reproducible in 5.1. It is not reproducible in 5.5,
       5.6 and trunk.
       The above scenarios 1, 2, 4 and 8 were fixed by the check
       added in sql_kill() function submitted as a patch for
       Scenario 1 of Bug#11790057.
    3. Scenario 5, 6, 7 are not reproducible as they are
       described in the bug report in any of the versions.
       When we CALL a successfully created stored procedure, and a
       KILL QUERY is issued before in another session, the CALL to
       stored procedure terminates with ER_QUERY_INTERRUPTED error.
       After that if we again CALL it, it works fine.
       The above is applicable to Scenarios 5, 6 and 7 mentioned
       in the bug report.
       Scenario 9 is special because here KILL is issued concurrently
       when we are calling the stored procedure.
       For Scenario 9 there are two cases:
       CASE 1:
       A SELECT SLEEP(10) query is present inside a stored procedure
       and a handler is declared for it. In this case, When we do KILL
       when the sleep is executing, it goes to  pthread_cond_timedwait()
       system call. This system call does not return an error code of EINTR,
       when it is killed. The system call passes and only the state of
       thread is set to KILL_QUERY.
       Since there is no error generated by the SLEEP(10), handler does not
       get activated in any of the versions. This behaviour is present in all
       the versions starting from 5.1 and is correct.
       CASE 2:
       A very long running SELECT query is present inside a Stored Procedure
       and a handler is declared for it. In 5.1, CONTINUE handler for this is
       executed when a FATAL error happens which should not be the behaviour.
       See Bug #15192 for more details.
       In 5.5, 5.6 and trunk, handler for this is not executed since it is a
       FATAL error. Therefore, this behaviour is correct.
    
    
    FIX FOR BUG#11754124:
    The fix for this bug is same as the patch submitted for Bug#11790057.
    Additional test coverage is submitted as patch for this bug.

[33mcommit 81cbc9024a8eadbd0adb5abcc56e057956a1e33f[m
Author: John David Duncan <john.duncan@oracle.com>
Date:   Thu Feb 7 15:54:35 2013 -0800

    Add a [1;31mflush[m at the start of lib_ascii test, and print more information
    if the first SET fails.

[33mcommit 5ece4a68df45beb438012d3cb9778328e80ce458[m
Author: Jon Olav Hauglid <jon.hauglid@oracle.com>
Date:   Thu Jan 24 15:02:44 2013 +0100

    WL#6613: Refactor logging code - split logging and binlogging code
    
    - Move replication code out of log.h and log.cc.
    - Remove MYSQL_LOG and move implementation to MYSQL_QUERY_LOG
      and MYSQL_BINARY_LOG.
    - Move Log_throttle, log_slow_applicable(), log_slow_statement()
      and log_slow_do() to log.h/log.cc.
    - Cleanup interaction between sys_vars and log, move code from sys_vars to log.
    - Make usage of LOCK_global_system_variables & LOCK_logger more consistent.
    - Remove external access to LOCK_logger.
    - Early exit if logs are closed (e.g. don't lock mutexes).
    - Cleanup construction/destruction.
    - Use common code for reopen/[1;31mflush[m.
    - Simplify LOGGER implementation - use enum_log_table_type more.
    - Remove some global functions, use LOGGER functions directly.
    - Make everything but LOGGER private.
    - Write to error log without using log_event_handler since we only support
      error logging to file anyway.
    - Splitt error logging and slow/general log (LOGGER).
    - Simplify error logging functions.
    - Remove sql_print_message_handlers[].
    - Remove dead code.
    - Update log.h/log.cc according to current code standard.
    - Use true/false rather than TRUE/FALSE.
    - Use bool rather than my_bool.
    - Use size_t.
    - Indentation/spacing cleanup.
    - Move function documentation to header file and make into doxygen.
    - Improve code documentation, add missing doxygen.
    - Add documentation of query logging sysvars.
    - Add simple unit test for Log_throttle
    - Renames:
      opt_log => opt_general_log
      opt_logname => opt_general_logname
      opt_log_raw => opt_general_log_raw
      mysql_log => mysql_general_log
      key_file_query_log => key_file_general_log
      LOGGER => Query_logger
      MYSQL_QUERY_LOG => File_query_log
      event_coordinates => rpl_event_coordinates

[33mcommit 3ea716063fa24cce4932735021c79e7770216840[m
Author: Bill Qu <bill.qu@Oracle.com>
Date:   Wed Jan 23 08:23:37 2013 +0800

    wl#6501 Added comment to make checkpoint during truncation and use log_buffer_[1;31mflush[m_to_disk() instead of sleep() for DBUG_EXECUTE_IF test

[33mcommit c68222d45d842507908bde519149fe2d239806d5[m
Author: Sujatha Sivakumar <sujatha.sivakumar@oracle.com>
Date:   Mon Jan 21 11:49:36 2013 +0530

    Bug#16061982:REPLICATION BETWEEN A LINUX MASTER AND WINDOWS
    SLAVE, WITH MIXED CASE BREAKS
    
    Problem:
    =======
    Replication between a Linux master and Windows slave, with
    mixed case database/table names, fails after a replicated
    table is opened locally on the slave. "FLUSH TABLES" command
    needs to be issued on the salve to see the latest updates
    that are applied on slave. The master and the slave should
    have the following case settings.
    
    Linux master:
    lower_case_table_names=0
    
    Windows slave:
    lower_case_table_names=2
    
    Analysis:
    ========
    When "lower_case_table_names" is set to 1 or 2 in a general
    scenario without replication the database names and the
    table names are converted to lower case.
    The following two function check for "lower_case_table_names>0"
    and convert the db and table name to lower case.
    "check_and_convert_db_name" and "st_select_lex::add_table_to_list"
    
    But when replication is enabled dabase names and table names
    are reassigned once again as per the events that are received
    from the binlog without checking for "lower_case_table_names=2".
    
    For example if the database name is "TestME" when the table
    is opened locally on windows a new table cache entry is
    created "testme.testnumberone" and when opened through replication
    another table cache entry "TestME.testnumberone" is created.
    Because of this problem users will not be able see the latest
    updates. "Flush Tables" [1;31mflush[mes both the cache entries hence
    they are able to see the updates.
    
    Fix:
    ===
    Existing code checks only for "if (lower_case_table_names == 1)".
    This has been replaced with "if (lower_case_table_names)".

[33mcommit 8e0efaf5122082d7607fdf315b272611f08f6e0e[m
Author: Astha Pareek <astha.pareek@oracle.com>
Date:   Fri Jan 18 12:05:32 2013 +0530

    Description
    The test, binlog.binlog_spurious_ddl_errors was failing on pb2 at the statement
    "UNINSTALL PLUGIN example;" with this warning:
    "Warning        1620    Plugin is busy and will be uninstalled on shutdown "
    
    Fix
    Spurious warnings occur in the test since we do not empty the Query cache,
    used by the example plugin at the time of creating tables using the plugin.
    Hence, the query chache is [1;31mflush[med before uninstalling the plugin.
    Also, as part of running the test across platforms, the plugin installation
    script is changed.

[33mcommit ba18e621035b86670d8d82de03897ace0a2d7f99[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Dec 17 14:56:57 2012 +0530

    - WL#6469: improved logic of turning off logging with adding dirty pages to [1;31mflush[m list

[33mcommit 8ab720c324b40f561081420e627fd79ca5ab4a7e[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Fri Dec 14 09:46:23 2012 +0530

    - WL#6469:
      - for temp-table we don't want to generate redo-logs and so we operate in
        mtr-log-none mode (which is not changed to more appropriate mtr-log-no-redo mode).
        mtr-log-no-redo mode had a major fallback that never [1;31mflush[med dirty pages.
        corrected this fallback.

[33mcommit 4080c4b96d985ea909902a65af08e2a32d2b7c3e[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Mon Dec 10 15:39:20 2012 +0530

    - WL#6469:
      - fixed issue where-in modified pages where not being added to [1;31mflush[m list
      resulting in reading of garbage page from disk.
      (reason: mtr->n_log_recs = 0 .... fix: ignore this if mode is set to mtr-log-none.)

[33mcommit 53fe6877465bbbb0700f6a6d7377c8db2882d6aa[m
Author: Krunal Bauskar krunal.bauskar@oracle.com <>
Date:   Thu Dec 6 09:20:18 2012 +0530

    - WL6470:
      - fixed following issues:
    
    
      - investigated and fixed issue of garbage read.
        (summary: page is alloted and written in-memory but it is still assumed as clean
         and so never [1;31mflush[med to disk. Next read from disk will fetch a corrupted page.
         reason traced: mtr->n_log_recs = 0 and so page is never notified as dirty page.)
    
      - investigated and fixed issue related to garbage undo-record read.
        (summary: during rollback, previous version are read.
         this loop stops if previous version is missing (due to purge which indicate latest
         state is consistent) or until it reaches first inserted record.
         With temp-table undo-logging turned-off it might never reach first inserted record
         and would hit roll_ptr=0 during intermeditate climbling.

[33mcommit c61244c0e6c58727cffebfb312ac415a463fa0fe[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Mon Nov 19 16:42:14 2012 +1100

    WL#6560 - Clean up the code.
    
     1. Refactor open_or_create_data_files()
     2. Get rid of srv_temp_tablespace_t.
     3. Get rid of open_or_create_temp_data_files()
    
    The refactored code is in srv0space.h and srv0space.cc. Add a new class
    Tablespace for handling multi-file tablespaces. Currently the only two
    tablespaces that use this class are the System tablespace and Temp tablespace.
    
    Make the create and open into separate steps. Get rid of the extra parameter
    to fil_read_first_page(). Always pass in valid min/max [1;31mflush[m LSN values.
    
    Simplify the error handling in innobase_init() in ha_innodb.cc.
    
    Change some fprintfs() to ib_logf().

[33mcommit 4445da4cc031aa86b2d04d5c2e0cc85a8b84f22d[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Aug 29 10:24:46 2012 +1000

    WL#6445 - Ignore [1;31mflush[m to disk during fast shutdown if --innodb-read-only set.
    Fix log message, move to new format.

[33mcommit 70490b1e6c5cdf9e1be1b9160c792d236fe0c507[m
Author: Sunny Bains <Sunny.Bains@Oracle.Com>
Date:   Wed Aug 29 09:58:58 2012 +1000

    WL#6445 - Disable opening of RBS when --innodb-read-only is set. Ignore [1;31mflush[m
    to disk requests from MySQL server layer when --innodb-read-only is set. Add
    some more assertions to catch requests for writes when in read-only-mode.
    Don't attempt to fix the .isl information during tablespace open if read-only
    mode is set.

[33mcommit a2c74205f2f9cff58fc1048ec0d324fc71f44c6d[m
Author: Mauritz Sundell <mauritz.sundell@oracle.com>
Date:   Wed Mar 7 12:50:14 2012 +0100

    ndb - fix of signal log serialization
    
    Previously, setting NDB_SIGNAL_LOG_MUTEX can be used to serialize writes
    to signal log, that protects against interleaving several signal
    descriptions with each other.  But [1;31mflush[m request was not serialized,
    leading to potential loss or repetition of parts of signal descriptions.
    
    Now, both [1;31mflush[mes and writes are serialized when NDB_SIGNAL_LOG_MUTEX
    are set.

[33mcommit 1ebda68bffec3359a7e7920318f9ae6a1a1486e2[m
Author: jonas oreland <jonas.oreland@oracle.com>
Date:   Tue Dec 13 12:01:30 2011 +0100

    ndb - bug#13340854 - force a [1;31mflush[m priviledges during startup
For keyword binaryLog:
